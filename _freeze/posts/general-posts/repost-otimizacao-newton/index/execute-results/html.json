{
  "hash": "ceb42c84b5392a19607e5d7754533d06",
  "result": {
    "markdown": "---\ntitle: \"Repost: Otimização numérica - métodos de Newton\"\ndate: '2019-09-28'\ncategories: ['econometria', 'tutorial-R', 'repost']\nexecute: \n  message: false\n  warning: false\n---\n\n\n\n\n# O método de Newton\n\nMétodos de otimização numérica são frequentemente necessários para estimar modelos econométricos e resolver problemas de máximo/mínimo em geral. Em particular, para encontrar estimadores de extremo --- como estimadores do método generalizado de momentos (GMM) ou estimadores de máxima verossimilhança (MLE) --- é necessário resolver um problema de otimização. Na maior parte dos casos, estes métodos rodam no background de funções prontas, mas não custa entender um pouco mais sobre como eles funcionam, até porque um dos métodos mais populares, o método de Newton (e suas variantes) é bastante simples e intuitivo.\n\n## Exemplo simples\n\nComo primeiro exemplo do método de Newton vou tomar emprestada uma sugestão deste [tweet](https://twitter.com/fermatslibrary/status/1177196132663009280).\n\n![](/static/images/Screenshot%202023-08-06%20at%2017.06.57.png){fig-align=\"center\"}\n\nO algoritmo descrito no tweet para encontrar $\\sqrt{y}$ é basicamente o seguinte:\n\n1.  Encontre um número $x$ tal que $x^2 \\approx y$ (ex: $y = 17$, $4^2 = 16$);\n2.  Compute a diferença $d = y - x^2$ ($d = 17 - 16 = 1$);\n3.  Atualize $x$ pela diferença $d/2x$ ($x = 4 + 1/8 = 4.125$)\n4.  Repita 2 e 3, com o novo valor de x.\n\nO código abaixo implementa os passos acima. Começo escolhendo um inteiro aleatório $y$ entre $0$ e $10^7$. O desafio será encontrar $\\sqrt{y}$. Usando o comando `set.seed(1984)` você poderá replicar os resultados abaixo. O número sorteado foi $6588047$ e $\\sqrt{6588047} = 2566.719$.\n\nPara demonstrar a poder do simples método acima vou começar com um *chute inicial* bem ruim, isto é, vou escolher um $x$ inicial igual a $1$. Como quero mostrar os passos intermediários do algoritmo vou criar um vetor numérico que salva todos os valores de x a cada iteração. Há diversas maneiras de escrever o loop abaixo e ao longo do post vou mostrar três tipos de loop (*for-loop*, *repeat* e *while*).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Para replicar os resultados\nset.seed(1984)\n# Sorteia um número inteiro\ny <- round(runif(n = 1, min = 0, max = 10^7))\n# Cria um vetor x de comprimento 15\nx <- vector(length = 20)\n# Primeiro valor de x = 1\nx[1] <- 1\n# Loop\nfor (i in 1:19) {\n  x_chute <- x[i]\n  # Computa a distância\n  dist <- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo <- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] <- x_chute_novo\n}\n```\n:::\n\n\nPodemos ver graficamente como o algortimo se aproxima do valor verdadeiro de $\\sqrt{y}$ rapidamente, mesmo partindo de um ponto inicial distante. Em vermelho, está o valor correto de $\\sqrt{y}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(2:20, x[2:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nComo os primeiros valores do gráfico acima são muito altos, faço um segundo gráfico usando somente os últimos valores que foram computados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(13:20, x[13:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNote que depois de 16 repetições, a diferença entre $x$ e $y$ é tão próxima de zero que o R arredonda ela para 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx - sqrt(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -2.565719e+03  3.291457e+06  1.644446e+06  8.209418e+05  4.091915e+05\n [6]  2.033204e+05  1.003928e+05  4.894506e+04  2.325311e+04  1.047078e+04\n[11]  4.204686e+03  1.305444e+03  2.200559e+02  8.688284e+00  1.465521e-02\n[16]  4.183812e-08  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n```\n:::\n:::\n\n\nMas como funciona este algoritmo? Basicamente, estamos procurando a raiz de um polinômio. No caso, estamos buscando resolver $f(x) = x^2 - y = 0$ (em que $y$ é dado). A solução proposta pelo método acima é de aproximar este polinômio linearmente (usando a primeira derivada) e então encontrar a raiz desta aproximação. Para facilitar o exemplo vamos tomar $y = 2$. Então temos o problema de encontrar a raiz do seguinte polinômio:\n\n$$\nP(x) = x^2 - 2\n$$\n\nVamos tomar o valor $x_{0} = 5$ como chute inicial. A aproximação linear de $P(x)$ é a derivada com respeito a $x$, isto é, $2x$. Avaliada no ponto $x_{0}$, temos $P'(5) = 10$. Assim, temos uma reta de inclinação $10$ que passa no ponto $(5, 23)$. Com isso podemos traçar a reta vermelha no gráfico abaixo e calcular o ponto em que esta reta corta o eixo-x. A equação da reta pode ser escrita como:\n\n$$\n\\frac{y_{2} - y_{1}}{x_{2} - x_{1}} = f'(x_{1})\n$$\n\nComo queremos encontrar o ponto em que a curva corta o eixo-x estamos, na verdade, buscando o ponto $(x_{2}, 0)$. Logo, podemos substituir $y_{2} = 0$ na expressão acima e resolver para $x_{2}$:\n\n$$\nx_{2} = x_{1} - \\frac{y_{1}}{f'(x_{1})} = x_{1} - \\frac{f(x_{1})}{f'(x_{1})}\n$$\n\nA expressão acima é essência do método de Newton. Substituindo nossos valores temos que:\n\n$$\n\\frac{23}{5 - x} = 10 \\longrightarrow x = 2.7\n$$\n\nNo gráfico abaixo podemos ver como funciona este processo. Parte-se de um ponto inicial $x = 5$. Encontra-se a reta tangente neste ponto (linha em vermelho) e computa-se o valor de $x$ em que esta reta cruza o eixo-x (ponto vermelho). O processo repete-se agora tomando o ponto em vermelho como ponto de partida. De maneira geral, o processo de atualização segue a regra abaixo:\n\n$$\n  x_{n + 1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- 2\nx <- vector(length = 10)\n# Primeiro valor de x = 1\nx[1] <- 5\n# Loop\nfor (i in 1:9) {\n  x_chute <- x[i]\n  # Computa a distância\n  dist <- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo <- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] <- x_chute_novo\n}\n```\n:::\n\n\nO gráfico abaixo mostra os próximos passos do algoritmo. Após poucas repetições já estamos muito próximos da resposta correta. Note que isto acontece porque estamos trabalhando com uma função bastante simples.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nPode-se generalizar o problema acima facilmente para encontrar a n-ésima raiz de qualquer valor. No código abaixo isto pode ser feito variando o valor de `n`. O código abaixo também é uma variação em relação aos anteiores. Ele é um *repeat-break*, ao invés do *for-loop* que estávamos usando acima. Isto é, ele repete uma operação enquanto alguma condição for válida. Assim, podemos exigir que o algoritmo pare quando algum critério de convergência for atingido. No caso abaixo, especifico que o loop pare quando a distância entre os valores sucessivos de $x$ for muito pequena (menor que $10^{-8}$) ou quando o número de iterações chegar a 100 repetições.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dá pra melhorar um pouco o loop (usando repeat) e agora generalizar para n\ny <- round(runif(n = 1, min = 0, max = 10^7))\nx <- vector(length = 100)\nx[1] <- (10^7 - 1) / 2\ni <- 1\nn <- 4\nrepeat {\n  x_chute <- x_chute_novo\n  dist <- y - x_chute^n\n  x_chute_novo <- x_chute + dist / (n * x_chute^(n - 1))\n  x[i + 1] <- x_chute_novo\n  # print(paste(\"iteration =\", i))\n  if (abs(x_chute_novo - x[i]) < 10^(-8) | i > 99) {\n    break\n  }\n  i <- i + 1\n}\nplot(x[1:i + 1] - y^(1 / n), main = \"Distância entre valor computado e valor verdadeiro\", xlab = \"Contagem da iteração\", ylab = \"Distância\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n# Minimizando funções\n\nO método de Newton, descrito acima, pode ser aplicado para encontrar o mínimo de funções. Há vários problemas de otimização que podem ser resolvidos igualando a primeira derivada a zero. Nestes casos, podemos usar o método de Newton para encontrar o valor de $x$ que faz com que função derivada $f'(x)$ seja igual a zero. Substituindo $f'(x)$ na expressão encontrada anteiormente:\n\n$$\n  x_{n + 1} = x_{n} - \\frac{f'(x)}{f''(x)}\n$$\n\nObviamente, é necessário que tanto $f'(x)$ como $f''(x)$ estejam bem definidas para que o método funcione. Na verdade, é preciso mais do que isto, mas vamos discutir estes problemas mais à frente. Vamos começar com o problema simples de encontrar o mínimo da função:\n\n$$\nf(x) = -2x^2 + (1/3)x^3\n$$\n\nDerivando e igualando a zero encontra-se os valores críticos $0$ e $4$. Usando a segunda derivada verifica-se que $4$ é mínimo local. O gráfico abaixo mostra este ponto na função.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nO código abaixo resolve este problema usando o método de Newton. Desta vez, uso o `while` apenas para exemplificar ainda mais uma forma de implementar este tipo de algoritmo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Objetos inicias\nx <- vector(length = 100)\nerror <- 10\ntheta <- 10\ndelta <- 10^(-6)\ni <- 0\n# Função objetivo\nf <- function(x) {\n  -2 * x^2 + (1 / 3) * x^3\n}\n# Loop\nwhile (error > delta & i < 10) {\n  i <- i + 1\n  theta_0 <- theta\n  G <- -4 * theta + theta^2\n  H <- -4 + 2 * theta\n  theta <- theta_0 - G / H\n  x[i + 1] <- theta\n  error <- abs((theta - theta_0) / theta_0)\n}\n```\n:::\n\n\nNovamente, pode-se visualizar o funcionamento do otimizador visualmente num gráfico. Após poucos passos, o algoritmo converigiu para um valor muito próximo do correto.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Na prática, a teoria é outra\n\nHá vários casos em que o otimizador de Newton falha em encontrar o valor desejado. Isto pode acontecer tanto porque a função viola alguma das hipóteses do método como também porque a função é complicada. A função abaixo, por exemplo, falha em convergir pois a sua derivada é descontínua no zero (há uma \"quina\" no zero). O resultado é que o método diverge para $\\infty$.\n\n$$\nf(x) = |x|^{1/3}\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nA tabela abaixo mostra os valores computados para as primeiras 10 iterações.\n\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Iteração </th>\n   <th style=\"text-align:center;\"> x </th>\n   <th style=\"text-align:center;\"> f(x) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 1.709976 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> -10 </td>\n   <td style=\"text-align:center;\"> 2.154435 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> -40 </td>\n   <td style=\"text-align:center;\"> 3.419952 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> -160 </td>\n   <td style=\"text-align:center;\"> 5.428835 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> -640 </td>\n   <td style=\"text-align:center;\"> 8.617739 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 6 </td>\n   <td style=\"text-align:center;\"> -2560 </td>\n   <td style=\"text-align:center;\"> 13.679808 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 7 </td>\n   <td style=\"text-align:center;\"> -10240 </td>\n   <td style=\"text-align:center;\"> 21.715341 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 8 </td>\n   <td style=\"text-align:center;\"> -40960 </td>\n   <td style=\"text-align:center;\"> 34.470955 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 9 </td>\n   <td style=\"text-align:center;\"> -163840 </td>\n   <td style=\"text-align:center;\"> 54.719230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> -655360 </td>\n   <td style=\"text-align:center;\"> 86.861364 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\nOutro exemplo curioso em que o algoritmo falha em convergir é quando temos:\n\n$$\nf(x) = \\left\\{\\begin{matrix}\n0 & \\text{se } x =0\\\\ \nx + x^2 sin(\\frac{2}{x})) & \\text{se } x \\neq 0 \n\\end{matrix}\\right.\n$$\n\nAinda que a função seja contínua, temos uma descontinuidade na sua derivada e, como resultado, o otmizador fica osciliando entre valores sem nunca convergir para o zero.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nA tabela abaixo mostra os primeiros dez resultados:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Iteração </th>\n   <th style=\"text-align:center;\"> x </th>\n   <th style=\"text-align:center;\"> f(x) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 5.0000000 </td>\n   <td style=\"text-align:center;\"> 14.7354586 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 0.1719653 </td>\n   <td style=\"text-align:center;\"> 0.1481520 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 0.4920907 </td>\n   <td style=\"text-align:center;\"> 0.2990380 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 0.2819031 </td>\n   <td style=\"text-align:center;\"> 0.3395412 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> -10.3200755 </td>\n   <td style=\"text-align:center;\"> -30.8312710 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 6 </td>\n   <td style=\"text-align:center;\"> -0.0854544 </td>\n   <td style=\"text-align:center;\"> -0.0782425 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 7 </td>\n   <td style=\"text-align:center;\"> -0.0171313 </td>\n   <td style=\"text-align:center;\"> -0.0169891 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 8 </td>\n   <td style=\"text-align:center;\"> -0.0109142 </td>\n   <td style=\"text-align:center;\"> -0.0110166 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 9 </td>\n   <td style=\"text-align:center;\"> -3.4206997 </td>\n   <td style=\"text-align:center;\"> -9.8789235 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> -0.2423414 </td>\n   <td style=\"text-align:center;\"> -0.2964612 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nPode-se visualizar o comportamento do otimizador na animação abaixo. Mostro os primeiros 50 resultados. Note como o algoritmo fica pulando para todos os lados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl <- data.frame(\n  x = x,\n  y = f(x),\n  label = 1:length(x)\n)\n\nggplot() +\n  geom_function(fun = ~ ifelse(.x == 0, 0, .x + .x^2 * sin(2 / .x))) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_point(data = tbl, aes(x = x, y = y), size = 2, color = \"firebrick\") +\n  geom_label(data = tbl, aes(x = x, y = y + 0.15, label = label)) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  transition_states(label) +\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.gif)\n:::\n:::\n\n\nEm geral, o método de Newton sofre problemas com:\n\n1.  *Pontos inciais ruins*. Às vezes uma escolha de ponto inicial ruim pode levar o otimizador a convergir para pontos diferentes. Em alguns casos, um ponto inicial ruim pode também levar o otimizador a ficar preso num ciclo.\n\nexemplo: $f(x) = x^3 - 2x + 2$ com $x_{0} = 0$. O otimizador fica preso entre $0$ e $1$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Iteração </th>\n   <th style=\"text-align:center;\"> x </th>\n   <th style=\"text-align:center;\"> f(x) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 3 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 6 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 7 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 8 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 9 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n2.  *Descontinuidades*. Descontinuidades tanto na raiz da função, como em outras partes da função (como também na sua derivada) podem levar a problemas sérios de convergência.\n\nexemplo: $f(x) = |x|^{1/3}$ como apresentado acima.\n\n3.  *Funções difíceis*. Algumas funções são simplesmente muito complicadas para o método de Newton. Em alguns casos, a convergência pode ser muito lenta e em outros o otimizador pode falhar completamente.\n\nexemplo: $f(x) = 7x - \\text{ln}(x)$. A função tem mínimo global em $x = 1/7$. Ainda assim, o método de Newton tem dificuldade em encontrar este valor (a não ser que o valor incial escolhido esteja muito próximo de $1/7$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) {\n  7 * x - log(x)\n}\ngrad_f <- function(x) {\n  7 - 1 / x\n}\nhess_f <- function(x) {\n  1 / x^2\n}\ntheta <- 5\nerror <- 5\ni <- 1\nx <- vector(length = 50)\nx[1] <- theta\nerror <- 1\nwhile (error > 10^(-8) & i < 50) {\n  theta_0 <- theta\n  G <- grad_f(theta)\n  theta <- theta_0 - grad_f(theta) / hess_f(theta)\n  x[i + 1] <- theta\n  error <- abs((theta - theta_0) / theta_0)\n  i <- i + 1\n}\n```\n:::\n\n\n# Usando funções de otimização\n\nNa prática, métodos de otimização comuns como os de Newton já estão implementados de maneira eficiente no R. A principal função que se para otimização é a `optim` que já está instalada no pacote base do R. Atualmente, contudo, ela já está um pouco defasada e novos pacotes foram criados para suplementá-la. Discuto um pouco mais sobre alternativas ao `optim` no meu post sobre [estimação por máxima verossimilhança](/estimacao-por-maxima-verossimilhanca-no-r).\n\n$$\nf(x,y) = (1-x)^2 + 100(y - x^2)^2\n$$\n\nA imagem abaixo usa o pacote `rgl` para mostrar a função em três dimensões.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfn_2 <- function(x, y) {\n  (1 - x)^2 + 100 * (y - x^2)^2\n}\n# Gráfico 3D\ngrid <- seq(from = -5, to = 5, length.out = 1000)\nx <- grid\ny <- grid\nz <- outer(x, y, fn_2)\npersp3d(x, y, z, col = \"skyblue\")\n```\n:::\n\n\nO código abaixo é, essencialmente, igual aos que foram apresentados acima. A diferença é que agora o gradiente tem que ser montado como um vetor e a hessiana tem que ser implementada como uma matriz. Além disso, há um pequeno detalhe técnico na hora de montar a função `fn`. Como há dois parâmetros, talvez a maneira mais natural de escrever isto seria:\n\n`fn2 = function (x, y) { ... }`\n\nMas isto não funciona bem com a função `optim`. O correto é montar uma função multivariada usando apenas um vetor numérico. Note que no código abaixo eu faço `fn2` em função de `theta` e escrevo `theta[1]` no lugar de $x$ e `theta[2]` no lugar de $y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# parâmetros iniciais\nx <- vector(length = 100)\nerror <- 10\ntheta <- 10\ndelta <- 10^(-6)\ni <- 0\n# a função\nfn2 <- function(theta) {\n  (1 - theta[1])^2 + 100 * (theta[2] - theta[1]^2)^2\n}\n# gradiente\ngrad_fn2 <- function(theta) {\n  x <- theta[1]\n  y <- theta[2]\n  gx <- -2 * (1 - x) - 400 * x * (y - x^2)\n  gy <- (1 - x)^2 + 200 * (y - x^2)\n  c(gx, gy)\n}\n# hessiana\nhess_fn2 <- function(theta) {\n  x <- theta[1]\n  y <- theta[2]\n  hxx <- 2 - 400 * (y - x^2) + 800 * x^2\n  hyy <- 200\n  hxy <- hyx <- -2 * (1 - x) - 400 * x\n  matrix(c(hxx, hxy, hyx, hyy), ncol = 2)\n}\n\n# Otimização usando loop e NR\ntheta <- c(-1.2, 1)\nhist <- matrix(NA, ncol = 2, nrow = 100)\nhist[1, ] <- theta\ni <- 0\nerror <- c(1, 1)\nwhile (i < 100 & error[1] > 10^(-8) & error[2] > 10^(-8)) {\n  theta_0 <- theta\n  G <- grad_fn2(theta)\n  H <- hess_fn2(theta)\n  theta <- theta_0 - solve(H) %*% G\n  hist[i + 1, ] <- t(theta)\n  error <- t(abs((theta - theta_0) / theta_0))\n  i <- i + 1\n}\n\nres <- na.omit(hist)\n```\n:::\n\n\nO gráfico abaixo mostra a convergência do algoritmo. Note que o valor correto de ambos os parâmetros é $1$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nPor fim, uso a função `optim` para resolver o problema acima; o método `BFGS` é uma variante do método de Newton.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Optim usando BFGS\noptim(par = c(-1.2, 1), fn = fn2, method = \"BFGS\", control = list(trace = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninitial  value 24.200000 \niter  10 value 1.370040\niter  20 value 0.132618\niter  30 value 0.001800\nfinal  value 0.000000 \nconverged\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$par\n[1] 0.9998044 0.9996084\n\n$value\n[1] 3.827383e-08\n\n$counts\nfunction gradient \n     118       38 \n\n$convergence\n[1] 0\n\n$message\nNULL\n```\n:::\n:::\n\n\n# Posts relacionados\n\n-   [Estimação por máxima verossimilhança no R](https://restateinsight.com/posts/general-posts/emv-no-r/)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}