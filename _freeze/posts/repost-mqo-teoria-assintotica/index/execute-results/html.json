{
  "hash": "0913819932960c6baa89a601525633a2",
  "result": {
    "markdown": "---\ntitle: \"MQO - teoria assintótica\"\ndate: '2019-03-23'\ncategories: ['econometria', 'tutorial-r']\ndescription: 'Simulação de alguns resultados assintóticos de MQO'\nexecute: \n  warning: false\n  message: false\n---\n\n\n# Disclaimer\n\nEste é um repost antigo que fiz ainda na época do mestrado em economia. Apesar de intuitivo o código dos loops abaixo é muito ineficiente. De maneira geral, `for`-loops são melhores do que loops feitos com `repeat`; melhor ainda é montar funções e usar `parallel::mclapply` ou `replicate`. Além disso, é importante pre-definir o tamanho do objeto antes de um loop, e.g., `x <- vector(\"numeric\", length = 10000)`.\n\n# Mínimos Quadrados \n\nA maior parte dos resultados assintóticos dos estimadores de mínimos quadrados (MQO) são um misto da LGN, do TCL e de outros resultados de convergência como o método delta e o teorema de Slutsky. Um resultado simples que podemos visualizar através de uma simulação é a propriedade de não-viés dos estimadores de MQO. Em linhas gerais, desde que o termo de erro seja ortogonal às variáveis independentes, os estimadores de MQO não serão viesados, i.e., os estimadores $\\hat{\\beta}$ vão convergir para os verdadeiros $\\beta$. Dizer que eles são ortogonais costuma ser o mesmo que dizer que eles são independentes. Isto é, na prática queremos que a esperança condicional de $u_{t}$, o erro, dado $x_{t}$, as variáveis explicativas, seja nulo:\n\n$$\n\\mathbb{E}(u_{t} | x_{t}) = 0\n$$\n\nSuponha que o modelo verdadeiro (o processo gerador de dados) seja da forma:\n\n\\begin{equation}\n\ty_{t} = 5 + 2.5x_{1t} -1.75x_{2t} + 5x_{3t} + u_{t}\n\\end{equation}\nonde\n\\begin{align}\n\tx_{1} & \\sim N(0,1)\\\\\n\tx_{2} & \\sim \\text{Beta}(2, 5)\\\\\n\tx_{3} & \\sim U(0,1)\\\\\n\tu_{t} & \\sim N(0,1)\n\\end{align}\n\nSe as variáveis $x_{1}, x_{2}$ e $x_{3}$ forem independentes de $u_{t}$ então o modelo:\n\\begin{equation}\n\ty_{t} = \\beta_{0} + \\beta_{1}x_{1t} + \\beta_{2}x_{2t} + \\beta_{3}x_{3t}\n\\end{equation}\nfornecerá estimativas consistentes para os betas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modelo verdadeiro (DGP)\nx1 <- rnorm(n = 200)\nx2 <- rbeta(n = 200, shape1 = 2, shape2 = 5)\nx3 <- runif(n = 200)\ny <- 5 + 2.5 * x1 - 1.75 * x2 + 5 * x3 + rnorm(200)\n\nsummary(fit <- lm(y ~ x1 + x2 + x3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2 + x3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.05015 -0.62926  0.03371  0.74542  2.73842 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.85230    0.21210  22.877  < 2e-16 ***\nx1           2.53860    0.07059  35.962  < 2e-16 ***\nx2          -1.63461    0.46950  -3.482 0.000614 ***\nx3           5.19868    0.26814  19.388  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.088 on 196 degrees of freedom\nMultiple R-squared:  0.8893,\tAdjusted R-squared:  0.8876 \nF-statistic:   525 on 3 and 196 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNote que os valores estimados estão bastante próximos dos valores verdadeiros. Podemos ver o impacto que o tamanho da amostra tem sobre as estimativas fazendo um loop para diferentes tamanhos. Para conseguir resultados mais consistentes podemos gerar os valores do DGP várias vezes. Abaixo os dados são gerados e estimados 100 vezes para diferentes amostras com $n = 10, 50, 10000$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\ntabela <- matrix(ncol = 4)\nfor (n in c(10, 50, 10000)) {\n\ti <- 0\n\tX <- matrix(ncol = 4, nrow = 100)\n\trepeat{\n\n\t\tx1 <- rnorm(n) \n\t\tx2 <- rbeta(n, 2, 5)\n\t\tx3 <- runif(n)\n\t\ty <- 5 + 2.5 * x1 - 1.75 * x2 + 5 * x3 + rnorm(n)\n\n\t\tfit <- lm(y ~ x1 + x2 + x3)\n\n\t\tcoeficientes <- coef(fit)\n\n\t\tX[i + 1, ] <- coeficientes\n\n\t\ti <- i + 1\n\t\tif (i == 100) {break}\n\t}\n\n\ttabela <- rbind(tabela, colMeans(X, na.rm = TRUE))\n\n\thist(X[, 1], freq = FALSE,\n\t\t main = bquote(beta[0]~\", n = \"~.(n)), xlab = \"\")\n\tabline(v = 5, col = \"red\")\n\thist(X[, 2], freq = FALSE,\n\t\t main = bquote(beta[1]~\", n = \"~.(n)), xlab = \"\")\n\tabline(v = 2.5, col = \"red\")\n\thist(X[, 3], freq = FALSE,\n\t\t main = bquote(beta[2]~\", n = \"~.(n)), xlab = \"\")\n\tabline(v = -1.75, col = \"red\")\n\thist(X[, 4], freq = FALSE,\n\t\t main = bquote(beta[3]~\", n = \"~.(n)), xlab = \"\")\n\tabline(v = 5, col = \"red\")\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\nA tabela abaixo mostra a média dos valores estimados para cada coeficiente\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-bordered\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> b0 = 5 </th>\n   <th style=\"text-align:center;\"> b1 = 2,5 </th>\n   <th style=\"text-align:center;\"> b2 = -1,75 </th>\n   <th style=\"text-align:center;\"> b3 = 5 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> n = 10 </td>\n   <td style=\"text-align:center;\"> 5.170063 </td>\n   <td style=\"text-align:center;\"> 2.425371 </td>\n   <td style=\"text-align:center;\"> -1.751129 </td>\n   <td style=\"text-align:center;\"> 4.649316 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> n = 50 </td>\n   <td style=\"text-align:center;\"> 5.030621 </td>\n   <td style=\"text-align:center;\"> 2.485392 </td>\n   <td style=\"text-align:center;\"> -1.746796 </td>\n   <td style=\"text-align:center;\"> 4.977591 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> n = 10000 </td>\n   <td style=\"text-align:center;\"> 5.003677 </td>\n   <td style=\"text-align:center;\"> 2.501104 </td>\n   <td style=\"text-align:center;\"> -1.755232 </td>\n   <td style=\"text-align:center;\"> 4.994201 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n# MQO viesado\n\n\n### Processo não-estacionário\n\nUm dos casos em que os estimadores de MQO se tornam viesados acontece quando o processo é auto-regressivo e não-estacionário, isto é, quando o DGP é da forma\n\n\\begin{equation}\n\ty_{t} = \\phi y_{t-1} + u_{t}\n\\end{equation}\n\nem que $|\\phi| \\geq 1$. O código abaixo simula o modelo acima mil vezes para diferentes valores de $\\phi$. Quando o processo é estacionário temos estimativas boas para o parâmetro, mas quando $\\phi = 1.1$ as estimativas tornam-se muito ruins. Quando $\\phi = 1$ temos um caso de raiz unitária que tem propriedades bastante específicas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dynlm)\npar(mfrow = c(2, 2))\nfor(phi in c(0.5, 0.9, 1, 1.1)){\n\tj = 0\n\tx <- c()\n\t\n\trepeat{\n\n\ty <- 0\n\tfor(i in 1:99) { \n\t\ty[i + 1] <- phi * y[i] + rnorm(1)\n\t}\n\t\n\ty <- ts(y)\n\tfit <- dynlm(y ~ lag(y))\n\tphi_hat <- coef(fit)[2]\n\tx <- c(x, phi_hat)\n\n\tj = j + 1\n\tif (j == 1000) {break}\n\t}\n\n\thist(x, freq = FALSE,\n\t\t main = bquote(phi==.(phi)),\n\t\t xlim = c(phi - 0.2, 1),\n\t\t xlab = \"\")\n\tabline(v = phi, col = \"red\")\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Erros ARMA\n\nQuando os erros do modelo são autocorrelacionados os estimadores de MQO perdem algumas de suas boas propriedades. Ainda assim, desde que o erro seja ortogonal aos regressores os estimadores continuaram sendo não-viesados. Há, contudo, um caso em que os estimadores de MQO são viesados: quando algum dos regressores independentes é uma defasagem da variável depedente.\n\nIsto acontece porque este tipo de autocorrelação implica que os regressores não são ortogonais aos erros. Tome o caso simples em que o erro $u_{t}$ segue um processo AR(1) da forma\n\n\\begin{equation}\n\tu_{t} = \\phi u_{t-1} + \\eta_{t}\n\\end{equation}\nem que $\\eta_{t}$ é ruído branco. E o modelo é \n\\begin{equation}\n\ty_{t} = \\beta_{0} + \\beta_{1}y_{t-1} + u_{t}\n\\end{equation}\nAgora note que \n$$y_{t - 1} = \\beta_{0} + \\beta_{1}y_{t - 2} + u_{t - 1}$$\nReescrevendo a expressão do erro e substituindo na equação acima temos que:\n$$y_{t - 1} = \\beta_{0} + \\beta_{1}y_{t - 2} + \\frac{u_{t} - \\eta_{t}}{\\phi}$$\nlogo $y_{t - 1} \\propto u_{t}$, isto é, $y_{t - 1}$ é correlacionado com o termo de erro $u_{t}$.\n\nPara exemplificar este resultado, o código abaixo simula a seguinte série 1000 vezes para diferentes valores de $\\phi$. O termo de erro segue um processo AR(1). Note como as estimativas $\\hat{\\phi}$ são bastante ruins. Para remediar este problema teríamos que levar em conta a estrutura autoregressiva do erro no modelo.\n\n\\begin{align}\n\ty_{t} & = \\phi y_{t-1} + u_{t}\\\\\n\tu_{t} & = 0.7u_{t-1} + \\eta_{t}\\\\\n\\end{align}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\n\nfor(phi in c(0.25, 0.5, 0.9, 1)) {\n\tj <- 0\n\tx <- c()\n\t\n\trepeat{\n\n\ty <- 0\n\terro <- arima.sim(n = 100, model = list(ar = .7))\n\n\tfor(i in 1:99) { y[i + 1] <- phi*y[i] + erro[i] }\n\t\n\ty <- ts(y)\n\n\tfit <- dynlm(y ~ lag(y))\n\t\n\tphi_hat <- coef(fit)[2]\n\n\tx <- c(x, phi_hat)\n\n\tj = j + 1\n\tif (j == 1000) {break}\n\t}\n\n\thist(x, main = bquote(phi==.(phi)), xlim = c(phi - 0.2, 1), xlab = \"\")\n\tabline(v = phi, col = \"red\")\n\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nEste resultado não muda assintoticamente, isto é, não importa qual o tamanho da amostra: as estimativas de $\\hat{\\phi}$ serão viesadas. Na verdade, os resultados vão piorando à medida que cresce o tamanho da amostra. Abaixo mostro isto para o caso de $\\phi = 0.5$ e n = $100, 200, 500, 10000$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nphi <- 0.5\nfor(n in c(100, 200, 500, 10000)){\n\tj = 0\n\tx <- c()\n\t\n\trepeat{\n\n\ty <- 0\n\terro <- arima.sim(n = n, model = list(ar = .7))\n\n\tfor(i in 1:(n-1)) { y[i + 1] <- phi*y[i] + erro[i] }\n\t\n\ty <- ts(y)\n\n\tfit <- dynlm(y ~ lag(y))\n\t\n\tphi_hat <- coef(fit)[2]\n\n\tx <- c(x, phi_hat)\n\n\tj = j + 1\n\tif (j == 1000) {break}\n\t}\n\n\thist(x,\n\t\t freq = FALSE,\n\t\t main = bquote(phi==.(phi)~\", n = \"~.(n)),\n\t\t xlim = c(0, 1),\n\t\t xlab = \"\")\n\tabline(v = phi, col = \"red\")\n\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nConsiderando a estrutura autoregressiva do modelo chegamos em estimativas melhores para os parâmetros. O código abaixo faz um ARMAX, isto é, uma estimativa de mínimos quadrados com erros ARMA. O modelo segue o processo\n\\begin{align}\n\ty_{t} & = \\beta_{0} + \\beta_{1}y_{t-1} + u_{t}\\\\\n\tu_{t} & = \\beta_{2} u_{t-1} + \\eta_{t}\\\\\n\\end{align}\n\nonde os valores foram definidos como $\\beta_{0} = 0$, $\\beta_{1} = 0.5$ e $\\beta_{2} = 0.7$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nn <- 1000\nparam <- matrix(nrow = n, ncol = 3)\nj <- 0\nrepeat {\n\n\ty <- 0\n\terro <- arima.sim(n = n, model = list(ar = .7))\n\n\tfor(i in 1:(n-1)) {\n\t\ty[i + 1] <- 0.5 * y[i] + erro[i + 1]\n\t}\n\t\n\ty <- ts(y)\n\tly <- lag(y, -1)\n\tx <- ts.intersect(y, ly, dframe = TRUE)\n\tfit <- arima(x$y, order = c(1, 0, 0), xreg = x$ly)\n\tparam[j + 1, ] <- coef(fit)\n\n\tj = j + 1\n\tif (j == 1000) {break}\n}\n\nhist(param[, 2], main = bquote(beta[0]==0~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0, col = \"red\")\nhist(param[, 1], main = bquote(beta[1]==0.5~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0.5, col = \"red\")\nhist(param[, 3], main = bquote(beta[2]==0.7~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0.7, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}