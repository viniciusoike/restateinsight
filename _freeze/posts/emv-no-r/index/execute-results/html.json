{
  "hash": "fa87a2b364a34c01255e16203459bf20",
  "result": {
    "markdown": "---\ntitle: \"EMV no R\"\ndate: '2020-02-01'\ncategories: ['econometria', 'tutorial-R', 'repost']\n---\n\n\n\n\n# Estimadores de Máxima Verossimilhança\n\nA estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é **consistente** (converge para o valor verdadeiro), **normalmente assintótico** (distribuição assintórica segue uma normal padrão) e **eficiente** (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.\n\nA intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros $\\mu$ e $\\sigma$ de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido \"gerada\" por uma normal).\n\nHá vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no `R`. Neste post vou me ater apenas a dois pacotes: o `optimx` e o `maxLik`. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.\n\nVale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função `optim` do `R`. Os dois pacotes simplesmente trazem algumas comodidades.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(maxLik)\nlibrary(optimx)\n# Para reproduzir os resultados\nset.seed(33)\n```\n:::\n\n\n# Exemplo com distribuição Poisson\n\nA ideia da estimação por máxima verossimilhança é de encontrar os parâmetros que maximizam a probabilidade de que um certo conjunto de dados sejam gerados por algum processo específico. Agora, em português: imagine que temos uma amostra aleatória; não sabemos qual distribuição os dados seguem, mas vamos *supor* que eles seguem alguma distribuição específica, por exemplo, uma Poisson. O formato da Poisson depende do parâmetro $\\lambda$ e a ideia então é de encontrar o valor de $\\lambda$ que melhor aproxima a distribuição empírica/observada dos dados. Isto é, dado uma amostra aleatória $x_{1}, x_{2}, \\dots , x_{n}$ buscamos o valor de $\\lambda$ que maximiza a probabilidade de que os dados tenham sido gerados por uma distribuição de Poisson.\n\nA Poisson é uma distribuição discreta parametrizada por um valor $\\lambda > 0$. O formato da distribuição varia com o valor de $\\lambda$. Lembrando que a função de distribuição da Poisson é dada pela expressão:\n\n$$\n  f(k, \\lambda) = \\text{Pr}(X = k) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\dots\n$$\n\nO código abaixo faz o gráfico da função acima para diferentes valores de $\\lambda$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nPara montar a função de log-verossimilhança começamos com a função de verossimilhança de uma observação $i$ qualquer e depois montamos a distribuição conjunta dos dados. Lembrando que a função de verossimilhança da Poisson para uma observação $i$ é dada por:\n\n$$\n  f(x_{i}, \\lambda) = \\frac{\\lambda^{x_{i}}e^{-\\lambda}}{x_{i}!}\n$$\n\nSupondo que a amostra é de fato aleatória, a distribuição conjunta dos dados é simplesmente o produtório das distribuições individuais, isto é:\n\n$$\n\\begin{align}\n  \\prod_{k = 1}^{n} f(x_{k}, \\lambda) & = \\prod_{k = 1}^{n} \\left( \\frac{\\lambda^{x_{k}}e^{-\\lambda}}{x_{k}!} \\right) \\\\\n  & = \\frac{\\lambda^{\\sum x_{k}} e^{-n\\lambda}}{\\prod_{k = 1}^{n}(x_{k}!)}\n\\end{align}\n$$\n\nAplicando log chegamos na função de log-verossimilhança:\n\n$$\n\\begin{align}\n  \\mathcal{L}(\\lambda ; x_{1}, \\dots , x_{n}) & = \\text{ln}(\\prod_{k = 1}^{n} f(x_{k}, \\lambda)) \\\\\n  & = \\text{ln}(\\lambda^{\\sum x_{k}}) + \\text{ln}(e^{-n\\lambda}) - \\text{ln}(\\prod_{k = 1}^{n}(x_{k}!)) \\\\\n  & = \\text{ln}(\\lambda)\\sum_{k = 1}^{n}x_{k} - n\\lambda - \\sum_{k = 1}^{n} \\text{ln}(x_{k}!)\n\\end{align}\n$$\n\nDada uma amostra aleatória $x_{1}, x_{2}, \\dots , x_{n}$ buscamos o valor de $\\lambda$ que maximiza a função acima. Isto é, temos o seguinte problema de otimização:\n\n$$\n  \\underset{\\lambda > 0}{\\text{Max}} \\quad \\text{ln}(\\lambda)\\sum_{k = 1}^{n}x_{k} - n\\lambda - \\sum_{k = 1}^{n} \\text{ln}(x_{k}!)\n$$\n\nonde os valores de $x_{k}$ são os nossos dados observados. Para implementar este procedimento no R primeiro montamos a função log-verossimilhança no código abaixo. Em seguida vamos usar a função base `optim` que resolve problemas de minimização. Como a função `optim` serve para *minimizar* funções precisamos implementar o negativo da função de log-verossimilhança (lembrando que maximizar $f(x)$ é equivalente à minimizar $-f(x)$).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nll_pois <- function(x, theta) {\n\tn <- length(x)\n\tll <- log(theta) * sum(x) - n * theta - sum(log(factorial(x)))\n\treturn(-ll)\n}\n```\n:::\n\n\nVamos simular 1000 observações $x_{i} \\sim \\text{Poisson}(\\lambda = 5)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\namostra <- rpois(1000, lambda = 5)\n```\n:::\n\n\nPodemos tornar mais claro o procedimento de estimação olhando para o gráfico da função. O código abaixo plota o gráfico da função de log-verossimilhança para valores de $\\lambda$ no intervalo $[0, 20]$. Note que a função parece atingir um máximo em torno de $5$, o valor verdadeiro do parâmetro.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neixo_lambda <- seq(0, 20, .001)\nplot(eixo_lambda, -ll_pois(amostra, eixo_lambda), type = \"l\",\n\t xlab = bquote(lambda),\n\t ylab = bquote(\"L(\"~lambda~\";\"~x[1]~\", ... ,\"~x[n]~\")\"),\n\t main = \"Função log-verossimilhança da Poisson\")\nabline(v = 5, col = \"indianred\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nPara estimar $\\lambda$ usamos a função `optim`. É preciso definir algum valor inicial para que o otimizador numérico comece a procurar pelo máximo global. Em casos mais simples, de funções globalmente côncavas, esta escolha não apresenta grandes consequências. Em casos mais complicados, o resultado da estimação pode ser bastante sensível à escolha de valor inicial porque o otimizador pode cair em máximos/mínimos locais. No final do post discuto brevemente algumas soluções. Neste exemplo escolho arbitrariamente começar no valor $1$.\n\nNeste caso também poderíamos usar o fato de que a esperança da Poisson é igual a $\\lambda$, logo, pela Lei dos Grandes Números, a média amostral já deve estar próxima do verdadeiro valor de $\\lambda$. Assim, poderíamos também ter escolhido `mean(amostra)` como valor inicial.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- optim(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n\tmethod = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$par\n[1] 4.938\n\n$value\n[1] 2202.837\n\n$counts\nfunction gradient \n      34        9 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n         [,1]\n[1,] 202.5112\n```\n:::\n:::\n\n\nA saída da função traz várias informações técnicas sobre a otimização numérica. A estimativa pontual computada foi de $4.938$ - bastante próxima do verdadeiro valor do parâmetro. Usando a função `str` podemos observar a estrutura do objeto `fit` criado acima. As principais informações que podemos extrair são as estimativas dos parâmetros `fit$par` e a hessiana calculada nestes parâmetros `fit$hessian`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstr(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 6\n $ par        : num 4.94\n $ value      : num 2203\n $ counts     : Named int [1:2] 34 9\n  ..- attr(*, \"names\")= chr [1:2] \"function\" \"gradient\"\n $ convergence: int 0\n $ message    : NULL\n $ hessian    : num [1, 1] 203\n```\n:::\n:::\n\n\nUsando a estimativa da hessiana podemos computar o erro-padrão da estimativa. Lembre que a variância assintótica do estimador de máxima verossimilhança é o inverso da matriz de informação de Fisher que pode ser expressa como o negativo do valor esperado da hessiana. Isto é, podemos encontrar a variância assintótica calculando o inverso da hessiana (como fizemos a minimização do negativo da função log-verossimilhança não precisamos calcular o negativo da hessiana).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(ep <- sqrt(1/fit$hess))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.0702709\n```\n:::\n:::\n\n\nCom o erro-padrão podemos calcular intervalos de confiança e estatística t.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(ic <- c(fit$par - 1.96 * ep, fit$par + 1.96 * ep))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.800269 5.075731\n```\n:::\n\n```{.r .cell-code}\n(est_t <- (fit$par - 5) / ep)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] -0.8823045\n```\n:::\n:::\n\n\n## Usando o pacote `optimx`\n\nA função `optim` já é bastante antiga e um novo pacote, chamado `optimx`, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são `optimx` e `optimr`. Mais informações sobre o pacote podem ser encontradas [aqui](https://rdrr.io/cran/optimx/).\n\nA sintaxe das funções é muito similar à sintaxe original do `optim`. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit <- optimx(par = 1, fn = ll_pois, x = amostra))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nNelder-Mead 4.9375 2202.837     32     NA    NA        0   NA   NA 0.001\nBFGS        4.9380 2202.837     34      9    NA        0   NA   NA 0.002\n```\n:::\n:::\n\n\nUma das principais vantagens do `optimx` é a possibilidade de usar vários métodos de otimização numérica numa mesma função.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- optimx(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n  method = c(\"nlm\", \"BFGS\", \"Rcgmin\", \"nlminb\")\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nnlm    4.937998 2202.837     NA     NA     8        0   NA   NA 0.002\nBFGS   4.938000 2202.837     34      9    NA        0   NA   NA 0.002\nRcgmin 4.938000 2202.837     15     10    NA        0   NA   NA 0.002\nnlminb 4.938000 2202.837     10     12     9        0   NA   NA 0.001\n```\n:::\n:::\n\n\nComo este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos.\n\n## Usando o pacote maxLik\n\nA função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, *não é preciso montar a função com sinal de menos* como fizemos acima; segundo, ela já *calcula erros-padrão e estatísticas-t dos coeficientes estimados*. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas [aqui](https://www.researchgate.net/publication/227354660_MaxLik_A_package_for_maximum_likelihood_estimation_in_R).\n\nPara usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o `dpois` que implementa a função densidade da Poisson. O argumento `log = TRUE` retorna as probabilidades $p$ como $log(p)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nll_pois <- function(x, theta) {\n\tll <- dpois(x, theta, log = TRUE)\n\treturn(sum(ll))\n}\n```\n:::\n\n\nO comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fit <- maxLik(ll_pois, start = 1, x = amostra))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 8 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(> t)    \n[1,]  4.93800    0.07617   64.83  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n```\n:::\n:::\n\n\nPodemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função `optimx`. Derivando a função de log-verossimilhança:\n\n$$\n\\begin{align}\n  \\frac{\\partial \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda} & = \\frac{1}{\\lambda}\\sum_{k = 1}^{n}x_{k} - n \\\\\n  \\frac{\\partial^2 \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda^2} & = -\\frac{1}{\\lambda^2}\\sum_{k = 1}^{n}x_{k}\n\\end{align}\n$$\n\nO código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrad_pois <- function(x, theta) {\n  (1 / theta) * sum(x) - length(x)\n  }\n\nhess_pois <- function(x, theta) {\n\t-(1 / theta^2) * sum(x)\n}\n\nfit2 <- maxLik(\n  ll_pois,\n  grad = grad_pois,\n  hess = hess_pois,\n  start = 1,\n  x = amostra\n  )\n\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 7 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(> t)    \n[1,]  4.93800    0.07027   70.27  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n```\n:::\n:::\n\n\n# Exemplo com a distribuição normal\n\nA distribuição normal tem dois parâmetros: $\\mu$ e $\\sigma$. Lembrando que o primeiro indica a média e o segundo a desvio-padrão. A função de densidade de probabilidade é dada por:\n\n$$\n  f(x, \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\text{exp}\\left(\\frac{-(x - \\mu)^2}{2\\sigma^{2}}\\right)\n$$\n\nonde $\\theta = (\\mu, \\sigma)$. O gráfico abaixo mostra como o formato da função varia conforme o valor destes parâmetros. Basicamente, quando a média aumenta, estamos \"deslocando para a direita\" e quando aumentamos o desvio-padrão estamos \"achatando\" a distribuição.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nLembrando que a função de distribuição de probabilidade da normal para uma observação $i$\n\n$$\n  f(x_{i}, \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{\\frac{-(x_{i} - \\mu)^2}{2\\sigma^{2}}}\n$$\n\nFazendo o produtório da expressão acima para cada $i$\n\n$$\n  \\prod_{i = 1}^{N}f(x_{i}, \\theta) = (2\\pi\\sigma^{2})^{-\\frac{N}{2}}\\text{exp}\\left( -\\frac{1}{2\\sigma^{2}}\\prod_{i = 1}^{N}(x_{i} - \\mu)^2\\right)\n$$\n\ne agora passando log, temos:\n\n$$\n  L(x_{i}, \\theta) = -\\frac{N}{2}\\text{ln}(2\\pi) -N\\text{ln}(\\sigma) -\\frac{1}{2\\sigma^{2}}\\sum_{i = 1}^{N}(x_{i} - \\mu)^{2}\n$$\n\nMontamos a função log-verossimilhança usando a função `dnorm`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nll_norm <- function(theta) {\n\tll <- dnorm(x, theta[1], theta[2], log = TRUE)\n\t-sum(ll)\n}\n```\n:::\n\n\nVamos simular uma amostra aleatória $x_{1}, x_{2}, \\dots, x_{1000}$ onde cada $x_{i}$ segue uma distribuição normal com média $2$ e desvio-padrão $3$ (i.e., variância $9$).\n\nPrimeiro vamos usar a função `optimx`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# warnings por causa dos valores negativos no log\nx <- rnorm(n = 1000, mean = 2, sd = 3)\n(fit <- optimx(par = c(1, 1), fn = ll_norm, method = \"BFGS\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           p1       p2    value fevals gevals niter convcode kkt1 kkt2 xtime\nBFGS 2.138082 3.058111 2536.736     47     20    NA        0 TRUE TRUE 0.003\n```\n:::\n:::\n\n\nNote que a função retorna mensagens de erro indicando que a função retornou *NaNs*. Isto acontece porque o otimizador experimenta valores não-positivos para $\\sigma$ e isto não é admissível pois $\\text{ln}(\\sigma)$, que aparece no segundo termo da equação acima, não é definido para $\\sigma < 0$. Além disso, $\\sigma$ não pode ser igual a zero pois ele aparece no denominador do último termo à direita da expressão da log-verossimilhança.\n\nIntuitivamente isto é bastante óbvio: $\\sigma$ representa o desvio-padrão da distribuição e $\\sigma^{2}$ a sua variância: não podemos ter valores negativos ou nulos para estas expressões.\n\nPodemos restringir os valores dos parâmetros usando os argumentos `upper` e `lower` para evitar as mensagens de `warning`, mas, na prática, isto não costuma fazer diferença no resultado final da estimação. Note que podemos deixar a restrição livre usando `Inf` e `-Inf` que correspondem a $\\infty$ e $-\\infty$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- optimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"L-BFGS-B\",\n  upper = c(Inf, Inf),\n  lower = c(-Inf, 0)\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               p1       p2    value fevals gevals niter convcode kkt1 kkt2\nL-BFGS-B 2.138093 3.058112 2536.736     11     11    NA        0 TRUE TRUE\n         xtime\nL-BFGS-B     0\n```\n:::\n:::\n\n\n# Propriedades dos estimadores de MV\n\nA teoria dos estimadores de máxima verossimilhança nos garante que eles são *consistentes* (i.e. que eles aproximam o valor verdadeiro dos parâmetros) e *normalmente assintóticos* (a distribuição assintótica segue uma distribuição normal) desde que algumas condições de regularidade sejam atentdidas.\n\nPodemos demonstrar ambas as propriedades fazendo algumas simulações no `R`.\n\n## Consistência\n\nVamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição $N(2, 3)$; computamos as estimativas para $\\mu$ e $\\sigma$ e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.\n\n1.  Simular uma amostra segundo uma distribuição.\n2.  Estimata os parâmetros da distribuição.\n3.  Calcula a variância assintótica dos estimadores.\n4.  Repete 5000 vezes os passos 1-3.\n\nO código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nr <- 5000\nn <- 1000\n\nestimativas <- matrix(ncol = 4, nrow = r)\n\nfor(i in 1:r) {\n\tx <- rnorm(n = n, mean = 2, sd = 3)\n\t\n\tfit <- optimr(\n\t  par = c(1, 1),\n\t  fn = ll_norm,\n\t  method = \"BFGS\",\n\t  hessian = TRUE\n\t  )\n\t# Guarda o valor estimado do parâmetro\n\testimativas[i, 1:2] <- fit$par\n\testimativas[i, 3:4] <- diag(n * solve(fit$hess))\n}\n```\n:::\n\n\nA *consistência* dos estimadores $\\hat{\\theta}_{MV}$ significa que eles aproximam os valores verdadeiros do parâmetros $\\theta_{0}$ à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande $\\mathbb{N} \\to \\infty$ então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros $\\hat{\\theta}_{\\text{MV}} \\to \\theta_{0}$\n\nO código abaixo calcula a média das estimativas para cada parâmetro - lembrando que $\\mu_{0} = 2$ e que $\\sigma_{0} = 3$. Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# | fig-width: 10\npar(mfrow = c(1, 2))\n# Consistência dos estimadores de MV\nmu <- estimativas[, 1]; sigma <- estimativas[, 2]\nmean(mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.000883\n```\n:::\n\n```{.r .cell-code}\nmean(sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.997335\n```\n:::\n\n```{.r .cell-code}\nhist(mu, main = bquote(\"Estimativas para \"~~mu), freq = FALSE, xlim = c(1.5, 2.5))\nabline(v = 2, col = \"indianred\")\nhist(sigma, main = bquote(\"Estimativas para \"~~sigma), freq = FALSE, xlim = c(2.7, 3.3))\nabline(v = 3, col = \"indianred\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n## Normalmente assintótico\n\nDizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:\n\n$$\nz_{\\theta} = \\sqrt{N}\\frac{\\hat{\\theta}_{MV} - \\theta}{\\sqrt{\\text{V}_{ast}}} \\to \\mathbb{N}(0, 1)\n$$\n\nonde $\\text{V}_{ast}$ é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\nNo loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Normalidade assintótica\n\n# Define objetos para facilitar a compreensão\nmu_hat <- estimativas[, 1]\nsigma_hat <- estimativas[, 2]\nvar_mu_hat <- estimativas[, 3]\nvar_sg_hat <- estimativas[, 4]\n\n# Centra a estimativa\nmu_centrado <- mu_hat - 2 \nsigma_centrado <- sigma_hat - 3\n# Computa z_mu z_sigma\nmu_normalizado <- sqrt(n) * mu_centrado / sqrt(var_mu_hat)\nsigma_normalizado <- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Monta o gráfico para mu\n\n# Eixo x\ngrid_x <- seq(-3, 3, 0.01)\n\nhist(\n  mu_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[mu]),\n  freq = FALSE,\n  xlim = c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[mu]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Monta o gráfico para sigma2\nhist(\n  sigma_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[sigma]),\n  freq = FALSE,\n  xlim =c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[sigma]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n## Escolha de valores inicias\n\nComo comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.\n\nO exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# sensível a escolha de valores inicias\nx <- rnorm(n = 1000, mean = 15, sd = 4)\nfit <- optim(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$par\n[1] 618.6792 962.0739\n\n$value\n[1] 7984.993\n\n$counts\nfunction gradient \n     107      100 \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]\n[1,]  0.001070703 -0.0013531007\n[2,] -0.001353101  0.0001884928\n```\n:::\n:::\n\n\nNote que as estimativas estão muito distantes dos valores corretos $\\mu = 15$ e $\\sigma = 4$. Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.\n\nO código abaixo usa os momentos empíricos como valores inicias para $\\mu$ e $\\sigma$:\n\n$$\n\\begin{align}\n  \\mu_{inicial} & = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\\\\n  \\sigma_{inicial} & = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} (x_{i} - \\mu_{inicial})^2}\n\\end{align}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(chute_inicial <- c(mean(x), sqrt(var(x))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.859702  3.930849\n```\n:::\n\n```{.r .cell-code}\n(est <- optimx(par = chute_inicial, fn = ll_norm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  p1       p2    value fevals gevals niter convcode kkt1 kkt2\nNelder-Mead 14.85997 3.929097 2787.294     47     NA    NA        0 TRUE TRUE\nBFGS        14.85970 3.928884 2787.294     15      2    NA        0 TRUE TRUE\n            xtime\nNelder-Mead 0.001\nBFGS        0.001\n```\n:::\n:::\n\n\nAgora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função `optimx` se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.\n\nNote como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método `bobyqa` retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos `nlminb` e `Nelder-Mead` ambos retornam os valores corretos.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Usando outros métodos numéricos\noptimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = c(\"BFGS\", \"Nelder-Mead\", \"CG\", \"nlminb\", \"bobyqa\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   p1         p2    value fevals gevals niter convcode  kkt1\nBFGS        618.67917 962.073907 7984.993    107    100    NA        1  TRUE\nNelder-Mead  14.85571   3.929621 2787.294     83     NA    NA        0  TRUE\nCG           46.43586 628.570987 7358.601    204    101    NA        1  TRUE\nnlminb       14.85970   3.928883 2787.294     23     47    19        0  TRUE\nbobyqa       15.20011   8.993240 3211.556    109     NA    NA        0 FALSE\n             kkt2 xtime\nBFGS        FALSE 0.007\nNelder-Mead  TRUE 0.001\nCG          FALSE 0.007\nnlminb       TRUE 0.001\nbobyqa      FALSE 0.033\n```\n:::\n:::\n\n\nVale notar também alguns detalhes técnicos da saída. Em particular, `convcode == 0` significa que o otimizador conseguiu convergir com sucesso, enquanto `convcode == 1` indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.\n\nJá o `kkt1` e `kkt2` verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte *necessária* do teorema enquanto a segunda condição verifica a parte *suficiente*. Note que o `bobyqa` falha em ambas as condições (pois ele não é feito para este tipo de problema).\n\nOs métodos que retornam os melhores valores, o `Nelder-Mead` e `nlminb` são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.\n\nMais detalhes sobre os métodos podem ser encontrados na página de ajuda da função `?optimx`.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}