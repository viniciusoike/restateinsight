---
title: "EMV no R"
date: '2020-02-01'
categories: ['econometria', 'tutorial-R', 'repost']
description: "Os estimadoes de máxima verossimilhança possuem várias boas propriedades. Neste post discuto tanto aspectos teóricos como aplicados, com exemplos, e faço algumas simulações para comprovar resultados assintóticos."
image: "/static/emv-r.svg"
image-alt: "/static/emv-r.svg"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  fig.asp = 0.618,
  out.width = "100%",
  fig.dpi = 72,
  fig.retina = 2,
  dev = "svg",
  fig.align = "center"
  )
```

# Estimadores de Máxima Verossimilhança

A estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é **consistente** (converge para o valor verdadeiro), **normalmente assintótico** (distribuição assintórica segue uma normal padrão) e **eficiente** (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.

A intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros $\mu$ e $\sigma$ de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido "gerada" por uma normal).

Há vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no `R`. Neste post vou me ater apenas a dois pacotes: o `optimx` e o `maxLik`. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.

Vale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função `optim` do `R`. Os dois pacotes simplesmente trazem algumas comodidades.

```{r, message = FALSE}
library(maxLik)
library(optimx)
# Para reproduzir os resultados
set.seed(33)
```

# Exemplo com distribuição Poisson

A ideia da estimação por máxima verossimilhança é de encontrar os parâmetros que maximizam a probabilidade de que um certo conjunto de dados sejam gerados por algum processo específico. Agora, em português: imagine que temos uma amostra aleatória; não sabemos qual distribuição os dados seguem, mas vamos *supor* que eles seguem alguma distribuição específica, por exemplo, uma Poisson. O formato da Poisson depende do parâmetro $\lambda$ e a ideia então é de encontrar o valor de $\lambda$ que melhor aproxima a distribuição empírica/observada dos dados. Isto é, dado uma amostra aleatória $x_{1}, x_{2}, \dots , x_{n}$ buscamos o valor de $\lambda$ que maximiza a probabilidade de que os dados tenham sido gerados por uma distribuição de Poisson.

A Poisson é uma distribuição discreta parametrizada por um valor $\lambda > 0$. O formato da distribuição varia com o valor de $\lambda$. Lembrando que a função de distribuição da Poisson é dada pela expressão:

$$
  f(k, \lambda) = \text{Pr}(X = k) = \frac{\lambda^{k}e^{-\lambda}}{k!}, \quad k = 0, 1, 2, \dots
$$

O código abaixo faz o gráfico da função acima para diferentes valores de $\lambda$.

```{r, echo = FALSE}
k <- seq(from = 0, to = 20, by = 1)
cores <- c("orange", "dodgerblue", "firebrick", "olivedrab")
plot(
  k, dpois(k, lambda = 1), type = "o", col = cores[1], pch = 19,
  xlab = "k",
  ylab = "P(X = k)",
  main = "Função densidade de probabilidade da Poisson"
  )
lines(k, dpois(k, lambda = 3), type = "o", col = cores[2], pch = 19)
lines(k, dpois(k, lambda = 7), type = "o", col = cores[3], pch = 19)
lines(k, dpois(k, lambda = 10), type = "o", col = cores[4], pch = 19)
legenda <- c(
  as.expression(bquote(lambda == 1)),
	as.expression(bquote(lambda == 3)),
	as.expression(bquote(lambda == 7)),
	as.expression(bquote(lambda == 10))
  )
grid()
legend("topright", legend = legenda, col = cores, lty = 1, pch = 19, cex = 1.2)
```

Para montar a função de log-verossimilhança começamos com a função de verossimilhança de uma observação $i$ qualquer e depois montamos a distribuição conjunta dos dados. Lembrando que a função de verossimilhança da Poisson para uma observação $i$ é dada por:

$$
  f(x_{i}, \lambda) = \frac{\lambda^{x_{i}}e^{-\lambda}}{x_{i}!}
$$

Supondo que a amostra é de fato aleatória, a distribuição conjunta dos dados é simplesmente o produtório das distribuições individuais, isto é:

$$
\begin{align}
  \prod_{k = 1}^{n} f(x_{k}, \lambda) & = \prod_{k = 1}^{n} \left( \frac{\lambda^{x_{k}}e^{-\lambda}}{x_{k}!} \right) \\
  & = \frac{\lambda^{\sum x_{k}} e^{-n\lambda}}{\prod_{k = 1}^{n}(x_{k}!)}
\end{align}
$$

Aplicando log chegamos na função de log-verossimilhança:

$$
\begin{align}
  \mathcal{L}(\lambda ; x_{1}, \dots , x_{n}) & = \text{ln}(\prod_{k = 1}^{n} f(x_{k}, \lambda)) \\
  & = \text{ln}(\lambda^{\sum x_{k}}) + \text{ln}(e^{-n\lambda}) - \text{ln}(\prod_{k = 1}^{n}(x_{k}!)) \\
  & = \text{ln}(\lambda)\sum_{k = 1}^{n}x_{k} - n\lambda - \sum_{k = 1}^{n} \text{ln}(x_{k}!)
\end{align}
$$

Dada uma amostra aleatória $x_{1}, x_{2}, \dots , x_{n}$ buscamos o valor de $\lambda$ que maximiza a função acima. Isto é, temos o seguinte problema de otimização:

$$
  \underset{\lambda > 0}{\text{Max}} \quad \text{ln}(\lambda)\sum_{k = 1}^{n}x_{k} - n\lambda - \sum_{k = 1}^{n} \text{ln}(x_{k}!)
$$

onde os valores de $x_{k}$ são os nossos dados observados. Para implementar este procedimento no R primeiro montamos a função log-verossimilhança no código abaixo. Em seguida vamos usar a função base `optim` que resolve problemas de minimização. Como a função `optim` serve para *minimizar* funções precisamos implementar o negativo da função de log-verossimilhança (lembrando que maximizar $f(x)$ é equivalente à minimizar $-f(x)$).

```{r}
ll_pois <- function(x, theta) {
	n <- length(x)
	ll <- log(theta) * sum(x) - n * theta - sum(log(factorial(x)))
	return(-ll)
}
```

Vamos simular 1000 observações $x_{i} \sim \text{Poisson}(\lambda = 5)$.

```{r}
amostra <- rpois(1000, lambda = 5)
```

Podemos tornar mais claro o procedimento de estimação olhando para o gráfico da função. O código abaixo plota o gráfico da função de log-verossimilhança para valores de $\lambda$ no intervalo $[0, 20]$. Note que a função parece atingir um máximo em torno de $5$, o valor verdadeiro do parâmetro.

```{r, echo = TRUE}
eixo_lambda <- seq(0, 20, .001)
plot(eixo_lambda, -ll_pois(amostra, eixo_lambda), type = "l",
	 xlab = bquote(lambda),
	 ylab = bquote("L("~lambda~";"~x[1]~", ... ,"~x[n]~")"),
	 main = "Função log-verossimilhança da Poisson")
abline(v = 5, col = "indianred")
```

Para estimar $\lambda$ usamos a função `optim`. É preciso definir algum valor inicial para que o otimizador numérico comece a procurar pelo máximo global. Em casos mais simples, de funções globalmente côncavas, esta escolha não apresenta grandes consequências. Em casos mais complicados, o resultado da estimação pode ser bastante sensível à escolha de valor inicial porque o otimizador pode cair em máximos/mínimos locais. No final do post discuto brevemente algumas soluções. Neste exemplo escolho arbitrariamente começar no valor $1$.

Neste caso também poderíamos usar o fato de que a esperança da Poisson é igual a $\lambda$, logo, pela Lei dos Grandes Números, a média amostral já deve estar próxima do verdadeiro valor de $\lambda$. Assim, poderíamos também ter escolhido `mean(amostra)` como valor inicial.

```{r}
fit <- optim(
  par = 1,
  fn = ll_pois,
  x = amostra,
	method = "BFGS",
  hessian = TRUE
  )

fit
```

A saída da função traz várias informações técnicas sobre a otimização numérica. A estimativa pontual computada foi de $4.938$ - bastante próxima do verdadeiro valor do parâmetro. Usando a função `str` podemos observar a estrutura do objeto `fit` criado acima. As principais informações que podemos extrair são as estimativas dos parâmetros `fit$par` e a hessiana calculada nestes parâmetros `fit$hessian`.

```{r}
str(fit)
```

Usando a estimativa da hessiana podemos computar o erro-padrão da estimativa. Lembre que a variância assintótica do estimador de máxima verossimilhança é o inverso da matriz de informação de Fisher que pode ser expressa como o negativo do valor esperado da hessiana. Isto é, podemos encontrar a variância assintótica calculando o inverso da hessiana (como fizemos a minimização do negativo da função log-verossimilhança não precisamos calcular o negativo da hessiana).

```{r}
(ep <- sqrt(1/fit$hess))
```

Com o erro-padrão podemos calcular intervalos de confiança e estatística t.

```{r}
(ic <- c(fit$par - 1.96 * ep, fit$par + 1.96 * ep))
(est_t <- (fit$par - 5) / ep)
```

## Usando o pacote `optimx`

A função `optim` já é bastante antiga e um novo pacote, chamado `optimx`, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são `optimx` e `optimr`. Mais informações sobre o pacote podem ser encontradas [aqui](https://rdrr.io/cran/optimx/).

A sintaxe das funções é muito similar à sintaxe original do `optim`. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead

```{r}
summary(fit <- optimx(par = 1, fn = ll_pois, x = amostra))
```

Uma das principais vantagens do `optimx` é a possibilidade de usar vários métodos de otimização numérica numa mesma função.

```{r}
fit <- optimx(
  par = 1,
  fn = ll_pois,
  x = amostra,
  method = c("nlm", "BFGS", "Rcgmin", "nlminb")
  )

fit
```

Como este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos.

## Usando o pacote maxLik

A função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, *não é preciso montar a função com sinal de menos* como fizemos acima; segundo, ela já *calcula erros-padrão e estatísticas-t dos coeficientes estimados*. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas [aqui](https://www.researchgate.net/publication/227354660_MaxLik_A_package_for_maximum_likelihood_estimation_in_R).

Para usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o `dpois` que implementa a função densidade da Poisson. O argumento `log = TRUE` retorna as probabilidades $p$ como $log(p)$.

```{r}
ll_pois <- function(x, theta) {
	ll <- dpois(x, theta, log = TRUE)
	return(sum(ll))
}
```

O comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.

```{r}
summary(fit <- maxLik(ll_pois, start = 1, x = amostra))
```

Podemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função `optimx`. Derivando a função de log-verossimilhança:

$$
\begin{align}
  \frac{\partial \mathcal{L}(\lambda ; x)}{\partial\lambda} & = \frac{1}{\lambda}\sum_{k = 1}^{n}x_{k} - n \\
  \frac{\partial^2 \mathcal{L}(\lambda ; x)}{\partial\lambda^2} & = -\frac{1}{\lambda^2}\sum_{k = 1}^{n}x_{k}
\end{align}
$$

O código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.

```{r}
grad_pois <- function(x, theta) {
  (1 / theta) * sum(x) - length(x)
  }

hess_pois <- function(x, theta) {
	-(1 / theta^2) * sum(x)
}

fit2 <- maxLik(
  ll_pois,
  grad = grad_pois,
  hess = hess_pois,
  start = 1,
  x = amostra
  )

summary(fit2)
```

# Exemplo com a distribuição normal

A distribuição normal tem dois parâmetros: $\mu$ e $\sigma$. Lembrando que o primeiro indica a média e o segundo a desvio-padrão. A função de densidade de probabilidade é dada por:

$$
  f(x, \theta) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\text{exp}\left(\frac{-(x - \mu)^2}{2\sigma^{2}}\right)
$$

onde $\theta = (\mu, \sigma)$. O gráfico abaixo mostra como o formato da função varia conforme o valor destes parâmetros. Basicamente, quando a média aumenta, estamos "deslocando para a direita" e quando aumentamos o desvio-padrão estamos "achatando" a distribuição.

```{r, echo = FALSE}
x <- seq(-4, 4, .01)
plot(
  x, dnorm(x, 0, 1), type = 'l', col = cores[1],
  ylim = c(0, 0.6),
  lwd = 2,
	xlab = "x",
  ylab = "P(X = x)",
	main = "Função densidade de probabilidade da Normal"
  )
lines(x, dnorm(x, 0, 2), type = 'l', col = cores[2], lwd = 2)
lines(x, dnorm(x, -1.5, 1), type = 'l', col = cores[3], lwd = 2)
lines(x, dnorm(x, 1, 0.75), type = 'l', col = cores[4], lwd = 2)
grid()

legenda <- c(
  as.expression(bquote(mu == 0 ~","~ sigma == 1)),
	as.expression(bquote(mu == 2 ~","~ sigma == 2)),
	as.expression(bquote(mu == -1.5 ~","~ sigma == 1)),
	as.expression(bquote(mu == 1 ~","~ sigma == 0.75))
  )

legend("topleft", legend = legenda, col = cores, lty = 1)
```

Lembrando que a função de distribuição de probabilidade da normal para uma observação $i$

$$
  f(x_{i}, \theta) = \frac{1}{\sqrt{2\pi\sigma^{2}}}e^{\frac{-(x_{i} - \mu)^2}{2\sigma^{2}}}
$$

Fazendo o produtório da expressão acima para cada $i$

$$
  \prod_{i = 1}^{N}f(x_{i}, \theta) = (2\pi\sigma^{2})^{-\frac{N}{2}}\text{exp}\left( -\frac{1}{2\sigma^{2}}\prod_{i = 1}^{N}(x_{i} - \mu)^2\right)
$$

e agora passando log, temos:

$$
  L(x_{i}, \theta) = -\frac{N}{2}\text{ln}(2\pi) -N\text{ln}(\sigma) -\frac{1}{2\sigma^{2}}\sum_{i = 1}^{N}(x_{i} - \mu)^{2}
$$

Montamos a função log-verossimilhança usando a função `dnorm`

```{r}
ll_norm <- function(theta) {
	ll <- dnorm(x, theta[1], theta[2], log = TRUE)
	-sum(ll)
}
```

Vamos simular uma amostra aleatória $x_{1}, x_{2}, \dots, x_{1000}$ onde cada $x_{i}$ segue uma distribuição normal com média $2$ e desvio-padrão $3$ (i.e., variância $9$).

Primeiro vamos usar a função `optimx`

```{r}
# warnings por causa dos valores negativos no log
x <- rnorm(n = 1000, mean = 2, sd = 3)
(fit <- optimx(par = c(1, 1), fn = ll_norm, method = "BFGS"))
```

Note que a função retorna mensagens de erro indicando que a função retornou *NaNs*. Isto acontece porque o otimizador experimenta valores não-positivos para $\sigma$ e isto não é admissível pois $\text{ln}(\sigma)$, que aparece no segundo termo da equação acima, não é definido para $\sigma < 0$. Além disso, $\sigma$ não pode ser igual a zero pois ele aparece no denominador do último termo à direita da expressão da log-verossimilhança.

Intuitivamente isto é bastante óbvio: $\sigma$ representa o desvio-padrão da distribuição e $\sigma^{2}$ a sua variância: não podemos ter valores negativos ou nulos para estas expressões.

Podemos restringir os valores dos parâmetros usando os argumentos `upper` e `lower` para evitar as mensagens de `warning`, mas, na prática, isto não costuma fazer diferença no resultado final da estimação. Note que podemos deixar a restrição livre usando `Inf` e `-Inf` que correspondem a $\infty$ e $-\infty$.

```{r}
fit <- optimx(
  par = c(1, 1),
  fn = ll_norm,
  method = "L-BFGS-B",
  upper = c(Inf, Inf),
  lower = c(-Inf, 0)
  )

fit
```

# Propriedades dos estimadores de MV

A teoria dos estimadores de máxima verossimilhança nos garante que eles são *consistentes* (i.e. que eles aproximam o valor verdadeiro dos parâmetros) e *normalmente assintóticos* (a distribuição assintótica segue uma distribuição normal) desde que algumas condições de regularidade sejam atentdidas.

Podemos demonstrar ambas as propriedades fazendo algumas simulações no `R`.

## Consistência

Vamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição $N(2, 3)$; computamos as estimativas para $\mu$ e $\sigma$ e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.

1.  Simular uma amostra segundo uma distribuição.
2.  Estimata os parâmetros da distribuição.
3.  Calcula a variância assintótica dos estimadores.
4.  Repete 5000 vezes os passos 1-3.

O código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.

```{r}
r <- 5000
n <- 1000

estimativas <- matrix(ncol = 4, nrow = r)

for(i in 1:r) {
	x <- rnorm(n = n, mean = 2, sd = 3)
	
	fit <- optimr(
	  par = c(1, 1),
	  fn = ll_norm,
	  method = "BFGS",
	  hessian = TRUE
	  )
	# Guarda o valor estimado do parâmetro
	estimativas[i, 1:2] <- fit$par
	estimativas[i, 3:4] <- diag(n * solve(fit$hess))
}
```

A *consistência* dos estimadores $\hat{\theta}_{MV}$ significa que eles aproximam os valores verdadeiros do parâmetros $\theta_{0}$ à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande $\mathbb{N} \to \infty$ então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros $\hat{\theta}_{\text{MV}} \to \theta_{0}$

O código abaixo calcula a média das estimativas para cada parâmetro - lembrando que $\mu_{0} = 2$ e que $\sigma_{0} = 3$. Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).

```{r}
# | fig-width: 10
par(mfrow = c(1, 2))
# Consistência dos estimadores de MV
mu <- estimativas[, 1]; sigma <- estimativas[, 2]
mean(mu)
mean(sigma)

hist(mu, main = bquote("Estimativas para "~~mu), freq = FALSE, xlim = c(1.5, 2.5))
abline(v = 2, col = "indianred")
hist(sigma, main = bquote("Estimativas para "~~sigma), freq = FALSE, xlim = c(2.7, 3.3))
abline(v = 3, col = "indianred")
```

## Normalmente assintótico

Dizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:

$$
z_{\theta} = \sqrt{N}\frac{\hat{\theta}_{MV} - \theta}{\sqrt{\text{V}_{ast}}} \to \mathbb{N}(0, 1)
$$

onde $\text{V}_{ast}$ é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.

No loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.

```{r}
# Normalidade assintótica

# Define objetos para facilitar a compreensão
mu_hat <- estimativas[, 1]
sigma_hat <- estimativas[, 2]
var_mu_hat <- estimativas[, 3]
var_sg_hat <- estimativas[, 4]

# Centra a estimativa
mu_centrado <- mu_hat - 2 
sigma_centrado <- sigma_hat - 3
# Computa z_mu z_sigma
mu_normalizado <- sqrt(n) * mu_centrado / sqrt(var_mu_hat)
sigma_normalizado <- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)
```

```{r}
# Monta o gráfico para mu

# Eixo x
grid_x <- seq(-3, 3, 0.01)

hist(
  mu_normalizado,
  main = bquote("Histograma de 5000 simulações para z"[mu]),
  freq = FALSE,
  xlim = c(-3, 3),
  breaks = "fd",
  xlab = bquote("z"[mu]),
  ylab = "Densidade"
  )
lines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = "dodgerblue")
legend("topright", lty = 1, col = "dodgerblue", legend = "Normal (0, 1)")
```

```{r}
# Monta o gráfico para sigma2
hist(
  sigma_normalizado,
  main = bquote("Histograma de 5000 simulações para z"[sigma]),
  freq = FALSE,
  xlim =c(-3, 3),
  breaks = "fd",
  xlab = bquote("z"[sigma]),
  ylab = "Densidade"
  )
lines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = "dodgerblue")
legend("topright", lty = 1, col = "dodgerblue", legend = "Normal (0, 1)")
```

## Escolha de valores inicias

Como comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.

O exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.

```{r}
# sensível a escolha de valores inicias
x <- rnorm(n = 1000, mean = 15, sd = 4)
fit <- optim(
  par = c(1, 1),
  fn = ll_norm,
  method = "BFGS",
  hessian = TRUE
  )

fit
```

Note que as estimativas estão muito distantes dos valores corretos $\mu = 15$ e $\sigma = 4$. Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.

O código abaixo usa os momentos empíricos como valores inicias para $\mu$ e $\sigma$:

$$
\begin{align}
  \mu_{inicial} & = \frac{1}{n}\sum_{i = 1}^{n}x_{i} \\
  \sigma_{inicial} & = \sqrt{\frac{1}{n} \sum_{i = 1}^{n} (x_{i} - \mu_{inicial})^2}
\end{align}
$$

```{r}
(chute_inicial <- c(mean(x), sqrt(var(x))))
(est <- optimx(par = chute_inicial, fn = ll_norm))
```

Agora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função `optimx` se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.

Note como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método `bobyqa` retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos `nlminb` e `Nelder-Mead` ambos retornam os valores corretos.

```{r}
# Usando outros métodos numéricos
optimx(
  par = c(1, 1),
  fn = ll_norm,
  method = c("BFGS", "Nelder-Mead", "CG", "nlminb", "bobyqa")
  )
```

Vale notar também alguns detalhes técnicos da saída. Em particular, `convcode == 0` significa que o otimizador conseguiu convergir com sucesso, enquanto `convcode == 1` indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.

Já o `kkt1` e `kkt2` verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte *necessária* do teorema enquanto a segunda condição verifica a parte *suficiente*. Note que o `bobyqa` falha em ambas as condições (pois ele não é feito para este tipo de problema).

Os métodos que retornam os melhores valores, o `Nelder-Mead` e `nlminb` são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.

Mais detalhes sobre os métodos podem ser encontrados na página de ajuda da função `?optimx`.
