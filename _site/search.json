[
  {
    "objectID": "posts/tutorial-ggplot2/3-grafico-histograma.html",
    "href": "posts/tutorial-ggplot2/3-grafico-histograma.html",
    "title": "Fundamentos: histograma",
    "section": "",
    "text": "Um histograma serve para visualizar a distribuição de um conjunto de dados. Ele consiste em colunas que representam a frequência de ocorrência de determinados valores nos dados: quanto mais alta for a coluna, mais frequente é uma observação. Isto permite ver a forma da distribuição dos dados e identificar padrões e tendências.\n\n\n\n\n\n\n\n\n\nHistogramas aparecem naturalmente na hora de visualizar, por exemplo:\n\nDistribuição de notas de alunos em testes padronizados.\nDistribuição da renda familiar na população de um país.\nDistribuição de preços de imóveis numa cidade.\nDistribuição da altura das pessoas.\nDistribuição de variáveis aleatórias em estatística.\n\nNeste post vamos entender como montar histogramas no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para visualizar a taxa de poupança nos EUA aos longo dos anos. Depois vamos trabalhar um exemplo mais complexo, analisando a distribuição do preço dos imóveis no Texas, EUA."
  },
  {
    "objectID": "posts/tutorial-ggplot2/3-grafico-histograma.html#ggplot2",
    "href": "posts/tutorial-ggplot2/3-grafico-histograma.html#ggplot2",
    "title": "Fundamentos: histograma",
    "section": "ggplot2",
    "text": "ggplot2\nPara criar um histograma com o pacote ggplot2 no R, usamos a função geom_histogram().\nA estrutura de um gráfico do ggplot2 parte de três elementos básicos: (1) a base de dados, isto é, um objeto data.frame; (2) um mapeamento de variáveis, feito com auxílio da função aes(); e (3) a escolha da forma do gráfico, feito com as funções geom.\nO ggplot2 funciona adicionando camadas e elementos subsequentemente sobre um gráfico inicial. Cada elemento novo que adicionamos ao gráfico é somado usando o operador +.\nPara resumir o processo: começamos com a função ggplot() e vamos adicionando geoms, funções auxiliares que especificam a forma do gráfico. Este processo construtivo de adicionar elementos a um gráfico é o principal diferencial do ggplot.\nOu seja, temos três elementos essenciais:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEsta estrutura básica é esquematizada no pseudo-código abaixo.\n\nggplot(data = base_de_dados, aes(x = variavel_x)) +\n  geom_histogram()\n\nVamos montar um exemplo usando a base economics, que vem carregada junto com o pacote ggplot2. Esta base compila uma série de informações econômicas e demográficas no período julho/1967 a abril/2014 nos EUA. Para explorar os dados podemos usar a função head() que exibe as primieras linhas da tabela.\n\nhead(economics)\n\n\n\n\n\n\ndate\npce\npop\npsavert\nuempmed\nunemploy\n\n\n\n\n1967-07-01\n507\n198712\n13\n4\n2944\n\n\n1967-08-01\n510\n198911\n13\n5\n2945\n\n\n1967-09-01\n516\n199113\n12\n5\n2958\n\n\n1967-10-01\n512\n199311\n13\n5\n3143\n\n\n1967-11-01\n517\n199498\n13\n5\n3066\n\n\n1967-12-01\n525\n199657\n12\n5\n3018\n\n\n\n\n\nInicialmente, vamos nos focar na coluna psavert, que é a taxa de poupança individual, isto é, o percentual da renda que as famílias poupam. O código abaixo monta um histograma desta variável.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram()\n\nVamos decompor o código acima em partes. Primeiro temos que informar onde estão os nossos dados. Fazemos isto dentro da função ggplot() usando o argumento data = economics.\nDepois, precisamos indicar qual a variável (coluna) que queremos visualizar, isto é, indicar qual é a variável que deve ser mapeada em um elemento visual. Fazemos isto usando a função aes(x = psavert).\nPor fim, como queremos desenhar um gráfico de histograma escolhemos o geom_histogram(). Esta última função é adicionada (somada) à função inicial com o sinal de soma +.\nSegue abaixo o código comentado junto com o gráfico produzido. Vemos que, historicamente, a taxa de poupança gira entre 5% e 15% da renda pessoal.\n\n# Chamada inical da função ggplot\nggplot(\n  # Define a base de dados\n  data = economics,\n  # Escolhe qual a variável deve ser visualizda\n  aes(x = psavert)\n) +\n  # Escolhe o tipo de gráfico (histograma)\n  geom_histogram()"
  },
  {
    "objectID": "posts/tutorial-ggplot2/3-grafico-histograma.html#elementos-estéticos",
    "href": "posts/tutorial-ggplot2/3-grafico-histograma.html#elementos-estéticos",
    "title": "Fundamentos: histograma",
    "section": "Elementos estéticos",
    "text": "Elementos estéticos\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nUm gráfico de histograma tem cinco elementos estéticos principais:\n\ncolor - Define a cor do contorno da coluna.\nfill - Define a cor que preenche a coluna.\nalpha - Define o nível de transparência das cores.\nbinwidth - Define a largura da coluna.\nbins - Define o número de colunas.\n\nOs dois últimos elementos são parâmetros estatísticos que são interpretados como estéticos neste contexto. Vamos explorar cada um destes elementos em exemplos abaixo.\nVale notar que o argumento x também é um elemento estético. Mais especificamente ele é um elemento estético variável, logo é mapeado com a função aes(), e é obrigatório (pois é exigido pela função geom_histogram())\n\nCores\nTemos duas opções principais de cores: color é a cor da linha do contorno da coluna e fill é a cor que preenche o interior da coluna. O código abaixo ilustra como utilizar estes argumentos dentro da função geom_histogram(). Note que ambos os elementos estéticos são constantes.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém podemos fazer referência a cores via código hexadecimal. No exemplo abaixo uso as cores \"#e76f51 (laranja-escuro) e \"#264653\" (azul-escuro).\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(color = \"#E76F51\", fill = \"#264653\")\n\n\n\n\n\n\n\n\n\n\nTransparência\nO parâmetro alpha controla o nível de transparência das cores. O valor dele deve estar sempre entre 0 e 1. Quanto mais próximo de 0, mais transparente será o gráfico final. Os gráficos abaixo mostram o efeito de alguns valores distintos de alpha.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.9)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.7)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.3)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\nColunas\nPodemos controlar o número de colunas do histograma de duas formas: (1) escolhendo o número via bins; (2) escolhendo o tamanho dos intervalos/colunas via binwidth.\nA escolha padrão da função geom_histogram() é definir bins = 30. Isto raramente resulta num gráfico ideal. O número ótimo de intervalos depende do tipo de dado que estamos visualizando.\nEm geral, um número muito pequeno resulta num gráfico agrupado demais, enquanto um número muito grande resulta num gráfico disperso demais. Em ambos os casos fica difícil enxergar o padrão nos dados.\nO código abaixo reduz o número de intervalos para 5. Note como as observações estão mais agrupadas.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 5)\n\n\n\n\n\n\n\n\nJá o código seguinte aumenta o número de intervalos para 70. Agora conseguimos identificar mais facilmente os outliers, mas as observações estão dispersas demais para conseguir enxergar algum tipo de padrão.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nPor fim, o gráfico abaixo tenta chegar num meio termo. Vemos que a taxa de poupança tem uma distribuição parecida com uma normal e possui alguns outliers tanto à esquerda como à direita.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nComo mencionado acima, podemos definir o tamanho dos intervalos usando binwidth. Como nossa variável está expressa em formato de percentual, podemos experimentar intervalos de tamanho unitário. O resultado, neste caso, é bastante satisfatório.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "posts/tutorial-ggplot2/3-grafico-histograma.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/tutorial-ggplot2/3-grafico-histograma.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: histograma",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  labs(\n    # Título\n    title = \"Taxa de poupança pessoal nos EUA\",\n    # Subtítulo\n    subtitle = \"Distribuição da taxa de poupança, como proporção da renda disponível, no período 1967-2014.\",\n    # Nome do eixo-x\n    x = \"Taxa de poupança (%)\",\n    # Nome do eixo-y\n    y = \"Frequência\",\n    # Nota de rodapé\n    caption = \"Fonte: FREDR\"\n  )"
  },
  {
    "objectID": "posts/tutorial-ggplot2/3-grafico-histograma.html#usando-cores-para-representar-variáveis",
    "href": "posts/tutorial-ggplot2/3-grafico-histograma.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: histograma",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes(). Como expliquei acima, esta função “transforma” nossos dados em elementos visuais. Nos casos acima, ela mapeia a variável x nas colunas do histograma.\nMas também podemos mapear uma coluna para um elemento estético como o fill, por exemplo. O resultado é um gráfico em que a cor de cada coluna vai corresponder a uma variável da nossa base de dados.\nAgora, vamos utilizar a base de dados txhousing que compila informações do mercado imobiliário das principais cidades do estado do Texas, nos EUA. Como a base inclui mais de 40 cidades vamos restringi-la para apenas quatro cidades: Austin, Dallas, Houston e San Angelo. Vamos visualizar a distribuição da variável median que registra o valor mediano de venda mensal dos imóveis em cada cidade. A variável city indica o nome da cidade.\n\nhead(txhousing)\n\n\n\n\n\n\ncity\nyear\nmonth\nsales\nvolume\nmedian\nlistings\ninventory\ndate\n\n\n\n\nAbilene\n2000\n1\n72\n5380000\n71400\n701\n6\n2000\n\n\nAbilene\n2000\n2\n98\n6505000\n58700\n746\n7\n2000\n\n\nAbilene\n2000\n3\n130\n9285000\n58100\n784\n7\n2000\n\n\nAbilene\n2000\n4\n98\n9730000\n68600\n785\n7\n2000\n\n\nAbilene\n2000\n5\n141\n10590000\n67300\n794\n7\n2000\n\n\nAbilene\n2000\n6\n156\n13910000\n66900\n780\n7\n2000\n\n\n\n\n\nQueremos um gráfico em que cada cidade tenha uma cor diferente, então, a variável city deve aparecer dentro da função aes(). O código abaixo primeiro organiza os dados e depois monta o gráfico. Agora, cada cidade tem uma cor diferente e as colunas são “empilhadas” umas sobre as outras.\n\n# Cria um vetor com as cidades selecionadas\ncities &lt;- c(\"Austin\", \"Dallas\", \"Houston\", \"San Angelo\")\n# Seleciona apenas as linhas que contêm informações sobre estas cidades\nsubtxhousing &lt;- subset(txhousing, city %in% cities)\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(aes(fill = city))\n\n\n\n\n\n\n\n\nNo gráfico acima, conseguimos ver, por exemplo, que o valor mais frequente de venda está em torno de 150 mil. Além disso, pode-se ver como os valores de venda em San Angelo costumam ser menores do que os valores de venda em Austin.\nPara ter maior controle sobre as cores e sobre a legenda usamos a função scale_fill_manual() e a função theme().\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(\n    # Mapeaia a variável city nas cores das colunas\n    aes(fill = city),\n    # Define o número de colunas\n    bins = 25,\n    # Define a cor (contorno) das colunas\n    color = \"white\"\n  ) +\n  # Controla as cores e a legenda\n  scale_fill_manual(\n    # Título da legenda\n    name = \"Cidade\",\n    # Cores das colunas\n    values = c(\"#264653\", \"#2a9d8f\", \"#f4a261\", \"#e76f51\")\n  ) +\n  # Posiciona a legenda acima do gráfico\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/tutorial-ggplot2/3-grafico-histograma.html#resumo",
    "href": "posts/tutorial-ggplot2/3-grafico-histograma.html#resumo",
    "title": "Fundamentos: histograma",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar alguns histogramas interessantes em poucas linhas de código. Em qualquer gráfico temos três elementos básicos:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos importantes:\n\nElementos estéticos podem ser constantes (números ou texto) ou variáveis (colunas da base de dados). Elementos variáveis precisam estar dentro da função aes().\nA escolha do número de colunas/intervalos depende do dado que queremos visualizar. Em geral, é preciso experimentar com números diferentes.\nSe o elemento fill for variável é preciso usar a função scale_fill_manual() para controlar as cores e a legenda de cores.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Tutoriais\n\nggplot2: Do básico ao avançado\nVisualização de dados é a base fundamental de uma boa análise e pode ser o fator que separara um trabalho amador de uma entrega profissional. Esta série de posts ensina os fundamentos do ggplot2 desde o início.\n\n\nData Science: tidyverse\nO tidyverse é uma coleção de pacotes R projetados para ciência de dados que compartilham uma filosofia comum de design, gramática e estruturas de dados. Esta série de tutoriais aborda algumas técnicas mais avançadas de manipulação básica de dados com {dplyr}.\n\n\n\nPosts\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMelhorando gráficos ruins\n\n\n\ndata-visualization\n\nggplot2\n\ndata-science\n\ntutorial-R\n\nbrasil\n\nLAPOP\n\n\n\nFazer o gráfico certo não é tarefa fácil e é um processo de aprendizado contínuo. Neste post, reviso algumas visualizações antigas e mostro como melhorá-las, usando…\n\n\n\nJul 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nO Novo Perfil Demográfico do Brasil\n\n\n\ndata-visualization\n\nggplot2\n\ncenso\n\nbrasil\n\ndemografia\n\n\n\nO Brasil está num momento singular na sua transição demográfica. Ao mesmo tempo em que a população está envelhecendo e menos pessoas estão nascendo, temos a maior geração de…\n\n\n\nJul 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nO crescimento das Regiões Metropolitanas Brasileiras\n\n\n\ncenso\n\nbrasil\n\ncidades\n\ndemografia\n\ndata-visualization\n\n\n\nOs resultados do Censo 2022 trouxeram à tona um fenômeno que tem gerado intensos debates sobre o futuro demográfico do Brasil: pela primeira vez em décadas, algumas das…\n\n\n\nJun 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n30DayChartChallenge: personal highlights\n\n\n\ndata-visualization\n\nggplot2\n\n\n\n\n\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Distribuição Racial do Brasil\n\n\n\nbrasil\n\nggplot2\n\ndata-visualization\n\ncenso\n\n\n\nUm mapa coroplético que mostra o grupo racial predominante em cada região do Brasil. A distribuição espacial dos dados revela um interessante padrão norte-sul, onde quase…\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nMelhores Posts de 2024\n\n\n\ntutorial-R\n\ndata-science\n\nmelhores-posts\n\n\n\nDestaco alguns dos melhores posts de 2024. Este post serve como guia do conteúdo feito no ano, destaca os melhores posts e também os mais compartilhados nas redes sociais.\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemographic Pyramids in R\n\n\n\nggplot2\n\ndata-visualization\n\nenglish\n\ntutorial-R\n\ndemographics\n\n\n\nDemographic pyramids are widely used in demographic analysis to visualize the structure of a population, providing valuable insights into trends like population growth…\n\n\n\nDec 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilm ratings over the decades: replicating a Nexo plot\n\n\n\ndata-visualization\n\nggplot2\n\nreplication\n\n\n\nIn this tutorial, we delve into the world of cinema ratings over the decades, examining how films from various eras are rated on IMDb. We’ll replicate a plot originally…\n\n\n\nDec 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPunchcard plots in R\n\n\n\ndata-visualization\n\nggplot2\n\ntutorial-R\n\n\n\nPunchcard plots are an alternative way to visualize data distributed across two categorical dimensions, such as days of the week and hours of the day. In this tutorial…\n\n\n\nDec 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nReplicating the Github Contributions plot in R using ggplot2\n\n\n\ndata-science\n\ntutorial-R\n\ndata-visualization\n\n\n\nIn this post I’ll show how to replicate the infamous GitHub contributions graphic using ggplot2\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\n\n\n\nLinha-4 Amarela Metrô de São Paulo\n\n\n\ndata-visualization\n\nggplot2\n\nbrazil\n\nmaps\n\n\n\nNeste post, analiso o fluxo de passageiros na Linha 4-Amarela do metrô de São Paulo. A demanda permanece abaixo dos níveis pré-pandemia, mesmo após a abertura da estação…\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding All Starbucks in Brazil\n\n\n\nstarbucks\n\nweb-scrapping\n\ndata-science\n\nbrazil\n\ntutorial-R\n\nfinding\n\nenglish\n\n\n\nIn this post, I show how to find all Starbucks in Brazil using web scraping in R. I also show how to use the Google Places API to add ratings information for all Starbucks…\n\n\n\nJul 14, 2024\n\n\n\n\n\n\n\n\n\n\n\nLine-4 Metro\n\n\n\ndata-science\n\nfinding\n\nsao-paulo\n\nsubway\n\nweb-scraping\n\ntutorial-R\n\n\n\nIn this post I show how to webscrape all publicly available information on passenger flow from the Line-4 Metro in São Paulo. This post is part of a larger series where I…\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nLocating all The Coffee shops in Brazil\n\n\n\ndata-science\n\nweb-scrapping\n\nfinding-all\n\ntutorial-r\n\nbrasil\n\n\n\nIn this post, I show how to map every The Coffee shop in Brazil using web scraping and geocoding. I also show how to combine this information with Goolge Maps ratings and…\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\n\n\n\n\nCensus Tracts in Brazil\n\n\n\ncensus\n\nbrazil\n\nsao-paulo\n\ndata-science\n\ndata-visualization\n\nmaps\n\nleaflet\n\nggplot2\n\n\n\nCensus tracts are the smallest administrative divisions that provide socio-economic and demographic data. They are highly useful for statistical and spatial analysis…\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\nAnalfabetismo no Brasil\n\n\n\ndata-visualization\n\nggplot2\n\nbrazil\n\nmaps\n\n\n\nOs dados mais recentes do IBGE revelam um padrão geográfico marcante na taxa de analfabetismo no Brasil. Os municípios do nordeste do país apresentam taxas de analfabetismo…\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\n\n\n\n\nGenerations in Brazil\n\n\n\nbrazil\n\ndemographics\n\ndata-visualization\n\nggplot2\n\nenglish\n\n\n\nBrazil has an enormous ‘young’ population that will both boost its labor force in the years to come. Failures in the education system and unequal access to opportunities…\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\n\n\n\n\nGDP in Brazil\n\n\n\ndata-visualization\n\nbrazil\n\nmaps\n\nggplot2\n\nenglish\n\n\n\nA choropleth map showing the spatial distribution of Brazilian GDP by city. Colors represent the economic sector with the highest share of contribution to total city GDP.…\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\n\n\n\n\nAdministrative and Statistical Divisions in Brazil\n\n\n\nbrazil\n\nmaps\n\ndata-science\n\ndata-visualization\n\nggplot2\n\nenglish\n\n\n\nIn this post I present the main administrative and statistical subdivisions of the Brazilian territory. All information comes from IBGE, the official statistical bureau. I…\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\n\n\n\n\nThe Greatest Films of All-time: a data approach\n\n\n\ndata-visualization\n\nggplot2\n\nmovies\n\nenglish\n\n\n\nIn this post I delve into the data to try to visualize patterns in movies rankings. Are movie critics biased towards a certain period of cinema? Are new movies overrated?…\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\n\n\n\n\nRadar Plots\n\n\n\ndata-visualization\n\nggplot2\n\nreal-estate\n\nenglish\n\n\n\nRadar plots, or spider plots, are commonly used in dashboards and general data visualizations. Radar plots show a static visualization of the numeric values of several…\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nÍndice de Envelhecimento no Brasil\n\n\n\ndata-visualization\n\nmapas\n\nggplot2\n\nbrasil\n\ndemografia\n\n\n\nUm mapa coroplético que apresenta o Índice de Envelhecimento por município do Brasil segundo os dados do mais recente Censo Demográfico do IBGE.\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\nNascimentos e Óbitos no Brasil\n\n\n\ndata-visualization\n\nmapas\n\nggplot2\n\nbrasil\n\ndemografia\n\n\n\nNeste post analiso os dados do mais recente Estatísticas do Registro Civil do IBGE. Esta base de dados estima o número total de nascidos vivos e de óbitos em cada município…\n\n\n\nApr 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nOs Pecados da Visualização de Dados\n\n\n\ndata-visualization\n\nggplot2\n\ntutorial-R\n\n\n\nNeste post apresento algumas recomendações práticas para melhorar uma visualização. O gráfico original é de um relatório do PewResearch Center sobre a religiosidade nos…\n\n\n\nApr 9, 2024\n\n\n\n\n\n\n\n\n\n\n\nImportando dados em PDF no R\n\n\n\ndata-science\n\nweb-scraping\n\ntutorial-R\n\n\n\nNeste post vou mostrar uma solução para importar dados em formato PDF de maneira fácil e prática usando R. Arquivos PDF não são um típico formato de armazenamento de…\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nRazão de Dependência no Brasil\n\n\nUm mapa coroplético interativo que mostra as medida de Razão de Dependência no Brasil por estado. O mapa destaca tanto a Razão de Dependência Total, como a RD Jovem e RD…\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nDomicilios em Sao Paulo\n\n\n\ndata-visualization\n\nmapas\n\nggplot2\n\nsao-paulo\n\n\n\nUm mapa coroplético em formato de grid retangular 100 x 100m mostrando a densidade de domicílios particulares em São Paulo. Os dados são do Censo mais recente, de 2022.\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapas Interativos com Leaflet e R\n\n\n\nmapas\n\ndata-visualization\n\ntutorial-R\n\ndata-science\n\nleaflet\n\ndemografia\n\n\n\nNeste post mostro como fazer mapas interativos usando o pacote leaflet no R. O pacote é bastante flexível na construção de mapas, permitindo muitas opções de customização.…\n\n\n\nMar 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncontrando todos os Starbucks do Brasil\n\n\n\nstarbucks\n\nweb-scrapping\n\ndata-science\n\nbrasil\n\ntutorial-R\n\n\n\nNeste post mostro como encontrar todos os Starbucks do Brasil usando webscrapping dentro do R. O web scraping é uma técnica de extração de dados bastante popular que nos…\n\n\n\nMar 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnriquecendo e coletando do Google Maps\n\n\n\ndata-science\n\ngoogle-maps\n\ntutorial-R\n\nmapas\n\nleaflet\n\nstarbucks\n\n\n\nNeste post mostro como usar o Google Places API para importar informação do Google Maps dentro do R. O foco deste post será no Places, que encontra informações sobre…\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\nCasas e Apartamentos\n\n\n\ndata-visualization\n\nggplot2\n\nreal-estate\n\ndata-science\n\ncenso\n\nbrasil\n\nhousing\n\nmoradia\n\n\n\nNeste post exploro os dados recentes do Censo sobre tipos de domicílios nas cidades brasileiras.\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\nAcidentes de Trânsito em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\n\n\nNeste post exploro o padrão espacial dos acidentes de trânsito em São Paulo. O detalhamento dos dados nos permite enxergar padrões sazonais intradiários e intrasemanais. De…\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nHousing Affordability em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\nmoradia\n\nhousing\n\n\n\nUm mapa coroplético que apresenta a distribuição do Price-Income-Ratio (PIR) em São Paulo por regiões. O PIR mede a acessibilidade financeira aos imóveis.\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\nAno de 2024 começa mais difícil\n\n\n\ndata-visualization\n\nggplot2\n\nbrasil\n\neconomia\n\n\n\nSegundo os dados mais recentes do IBGE, o carry-over do PIB de 2023 para 2024 é de apenas 0,2%, menor valor da série histórica (excluindo anos de recessão).\n\n\n\nMar 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nRecessões no Brasil\n\n\n\ndata-visualization\n\nbrasil\n\neconomia\n\nggplot2\n\n\n\nNeste post, exploro as recessões econômicas mais recentes na histórica econômica do Brasil. Mostro como comparar as recessões de diferentes maneiras e construo uma tabela…\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nÍndices de Preços Imobiliários no Brasil\n\n\n\ndata-visualization\n\nbrasil\n\nggplot2\n\nreal-estate\n\n\n\nNeste post discuto a teoria sobre índices de preços imobiliários e apresento os principais índices disponíveis no Brasil. Pela discussão teórica deve ficar claro as…\n\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreços de Aluguel e de Venda de Imóveis\n\n\n\ndata-visualization\n\nggplot2\n\nweekly-viz\n\nbrasil\n\nreal-estate\n\n\n\nOs mercados de aluguel e de venda de imóveis são interligados e deve existir algum tipo de equilíbrio entre eles. Neste post exploro macrotendências destes mercados usando a…\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\nIDH por região em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a distribuição do IDH por regiões em São Paulo. Os dados são importados do projeto Atlas Brasil e estão a nível de UDH (Unidade de…\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nMédias móveis\n\n\n\ndata-science\n\neconomia\n\ntutorial-R\n\neconometria\n\ntime-series\n\n\n\nO filtro de médias móveis serve para suavizar séries de tempo e encontrar tendências nos dados. Este filtro é bastante simples e pode ser escalado com facilidade para lidar…\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\nEnergia Elétrica e Crescimento Econômico no Brasil\n\n\n\ndata-visualization\n\nggplot2\n\nweekly-viz\n\ntime-series\n\n\n\nA demanda por energia elétrica é geralmente entendida como uma proxy do nível de produção de um país. Na prática, ela funciona como uma proxy mensal do PIB. Neste…\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltro HP e Filtro de Hamilton\n\n\n\neconometria\n\ntime-series\n\ntutorial-R\n\neconomia\n\n\n\nCiclos variam de periodicidade e há várias abordagens para decompor uma série de tempo, separando a tendência do ruído. Neste post, discuto brevemente sobre os tipos de…\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\n\n\n\nAcessibilidade financeira à moradia em São Paulo\n\n\n\ndata-science\n\ndata-visualization\n\nsao-paulo\n\nreal-estate\n\n\n\nNeste post exploro em maiores detalhes indicadores de acessibilidade financeira à moradia. Apresento uma análise histórica a nível nacional e o retrato da desigualdade do…\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nDistribuição de Renda em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a distribuição de renda em São Paulo. Os dados são importados do projeto Acesso a Oportunidades do IPEA e apresentados no padrão H3 da Uber.\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent\n\n\n\ndata-science\n\ntutorial-R\n\neconometria\n\n\n\nO algoritmo de Gradient Descent é um otimizador amplamente utilizado em Machine Learning para minimizar funções de custo. Este otimizador simples é baseado na direção do…\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\n\n\n\n\nPreços de Imóveis no Brasil\n\n\n\ndata-visualization\n\nggplot2\n\nweekly-viz\n\nbrasil\n\nreal-estate\n\n\n\nExiste uma impressão generalizada de que o preço dos imóveis no Brasil cresceu muito nos últimos anos. Neste post mostro como, em termos reais, o preço médio dos imóveis no…\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nTendência e Sazonalidade\n\n\n\ndata-science\n\neconomia\n\ntutorial-R\n\neconometria\n\ntime-series\n\n\n\nSéries de tempo apresentam diversos padrões sazonais. Em econometria, o interesse nem sempre está na sazonalidade, em si, mas na sua eliminação para chegar na tendência de…\n\n\n\nJan 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nAcesso a Hospitais e Leitos em São Paulo\n\n\n\ndata-science\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\ntransporte\n\n\n\nNeste post, vou mapear a acessibilidade a hospitais e leitos em São Paulo. Para avaliar quantitativamente o nível de acessibilidade vou montar uma métrica bastante simples…\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\nCarros e Renda em Sao Paulo\n\n\n\ndata-science\n\nsao-paulo\n\ntransporte\n\n\n\nEste post analisa a relação entre a posse de automóveis e a renda domiciliar usando dados da Pesquisa Origem e Destino do Metrô de 2017. Um modelo de escolha discreta revela…\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nUruguay in numbers\n\n\n\ndata-visualization\n\nlatin-america\n\nggplot2\n\nenglish\n\n\n\nUruguay has earned acclaim as a model democracy, leading the Economist’s Democracy Index and holding the status of the least corrupt country in Latin America. With a GINI…\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBump Plots\n\n\n\nggplot2\n\ntutorial-R\n\n\n\nUm ‘bump chart’ mostra diferentes valores de uma variável em contextos distintos. É similar a um gráfico de tendências paralelas mas com linhas mais suaves. Neste post…\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Brazilian Inflation\n\n\n\ndata-visualization\n\nweekly-viz\n\nenglish\n\nggplot2\n\nbrasil\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\nReplicando gráficos\n\n\n\ndata-visualization\n\nggplot2\n\n\n\nNeste post mostro como replicar alguns gráficos de publicações usando apenas o ggplot2.\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Car Dependency in São Paulo\n\n\n\ndata-visualization\n\nsao-paulo\n\nggplot2\n\nweekly-viz\n\ntransporte\n\n\n\nSão Paulo boasts a predominantly clean energy matrix, with the city’s primary challenge lying in the transportation sector. A simple manner to capture car dependency is to…\n\n\n\nNov 24, 2023\n\n\n\n\n\n\n\n\n\n\n\nCenso 2022: O que houve de errado?\n\n\n\nbrasil\n\ndemografia\n\ncenso\n\n\n\nHouve certo rebuliço nas redes sociais, quando do lançamento dos dados mais recentes do Censo Demográfico de 2022. O fato carregado na maior parte das manchetes do Brasil…\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\nCarros em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\ntransporte\n\n\n\nUm mapa coroplético que apresenta o número de carros por domicílios em São Paulo. Enquanto o Centro Antigo tem algumas das taxas mais baixas, bairros centrais como Pacembu e…\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\nNascimentos no Brasil\n\n\n\ndata-science\n\ndata-visualization\n\nbrasil\n\ndemografia\n\ntime-series\n\n\n\nTende-se a pensar que a data de nascimento de um indivíduo é algo completamente aleatório. Neste post faço algumas visualizações para investigar o padrão de sazonalidade nos…\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n\n\n\n\n\nEnsino Superior em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta o percentual de adultos com ensino superior. Vê-se nos dados o conhecido padrão da cidade: indicadores de ensino superior altos dentro do…\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nExpectativa de Vida em São Paulo\n\n\n\ndata-visualization\n\nmapas\n\nsao-paulo\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a desigualdade na expectativa de vida nos distritos de São Paulo. Do Jardim Paulista até Anhanguera são 23 anos de diferença na idade média…\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Transportation in São Paulo\n\n\n\ndata-visualization\n\nweekly-viz\n\nenglish\n\nsao-paulo\n\ntransporte\n\n\n\nThis week, I delve into the transportation modes within the Greater São Paulo Region by analyzing data sourced from the Origin Destination Survey. The achievement of…\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\n\n\n\n\nEnvelhecimento no Brasil\n\n\n\ndata-visualization\n\nggplot2\n\ncenso\n\ndemografia\n\nbrasil\n\n\n\nO futuro demográfico do Brasil, em grande parte, já é conhecido. Assim como no resto do mundo, a combinação de queda de taxa de fecundidade e aumento de expectativa de vida…\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz - Brazilian Census\n\n\n\ndata-visualization\n\nweekly-viz\n\nenglish\n\nbrasil\n\nggplot2\n\n\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\n\n\n\n\nUm pacote com dados do mercado imobiliário\n\n\n\ntutorial-R\n\ndata-science\n\nreal-estate\n\n\n\nConsumir os dados do mercado imobiliário brasileiro não é tarefa fácil. Pensando em simplificar este processo eu criei um pacote do R, que importa e limpa diversas bases de…\n\n\n\nOct 26, 2023\n\n\n\n\n\n\n\n\n\n\n\nBrazil in Charts: Unemployment\n\n\n\ndata-visualization\n\nbrazil\n\nweekly-viz\n\nggplot2\n\nenglish\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\nImportando arquivos, visualizando linhas\n\n\n\ntutorial-R\n\ndata-science\n\n\n\nToda análise de dados passa por tarefas de rotina: importar dados, trocar nomes de colunas, remover observações vazias, etc. Por que não facilitar a sua vida e tornar essas…\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCidades Brasil\n\n\n\ntutorial-R\n\nmapas\n\nggplot2\n\n\n\nEste tutorial ensina como fazer um mapa da altitude de ruas de uma cidade usando programação funcional. O princípio da programação funcional é de decompor uma tarefa…\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Nascimentos no Brasil\n\n\n\nggplot2\n\ndata-visualization\n\nbrasil\n\nweekly-viz\n\n\n\nNesta semana resolvi olhar os nascimentos no Brasil. Há menos pessoas nascendo? Em quais meses nascem mais bebês?\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\nLife Satisfaction and GDP per capita\n\n\n\ndata-visualization\n\ntutorial-R\n\nggplot2\n\nenglish\n\n\n\nIn this post I make a step-by-step replication of a plot from OurWorldInData. The plot shows the correlation between average self-reported life satisfaction and GDP per…\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPipes\n\n\n\ndata-science\n\ntutorial-R\n\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Ruas de Porto Alegre\n\n\n\nmapas\n\nggplot2\n\nweekly-viz\n\ndata-visualization\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nMapa de altitude de ruas de Brasília\n\n\n\ndata-visualization\n\nmapas\n\n\n\n\n\n\n\nSep 3, 2023\n\n\n\n\n\n\n\n\n\n\n\nShiny Dashboard: IDH municípios\n\n\n\nshiny\n\ndata-visualization\n\nbrasil\n\n\n\nNeste post apresento o Dashboard do IDH de municípios que fiz com base nos dados da Firjan. O Dashboard foi feito com Shiny.\n\n\n\nAug 25, 2023\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Recife em mapas\n\n\n\nmapas\n\nggplot2\n\nweekly-viz\n\ndata-visualization\n\n\n\n\n\n\n\nAug 23, 2023\n\n\n\n\n\n\n\n\n\n\n\nO impacto dos juros na demanda imobiliário\n\n\n\nreal-estate\n\neconomia\n\n\n\nA taxa de juros é talvez a variável macroeconômica mais importante para se observar quando se pensa em financiamento imobiliário. Neste post apresento o básico do…\n\n\n\nAug 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportando dados do SIDRA\n\n\n\ndata-science\n\neconomia\n\ntutorial-R\n\n\n\nImportando dados abertos do IBGE via API usando o sidrar no R.\n\n\n\nAug 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinindo objetos no R. = ou &lt;- ?\n\n\n\ntutorial-R\n\n\n\n\n\n\n\nJul 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPacotes Essenciais R\n\n\n\ndata-science\n\neconometria\n\ntutorial-R\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizando uma única variável\n\n\n\ndata-visualization\n\ntutorial-R\n\nrepost\n\nggplot2\n\n\n\nAlgumas ideias soltas sobre como visualizar uma única variável usando vários tipos de gráficos distintos em ggplot2.\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\n\n\n\nPreços de Imóveis e Demografia\n\n\n\nreal-estate\n\neconomia\n\n\n\nEm algum sentido, o preço dos imóveis reflete tendências demográficas de longo prazo: a mais simples delas é o crescimento populacional. Neste post analiso padrões de…\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSéries de Tempo no R\n\n\n\ntime-series\n\ntutorial-R\n\neconometria\n\n\n\nNeste post faço um panorama geral de como estimar um modelo SARIMA no R usando majoritariamente funções base. O R vem equipado com diversas funções úteis e poderosas para…\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsando fontes com showtext no R\n\n\n\ntutorial-R\n\n\n\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar. Neste post ensino como importar fontes no R para criar visualizações mais refinadas e…\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizando o IPCA\n\n\n\ndata-visualization\n\nbrasil\n\neconomia\n\ntutorial-R\n\nggplot2\n\n\n\nNeste post discuto e apresento diversas maneiras de visualizar o IPCA incluindo alguns gráficos peculiares para enxergar a distribuição dos valores da inflação.\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\nARMA: um exemplo simples\n\n\n\neconometria\n\ntime-series\n\nrepost\n\ntutorial-R\n\n\n\nNeste post mostro como modelar um ARMA simples no R usando o pacote {astsa}.\n\n\n\nMar 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nEMV no R\n\n\n\neconometria\n\ntutorial-R\n\nrepost\n\n\n\nOs estimadoes de máxima verossimilhança possuem várias boas propriedades. Neste post discuto tanto aspectos teóricos como aplicados, com exemplos, e faço algumas simulações…\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAquecimento Global\n\n\n\ndata-visualization\n\ntutorial-R\n\nrepost\n\nggplot2\n\n\n\nCada listra nesta imagem representa a temperatura de um ano desde o 1850 até o presente. A mensagem é bastante clara: o planeta está cada ano mais quente e é nos anos…\n\n\n\nJan 10, 2020\n\n\n\n\n\n\n\n\n\n\n\nOLS com matrizes\n\n\n\neconometria\n\nrepost\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\n\n\n\n\n\nRepost: Otimização numérica - métodos de Newton\n\n\n\neconometria\n\ntutorial-R\n\nrepost\n\n\n\n\n\n\n\nSep 28, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrescimento do PIB per capita no mundo\n\n\n\nrepost\n\ndata-visualization\n\nggplot2\n\ntutorial-R\n\n\n\nReproduzindo uma visualização do portal Nexo\n\n\n\nJun 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepost: Expectativa de vida e Crescimento Econômico\n\n\n\ndata-visualization\n\neconomia\n\nrepost\n\ntutorial-R\n\nggplot2\n\n\n\nNeste tutorial mostro como criar visualizações ricas mostrando correlações, usando os dados de crescimento econômico e expectativa de vida do Gapminder.\n\n\n\nMay 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nMQO - teoria assintótica\n\n\n\neconometria\n\ntutorial-R\n\nrepost\n\n\n\nNeste post discuto e apresento simulações de alguns resultados assintóticos de MQO. Estes resultados permitem entender a distribuição dos estimadores MQO e o comportamento…\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeoria Assintótica - LGN e TCL\n\n\n\neconometria\n\ntutorial-R\n\nrepost\n\n\n\nSimulações de dois resultados centrais para a econometria: a Lei dos Grandes Números e o Teorema Central do Limite\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressão Linear com Séries de Tempo\n\n\n\neconometria\n\ntime-series\n\nrepost\n\ntutorial-R\n\n\n\nCursos de econometria em séries de tempo às vezes omitem o uso de MQO num contexto de séries de tempo. Neste post discuto um pouco da teoria e de aplicações no R.\n\n\n\nJan 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nSARIMA no R\n\n\n\neconometria\n\ntime-series\n\nrepost\n\ntutorial-R\n\n\n\nUm tutorial conciso sobre como utilizar o modelo SARIMA para análise e previsão de séries temporais no R. Aprenda a identificar componentes sazonais, ajustar parâmetros e…\n\n\n\nJan 1, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorial-ggplot2.html",
    "href": "tutorial-ggplot2.html",
    "title": "ggplot2: Do básico ao intermediário",
    "section": "",
    "text": "Introdução\n\n\nComeçe por aqui\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApêndice: manipular para enxergar\n\n\nEste post revisa as principais funções para manipulação de dados do Tidyverse. O material serve mais como referência e apoio ao tutorial de ggplot2 do que como introdução ao assunto.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos: gráfico de dispersão\n\n\nO gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente a correlação entre as duas variáveis escolhidas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos: gráfico de coluna\n\n\nUm gráfico do colunas é uma ferramenta de visualização poderosa e versátil para visualizar a diferença de valores entre classes e também a evolução de valores ao longo do tempo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos: histograma\n\n\nUm histograma serve para visualizar a distribuição de um conjunto de dados. É uma visualização estatística poderosa para entender o comportamento dos dados\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentos: gráfico de linha\n\n\nGráficos de linha são frequentemente usados para representar séries de tempo, isto é, valores que mudam ao longo do tempo. Estes gráficos revelam a evolução de uma variável ao longo do tempo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstético: Destacando informação\n\n\nUm gráfico deve ser autoexplicativo. Neste post, discutiremos três estratégias simples para realçar informações em um gráfico: usar linhas com geom_vline(), geom_hline() e geom_abline() para destacar eixos ou informações numéricas; realçar seções do gráfico com geom_rect(); e destacar informações numéricas e texto usando geom_text() e annotate().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstético: Escalas e Cores\n\n\nEscalas, legendas e cores são elementos essenciais numa boa visualização. Este post apresenta a lógica das funções que controlam as escalas do gráfico e as suas cores com diversos exemplos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstético: Tipografia e temas\n\n\nEste post encerra a discussão de elementos ‘estéticos’ de gráficos. Primeiro apresento, brevemente, uma discussão sobre tipografias e como utilizar fontes em gráficos de ggplot2. Depois, entro numa discussão mais detalhada sobre a função theme, que controla todos os aspectos ‘temáticos’ do gráfico\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndo Além: Lollipops\n\n\nPost intermediário que ensina a fazer gráficos de lollipop no R usando o pacote ggplot2. Os gráficos de lollipop consistem de barras com círculos no topo, que representam os valores das observações. Eles são utilizados tanto para substituir gráficos de coluna convencionais, como para destacar e comparar valores entre diferentes categorias ou momentos no tempo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndo além: facets\n\n\nFacets são pequenos gráficos que, lado a lado, ajudam a comparar várias informações ao mesmo tempo. Este post intermediário ensina a fazer gráficos de facets no R usando o ggplot2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndo além: empilhando áreas\n\n\nPost intermediário que ensina a fazer gráficos de área no R usando o pacote ggplot2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndo além: mapas de clusters\n\n\nMapas de calor ou de clusters apresentam a variação de uma variável num plano bidemensional para sugerir padrões, tendências, ou mesmo para visualizar a evolução de uma variável num grupo de classes. Este post intermediário ensina a fazer este gráfico no R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou need a map - Parte 1\n\n\nO ggplot2 tem uma ótima interface para a produção de mapas de alta qualidade. Neste primeiro post, mostro como fazer mapas usando os conhecimentos adquiridos até aqui sem a necessidade de se preocupar com objetos geométricos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou need a map - Parte 2\n\n\nNeste segundo post mostro o básico de como trabalhar com objetos espaciais e como montar mapas coropléticos. Estes mapas apresentam a distribuição espacial de uma variável numérica e são extremamente úteis. Apresento também alguns mapas temáticas como o mapa de quebras naturais e o mapa de desvios-padrão.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Atlas Brasil\n\n\n\n\n\n\n\n\n\n\nIDH dos municípios do Brasil\n\n\n\n\n\n\n\n\n\n\nMelhorando gráficos ruins\n\n\n\n\n\nJul 15, 2025\n\n\n\n\n\n\n\nO Novo Perfil Demográfico do Brasil\n\n\n\n\n\nJul 4, 2025\n\n\n\n\n\n\n\nO crescimento das Regiões Metropolitanas Brasileiras\n\n\n\n\n\nJun 15, 2025\n\n\n\n\n\n\n\n30DayChartChallenge: personal highlights\n\n\n\n\n\nMay 1, 2025\n\n\n\n\n\n\n\nA Distribuição Racial do Brasil\n\n\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\nMelhores Posts de 2024\n\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\nDemographic Pyramids in R\n\n\n\n\n\nDec 29, 2024\n\n\n\n\n\n\n\nFilm ratings over the decades: replicating a Nexo plot\n\n\n\n\n\nDec 9, 2024\n\n\n\n\n\n\n\nPunchcard plots in R\n\n\n\n\n\nDec 4, 2024\n\n\n\n\n\n\n\nReplicating the Github Contributions plot in R using ggplot2\n\n\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\nLinha-4 Amarela Metrô de São Paulo\n\n\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n\nFinding All Starbucks in Brazil\n\n\n\n\n\nJul 14, 2024\n\n\n\n\n\n\n\nLine-4 Metro\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\nLocating all The Coffee shops in Brazil\n\n\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\nCensus Tracts in Brazil\n\n\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\nAnalfabetismo no Brasil\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\nGenerations in Brazil\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nGDP in Brazil\n\n\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\nAdministrative and Statistical Divisions in Brazil\n\n\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\nThe Greatest Films of All-time: a data approach\n\n\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\nRadar Plots\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\nÍndice de Envelhecimento no Brasil\n\n\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\nNascimentos e Óbitos no Brasil\n\n\n\n\n\nApr 10, 2024\n\n\n\n\n\n\n\nOs Pecados da Visualização de Dados\n\n\n\n\n\nApr 9, 2024\n\n\n\n\n\n\n\nImportando dados em PDF no R\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n\nRazão de Dependência no Brasil\n\n\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\nDomicilios em Sao Paulo\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\nMapas Interativos com Leaflet e R\n\n\n\n\n\nMar 25, 2024\n\n\n\n\n\n\n\nEncontrando todos os Starbucks do Brasil\n\n\n\n\n\nMar 23, 2024\n\n\n\n\n\n\n\nEnriquecendo e coletando do Google Maps\n\n\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n\nCasas e Apartamentos\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\nAcidentes de Trânsito em São Paulo\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\nHousing Affordability em São Paulo\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\nAno de 2024 começa mais difícil\n\n\n\n\n\nMar 4, 2024\n\n\n\n\n\n\n\nRecessões no Brasil\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\nÍndices de Preços Imobiliários no Brasil\n\n\n\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\nPreços de Aluguel e de Venda de Imóveis\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\nIDH por região em São Paulo\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\nMédias móveis\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\nEnergia Elétrica e Crescimento Econômico no Brasil\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\nFiltro HP e Filtro de Hamilton\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\nAcessibilidade financeira à moradia em São Paulo\n\n\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\nDistribuição de Renda em São Paulo\n\n\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\nGradient Descent\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\nPreços de Imóveis no Brasil\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\nTendência e Sazonalidade\n\n\n\n\n\nJan 23, 2024\n\n\n\n\n\n\n\nAcesso a Hospitais e Leitos em São Paulo\n\n\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\nCarros e Renda em Sao Paulo\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\nO novo tidyverse: summarise\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\nO novo tidyverse: mutate\n\n\n\n\n\nJan 11, 2024\n\n\n\n\n\n\n\nO novo tidyverse: filter\n\n\n\n\n\nJan 10, 2024\n\n\n\n\n\n\n\nO novo tidyverse: rename\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n\nO novo tidyverse: select\n\n\n\n\n\nJan 6, 2024\n\n\n\n\n\n\n\nUruguay in numbers\n\n\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\nYou need a map - Parte 2\n\n\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\nYou need a map - Parte 1\n\n\n\n\n\nJan 3, 2024\n\n\n\n\n\n\n\nBump Plots\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\nWeekly Viz: Brazilian Inflation\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\nReplicando gráficos\n\n\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\nWeekly Viz: Car Dependency in São Paulo\n\n\n\n\n\nNov 24, 2023\n\n\n\n\n\n\n\nCenso 2022: O que houve de errado?\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\nCarros em São Paulo\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\nIndo além: mapas de clusters\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\nNascimentos no Brasil\n\n\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n\nEnsino Superior em São Paulo\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\nExpectativa de Vida em São Paulo\n\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\nWeekly Viz: Transportation in São Paulo\n\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\nIndo além: empilhando áreas\n\n\n\n\n\nNov 1, 2023\n\n\n\n\n\n\n\nEnvelhecimento no Brasil\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n\nWeekly Viz - Brazilian Census\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\nUm pacote com dados do mercado imobiliário\n\n\n\n\n\nOct 26, 2023\n\n\n\n\n\n\n\nIndo além: facets\n\n\n\n\n\nOct 24, 2023\n\n\n\n\n\n\n\nBrazil in Charts: Unemployment\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\nImportando arquivos, visualizando linhas\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n\nIndo Além: Lollipops\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\nCidades Brasil\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\nWeekly Viz: Nascimentos no Brasil\n\n\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\nEstético: Tipografia e temas\n\n\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\nA filosofia do tidyverse\n\n\n\n\n\nOct 9, 2023\n\n\n\n\n\n\n\nLife Satisfaction and GDP per capita\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\nPipes\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\nWeekly Viz: Ruas de Porto Alegre\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\nEstético: Escalas e Cores\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\nMapa de altitude de ruas de Brasília\n\n\n\n\n\nSep 3, 2023\n\n\n\n\n\n\n\nEstético: Destacando informação\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\n\nShiny Dashboard: IDH municípios\n\n\n\n\n\nAug 25, 2023\n\n\n\n\n\n\n\nWeekly Viz: Recife em mapas\n\n\n\n\n\nAug 23, 2023\n\n\n\n\n\n\n\nFundamentos: gráfico de linha\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\nO impacto dos juros na demanda imobiliário\n\n\n\n\n\nAug 17, 2023\n\n\n\n\n\n\n\nFundamentos: histograma\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n\nImportando dados do SIDRA\n\n\n\n\n\nAug 10, 2023\n\n\n\n\n\n\n\nFundamentos: gráfico de coluna\n\n\n\n\n\nAug 8, 2023\n\n\n\n\n\n\n\nFundamentos: gráfico de dispersão\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\n\nDefinindo objetos no R. = ou &lt;- ?\n\n\n\n\n\nJul 11, 2023\n\n\n\n\n\n\n\nApêndice: manipular para enxergar\n\n\n\n\n\nJul 7, 2023\n\n\n\n\n\n\n\nIntrodução\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\nPacotes Essenciais R\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\nVisualizando uma única variável\n\n\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\nPreços de Imóveis e Demografia\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\nSéries de Tempo no R\n\n\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\nUsando fontes com showtext no R\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\nVisualizando o IPCA\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\nARMA: um exemplo simples\n\n\n\n\n\nMar 1, 2021\n\n\n\n\n\n\n\nEMV no R\n\n\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\nAquecimento Global\n\n\n\n\n\nJan 10, 2020\n\n\n\n\n\n\n\nOLS com matrizes\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\n\nRepost: Otimização numérica - métodos de Newton\n\n\n\n\n\nSep 28, 2019\n\n\n\n\n\n\n\nCrescimento do PIB per capita no mundo\n\n\n\n\n\nJun 1, 2019\n\n\n\n\n\n\n\nRepost: Expectativa de vida e Crescimento Econômico\n\n\n\n\n\nMay 6, 2019\n\n\n\n\n\n\n\nMQO - teoria assintótica\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\nTeoria Assintótica - LGN e TCL\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\nRegressão Linear com Séries de Tempo\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\n\nSARIMA no R\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "On this website, you’ll find various posts about economics, data visualization, and data analysis. I also write several tutorials about R, the programming language in which I’m most proficient. Most of the posts and texts are under the Blog tab. I have some applications, developed with Shiny, under the Apps tab.\nThis site was originally launched in late 2019, using RMarkdown and the blogdown package. The current version was officially released in 2023, using Quarto, an integrated language for presenting data analyses, which combines R, Python, Julia, and several other programming languages.\n\n\n\nMy name is Vinícius Reginatto, I’m an economist and data scientist currently based in São Paulo, Brazil. I have been working as an economic consultant since 2019, focusing on urban economics and the real estate market. I’ve also worked at tech startups as a data scientist and spokesperson. Link to my CV."
  },
  {
    "objectID": "aboutme.html#about-the-site",
    "href": "aboutme.html#about-the-site",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "On this website, you’ll find various posts about economics, data visualization, and data analysis. I also write several tutorials about R, the programming language in which I’m most proficient. Most of the posts and texts are under the Blog tab. I have some applications, developed with Shiny, under the Apps tab.\nThis site was originally launched in late 2019, using RMarkdown and the blogdown package. The current version was officially released in 2023, using Quarto, an integrated language for presenting data analyses, which combines R, Python, Julia, and several other programming languages."
  },
  {
    "objectID": "aboutme.html#about-me",
    "href": "aboutme.html#about-me",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "My name is Vinícius Reginatto, I’m an economist and data scientist currently based in São Paulo, Brazil. I have been working as an economic consultant since 2019, focusing on urban economics and the real estate market. I’ve also worked at tech startups as a data scientist and spokesperson. Link to my CV."
  },
  {
    "objectID": "aboutme.html#sobre-o-site",
    "href": "aboutme.html#sobre-o-site",
    "title": "Vinicius Oike Reginatto",
    "section": "Sobre o Site",
    "text": "Sobre o Site\nNeste site você vai encontrar vários posts sobre economia, mercado, visualização e análise de dados. Também escrevo vários tutoriais sobre R, a linguagem de programação que tenho maior proficiência. A maior parte dos posts e textos estão na aba Blog. Tenho alguns aplicativos, desenvolvidos com Shiny, na aba Apps.\nEste site foi originalmente lançado no final de 2019, usando RMarkdown e o pacote blogdown. A versão atual foi lançada oficialmente em 2023, usando Quarto uma linguagem integrada para apresentação de análises de dados, que junta R, Python, Julia e diversas outras linguagens de programação."
  },
  {
    "objectID": "aboutme.html#sobre-mim",
    "href": "aboutme.html#sobre-mim",
    "title": "Vinicius Oike Reginatto",
    "section": "Sobre Mim",
    "text": "Sobre Mim\nMeu nome é Vinicius Oike Reginatto, sou mestre em Economia pela Universidade de São Paulo. No meu mestrado, estudei sobre teorias de crescimento econômico e escrevi sobre a história da matematização da teoria econômica.\nTrabalho como consultor econômico desde 2019. Tenho experiência com projetos de infraestrutura urbana (TOD) e análise de mercado e de conjuntura econômica. Tenho um forte background quantitativo e expertise em análise de dados (R/Python).\n\nVinicius Oike, São Paulo, Brazil. Contact: viniciusoike@gmail.com\n\nSome of my work (outside of the this blog)\n\nHousing Affordability in São Paulo: overview and measurement presented at the 2021 Latin American Real Estate Society.\nMicroapartamentos: mercado e tendências (2022)\nO Mercado Residencial na América Latina (2022)"
  },
  {
    "objectID": "posts/shiny-apps/ifdm.html",
    "href": "posts/shiny-apps/ifdm.html",
    "title": "IDH dos municípios do Brasil",
    "section": "",
    "text": "Sobre o aplicativo\nLink para o aplicativo\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\nEste aplicativo foi construído no R usando {shiny} e {shinydashboardplus}. Para acessar o código do aplicativo confira o repositório no GitHub.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. Os quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\n\n\n\n\n\nA lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque. O dashboard também mostra a evolução dos indicadores de desenvolvimento humano ao longo do tempo. Nesta comparação fica evidente o impacto da Crise de 2014-16.\n\n\n\n\n\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html",
    "href": "posts/general-posts/repost-aquecimento-global/index.html",
    "title": "Aquecimento Global",
    "section": "",
    "text": "Uma recente edição da revista inglesa The Economist exibe uma série de listras coloridas em sua capa. Elas formam um degradê que vai de um azul escuro até um vermelho intenso. Cada listra representa a temperatura de um ano e a linha do tempo vai desde o 1850 até o presente. A mensagem é bastante clara: o planeta esta cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Esta imagem é creditada a Ed Hawkings, editor do Climate Lab Book.\nPara ser preciso, a imagem não plota a temperatura de cada ano, mas sim o quanto cada ano se desvia da temperatura média do período 1971-2000. Isto é, anos acima dessa média têm um valor positivo, valores abaixo dessa média, valores negativos. Esta é uma forma bastante comum de representar este tipo de dado climático. De imediato, quando vi a imagem me ocorreu que seria bastante simples reproduzir uma versão aproximada dela usando o R."
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html#o-código",
    "href": "posts/general-posts/repost-aquecimento-global/index.html#o-código",
    "title": "Aquecimento Global",
    "section": "O código",
    "text": "O código\nO código necessário para gerar a imagem é bastante enxuto. Vou descrever em linhas gerais o que ele faz:\nPrimeiro carrego dois pacotes (linhas 1, 2), depois a série de temperatura (linha 3), faço algumas transformações nos dados (linhas 4, 5) e, por fim, ploto os dados (linhas 6, 7, 8). O resultado inicial já é bastante satisfatório e a partir destas poucas linhas de código pode-se chegar num resultado muito próximo ao da imagem original. Vale notar que a imagem fica um pouco diferente da original porque eu uso uma base de dados diferente.\n\n# Carrega pacotes\nlibrary(ggplot2)\nlibrary(astsa)\n# Carrega a base de dados 'xglobtemp'\ndata(\"xglobtemp\")\n# Converte o objeto para data.frame\ndf &lt;- data.frame(ano = as.numeric(time(xglobtemp)),\n temp = as.numeric(xglobtemp))\n\n# Monta o gráfico\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")"
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html#os-detalhes-do-código",
    "href": "posts/general-posts/repost-aquecimento-global/index.html#os-detalhes-do-código",
    "title": "Aquecimento Global",
    "section": "Os detalhes do código",
    "text": "Os detalhes do código\nVou explicar cada linha de código para ser didático. O R funciona, grosso modo, como um repositório de pacotes: cada pacote contem funções e, às vezes, bases de dados. O primeiro pacote que carrego é o ggplot2. Ele serve para fazer visualizações de dados. O pacote astsa traz várias funções para fazer análise de séries de tempo, mas eu carrego ele somente para usar a base de dados xglobtemp, que traz informação sobre a temperatura anual da terra coletada pela NASA.\nO objeto xglobtemp é uma série de tempo (um objeto da classe ts), que tem alguns atributos especiais. Um deles pode ser acesado pela função time que extrai um vetor numérico com as datas desta série de tempo. No código abaixo mostro os primeiros dez valores do time(xglobtemp).\n\nclass(xglobtemp)\n\n[1] \"ts\"\n\ntime(xglobtemp)[1:10]\n\n [1] 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n\n\nPara extrair somente os valores da série, uso a função as.numeric, que converte o vetor de ts para numeric (numérico). Este tipo de função é bastante comum já que frequentemente é preciso trocar a classe de um objeto. O objetivo destes primeiros passos é de inserir as informações do xglobtemp num data.frame em que a data aparece na primeira coluna e os valores da série são armazenados na segunda coluna. O procedimento pode parecer um tanto trabalhoso (e acho que é mesmo), mas é o jeito. Um data.frame é como uma tabela com dados. Este é um objeto bastante típico em análise de dados e é necessário para usar a função ggplot que vai fazer o gráfico. Abaixo pode-se ver as primeiras linhas desta tabela.\n\nhead(df)\n\n   ano  temp\n1 1880 -0.20\n2 1881 -0.11\n3 1882 -0.10\n4 1883 -0.20\n5 1884 -0.28\n6 1885 -0.31\n\n\nAgora que tenho os dados no formato apropriado posso usar o ggplot. O argumento que pode ser um pouco confuso é o aes. Nele especifica-se quais dados serão mapeados no gráfico. Depois disso adicionamos um geom. Há vários tipos de geom (geom_line, geom_bar, geom_histogram, etc.) e cada um deles produz uma imagem diferente. O geom_tile faz um pequeno quadrado. Para que a função consiga desenhar o quadrado é preciso informar uma variável x e uma variável y. Além disso, também especifico fill = temp. O fill se refere à cor que vai preencher (fill) o quadrado. Como especifico fill = temp a cor do quadrado vai representar a variável temp (temperatura).\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile()\n\n\n\n\n\n\n\n\nO resultado é exatamente como o esperado, mas ainda é preciso mudar a escala de cores. Faço isto com o scale_fill_gradient2. Aqui cada termo tem um signficado: scale_fill pois estamos mudando a escala do fill (outra opção seria scale_color que muda a escala do color). scale_fill_gradient pois queremos um gradiente (degradê) de cores. Por fim, o 2 é adicionado no final pois queremos um escala que diferencie dois grupos distintos: temperaturas acima da média em vermelho, temperaturas abaixo da média em azul. A escala de cores é determinada pelos argumentos low, mid e high.\nOs valores negativos serão coloridos pelo low, os próximos de zero pelo mid e os valores grandes pelo high. Abaixo escrevo as cores em hexa-decimal, mas elas podem ser lidas, essencialmente, como: azul-escuro, cinza-azulado-claro e vermelho-escuro.\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")\n\n\n\n\n\n\n\n\nComo comentei acima, pode-se melhorar o gráfico acima adicionando outros elementos e detalhes. A versão final que fiz do gráfico fica no código abaixo.\n\n# Pacote para carregar fontes externas no R\n# Necessário para utilizar 'Georgia' no gráfico\nlibrary(extrafont)\n# Data.frames auxiliares para plotar as anotações de texto\ndf_aux_title &lt;- data.frame(x = 1930, y = 0, label = \"The Climate Issue\")\ndf_aux_anos &lt;- data.frame(\n  label = c(1880, 1920, 1960, 2000),\n  x = c(1890, 1925, 1960, 1995)\n  )\n\nggplot() +\n  geom_tile(data = df, aes(x = ano, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 6,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = 1950, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 11,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_gradientn(\n    colors = c(\"#213A82\", \"#3B60CE\", \"#8DA2E2\", \"#DE2E02\", \"#9d0208\")\n   ) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n    )"
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html",
    "title": "Casas e Apartamentos",
    "section": "",
    "text": "A vasta maioria dos domicílios residenciais no Brasil são casas1. De fato, em apenas 3, das mais de 5000 cidades brasileiras, o número de apartamentos é superior ao número de casas. Isto é, apenas em Santos, Balneário Camboriú e São Caetano do Sul, existe uma proporção maior de apartamentos do que de casas. Embora São Paulo seja famosa por seus prédios, os apartamentos representam apenas um terço das moradias na cidade.\nA tabela abaixo resume os dados para as vinte cidade mais “verticalizadas” do Brasil.\n\n\n\n\n\n\n\n\nNome Cidade\nUF\nCasa\nApartamento\nOutros\nTotal Domicílios\n\n\n\n\nSantos\nSP\n31,9%\n67,1%\n1,0%\n167.478\n\n\nBalneário Camboriú\nSC\n36,4%\n63,3%\n0,3%\n57.862\n\n\nSão Caetano do Sul\nSP\n46,3%\n52,5%\n1,2%\n61.995\n\n\nPorto Alegre\nRS\n50,3%\n49,6%\n0,2%\n558.252\n\n\nVitória\nES\n50,4%\n49,0%\n0,6%\n128.617\n\n\nSão José\nSC\n53,9%\n45,9%\n0,2%\n105.580\n\n\nViçosa\nMG\n54,8%\n45,0%\n0,2%\n29.055\n\n\nFlorianópolis\nSC\n55,7%\n44,0%\n0,3%\n219.739\n\n\nNiterói\nRJ\n55,8%\n44,0%\n0,2%\n194.492\n\n\nItapema\nSC\n56,2%\n43,5%\n0,3%\n27.423\n\n\nJoão Pessoa\nPB\n58,5%\n41,3%\n0,2%\n296.249\n\n\nVila Velha\nES\n58,8%\n40,8%\n0,4%\n177.116\n\n\nRio de Janeiro\nRJ\n59,7%\n39,5%\n0,8%\n2.436.971\n\n\nValparaíso de Goiás\nGO\n60,2%\n38,9%\n0,9%\n71.169\n\n\nBelo Horizonte\nMG\n60,2%\n38,9%\n0,9%\n889.584\n\n\nBento Gonçalves\nRS\n62,3%\n37,7%\n0,0%\n48.813\n\n\nJuiz de Fora\nMG\n62,7%\n37,1%\n0,2%\n210.953\n\n\nCastelo\nES\n64,7%\n35,2%\n0,1%\n14.229\n\n\nSanto André\nSP\n64,7%\n34,7%\n0,6%\n280.389\n\n\nBrasília\nDF\n64,9%\n34,2%\n0,9%\n988.191\n\n\n\n\n\n\n\n\n\nPode-se imaginar que cidades maiores são mais verticalizadas, na média, do que cidades menores. Isto não parece ser o caso no Brasil. O gráfico abaixo mostra a relação entre o número total de domicílios e o share de apartamentos nas maiores cidades brasileiras. Considerou-se somente cidades com pelo menos 50 mil domicílios (grosso modo, 150 mil habitantes).\nOs dados apresentam uma correlação muito fraca e grande variabilidade. De fato, não parece haver relação alguma entre os dados.\n\n\n\n\n\n\n\n\n\nA tabela abaixo ilustra este fato. A tabela mostra todas as cidades na faixa de 150 a 200 mil domicílios. A cidade com maior percentual de apartamentos é Santos: 67% dos seus 167 mil domicílios são apartamentos; um contraste enorme com São João de Meriti: menos de 5% dos seus 168 domicílios são apartamentos.\n\n\n\n\n\n\n\n\nNome Cidade\nUF\nApartamento\nTotal Domicílios\n\n\n\n\nSantos\nSP\n67,1%\n167.478\n\n\nNiterói\nRJ\n44,0%\n194.492\n\n\nVila Velha\nES\n40,8%\n177.116\n\n\nCaxias do Sul\nRS\n33,9%\n184.843\n\n\nJundiaí\nSP\n30,6%\n163.219\n\n\nMaringá\nPR\n29,1%\n156.517\n\n\nPiracicaba\nSP\n23,5%\n155.428\n\n\nMogi das Cruzes\nSP\n22,3%\n158.693\n\n\nSão José do Rio Preto\nSP\n20,9%\n185.633\n\n\nSerra\nES\n20,0%\n191.936\n\n\nPorto Velho\nRO\n16,2%\n151.905\n\n\nCampos dos Goytacazes\nRJ\n13,3%\n175.744\n\n\nAnanindeua\nPA\n12,9%\n154.891\n\n\nMauá\nSP\n12,2%\n152.619\n\n\nAparecida de Goiânia\nGO\n10,5%\n186.221\n\n\nBelford Roxo\nRJ\n7,3%\n180.893\n\n\nSão João de Meriti\nRJ\n4,3%\n168.771\n\n\n\n\n\n\n\nComparando o share de apartamentos com o tamanho do município2, chega-se na mesma conclusão. O gráfico abaixo mostra o percentual de domicílios contra a área total do município. Seria intuitivo que municípios menores, onde há menos oferta de terra, exibissem maiores percentuais de apartamentos. Contudo, não parece haver relação entre as variáveis.\n\n\n\n\n\n\n\n\n\nNa faixa de 450 a 500 hectares, temos 20 municípios, a maioria com menos de 5% de apartamentos. Neste contexto, Balneário Camboriú é um outlier notável com mais de 60% de apartamentos.\n\n\n\n\n\n\n\n\nNome Cidade\nUF\nApartamento\nTotal Domicílios\nÁrea (ha.)\n\n\n\n\nBalneário Camboriú\nSC\n63,3%\n57.862\n475\n\n\nLindóia\nSP\n23,7%\n2.591\n495\n\n\nCachoeirinha\nRS\n14,6%\n52.484\n453\n\n\nSão José da Lapa\nMG\n10,4%\n8.973\n480\n\n\nHarmonia\nRS\n9,0%\n2.022\n462\n\n\nVale Real\nRS\n8,5%\n2.263\n466\n\n\nSão Sebastião de Lagoa de Roça\nPB\n6,9%\n3.764\n500\n\n\nBandeira do Sul\nMG\n3,5%\n2.232\n478\n\n\nFrancisco Morato\nSP\n3,4%\n57.668\n498\n\n\nSantana do São Francisco\nSE\n1,6%\n2.366\n453\n\n\nMato Leitão\nRS\n1,3%\n1.883\n476\n\n\nBom Jesus\nPB\n0,5%\n805\n475\n\n\nRibeirão Vermelho\nMG\n0,4%\n1.470\n496\n\n\nNova Floresta\nPB\n0,4%\n3.577\n477\n\n\nChã de Alegria\nPE\n0,3%\n4.384\n487\n\n\nCarmópolis\nSE\n0,2%\n4.689\n461\n\n\nVila Flor\nRN\n0,1%\n987\n477\n\n\nBelém\nAL\n0,1%\n1.676\n489\n\n\nPalestina\nAL\n0,0%\n1.321\n490\n\n\nTelha\nSE\n0,0%\n1.117\n487\n\n\n\n\n\n\n\n\n\n\nA lista completa dos dados pode ser acessada na tabela abaixo."
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html#tamanho-do-município",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html#tamanho-do-município",
    "title": "Casas e Apartamentos",
    "section": "",
    "text": "Pode-se imaginar que cidades maiores são mais verticalizadas, na média, do que cidades menores. Isto não parece ser o caso no Brasil. O gráfico abaixo mostra a relação entre o número total de domicílios e o share de apartamentos nas maiores cidades brasileiras. Considerou-se somente cidades com pelo menos 50 mil domicílios (grosso modo, 150 mil habitantes).\nOs dados apresentam uma correlação muito fraca e grande variabilidade. De fato, não parece haver relação alguma entre os dados.\n\n\n\n\n\n\n\n\n\nA tabela abaixo ilustra este fato. A tabela mostra todas as cidades na faixa de 150 a 200 mil domicílios. A cidade com maior percentual de apartamentos é Santos: 67% dos seus 167 mil domicílios são apartamentos; um contraste enorme com São João de Meriti: menos de 5% dos seus 168 domicílios são apartamentos.\n\n\n\n\n\n\n\n\nNome Cidade\nUF\nApartamento\nTotal Domicílios\n\n\n\n\nSantos\nSP\n67,1%\n167.478\n\n\nNiterói\nRJ\n44,0%\n194.492\n\n\nVila Velha\nES\n40,8%\n177.116\n\n\nCaxias do Sul\nRS\n33,9%\n184.843\n\n\nJundiaí\nSP\n30,6%\n163.219\n\n\nMaringá\nPR\n29,1%\n156.517\n\n\nPiracicaba\nSP\n23,5%\n155.428\n\n\nMogi das Cruzes\nSP\n22,3%\n158.693\n\n\nSão José do Rio Preto\nSP\n20,9%\n185.633\n\n\nSerra\nES\n20,0%\n191.936\n\n\nPorto Velho\nRO\n16,2%\n151.905\n\n\nCampos dos Goytacazes\nRJ\n13,3%\n175.744\n\n\nAnanindeua\nPA\n12,9%\n154.891\n\n\nMauá\nSP\n12,2%\n152.619\n\n\nAparecida de Goiânia\nGO\n10,5%\n186.221\n\n\nBelford Roxo\nRJ\n7,3%\n180.893\n\n\nSão João de Meriti\nRJ\n4,3%\n168.771\n\n\n\n\n\n\n\nComparando o share de apartamentos com o tamanho do município2, chega-se na mesma conclusão. O gráfico abaixo mostra o percentual de domicílios contra a área total do município. Seria intuitivo que municípios menores, onde há menos oferta de terra, exibissem maiores percentuais de apartamentos. Contudo, não parece haver relação entre as variáveis.\n\n\n\n\n\n\n\n\n\nNa faixa de 450 a 500 hectares, temos 20 municípios, a maioria com menos de 5% de apartamentos. Neste contexto, Balneário Camboriú é um outlier notável com mais de 60% de apartamentos.\n\n\n\n\n\n\n\n\nNome Cidade\nUF\nApartamento\nTotal Domicílios\nÁrea (ha.)\n\n\n\n\nBalneário Camboriú\nSC\n63,3%\n57.862\n475\n\n\nLindóia\nSP\n23,7%\n2.591\n495\n\n\nCachoeirinha\nRS\n14,6%\n52.484\n453\n\n\nSão José da Lapa\nMG\n10,4%\n8.973\n480\n\n\nHarmonia\nRS\n9,0%\n2.022\n462\n\n\nVale Real\nRS\n8,5%\n2.263\n466\n\n\nSão Sebastião de Lagoa de Roça\nPB\n6,9%\n3.764\n500\n\n\nBandeira do Sul\nMG\n3,5%\n2.232\n478\n\n\nFrancisco Morato\nSP\n3,4%\n57.668\n498\n\n\nSantana do São Francisco\nSE\n1,6%\n2.366\n453\n\n\nMato Leitão\nRS\n1,3%\n1.883\n476\n\n\nBom Jesus\nPB\n0,5%\n805\n475\n\n\nRibeirão Vermelho\nMG\n0,4%\n1.470\n496\n\n\nNova Floresta\nPB\n0,4%\n3.577\n477\n\n\nChã de Alegria\nPE\n0,3%\n4.384\n487\n\n\nCarmópolis\nSE\n0,2%\n4.689\n461\n\n\nVila Flor\nRN\n0,1%\n987\n477\n\n\nBelém\nAL\n0,1%\n1.676\n489\n\n\nPalestina\nAL\n0,0%\n1.321\n490\n\n\nTelha\nSE\n0,0%\n1.117\n487"
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html#tabela-completa",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html#tabela-completa",
    "title": "Casas e Apartamentos",
    "section": "",
    "text": "A lista completa dos dados pode ser acessada na tabela abaixo."
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html#footnotes",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html#footnotes",
    "title": "Casas e Apartamentos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale notar que os dados refletem o número total de domicílios e não o número total de pessoas morando em domicílios (por tipo de domicílio).↩︎\nExiste uma leve distorção no cálculo da área do município. Calculou-se a área total do perímetro municipal, o que penaliza cidades com muitas áreas verdes. Cidades como Campo Grande e Manaus, por exemplo, tem uma área total muito superior à área urbana. Conseguir o shapefile somente da área urbana da cidade, contudo, não é tarefa simples.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html",
    "title": "Census Tracts in Brazil",
    "section": "",
    "text": "Census tracts are the smallest administrative areas for which socioeconomic and demographic data are available. Broadly speaking, census tracts are small areas that exhibit similar socioeconomic and demographic patterns. In another post, I presented all of the Brazilian administrative and statistical subdivisions.\nCensus tracts are the strata used by National Bureau of Statistics and Geography (IBGE) in their decennial Census. The shape of each census tract usually respects administrative borders, land barriers, public spaces (e.g. parks, beaches, etc.), and follows the shape of roads, highways, or city blocks.\nThough they are typically small, census tracts size vary. Large plots of uninhabited land are commonly grouped into a single tract. In dense urban areas, census tracts are very small.\nCensus tracts exhibit relatively homogeneous socioeconomic and demographic characteristics. This makes census tracts a very useful statistical tool in regression analysis and classification.\nThe map below shows the 2022 census tracts in Curitiba, a major city in the Southern part of Brazil.\n\n\nCode\ncuritiba_setores &lt;- cur_setores %&gt;%\n  mutate(across(pop:dom_prt_ocup, as.numeric)) %&gt;%\n  mutate(\n    area_km2 = as.numeric(area_km2),\n    pop_dens_area = pop / area_km2,\n    pop_dens_hh = pop / dom_prt_ocup\n  )\n\nlabels &lt;- sprintf(\n  \"&lt;b&gt;Code Tract&lt;b/&gt;: %s &lt;br&gt;\n  &lt;b&gt;Code Subdistrict&lt;b/&gt;: %s &lt;br&gt;\n  &lt;b&gt;Name Subdistrict&lt;b/&gt;: %s\",\n  curitiba_setores$code_tract,\n  curitiba_setores$code_subdistrict,\n  curitiba_setores$name_subdistrict\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\n# center &lt;- curitiba_setores %&gt;%\n#   summarise(geom = st_union(.)) %&gt;%\n#   st_centroid() %&gt;%\n#   st_coordinates()\n\ncuritiba_center &lt;- c(\"lng\" = -49.28824, \"lat\" = -25.47788)\n\nleaflet(curitiba_setores) |&gt; \n  addTiles() |&gt; \n  addPolygons(weight = 2, fillColor = \"gray80\", color = \"#feb24c\", label = labels) |&gt; \n  addProviderTiles(provider = \"CartoDB.Positron\") |&gt; \n  setView(curitiba_center[[1]], curitiba_center[[2]], zoom = 15)\n\n\n\n\n\n\n\n\n\n\nThe data can be downloaded from IBGE’s website.\n\n\n\nThe package geobr (available for both Python and R) provides a convenient way to import census tracts directly into a session. The example code below shows how to import the most recent shape file for São Paulo’s census tracts1.\n\n# Import census tracts for São Paulo\nspo_tract &lt;- geobr::read_census_tract(3550308, year = 2022)\n\n\n\n\n\nCensus tracts come with a large variety of socioeconomic and demographic data. The 2022 Census currently only has a limited set of variables, that include total population and the total number of households. More data should be released in the future.\nThe 2010 census tracts offers a much richer set of variables including demographic information on age, sex, and race as well as income and education. There is a clear trade-off however as this data is almost 15 years old.\n\n\nThe maps below show basic demographic information from the 2022 Census highlighting the central district in Curitiba. For simplicity, I omit the color legend but darker shades of blue represent higher values, while lighter/whiter shades of blue represent lower values. Technically, these are quintile maps, where the underlying numerical data was ordered and binned into five equal-sized groups.\n\n\nCode\nsetores_centro &lt;- cur_setores |&gt; \n  # Administração Regional da Matriz\n  filter(code_subdistrict == 41069020501) |&gt; \n  mutate(\n    pop = as.numeric(pop),\n    pop_quintile = ntile(pop, 5),\n    area_km2 = as.numeric(area_km2),\n    pop_dens = pop / area_km2 * 10,\n    pop_dens = ntile(pop_dens, 5),\n    pop_dom = ntile(pop_dom, 5)\n    )\n\ntheme_map &lt;- theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 14, hjust = 0.5)\n  )\n\nm1 &lt;- ggplot(setores_centro) +\n  geom_sf() +\n  ggtitle(\"Census tracts\") +\n  theme_map\n\nm2 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_quintile), lwd = 0.15, color = \"white\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  ggtitle(\"Population\") +\n  theme_map\n\nm3 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dens), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Pop. Density\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\nm4 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dom), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Persons per Household\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore sophisticated analysis can be made using census tracts, but - currently - this is only possible using 2010 data. The map below shows household income data at a census tract level for the entire city of Curitiba. The map shows higher levels of income in the city center and lower levels of income in the city’s periphery.\nWhile the absolute values of income are certainly out of date, the overall spatial distribution of the data might still be similar. Since this maps uses deciles of income, instead of the actual income value, it can still communicate valuable information. This is, of course, a strong hypothesis that might be more or less valid in different contexts.\n\n\nCode\ncuritiba_income &lt;- curitiba_income |&gt; \n  mutate(decile = ntile(income_pc, 10))\n\nggplot(curitiba_income) +\n  geom_sf(aes(fill = decile, color = decile)) +\n  scale_fill_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  scale_color_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  labs(title = \"Curitiba: Household Income per capita (deciles)\") +\n  ggthemes::theme_map() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.position = c(0.9, 0.05)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#getting-the-data",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#getting-the-data",
    "title": "Census Tracts in Brazil",
    "section": "",
    "text": "The data can be downloaded from IBGE’s website.\n\n\n\nThe package geobr (available for both Python and R) provides a convenient way to import census tracts directly into a session. The example code below shows how to import the most recent shape file for São Paulo’s census tracts1.\n\n# Import census tracts for São Paulo\nspo_tract &lt;- geobr::read_census_tract(3550308, year = 2022)"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#using-census-tracts",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#using-census-tracts",
    "title": "Census Tracts in Brazil",
    "section": "",
    "text": "Census tracts come with a large variety of socioeconomic and demographic data. The 2022 Census currently only has a limited set of variables, that include total population and the total number of households. More data should be released in the future.\nThe 2010 census tracts offers a much richer set of variables including demographic information on age, sex, and race as well as income and education. There is a clear trade-off however as this data is almost 15 years old.\n\n\nThe maps below show basic demographic information from the 2022 Census highlighting the central district in Curitiba. For simplicity, I omit the color legend but darker shades of blue represent higher values, while lighter/whiter shades of blue represent lower values. Technically, these are quintile maps, where the underlying numerical data was ordered and binned into five equal-sized groups.\n\n\nCode\nsetores_centro &lt;- cur_setores |&gt; \n  # Administração Regional da Matriz\n  filter(code_subdistrict == 41069020501) |&gt; \n  mutate(\n    pop = as.numeric(pop),\n    pop_quintile = ntile(pop, 5),\n    area_km2 = as.numeric(area_km2),\n    pop_dens = pop / area_km2 * 10,\n    pop_dens = ntile(pop_dens, 5),\n    pop_dom = ntile(pop_dom, 5)\n    )\n\ntheme_map &lt;- theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 14, hjust = 0.5)\n  )\n\nm1 &lt;- ggplot(setores_centro) +\n  geom_sf() +\n  ggtitle(\"Census tracts\") +\n  theme_map\n\nm2 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_quintile), lwd = 0.15, color = \"white\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  ggtitle(\"Population\") +\n  theme_map\n\nm3 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dens), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Pop. Density\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\nm4 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dom), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Persons per Household\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore sophisticated analysis can be made using census tracts, but - currently - this is only possible using 2010 data. The map below shows household income data at a census tract level for the entire city of Curitiba. The map shows higher levels of income in the city center and lower levels of income in the city’s periphery.\nWhile the absolute values of income are certainly out of date, the overall spatial distribution of the data might still be similar. Since this maps uses deciles of income, instead of the actual income value, it can still communicate valuable information. This is, of course, a strong hypothesis that might be more or less valid in different contexts.\n\n\nCode\ncuritiba_income &lt;- curitiba_income |&gt; \n  mutate(decile = ntile(income_pc, 10))\n\nggplot(curitiba_income) +\n  geom_sf(aes(fill = decile, color = decile)) +\n  scale_fill_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  scale_color_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  labs(title = \"Curitiba: Household Income per capita (deciles)\") +\n  ggthemes::theme_map() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.position = c(0.9, 0.05)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#higher-resolution",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#higher-resolution",
    "title": "Census Tracts in Brazil",
    "section": "Higher Resolution",
    "text": "Higher Resolution\nAn obvious improvement from the 2010 census tracts is the gain in spatial resolution. The maps below show the central area of Curitiba where there was a 33% increase in the number of total census tracts."
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#dealing-with-spatial-inconsistency",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#dealing-with-spatial-inconsistency",
    "title": "Census Tracts in Brazil",
    "section": "Dealing with spatial inconsistency",
    "text": "Dealing with spatial inconsistency\n\nSpatial interpolation\nA simple strategy to deal with census tracts’ spatial inconsistency is to define a common spatial grid. This means choosing either a (1) grid of triangles; (2) grid of squares; or (3) grid of hexagons. In this example, I choose a simple squared grid.\nThe process of converting one set of data (stored in a particular shape) into another shape is called spatial interpolation. Spatial interpolation is also referred to as areal interpolation or dasymetric interpolation.\nEssentially, the problem we are tryting to solve is: we have some data stored in a (big) shape and we wish to estimate the same data in another (smaller) shape. In this case, we will dissolve population count data, stored in the shape of the 2010 census tracts and 2020 census tracts, into a finer squared grid.\nTo convert the data from one shape to the other we implicitly assume that the variable (population) is uniformly distributed over the shape’s space. That is, we assume that every single person is evenly distributed across each census tract. This assumption works well in small densely populated tracts, but doesn’t hold as well in larger tracts.\n\n\nSpatial grid for Census Data\nThe maps below show the same Census household data in a squared 500x500m grid. While I omit the color legend of the plots, I scaled them equally as to make them directly comparable; also, darker shades of green indicate higher values, while lighter shades of green indicate lower values.\nThe 2010 Census data was directly imported using the censobr R package. To estimate a simple areal interpolation I use the areal package. Executing the areal interpolation involves a few intermediary steps such as choosing a valid UTM CRS.\nThe final result shows the number of households in 2010 and 2022 in a common spatial grid. This process can be replicated across the entire city to allow an easier comparison of the data.\n\n\nCode\nlibrary(censobr)\nlibrary(areal)\n# Get demographic data for 2010 census\nbasico &lt;- censobr::read_tracts(dataset = \"Basico\")\n# Get number of households (private)\nbasico &lt;- basico %&gt;%\n  filter(code_muni == 4106902) %&gt;%\n  select(code_tract, dom = V001) %&gt;%\n  collect()\n# Join with census tract shape\nsetores_centro_antigo &lt;- left_join(setores_centro_antigo, basico, by = \"code_tract\")\n\n# Create a grid\ngrid_centro &lt;- setores_centro %&gt;%\n  st_union() %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  st_make_grid(cellsize = 500) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  mutate(gid = row_number(.))\n\n# Join census data with the new grid\ngrid_2010 &lt;- setores_centro_antigo %&gt;%\n  select(dom) %&gt;%\n  st_interpolate_aw(grid_centro, extensive = TRUE) \n# Join 2020 census data with the new grid\ngrid_2020 &lt;- setores_centro %&gt;%\n  select(dom_prt) %&gt;%\n  st_interpolate_aw(grid_centro, extensive = TRUE)\n\nd1 &lt;- grid_2010 %&gt;%\n  st_centroid() %&gt;%\n  st_join(grid_centro) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  rename(dom_2010 = dom)\n\nd2 &lt;- grid_2020 %&gt;%\n  st_centroid() %&gt;%\n  st_join(grid_centro) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  rename(dom_2020 = dom_prt)\n\nfull_grid &lt;- grid_centro %&gt;%\n  left_join(d1) %&gt;%\n  left_join(d2)\n\nbbox &lt;- setores_centro |&gt; \n  st_union() %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  st_buffer(dist = 100) %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  st_bbox()\n\nm3 &lt;- full_grid %&gt;%\n  mutate(\n    chg = (dom_2020 / dom_2010 - 1) * 100,\n    chg_bin = cut(chg, breaks = c(-Inf, 0, 50, 100, Inf))\n  )  %&gt;%\n  st_make_valid() %&gt;%\n  filter(!is.na(chg_bin)) %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = chg_bin), color = \"white\") +\n  scale_fill_brewer(palette = \"Greens\") +\n  guides(fill = \"none\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#related-posts",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#related-posts",
    "title": "Census Tracts in Brazil",
    "section": "Related posts",
    "text": "Related posts\n\nStatistical and administrative divisions of Brazil"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#footnotes",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#footnotes",
    "title": "Census Tracts in Brazil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of June 2024, the shape↩︎\nCensus tracts where there either 0 persons per household or more than 4 persons per household are considered outliers.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html",
    "title": "Cidades Brasil",
    "section": "",
    "text": "O objetivo deste post é conseguir gerar o gráfico abaixo com uma única linha de código. Isto será possível, pois todo o processamento dos dados será feito por funções auxiliares. A inspiração do mapa vem diretamente de BlakeRMills, criador do pacote {MetBrewer}.\nAbaixo segue a lista de todos os pacotes utilizados neste post. Não é necessário chamar todos eles com library mas é preciso ter todos eles instalados1. Com exceção dos três primeiros, usaremos somente uma ou duas funções de cada um dos pacotes.\n\n#&gt; Principais pacotes\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(osmdata)\nlibrary(elevatr)\nlibrary(raster)\nlibrary(geobr)\n#&gt; Pacotes auxiliares\nlibrary(purrr)\nlibrary(furrr)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(BAMMtools)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(ggthemes)\nlibrary(viridis)\nlibrary(showtext)"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#baixando-os-dados",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#baixando-os-dados",
    "title": "Cidades Brasil",
    "section": "Baixando os dados",
    "text": "Baixando os dados\nO objetivo é gerar o mapa de altitude das principais cidades do Brasil. Ao invés de ranquear as cidades por área, eu prefiro ordená-las por tamanho de população. Uma maneira fácil de conseguir esta informação é buscando ela diretamente na Wikipedia. No link temos uma tabela com código do IBGE, nome do município, nome do estado e população total em 2022.\nÉ bastante simples importar esta tabela no R. O código abaixo, interpreta a página com base na url, encontra todas as tabelas2 e escolhe especificamente a tabela principal.\n\nurl = \"https://pt.wikipedia.org/wiki/Lista_de_municípios_do_Brasil_por_população\"\n\ntab = xml2::read_html(url) |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(2)\n\nhead(tab)\n\n# A tibble: 6 × 5\n  Posição `Código IBGE` Município      `Unidade federativa` População \n  &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;                &lt;chr&gt;     \n1 1º      3550308       São Paulo      São Paulo            11 451 245\n2 2º      3304557       Rio de Janeiro Rio de Janeiro       6 211 423 \n3 3º      5300108       Brasília       Distrito Federal     2 817 068 \n4 4º      2304400       Fortaleza      Ceará                2 428 678 \n5 5º      2927408       Salvador       Bahia                2 418 005 \n6 6º      3106200       Belo Horizonte Minas Gerais         2 315 560 \n\n\nEste dado está um pouco sujo então eu limpo a tabela para facilitar nosso trabalho. Após limpar os dados, eu seleciono as 200 cidades mais populosas.\n\n\nCode\nas_numeric_char = Vectorize(function(x) {\n  ls = stringr::str_extract_all(x, \"[[:digit:]]\")\n  y = paste(ls[[1]], collapse = \"\")\n  as.numeric(y)\n})\n\nclean_tab = tab |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    code_muni = codigo_ibge,\n    name_muni = municipio,\n    rank = posicao,\n    name_state = unidade_federativa,\n    pop = populacao\n  ) |&gt; \n  filter(name_muni != \"Brasil\") |&gt; \n  mutate(\n    code_muni = as.numeric(code_muni),\n    pop = as_numeric_char(pop),\n    rank = rank(-pop),\n    name_muni = stringr::str_to_title(name_muni)\n  )\n\ntop200 = slice_max(clean_tab, pop, n = 200)"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade",
    "title": "Cidades Brasil",
    "section": "Shape da cidade",
    "text": "Shape da cidade\nAgora que temos uma lista de cidades podemos importar o shapefile com os limites territoriais do município do Recife. Graças ao pacote {geobr} isto é muito simples. A função read_municipality faz justamente isto e precisa apenas do código de 7 dígitos do IBGE.\n\n#&gt; Encontra o código do IBGE de Recife\ncode_muni = top200 |&gt; \n  filter(name_muni == \"Recife\") |&gt; \n  pull(code_muni)\n\n#&gt; Importa o shape dos limites do município\nborder = geobr::read_municipality(code_muni, showProgress = FALSE)\n\n#&gt; Um mapa simples para checar o resultado\nggplot(border) +\n  geom_sf() +\n  ggtitle(\"Limites do município\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias",
    "title": "Cidades Brasil",
    "section": "Principais vias",
    "text": "Principais vias\nO segundo passo é importar o shape das principais vias da cidade. Aqui, uso o pacote {osmdata}. O código abaixo importa as vias como “linhas” e depois usa os limites do município para remover os segmentos de linhas que estão fora da cidade.\n\n#&gt; Define os \"limites\" da busca. Monta um bounding box em torno do Recife\nrec = opq(bbox = getbb(\"Recife, Pernambuco, Brazil\"))\n\n#&gt; Pega as principais vias\nstreets = add_osm_feature(\n  rec,\n  key = \"highway\",\n  value = c(\"primary\", \"secondary\", \"tertiary\", \"residential\")\n  )\n\n#&gt; Converte o objeto para sf (LINESTRING)\nstreets = osmdata_sf(streets)\nstreets = streets$osm_lines\nstreets = select(streets, osm_id, name)\nstreets = st_transform(streets, crs = 4674)\n#&gt; Encontra a intersecção entre as vias e os limites do município\nstreets_border = st_intersection(streets, border)\n\nPara tornar mais evidente o que está acontecendo mostro primeiro o resultado geral, com todas as vias.\n\nggplot(streets) + \n  geom_sf(linewidth = 0.15) +\n  theme_void()\n\n\n\n\n\n\n\n\nE agora o resultado após a intersecção entre as vias e os limites do município.\n\nggplot() +\n  geom_sf(data = border, fill = NA) +\n  geom_sf(data = streets_border, linewidth = 0.15, color = \"gray20\") +\n  theme_void()"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude",
    "title": "Cidades Brasil",
    "section": "Altitude",
    "text": "Altitude\nO terceiro passo é importar os dados de altitude da cidade. Isto é feito com o pacote {elevatr}. Como os dados de altitude são armazenados como raster preciso convertê-los para dados em formato de vetor3. Novamente eu faço a intersecção destes dados com os limites do município para ficar somente com os valores que nos interessam.\n\n#&gt; Importa os dados de altitude\naltitude = elevatr::get_elev_raster(border, z = 9, clip = \"bbox\")\n#&gt; Converte para 'vector'\nrec_alti = raster::rasterToPolygons(altitude)\nrec_alti = sf::st_as_sf(rec_alti)\nnames(rec_alti)[1] = \"elevation\"\n\n#&gt; Converte o CRS e intersecta com os limites do município\nrec_alti = rec_alti %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  st_intersection(border) %&gt;%\n  #&gt; Remove geometrias inválidas\n  filter(st_is_valid(.))\n\nO gráfico abaixo mostra o resultado final.\n\n#&gt; Mapa\nggplot(rec_alti) +\n  geom_sf(aes(fill = elevation)) +\n  scale_fill_viridis_c(name = \"Altitude\", option = \"inferno\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nClassificando a altitude\nPara facilitar a visualização dos dados, classifica-se eles em grupos. Eu uso o algoritmo de Jenks para classificar os dados de elevação em 7 grupos distintos. O algoritmo de Jenks, também conhecido como “quebras naturais”, é bastante utilizado com dados espaciais.\nComo todo algoritmo de clustering, o algoritmo de Jenks busca minimizar a distância intraclasse enquanto tenta maximizar a distância entre as classes; isto é, ele busca observações parecidas e juntas elas em um grupo e busca separar os grupos o máximo possível.\nA medida de distância/dissemelhança que o algoritmo usa é a soma do quadrado dos desvios (em relação à média do grupo). O algortimo busca minimizar esta “variância” em cada um dos grupos para encontrar os grupos mais “parecidos” possíveis. O número de grupos é arbitrário e precisa ser selecionado manualmente4. Eu costumo escolher algum número entre 3 e 9.\n\njbreaks = BAMMtools::getJenksBreaks(rec_alti$elevation, k = 7)\njbreaks = round(jbreaks, -1)\n\nrec_alti = rec_alti %&gt;%\n  mutate(\n    jenks_group = cut(elevation, jbreaks)\n  )\n\nO resultado do agrupamento pode ser visto no histograma abaixo, onde as linhas verticais representam as quebras entre os grupos.\n\n\nCode\nggplot(rec_alti, aes(x = elevation)) +\n  geom_histogram(bins = 40, color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = jbreaks) +\n  scale_x_continuous(limits = c(-1, NA)) +\n  labs(\n    title = \"Distribuição da altitude em Recife\",\n    subtitle = \"Linhas verticais mostram o agrupamento do algoritmo de Jenks\"\n    ) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra o formato dos dados de altitude. Temos um grid retangular que mostram a altura, em metros, da cidade do Recife.\n\n#&gt; Mapa\nggplot(rec_alti) +\n  geom_sf(aes(fill = jenks_group)) +\n  scale_fill_viridis_d(option = \"inferno\") +\n  theme_void()"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes",
    "title": "Cidades Brasil",
    "section": "Juntando as partes",
    "text": "Juntando as partes\nA próxima etapa é um pouco mais complexa então vamos proceder em passos curtos. Os dados de altitude estão agrupados em grupos, definidos pelo algoritmo de jenks. Nosso objetivo é encontrar todas as ruas que pertencem a cada um destes grupos de altitude.\n\nlevels(rec_alti$jenks_group)\n#&gt; [1] \"(-40,20]\" \"(20,30]\"  \"(30,50]\"  \"(50,70]\"  \"(70,90]\"  \"(90,120]\"\n\njgroups = levels(rec_alti$jenks_group)\n\nO mapa abaixo mostra todos as áreas com altitude dentro do grupo 6 (90, 120].\n\njgroups = levels(rec_alti$jenks_group)\n\n\nsub = rec_alti %&gt;%\n  filter(jenks_group == jgroups[6]) %&gt;%\n  st_union(.) %&gt;%\n  st_as_sf()\n\nggplot(sub) + geom_sf()\n\n\n\n\n\n\n\n\nFazendo a intereseção entre este subconjunto do grid de altitude com o shape das ruas, encontra-se somente as vias que estão neste grupo de altitude.\n\nsubstreet = streets %&gt;%\n  st_intersection(sub) %&gt;%\n  filter(st_is_valid(.))\n\nggplot() +\n  geom_sf(data = substreet, linewidth = 0.5) +\n  geom_sf(data = sub, fill = NA)\n\n\n\n\n\n\n\n\nJuntando as duas etapas num mesmo bloco de código temos as linhas abaixo. Primeiro, encontra-se o subconjunto do grid de altitude dentro de um grupo. Depois, faz-se a interseção deste grupo com as vias da cidade.\n\npoly = rec_alti %&gt;%\n  filter(jenks_group == jgroups[6]) %&gt;%\n  st_union(.) %&gt;%\n  st_as_sf()\n\njoined = streets %&gt;%\n  st_intersection(poly) %&gt;%\n  filter(st_is_valid(.))\n\nggplot(joined) + geom_sf()\n\n\n\n\n\n\n\n\nPara repetir o código acima em todos os grupos, faz-se um loop simples. Os resultados são gravados numa lista chamada streets_altitude.\n\n#&gt; Cria uma lista para gravar os resultados\nstreets_altitude = list()\n\n#&gt; For-loop em todos os elementos de jgroups\nfor (i in seq_along(jgroups)) {\n\n  #&gt; Seleciona um nível do grupo em particular  \n  group = levels(rec_alti$jenks_group)[[i]]\n  \n  #&gt; Encontra o subconjunto de do grid de altitude que corresponde a este grupo\n  poly = rec_alti %&gt;%\n    filter(jenks_group == group) %&gt;%\n    st_union(.) %&gt;%\n    st_as_sf()\n  #&gt; Faz a interseção deste subconjunto com as vias da cidade\n  joined = streets %&gt;%\n    st_intersection(poly) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    mutate(level = factor(i))\n  #&gt; Grava o resultado como elemento da lista streets_altitude\n  streets_altitude[[i]] &lt;- joined\n  \n}\n\nFeito isto, pode-se verificar visualmente o resultado.\n\nrec_streets_altitude &lt;- bind_rows(streets_altitude)\n\nggplot(rec_streets_altitude) +\n  geom_sf(aes(fill = level, color = level), linewidth = 0.2) +\n  scale_fill_viridis_d(name = \"Altitude\", option = \"inferno\") +\n  scale_color_viridis_d(name = \"Altitude\", option = \"inferno\") +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_rect(color = NA, fill = \"gray75\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa-final",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa-final",
    "title": "Cidades Brasil",
    "section": "Mapa final",
    "text": "Mapa final\nPara montar a versão finalizada do mapa, adiciona-se alguns elementos temáticos. Para saber mais sobre elementos temáticos e a função theme, consulte meu post.\n\ncores = c(\n  \"#0D0887FF\", \"#5402A3FF\", \"#8B0AA5FF\", \"#B93289FF\", \"#DB5C68FF\", \n  \"#F48849FF\", \"#FEBC2AFF\"\n  )\n\njlabels = paste(jbreaks, jbreaks[-1], sep = \"–\")\njlabels[1] = paste(\"&lt;\", min(jbreaks[-1]))\njlabels[length(jlabels)] = paste(\"&gt;\", max(jbreaks))\n\nggplot(data = rec_streets_altitude) +\n  geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n  scale_color_manual(name = \"Altitude\", values = cores, labels = jlabels) +\n  scale_fill_manual(name = \"Altitude\", values = cores, labels = jlabels) +\n  guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n  ggtitle(\"Recife\") +\n  ggthemes::theme_map() +\n  coord_sf() +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 8),\n    panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n    plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n    legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#o-básico-de-funções",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#o-básico-de-funções",
    "title": "Cidades Brasil",
    "section": "O básico de funções",
    "text": "O básico de funções\nUma função transforma um input num output seguindo uma série de comandos. Uma função no R é composta de três elementos: (1) formals; (2) body; e (3) environment. O primeiro elemento corresponde aos argumentos da função e o segundo elemento corresponde à função, propriamente dita. Tomando um exemplo da matemática considere a função abaixo\n\\[\nf(x) = x^2 + 1\n\\]\nNeste caso, o formals seria simplesmente \\(x\\) e o body seria \\(x^2 + 1\\) . Podemos ver como isto ocorre dentro do R.\n\nf &lt;- function(x) {\n  x^2 + 1\n}\n\nformals(f)\n#&gt; $x\nbody(f)\n#&gt; {\n#&gt;    x^2 + 1\n#&gt; }\nenvironment(f)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\nO environment é o ambiente onde a função existe; geralmente, o environment é simplesmente R_GlobalEnv, o ambiente “global” onde habitam todos os objetos que você cria como, por exemplo, x &lt;- 1:5. Este aspecto é um pouco mais técnico, mas é o que permite que funções habitem dentro de funções.\nO exemplo abaixo é adaptado do livro Advanced R e ilustra como funcionam diferentes environments quando se tem funções dentro de funções.\nFunções compartimentalizam o seu ambiente de trabalho. O que acontece dentro da função, fica dentro da função. Assim, a linha y &lt;- 2 abaixo “existe” apenas dentro do contexto da função j. Os objetos definidos dentro de funções não interferem com objetos criados fora da função.\n\nprint(environment())\n#&gt; &lt;environment: R_GlobalEnv&gt;\nj &lt;- function(x) {\n  y &lt;- 2\n  print(environment())\n  #&gt; &lt;environment: 0x16eaa2320&gt;\n  function() {\n    print(environment())\n    #&gt; &lt;environment: 0x16e9f8d40&gt;\n    c(x, y)\n  }\n}\nk &lt;- j(1)\nk()\n#&gt; [1] 1 2\nprint(y)\n#&gt; Error in print(y) : object 'y' not found\n\nUma função executa um conjunto de ações sobre seus inputs dentro de um “ambiente controlado” e devolve apenas o resultado final, output, para a sessão ativa do R. Assim, funções permitem isolar partes do código e dispensar resultados intermediários.\nO próximo exemplo mostra como filtrar linhas de um data.frame. No fundo, esta função simplesmente chama dplyr::filter e fornece argumentos numa determinada maneira. Note que declaro o nome do pacote dplyr o que evita a necessidade de chamar library(dplyr) e torna a função filtrar_linhas portátil5.\n\ndados = data.frame(\n  grupo = c(\"A\", \"A\", \"A\", \"B\", \"C\"),\n  y = c(1, 3, 7, 4, 1)\n)\n\n\nfiltrar_linhas = function(df, filter_val) {\n  dplyr::filter(df, grupo == filter_val)\n}\n\nfiltrar_linhas(dados, \"B\")\n\n  grupo y\n1     B 4\n\n\nEste é um ponto importante: em geral, nossas funções simplesmente chamam outras funções prontas numa ordem específica e com argumentos específicos.\nO próximo é exemplo é um pouco mais sofisticado. Agora temos uma função que calcula a média geométrica de um vetor numérico. Neste caso há uma estrutura de if/else dentro do corpo da função que verifica o input antes de executar os cálculos. Especificamente, como a média geométrica é definida apenas para números positivos faz-se um teste para verificar se o input contém apenas números positivos.\nNote que as funções exp, mean e log são utilizadas sem o sinal :: pois estas funções do base-R. Isto é, são funções básicas, que permitem que o R funcione enquanto linguagem de programação, e estão sempre disponíveis.\nAlém disso, desta vez uso return para explicitamente declarar qual objeto a função deve retornar. Vale reforçar que o objeto z &lt;- exp(mean(log(x))) passa a existir somente dentro da função. Ou seja, ele não interfere com algum objeto z que exista ou que possa vir a existir no ambiente global.\n\ngeometric_mean &lt;- function(x) {\n  #&gt; Verifica o input\n  if (is.numeric(x) && all(x &gt; 0)) {\n    #&gt; Calcula a média geométrica\n    z &lt;- exp(mean(log(x)))\n    #&gt; Retorna o vetor z\n    return(z)\n    \n  } else {\n    #&gt; Retorna um erro\n    stop(\"Non-positive values found in x.\")\n    \n  }\n  \n}\n\ny &lt;- c(1, 4, 5, 10, 15, 6)\ngeometric_mean(y)\n#&gt; [1] 5.119318\nz &lt;- c(-1, 4, 5, 10, 15, 6)\ngeometric_mean(z)\n#&gt; Error in geometric_mean(z) : Non-positive values found in x.\n\nNa sua essência, a programação funcional enfatiza modularidade e previsibilidade. Quebra-se uma tarefa complexa em partes menores. Cada uma destas partes menores vira uma função individual com input e output bem estabelecidos. Códigos escritos desta maneira também funcionam melhor com processamento paralelo, ou seja, há também um ganho de eficiência.\nVamos reconstruir todos os passos acima usando funções."
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade-1",
    "title": "Cidades Brasil",
    "section": "Shape da cidade",
    "text": "Shape da cidade\nPor conveniência, eu repito abaixo o código que se usou para chegar no shape dos limites territoriais do Recife. Note o que acontece: (1) o primeiro passo filtra um data.frame para encontrar o código do IBGE do respectivo município; (2) usando o código, a função geobr::read_municipality() importa o shape.\nHá dois objetos importantes nesta função: a tabela que contém as informações de ‘nome da cidade’ e ‘código do ibge’; e o string com o ‘nome da cidade’.\n\ncode_muni = top200 |&gt; \n  filter(name_muni == \"Recife\") |&gt; \n  pull(code_muni)\n\nborder = geobr::read_municipality(code_muni)\n\nPara transformar o código acima numa função, basta incluir estes objetos como argumentos. Como a tabela usada é sempre a mesma coloca-se o objeto top200 dentro da função6.\n\nget_border = function(city) {\n  #&gt; Filtra a tabela top200 e encontra o código do município\n  code_muni = top200 |&gt; \n    dplyr::filter(name_muni == city) |&gt; \n    dplyr::pull(code_muni)\n  #&gt; Importa o shape do município\n  border = geobr::read_municipality(code_muni, showProgress = FALSE)\n  \n  return(border)\n  \n}\n\nA função get_border, definida acima, possui um único argumento city, o nome da cidade, e retorna um único objeto, border, um spatial data.frame que contém o shapefile com os limites territoriais do município."
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias-1",
    "title": "Cidades Brasil",
    "section": "Principais vias",
    "text": "Principais vias\nO processo de transformar o código numa função consiste em entender quais são os elementos essenciais e circustanciais. No código abaixo o nome da cidade \"Recife, Pernambuco, Brazil\" é um elemento mutável enquanto opq(bbox = getbb(…)) é a parte essencial. Similarmente, todos os comandos abaixo de streets são essenciais, mas o nome do objeto rec é inteiramente acidental.\n\n#&gt; Define os \"limites\" da busca. Monta um bounding box em torno do Recife\nrec = opq(bbox = getbb(\"Recife, Pernambuco, Brazil\"))\n\n#&gt; Pega as principais vias\nstreets = add_osm_feature(\n  rec,\n  key = \"highway\",\n  value = c(\"primary\", \"secondary\", \"tertiary\", \"residential\")\n  )\n\n#&gt; Converte o objeto para sf (LINESTRING)\nstreets = osmdata_sf(streets)\nstreets = streets$osm_lines\nstreets = select(streets, osm_id, name)\nstreets = st_transform(streets, crs = 4674)\n#&gt; Encontra a intersecção entre as vias e os limites do município\nstreets_border = st_intersection(streets, border)\n\nVamos dividir o código acima em duas etapas. Na primeira, vamos usar o nome da cidade para encontrar o nome do estado do município. Na segunda, vamos encontrar todas as ruas da cidade selecionada.\nA função get_state é bastante simples: ela filtra a tabela que contém as informações das cidades e encontra o nome do estado do município selecionado.\n\nget_state = function(city) {\n  top200 |&gt; \n    dplyr::filter(name_muni == city) |&gt; \n    dplyr::pull(name_state)\n}\n\nget_state(\"Recife\")\n\n[1] \"Pernambuco\"\n\n\nA função get_streets usa o nome da cidade e o shape dos limites territoriais (gerado pela função get_border()) para encontrar todas as vias contidas dentro da cidade. Note como a função get_streets chama a função get_state.\n\nget_streets = function(city, border) {\n  \n  #&gt; Encontra o nome da Unidade Federativa\n  nome_uf = get_state(city)\n  #&gt; Monta o nome do local\n  name_place = stringr::str_glue(\"{city}, {nome_uf}, Brazil\")\n  #&gt; Monta a query\n  place = osmdata::opq(bbox = osmdata::getbb(name_place))\n  \n  #&gt; Importa todas as principais vias da cidade\n  streets = osmdata::add_osm_feature(\n    place,\n    key = \"highway\",\n    value = c(\"primary\", \"secondary\", \"tertiary\", \"residential\")\n    )\n  \n  #&gt; Converte o dado\n  streets = streets %&gt;%\n    osmdata::osmdata_sf() %&gt;%\n    .$osm_lines %&gt;%\n    dplyr::select(osm_id, name) %&gt;%\n    sf::st_transform(crs = 4674)\n  \n  #&gt; Enconrtra a intersecção entre as estradas e o limites do município\n  streets_border = sf::st_intersection(streets, border)\n  #&gt; Retorna o objeto streets_border\n  return(streets_border)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude-1",
    "title": "Cidades Brasil",
    "section": "Altitude",
    "text": "Altitude\nO código que encontra a altitude da cidade é muito similar ao código que encontra as principais vias. Neste caso, contudo, pode ser interessante manter controle sobre o argumento z da função get_elev_raster. Este argumento controla o nível de resolução da imagem de altitude que se importa.\n\n\nCode\n#&gt; Importa os dados de altitude\naltitude = elevatr::get_elev_raster(border, z = 9, clip = \"bbox\")\n#&gt; Converte para 'vector'\nrec_alti = raster::rasterToPolygons(altitude)\nrec_alti = sf::st_as_sf(rec_alti)\nnames(rec_alti)[1] = \"elevation\"\n\n#&gt; Converte o CRS e intersecta com os limites do município\nrec_alti = rec_alti %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  st_intersection(border) %&gt;%\n  #&gt; Remove geometrias inválidas\n  filter(st_is_valid(.))\n\n\nA função get_elevation utiliza o shape dos limites do município e retorna um spatial data.frame com o grid retangular que contém os dados de altitude do município. O argumento z controla o nível de resolução e varia de 1 a 14. Quanto maior, maior será a resolução (o “zoom” da imagem), ou seja, mais detalhado (e mais pesado) será o resultado.\nNa definição da função, deixa-se o valor padrão pré-definido em z = 9. Isto significa que o argumento pode ser omitido pelo usuário.\n\nget_elevation = function(border, z = 9) {\n  #&gt; Importa os dados de altitude\n  altitude = elevatr::get_elev_raster(border, z = z, clip = \"bbox\")\n  #&gt; Converte para 'vector'\n  altitude = raster::rasterToPolygons(altitude)\n  altitude = sf::st_as_sf(altitude)\n  names(altitude)[1] = \"elevation\"\n  \n  #&gt; Converte o CRS e intersecta com os limites do município\n  altitude = sf::st_transform(altitude, crs = 4674)\n  altitude = suppressWarnings(sf::st_intersection(altitude, border))\n  altitude = dplyr::filter(altitude, sf::st_is_valid(altitude))\n  \n  return(altitude)\n  \n}\n\n\nClassificando\nPara classificar os dados utiliza-se o algoritmo de Jenks. As duas funções abaixo retornam esse agrupamento tomando um spatial data.frame como argumento principal. Eu adicionei alguns argumento adicionais que facilitam a escolha do número dos grupos e (opcionalmente) permite os limites dos grupos sejam arredondados para gerar número mais bonitos.\nA função add_jenks_breaks retorna uma lista com dois elementos: o primeiro elemento é o spatial data.frame, acrescido de uma coluna com o argupamento; o segundo elemento é um vetor character com a “legenda” do grupo, i.e., \"&gt;50\", \"50-100\", ….\n\nadd_jenks_breaks = function(shp, k = 7, round = FALSE) {\n  #&gt; Classifica os dados de altitude em k grupos segundo o algo. de Jenks\n  jbreaks = BAMMtools::getJenksBreaks(shp$elevation, k = k)\n  #&gt; Arredonda os números para chegar numa legenda menos quebrada\n  if (round) {\n    jbreaks[1] = floor(jbreaks[1])\n    jbreaks[length(jbreaks)] = ceiling(jbreaks)\n    jbreaks[2:(length(jbreaks) - 1)] = round(jbreaks)\n  }\n  #&gt; Cria a coluna 'jenks_group' que classifica cada valor num grupo\n  shp = shp |&gt; \n    dplyr::mutate(\n      jenks_group = findInterval(elevation, jbreaks, rightmost.closed = TRUE),\n      jenks_group = factor(jenks_group, labels = get_jenks_labels(jbreaks))\n    )\n  \n  #&gt; Verifica se todas as observações tem um grupo\n  check = any(is.na(shp$jenks_group))\n  if (check) {\n    warning(\"Some observations have failed to be grouped\")\n  }\n  \n  #&gt; Transforma os groups em legendas\n  labels = get_jenks_labels(jbreaks)\n  \n  #&gt; Retorna o output numa lista\n  out = list(shp = shp, labels = labels)\n  return(out)\n  \n}\n\nget_jenks_labels = function(x) {\n  labels = paste(x, x[-1], sep = \"–\")\n  labels[1] = paste(\"&lt;\", x[2])\n  labels = labels[1:(length(labels) - 1)]\n  return(labels)\n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes-1",
    "title": "Cidades Brasil",
    "section": "Juntando as partes",
    "text": "Juntando as partes\nO código que junta o grid de altitude com as ruas usava um for-loop em cada um dos grupos. Costuma ser relativamente simples substituir loops por funções. Transforma-se a essência do loop numa função e aplica-se a esta função no objeto usando alguma função map_* ou lapply. No exemplo abaixo, a parte essencial é o código que filtra o grid de alitutude e faz a sua intersecção com o shape das ruas.\nFunções têm algumas vantanges sobre loops. Para rodar um loop quase sempre é necessário criar objetos temporários, que vão armazenar os resultados parciais do loop. Além disso, loops costumam ser mais lentos do que funções rodando em paralelo7. Loops mal construídos, em particular, podem ser bastante ineficientes.\n\n\nCode\n#&gt; Cria uma lista para gravar os resultados\nstreets_altitude &lt;- list()\n\n#&gt; For-loop em todos os elementos de jgroups\nfor (i in seq_along(jgroups)) {\n\n  #&gt; Seleciona um nível do grupo em particular  \n  group = levels(rec_alti$jenks_group)[[i]]\n  \n  #&gt; Encontra o subconjunto de do grid de altitude que corresponde a este grupo\n  poly = rec_alti %&gt;%\n    filter(jenks_group == group) %&gt;%\n    st_union(.) %&gt;%\n    st_as_sf()\n  #&gt; Faz a interseção deste subconjunto com as vias da cidade\n  joined = streets %&gt;%\n    st_intersection(poly) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    mutate(level = factor(i))\n  #&gt; Grava o resultado como elemento da lista streets_altitude\n  streets_altitude[[i]] &lt;- joined\n  \n}\n\nrec_streets_altitude &lt;- bind_rows(streets_altitude)\n\n\nA função get_street_altitude abaixo pega o grid de altitude da cidade e o shape das principais vias e retorna um spatial data.frame único que contém as ruas da cidade agrupadas pela sua altura.\nNesta função, define-se uma função “auxiliar” join_streets que existe somente dentro do contexto da função get_streets_altitude. Aplica-se esta função em paralelo usando furrr::future_map8.\n\nget_streets_altitude = function(altitude, streets) {\n  \n  stopifnot(any(colnames(altitude) %in% \"jenks_group\"))\n  \n  #&gt; Encontra todos os grupos\n  groups = levels(altitude$jenks_group)\n  \n  #&gt; Define uma função auxiliar\n\n  #&gt; Esta função filtra o grid de altitude e faz a sua interseção\n  #&gt; com o shape das princiapis vias\n  join_streets = function(group) {\n    \n    poly = altitude %&gt;%\n      dplyr::filter(jenks_group == group) %&gt;%\n      sf::st_union(.) %&gt;%\n      sf::st_as_sf() %&gt;%\n      sf::st_make_valid()\n    \n    joined = suppressWarnings(sf::st_intersection(streets, poly))\n    \n    return(joined)\n    \n  }\n  #&gt; Aplica a função acima em todos os grupos em paralelo\n  street_levels = furrr::future_map(groups, join_streets)\n  #&gt; \"Empilha\" o objeto num único spatial data.frame\n  out = dplyr::bind_rows(street_levels, .id = \"level\")\n  \n  return(out)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa",
    "title": "Cidades Brasil",
    "section": "Mapa",
    "text": "Mapa\nTransformar o mapa final numa função é bastante simples já que quase todos os argumentos das funções vão continuar exatamente iguais. A única diferença importante a se notar é na escolha da paleta de cores. Como o número de grupos de altitude pode mudar, faz sentido que a paleta de cores também se altere. Além disso, adiciono também uma opção de usar a fonte Roboto Condensed com o pacote showtext.\nNote que suprimo o uso de ggplot2:: para deixar o código menos carregado.\n\nmap_plot = function(shp, labels, title, showtext = TRUE) {\n  \n  cores = viridis::plasma(n = length(labels) + 1)\n  cores = cores[-length(cores)]\n  \n  font = ifelse(showtext == TRUE, \"Roboto Condensed\", \"sans\")\n  \n  plot =\n    ggplot(data = shp) +\n    geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n    scale_color_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = cores\n      ) +\n    scale_fill_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = cores\n      ) +\n    guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n    ggtitle(title) +\n    ggthemes::theme_map() +\n    coord_sf() +\n    theme(\n      plot.title = element_text(\n        size = 30,\n        hjust = 0.5,\n        family = font\n        ),\n      legend.title = element_text(\n        size = 20,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.text = element_text(\n        size = 14,\n        family = font,\n        color = \"gray10\"\n        ),\n      legend.position = \"top\",\n      legend.direction = \"horizontal\",\n      plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n    )\n  \n  return(plot)\n  \n}\n\n#&gt; Para adicionar a fonte\n# sysfonts::font_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\n# showtext::showtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#uma-função-final",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#uma-função-final",
    "title": "Cidades Brasil",
    "section": "Uma função final",
    "text": "Uma função final\nAgora que temos funções para cada uma das principais tarefas, podemos criar uma última função que vai executar estas funções na ordem apropriada.\nA função map_altitude utiliza apenas o nome da cidade para gerar o mapa de altitude. Opcionalmente, pode-se alterar os argumentos k, que altera o número de grupos e z que aumenta/diminui a resolução do mapa de altura.\nComo esta função executa vários passos intermediários, e alguns destes podem ser bastante demorados, eu incluo algumas mensagens que informam sobre o andamento da função. O resultado final é armazenado numa lista, que retorna alguns dos objetos intermediários como o shape de altitude.\n\nmap_altitude = function(city, k = 6, z = 7) {\n  \n  #&gt; Importa o shape do limite do município\n  message(\"Importando os limites do município: \", city)\n  city_border = get_border(city)\n  #&gt; Importa as principais vias da cidade e junta com o limite do muni\n  message(\"Importando as vias.\")\n  city_street = get_streets(city, city_border)\n  #&gt; Importa a altitude da cidade\n  message(\"Importando a altitude.\")\n  city_elevation = suppressMessages(get_elevation(city_border, z = z))\n  #&gt; Classifica a altitude em grupos\n  message(\"Classificando e juntando os shapefiles.\")\n  jenks = add_jenks_breaks(city_elevation, k = k)\n  city_elevation = jenks[[\"shp\"]]\n  labels = jenks[[\"labels\"]]\n  #&gt; Junta a altitude (agrupada) com as vias\n  city_street_elevation = get_streets_altitude(city_elevation, city_street)\n  \n  #&gt; Monta o mapa final\n  message(\"Gerando o mapa final.\")\n  plot = map_plot(city_street_elevation, labels = labels, title = city)\n  message(\"Feito.\")\n  #&gt; Retorna o output numa lista\n  out = list(\n    shp = city_street_elevation,\n    streets = city_street,\n    elevation = city_elevation,\n    plot = plot\n    )\n  \n  return(out)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#testando-a-função",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#testando-a-função",
    "title": "Cidades Brasil",
    "section": "Testando a função",
    "text": "Testando a função\nFeito tudo isto. Agora podemos testar a função com outras cidades.\n\nSão Caetano do Sul\nSão Caetano do Sul é uma boa cidade teste, porque ela é bem pequena então os resultados não demoram tanto para carregar.\n\nscs = map_altitude(\"São Caetano Do Sul\", k = 4, z = 8)\nscs$plot\n\n\n\n\n\n\n\n\nOs demais mapas gerados foram feitos em resolução bastante elevada, então os códigos podem demorar várias horas para executar. Recomendo começar testando com valores menores de z. Mesmo em alta resolução, há melhorias possíveis nos gráficos como mudanças no “enquadramento” do mapa e também na escolha das quebras da legenda. Ainda assim, o resultado é muito bom.\n\n\nFortaleza\n\nfortaleza = map_altitude(\"Fortaleza\", z = 13)\nfortaleza$plot\n\n\n\n\n\n\n\n\nOsasco\n\nosa &lt;- map_altitude(\"Osasco\", k = 7, z = 12)\nosa$plot\n\n\n\n\n\n\n\n\nBelo Horizonte\n\nmap_altitude(\"Belo Horizonte\", k = 7, z = 13)$plot\n\n\n\n\n\n\n\n\nPorto Alegre\n\nmap_altitude(\"Porto Alegre\", k = 6, z = 13)\n\n\n\n\n\n\n\n\nCuritiba\n\nmap_altitude(\"Curitiba\", k = 8, z = 11)\n\n\n\n\n\n\n\n\nBrasília\n\nmap_altitude(\"Brasília\", k = 7, z = 11)"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#footnotes",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#footnotes",
    "title": "Cidades Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChamar muitos pacotes numa mesma sessão pode levar a muitos conflitos entre funções com o mesmo nome. Isto não é um problema muito sério já que sempre é possível especificar nome_pacote::nome_funcao.↩︎\nNa verdade, o código encontra todos os elementos com a classe table na página.↩︎\nPara uma introdução aos tipos de objetos espaciais (raster e vector) veja Lovelace (2023).↩︎\nExistem alguns métodos que ajudam a escolher um número “ótimo” de clusters, como o “elbow method” mas vale lembrar que clustering é muito mais arte do que ciência. Clustering envolve agrupar dados semelhantes em um número finito de grupos, mas há inúmeras maneiras de definir “semelhante”; além disso, o algoritmo de clustering sempre chega num agrupamento, qualquer que seja a escolha do número de grupos. Assim, é importante frisar que estes resultados são mais explortatórios, por assim dizer.↩︎\nDeclarar explicitamente o pacote utilizado é a melhor maneira de escrever novas funções. Isto garante que a função vai funcionar em qualquer contexto onde o pacote estiver instalado. A longo prazo, isto também permite que se use a mesma função em outro projeto ou até mesmo dentro de um novo pacote. Vale notar que algumas funções base como mean, round, findInterval, etc. não precisam ser declaradas pois elas fazem parte do base-R.↩︎\nNote que isto torna a função limitada, pois temos dados somente das 200 cidades mais populosas do Brasil. Outro fator importante é que estamos usando o nome da cidade como input. Se a tabela fosse expandida para incluir todas as cidades do Brasil seria necessário alterar o argumento pois há cidades com nome duplicado.\nEste tipo de generalização está fora do escopo deste post.↩︎\nEm geral, desde que o loop seja bem feito ele não será muito lento. Para consultar boas práticas de como fazer bons loops consulte Best Coding Practices for R. Uma comparação recente da velocidade de loops no R está disponível em On the performance of for loops in R.↩︎\nAlternativamente, pode-se usar parallel::mclapply, mas esta função, infelizmente só funciona em sistemas MacOs e Linux.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "",
    "text": "In this post I’ll show how to replicate the infamous GitHub contributions graphic. This visualization shows all the days of the year and highlights the most “productive” days in darker colors. It’s similar to a heat map and also known as a cluster map. You can check other examples of this plot in my tutorial post (in Portuguese). In a way, it’s also fairly similar to a “punchcard” plot."
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#libraries",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#libraries",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Libraries",
    "text": "Libraries\nI use basic tidyverse libraries to replicate the plot. The import::from is a neat way to get only some functions from a large package. This is not strictly necessary but is a good coding practice. Since I use many functions from the ggplot2 and lubridate packages, I import them fully.\nFor this tutorial I’m using ggplot2_3.5.1.\n\nlibrary(ggplot2)\nlibrary(lubridate)\n\nimport::from(dplyr, mutate, if_else, summarise)\nimport::from(tidyr, expand_grid)\nimport::from(forcats, fct_rev)\nimport::from(RQuantLib, isBusinessDay)\n\nAn important feature of the original plot is that each square has round borders. Unfortunately, drawing these type of squares isn’t very straightforward with ggplot2. To solve this, I draw from a solution posted on StackOverflow, which copies part of the code from the statebins package to make a geom_rtile function.\nWhile the code may seem daunting, using it is very simple.\n\n\nCode\n`%||%` &lt;- function(a, b) {\n  if(is.null(a)) b else a\n}\n\nGeomRtile &lt;- ggplot2::ggproto(\n  \"GeomRtile\", \n  statebins:::GeomRrect, # 1) only change compared to ggplot2:::GeomTile\n                     \n  extra_params = c(\"na.rm\"),\n  setup_data = function(data, params) {\n    data$width &lt;- data$width %||% params$width %||% resolution(data$x, FALSE)\n    data$height &lt;- data$height %||% params$height %||% resolution(data$y, FALSE)\n\n    transform(data,\n      xmin = x - width / 2,  xmax = x + width / 2,  width = NULL,\n      ymin = y - height / 2, ymax = y + height / 2, height = NULL\n    )\n  },\n  default_aes = ggplot2::aes(\n    fill = \"grey20\", colour = NA, size = 0.1, linetype = 1,\n    alpha = NA, width = NA, height = NA\n  ),\n  required_aes = c(\"x\", \"y\"),\n\n  # These aes columns are created by setup_data(). They need to be listed here so\n  # that GeomRect$handle_na() properly removes any bars that fall outside the defined\n  # limits, not just those for which x and y are outside the limits\n  non_missing_aes = c(\"xmin\", \"xmax\", \"ymin\", \"ymax\"),\n  draw_key = draw_key_polygon\n)\n\ngeom_rtile &lt;- function(mapping = NULL, data = NULL,\n                       stat = \"identity\", position = \"identity\",\n                       radius = grid::unit(6, \"pt\"), # 2) add radius argument\n                       ...,\n                       na.rm = FALSE,\n                       show.legend = NA,\n                       inherit.aes = TRUE) {\n  ggplot2::layer(\n    data = data,\n    mapping = mapping,\n    stat = stat,\n    geom = GeomRtile,\n    position = position,\n    show.legend = show.legend,\n    inherit.aes = inherit.aes,\n    params = rlang::list2(\n      radius = radius,\n      na.rm = na.rm,\n      ...\n    )\n  )\n}"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#data",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#data",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Data",
    "text": "Data\nFor the visualization, we need 365 observations indicating how many contributions were submitted each day of the year. To make the simulated data more realistic I’ll account for holidays and weekends using the RQuantLib package.\nOn an average workday I assume the number of contributions follows a Poisson distribution with \\(\\lambda = 6.5\\)1. This returns an average of around 4-8 contributions per day and guarantees that the number of contributions will be integer and non-negative. For weekdays or holidays I assumed a small chance (10%) that our worker will have to do some light work.\n\n# Tibble that contains all days of the year\ncontributions &lt;- expand_grid(\n  date = seq(as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), by = \"1 day\")\n)\n\n# Function to simulate number of contributions conditional on workday\nsample_contribution &lt;- Vectorize(function(x) {\n  if (x == 1) {\n    # Normal day of work\n    rpois(1, 6.5)\n  } else {\n    # Small chance of working overtime on weekends or holidays\n    sample(c(0, 1, 2), 1, prob = c(0.9, 0.05, 0.05))\n  }}\n)\n\ncontributions &lt;- contributions |&gt; \n  mutate(\n    # Gets the number of the week in the year\n    n_week = week(date),\n    # Gets weekday number - Starts the week at sunday\n    n_day = wday(date, week_start = 7),\n    # Weekday labels for the plot\n    weekday_label = wday(date, week_start = 7, label = TRUE, abbr = TRUE),\n    weekday_label = fct_rev(weekday_label),\n    # Month labels for the plot\n    month = month(date, label = TRUE, abbr = TRUE),\n    is_workday = as.numeric(RQuantLib::isBusinessDay(\"Brazil\", date)),\n    # is_weekday = if_else(n_day == 7 | n_day == 1, 0L, 1L),\n    n = sample_contribution(is_workday)\n  )\n\ncontributions &lt;- contributions |&gt; \n  mutate(\n    n = if_else(n == 0, NA, n),\n    n_week = if_else(n_day == 1, n_week + 1, n_week)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#plotting",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#plotting",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Plotting",
    "text": "Plotting\n\nBase plot\nThe basic version of this plot is essentially a cluster map. On the horizontal axis, the weeks of the year are plotted; on the vertical axis, the days of each year are plotted. The intensity of the color highlights the amount of GitHub contributions.\n\nggplot(contributions, aes(n_week, n_day)) +\n  geom_rtile(\n    aes(fill = n),\n    color = \"white\",\n    radius = unit(2, \"pt\"),\n    width = 0.9,\n    height = 0.9\n    )\n\n\n\n\n\n\n\n\n\n\nFinal plot\nTo arrive at a more polished version of this visualization I adjusted the scales and tweaked some minor thematic elements. To my surprise this was easier than I had anticipated.\n\n# Find the positions of the month labels\ntab &lt;- contributions |&gt; \n  summarise(nmin = min(n_week), .by = \"month\")\n\nggplot(contributions, aes(n_week, weekday_label)) +\n  geom_rtile(\n    aes(fill = n),\n    color = \"white\",\n    radius = unit(2, \"pt\"),\n    width = 0.9,\n    height = 0.9\n    ) +\n  # Highlight the months on the horizontal axis\n  scale_x_continuous(\n    breaks = tab$nmin,\n    labels = as.character(tab$month),\n    position = \"top\",\n    expand = c(0, 0)\n  ) +\n  # Highlight days of the week on the vertical axis\n  scale_y_discrete(breaks = c(\"Mon\", \"Wed\", \"Fri\")) +\n  # Adjust color palette\n  scale_fill_distiller(\n    palette = \"Greens\",\n    direction = 1,\n    na.value = \"gray90\") +\n  # Removes x and y labels\n  labs(x = NULL, y = NULL) +\n  # Removes the color legend\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    text = element_text(color = \"gray10\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#footnotes",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#footnotes",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis number was a guess roughly based on my own history of contributions. The actual distribution might be more right-skewed than a Poisson, but for the purposes of this post it works fine.↩︎"
  },
  {
    "objectID": "posts/general-posts/2025-05-chart-challenge/index.html",
    "href": "posts/general-posts/2025-05-chart-challenge/index.html",
    "title": "30DayChartChallenge: personal highlights",
    "section": "",
    "text": "The 30DayChartChallenge is a yearly community-driven online event that presents 30 prompts for data visualizations. The challenge is organized by Cédric Scherer and Dominic Royé, with prompts categorized into themes (e.g. comparisons, distributions, relationships, etc.).\n\n\n\n\n\n\n\nThis was my first year participating in the challenge. For the sake of consistency and speed, I created almost all of my visualizations using ggplot2 and R. The first few days were tougher than I expected—it took me a while to grasp the format of the challenge. I wasted a lot of time experimenting with different datasets, trying out alternative visualizations that weren’t used. Over time, though, I got better at quickly prototyping simple versions of plots and made my code more reusable and efficient.\nMy simple lessons are summarized below.\n\nVisualize before coding\n\nIt’s usually hard to visualize the final plot, specially when dealing with an unknown dataset. I found that making very simple plots or even drawing them by hand, helped me structure the final version of the code.\n\nReuse code\n\nCode reusability is a clear advantage of using a programming language over GUI-based software, yet I’ve often fallen into the (bad) habit of starting each new visualization from scratch. As the challenge progressed, though, I began adapting and reusing more of my own code—and things became much more efficient and productive as a result.\n\nThe ragg package\n\nI’ve always used showtext to display custom fonts in plots, but ragg seems to be the superior alternative. Plots render faster and there’s no DPI issue when exporting plots. While you’re limited to local fonts, this can actually help streamline your workflow by reducing options. I found myself gravitating towards Avenir, Futura, Helvetica, and Gill Sans.\nYou can also download fonts from Google Fonts if you find yourself missing Roboto.\n\nUsing variables for fonts and colors\n\nThis was a simple adaptation I made to speed up prototyping. The code snippet below is from the Extraterrestrial plot. With this setup, it’s much easier to test different fonts, colors, or even entire palettes\n\nfont_title &lt;- \"Fira Code\"\nfont_text &lt;- \"IBM Plex Mono\"\n\npal_scifi &lt;- c(\n  \"#06d0ce\",\n  \"#09f6f8\",\n  \"#076a6c\",\n  \"#051416\",\n  \"#d5dadb\",\n  \"#04454a\",\n  \"#098b94\",\n  \"#05947c\",\n  \"#fefefe\",\n  \"#022123\"\n)\n\nspace_background &lt;- pal_scifi[10]\ncolor_text_label &lt;- pal_scifi[2]\ncolor_axis_label &lt;- pal_scifi[5]\nfill_text_label &lt;- pal_scifi[4]\nfill_column &lt;- pal_scifi[3]\n\nThe code for all of my plots can be found on this GitHub repository.\n\n\n\n\n\nThese circular maps were my favorite visualization from the Challenge. The maps estimate the number of people living within a 6 km radius of their city’s downtown. The street colors represent nearby population density: darker shades indicate lower values, lighter shades represent higher.\nI adapted the code from this post, which shows how to map altitude into streets, to show population counts. The population data was collected from the recent 2022 Census, which just recently published population couts in census tracts. The style of the maps was inspired by the prettymaps Python package made by Marcelo Prates.\nOverall, these maps took a very long time to make, and were unpractical for the challenge. Running the code takes a long time, since several OSM features need to be downloaded.\n\n\n\n\n\n\n\n\n\n\nLink to code\n\n\n\nThe idea for this visualization is to show how choropleth maps of Brazil are deceiving. Brazil’s Midwest and North regions cover about 65% of the country’s area but house only 16% of its population. As a result, these choropleth maps tend to be biased, since larger chunks of land dominate the visualization, drawing more attention regardless of their actual relevance to the underlying data. This obscures important patterns in more densely populated areas, like the Southeast and Northeast, which occupy smaller land areas but contain the majority of Brazil’s population.\n\n\n\n \n\n\n\n\n\n\nBy the second half of the challenge, I felt more confortable with the challenge, and the plots became more streamlined. This chart shows the growing gap between house sales prices and rental prices in Brazil. This is something I explored in greater detail in this post (in Portuguese).\n\n\n\n\n\nLink to code\n\n\n\nThis was more of a “fun” prompt. The chart shows different smoothing techniques applied to recent S&P 500 data. Naturally, the index performed poorly in April, reflecting the turbulence sparked by Trump’s tariff policies. Making this plot was relatively straightforward, since the base stats package from R already has several smoothers, such as loess, smooth, and smooth.spline. The TTR package also provides variations of MA filters, commonly used in financial analysis.\n\n\n\n\n\nLink to code\n\n\n\nThis was one of the quickest plots to make and shows how pre-planning really is key for this challenge. I experimented with alternative datasets from OurWorldInData and settled on the raw consumption of fossil fuels in the past 100 years.\n\n\n\n\n\nLink to code\n\n\n\nI already knew I wanted to use the sunspots dataset for this prompt, but Jevons helped make it more compelling. In the late 19th century, British economist Stanely Jevons theorized that sunspot cycles had an influence over agricultural productivity and, by extension, economic cycles. Although he never managed to prove his theory, many still claim the economic cycles are regulated by sunspot cycles. The last two major recessions (more or less) coincided sunspot cycle troughs, giving the theory a bit of renewed attention.\n\n\n\n\n\nLink to code\n\n\n\nThis plot shows the EMBI+ index over the years, highlighting key political and economic events in Brazilian history. The code for this plot was adapted from my post on Brazilian hyperinflation.\n\n\n\n\n\nLink to code\n\n\n\nThis plot shows the decoupling of electric energy consumption and GDP growth in Brazil. The consumption of electric energy is typically considered a good proxy for economic growth, but, since the 2014-16 recession, these series have diverged.\nThe code for this plot was adapted by a similar analysis I did in this post (in Portuguese).\n\n\n\n\n\nLink to code\n\n\n\nThis plot shows the box-office earnings from films that feature aliens. Making this plot was much easier than I imagined. I combined a Wikipedia list of alien-themed movies with domestic box-office data from BoxOfficeMojo, and used FRED to adjust the values for inflation to ensure comparability.\n\n\n\n\n\nLink to code"
  },
  {
    "objectID": "posts/general-posts/2025-05-chart-challenge/index.html#learning",
    "href": "posts/general-posts/2025-05-chart-challenge/index.html#learning",
    "title": "30DayChartChallenge: personal highlights",
    "section": "",
    "text": "This was my first year participating in the challenge. For the sake of consistency and speed, I created almost all of my visualizations using ggplot2 and R. The first few days were tougher than I expected—it took me a while to grasp the format of the challenge. I wasted a lot of time experimenting with different datasets, trying out alternative visualizations that weren’t used. Over time, though, I got better at quickly prototyping simple versions of plots and made my code more reusable and efficient.\nMy simple lessons are summarized below.\n\nVisualize before coding\n\nIt’s usually hard to visualize the final plot, specially when dealing with an unknown dataset. I found that making very simple plots or even drawing them by hand, helped me structure the final version of the code.\n\nReuse code\n\nCode reusability is a clear advantage of using a programming language over GUI-based software, yet I’ve often fallen into the (bad) habit of starting each new visualization from scratch. As the challenge progressed, though, I began adapting and reusing more of my own code—and things became much more efficient and productive as a result.\n\nThe ragg package\n\nI’ve always used showtext to display custom fonts in plots, but ragg seems to be the superior alternative. Plots render faster and there’s no DPI issue when exporting plots. While you’re limited to local fonts, this can actually help streamline your workflow by reducing options. I found myself gravitating towards Avenir, Futura, Helvetica, and Gill Sans.\nYou can also download fonts from Google Fonts if you find yourself missing Roboto.\n\nUsing variables for fonts and colors\n\nThis was a simple adaptation I made to speed up prototyping. The code snippet below is from the Extraterrestrial plot. With this setup, it’s much easier to test different fonts, colors, or even entire palettes\n\nfont_title &lt;- \"Fira Code\"\nfont_text &lt;- \"IBM Plex Mono\"\n\npal_scifi &lt;- c(\n  \"#06d0ce\",\n  \"#09f6f8\",\n  \"#076a6c\",\n  \"#051416\",\n  \"#d5dadb\",\n  \"#04454a\",\n  \"#098b94\",\n  \"#05947c\",\n  \"#fefefe\",\n  \"#022123\"\n)\n\nspace_background &lt;- pal_scifi[10]\ncolor_text_label &lt;- pal_scifi[2]\ncolor_axis_label &lt;- pal_scifi[5]\nfill_text_label &lt;- pal_scifi[4]\nfill_column &lt;- pal_scifi[3]\n\nThe code for all of my plots can be found on this GitHub repository."
  },
  {
    "objectID": "posts/general-posts/2025-05-chart-challenge/index.html#highlights",
    "href": "posts/general-posts/2025-05-chart-challenge/index.html#highlights",
    "title": "30DayChartChallenge: personal highlights",
    "section": "",
    "text": "These circular maps were my favorite visualization from the Challenge. The maps estimate the number of people living within a 6 km radius of their city’s downtown. The street colors represent nearby population density: darker shades indicate lower values, lighter shades represent higher.\nI adapted the code from this post, which shows how to map altitude into streets, to show population counts. The population data was collected from the recent 2022 Census, which just recently published population couts in census tracts. The style of the maps was inspired by the prettymaps Python package made by Marcelo Prates.\nOverall, these maps took a very long time to make, and were unpractical for the challenge. Running the code takes a long time, since several OSM features need to be downloaded.\n\n\n\n\n\n\n\n\n\n\nLink to code\n\n\n\nThe idea for this visualization is to show how choropleth maps of Brazil are deceiving. Brazil’s Midwest and North regions cover about 65% of the country’s area but house only 16% of its population. As a result, these choropleth maps tend to be biased, since larger chunks of land dominate the visualization, drawing more attention regardless of their actual relevance to the underlying data. This obscures important patterns in more densely populated areas, like the Southeast and Northeast, which occupy smaller land areas but contain the majority of Brazil’s population.\n\n\n\n \n\n\n\n\n\n\nBy the second half of the challenge, I felt more confortable with the challenge, and the plots became more streamlined. This chart shows the growing gap between house sales prices and rental prices in Brazil. This is something I explored in greater detail in this post (in Portuguese).\n\n\n\n\n\nLink to code\n\n\n\nThis was more of a “fun” prompt. The chart shows different smoothing techniques applied to recent S&P 500 data. Naturally, the index performed poorly in April, reflecting the turbulence sparked by Trump’s tariff policies. Making this plot was relatively straightforward, since the base stats package from R already has several smoothers, such as loess, smooth, and smooth.spline. The TTR package also provides variations of MA filters, commonly used in financial analysis.\n\n\n\n\n\nLink to code\n\n\n\nThis was one of the quickest plots to make and shows how pre-planning really is key for this challenge. I experimented with alternative datasets from OurWorldInData and settled on the raw consumption of fossil fuels in the past 100 years.\n\n\n\n\n\nLink to code\n\n\n\nI already knew I wanted to use the sunspots dataset for this prompt, but Jevons helped make it more compelling. In the late 19th century, British economist Stanely Jevons theorized that sunspot cycles had an influence over agricultural productivity and, by extension, economic cycles. Although he never managed to prove his theory, many still claim the economic cycles are regulated by sunspot cycles. The last two major recessions (more or less) coincided sunspot cycle troughs, giving the theory a bit of renewed attention.\n\n\n\n\n\nLink to code\n\n\n\nThis plot shows the EMBI+ index over the years, highlighting key political and economic events in Brazilian history. The code for this plot was adapted from my post on Brazilian hyperinflation.\n\n\n\n\n\nLink to code\n\n\n\nThis plot shows the decoupling of electric energy consumption and GDP growth in Brazil. The consumption of electric energy is typically considered a good proxy for economic growth, but, since the 2014-16 recession, these series have diverged.\nThe code for this plot was adapted by a similar analysis I did in this post (in Portuguese).\n\n\n\n\n\nLink to code\n\n\n\nThis plot shows the box-office earnings from films that feature aliens. Making this plot was much easier than I imagined. I combined a Wikipedia list of alien-themed movies with domestic box-office data from BoxOfficeMojo, and used FRED to adjust the values for inflation to ensure comparability.\n\n\n\n\n\nLink to code"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-cars/index.html",
    "href": "posts/general-posts/2023-11-wz-cars/index.html",
    "title": "Weekly Viz: Car Dependency in São Paulo",
    "section": "",
    "text": "Automobiles pose a significant challenge in the global effort to combat climate change. Transportation contributes to roughly a fifth of total carbon emissions worldwide, and among contemporary transportation options, petrol-burning cars are notably inefficient. Beyond air pollution, cars require substantial space for operation and inflict various negative externalities on cities.\nIn this post, I revisit São Paulo to assess the extent of the city’s dependence on cars. The data used is available via my R package tidypod.\n\n\nSão Paulo is a megacity with over 12 million inhabitants. The city recently declared its commitment to a significant reduction in greenhouse gas emissions, aiming for net-zero emissions by 2050. Achieving this goal requires a strategic focus on improving public transit options and diminishing reliance on cars. In the Greater São Paulo area, cars currently represent nearly 27% of all trips.\nSimilar to much of the southeastern region of Brazil, São Paulo boasts a predominantly clean energy matrix. The city’s primary challenge lies in the realm of transportation. Specifically, there is a need to reduce the size of the car fleet and enhance public transportation alternatives.\nOne effective measure of car-dependency is assessing the prevalence of car-free households. Using the most recent Origin Destination Survey data I estimate the share of households without cars. The data is aggregated by OD zone: after cleaning, there are 329 valid OD zones in São Paulo1. To quickly get this data I use the tidypod package.\nThe data suggest that constraints related to income, rather than preferences, predominantly keep car usage low. Interestingly, some of the OD zones with the lowest share of car-free households are affluent neighborhoods spatially close to key business districts in the city.\nIt’s important to note that reducing car-dependency doesn’t necessarily mean removing cars from the streets. As professor Dorel Soares Ramos, from the University of São Paulo, explains “renewable gaseous fuels” such as “biomethane” are considered “green for fleet adaptation”2. Similarly, electric automobiles will contribute to lowering carbon emissions. However, irrespective of the energy source for cars, we must still address the numerous negative externalities associated with their use.\n\n\nCar-free households seem to be spatially concentrated at peripheral regions of the city, very far away from any of the cities CBDs. The notable exception is the green mass right at the “old” CBD (Centro Histórico). While this region has very high share of car-free households, it should be noted that is also is not a very populated region. The “newer” CBDs of Paulista and Itaim have much lower shares of car-free households.\n\n\n\n\n\n\n\n\n\nClose to half of São Paulo’s households are entirely car-free, and in certain regions, virtually every household opts to go without any cars.\n\n\n\n\n\n\n\n\n\nA distinct negative correlation exists between income and car-dependency in São Paulo. As income rises, households tend to demand more cars and the likelihood of finding car-free households decreases. The plot below shows average household income (in log-scale) against the percentage share of car-free households across all 329 zones. The size of each circle is scaled to represent the total population living in each zone. The richer, less populated zones, have the lowest shares of car-free households. Meanwhile, the more populated and poor households have some of the highest shares of car-free households3."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-cars/index.html#são-paulo",
    "href": "posts/general-posts/2023-11-wz-cars/index.html#são-paulo",
    "title": "Weekly Viz: Car Dependency in São Paulo",
    "section": "",
    "text": "São Paulo is a megacity with over 12 million inhabitants. The city recently declared its commitment to a significant reduction in greenhouse gas emissions, aiming for net-zero emissions by 2050. Achieving this goal requires a strategic focus on improving public transit options and diminishing reliance on cars. In the Greater São Paulo area, cars currently represent nearly 27% of all trips.\nSimilar to much of the southeastern region of Brazil, São Paulo boasts a predominantly clean energy matrix. The city’s primary challenge lies in the realm of transportation. Specifically, there is a need to reduce the size of the car fleet and enhance public transportation alternatives.\nOne effective measure of car-dependency is assessing the prevalence of car-free households. Using the most recent Origin Destination Survey data I estimate the share of households without cars. The data is aggregated by OD zone: after cleaning, there are 329 valid OD zones in São Paulo1. To quickly get this data I use the tidypod package.\nThe data suggest that constraints related to income, rather than preferences, predominantly keep car usage low. Interestingly, some of the OD zones with the lowest share of car-free households are affluent neighborhoods spatially close to key business districts in the city.\nIt’s important to note that reducing car-dependency doesn’t necessarily mean removing cars from the streets. As professor Dorel Soares Ramos, from the University of São Paulo, explains “renewable gaseous fuels” such as “biomethane” are considered “green for fleet adaptation”2. Similarly, electric automobiles will contribute to lowering carbon emissions. However, irrespective of the energy source for cars, we must still address the numerous negative externalities associated with their use.\n\n\nCar-free households seem to be spatially concentrated at peripheral regions of the city, very far away from any of the cities CBDs. The notable exception is the green mass right at the “old” CBD (Centro Histórico). While this region has very high share of car-free households, it should be noted that is also is not a very populated region. The “newer” CBDs of Paulista and Itaim have much lower shares of car-free households.\n\n\n\n\n\n\n\n\n\nClose to half of São Paulo’s households are entirely car-free, and in certain regions, virtually every household opts to go without any cars.\n\n\n\n\n\n\n\n\n\nA distinct negative correlation exists between income and car-dependency in São Paulo. As income rises, households tend to demand more cars and the likelihood of finding car-free households decreases. The plot below shows average household income (in log-scale) against the percentage share of car-free households across all 329 zones. The size of each circle is scaled to represent the total population living in each zone. The richer, less populated zones, have the lowest shares of car-free households. Meanwhile, the more populated and poor households have some of the highest shares of car-free households3."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-cars/index.html#footnotes",
    "href": "posts/general-posts/2023-11-wz-cars/index.html#footnotes",
    "title": "Weekly Viz: Car Dependency in São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcluding zero-income and zero-population OD zones.↩︎\nOriginal quote: “Um ponto muito importante também é utilização, por exemplo, de combustíveis como o biometano e os combustíveis gasosos renováveis que são considerados verdes para a adaptação da frota”.↩︎\nGiven the considerable variation in OD zone sizes, one could argue that a more appropriate measure would be to utilize population density instead of total population. However, showcasing total population figures makes it easier to understand the overall impact on emissions.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "While some find it odd to rank movie tastes, I’ve always enjoyed “greatest of all times” lists. When not taken too literally, these lists provide a resourceful almanac of good movies, including classics, and culturally relevant movies. By construction, these lists will always be lacking and always be biased: it’s a subjective evaluation of the best of the best films after all.\nTraditionally, these lists were penned by movie critics or experts; more recently, however, there are several “greatest of all times” lists voted by the public. Perhaps the most well known of these is the Top 250 IMDB. As of December 2024, the top 250 films alone aggregate almost 180 million votes.\nWhen comparing these lists, the ones made by critics and those voted by the broader public, one notices a kind “temporal bias” in the rankings. New movies, released in the past 10-15 years, rarely appear in prestigious lists such as the BFI’s Greatest Films. In fact, in it’s most recent edition, published in 2022, only 24 of the 264 films were released in the 21st Century (less than 10%). On the flip side, online rankings such as the Top 250 IMDB are flooded with new releases. Almost 36% of the films featured in the Top 250 IMDB were released from 2001 to 2021.\nIn this post I delve into the data to try to visualize these patterns. Are movie critics biased towards a certain period of cinema? Are new movies overrated? Are they underrated?\n\n\nThere are several “greatest movies of all time” lists. Comparing them directly is often impossible due to conflicting methodologies. For simplicity, I’ll delve mostly into the Top 250 IMDB and the BFI Greatest Films of All Time. IMDB’s ranking compiles the largest number of online votes and has a relatively sturdy methodology for both its ratings and its rankings. The BFI list compiles votes from academics, critics, curators, archivists, and programmers. Its most recent edition was published in 2022.\nThe lists are relatively comparable in size: IMDB’s has (obviously) 250 films while BFI’s has 264. While BFI’s list has ties, I ignore these to make comparisons simpler.\nIn a latter part of the post I extend the analysis to other lists published by media outlets such as Variety and TimeOut and also to other online sites such as Letterboxd.\n\n\nGetting all the data involves a lot of webscraping. More information on the data collection process can be found on my Github.\n\n\n\n\n\n\nLooking at the data by decade seems to reveal a clear pattern. BFI’s list peaks around the 1960’s, while IMDB’s list peaks around the 2000’s. There’s a small bump in the 1950’s but nothing noteworthy.\n\n\n\n\n\n\n\n\n\nWhile one could argue that IMDB’s ranking suggests some level of recency bias among the public, the numbers might also be a reflection of (1) the age demographic of online voters; (2) the (lack of) availability of older films, specially in mainstream streaming services.\n\n\n\nGoing into greater details reveals some curious features about how these lists compare. Some films appear to be appreciated by both the public and the critics. Films like Kurosawa’s Seven Samurai, Hitchcock’s Psycho, and Coppola’s The Godfather all rank highly in both lists.\nThere are, of course, divergences: some films, like Metropolis (1927) and The Apartment (1960) are slightly favored by the BFI’s critics; meanwhile, Ran (1985) and Modern Times (1936) are favored by the public. These divergences don’t seem to correlate directly with the year of release of each movie or even with its director.\nIts important to note that rankings work in a counter-intuitive fashion to most graphics, since the best (highest-ranking) films are associated with lower numbers and appear on the left. To make reading the graphics easier, colors and symbols were used to identify which films ranked higher on each list.\n\n\n\n\n\n\n\n\n\n\n\n\nLooking even deeper into the discrepancies between the lists reveals an interesting trend. The movies on the top of the plot are overwhelmingly favored by the critics: these include Tokyo Story, Citizen Kane, Barry Lyndon, and The Third Man. Movies more to the bottom of the list are favorites of the public, including: Raiders of Lost Ark, The Matrix, Star Wars - New Hope, and Pulp Fiction.\nWhile year of release seems to play a significant role, one can also theorize that the content of these films plays a much larger one.\n\n\n\n\n\n\n\n\n\nFinally, it’s also important to note movies that appear exclusively in one list. These last two tables more clearly reflect the difference between critically appraised movies and favorites of the public.\nThe best ranking films in BFI’s list that don’t appear at all in IMDB’s list include The Rules of the Game (1939), Persona (1966), and Mulholland Drive (2001). This is definitely a very snobish list, but again, year of release doesn’t seem to matter much.\n\n\n\n\n\n\nHighest ranked BFI films not appearing on IMDB\n\n\nName\nYear\nRank (BFI)\n\n\n\n\nJeanne Dielman, 23, quai du commerce, 1080 Bruxelles\n1975\n1\n\n\nIn the Mood for Love\n2000\n5\n\n\nBeau Travail\n1999\n7\n\n\nMulholland Drive\n2001\n8\n\n\nMan with a Movie Camera\n1929\n9\n\n\nSunrise\n1927\n11\n\n\nThe Rules of the Game\n1939\n13\n\n\nCléo from 5 to 7\n1962\n14\n\n\nThe Searchers\n1956\n15\n\n\nMeshes of the Afternoon\n1943\n16\n\n\nClose-Up\n1990\n17\n\n\nPersona\n1966\n18\n\n\nLate Spring\n1949\n22\n\n\nPlaytime\n1967\n23\n\n\nDo the Right Thing\n1989\n24\n\n\n\n\n\n\n\nThe best ranking films in IMDB’s list that don’t appear at all in BFI’s list include The Lord of the Rings trilogy (2001-2003), The Shawshank Redemption (1994), and several Christopher Nolan movies. Outside of Lumet’s 12 Angry Men (1957) most of this list heavily skewed to the 1990’s onwards.\n\n\n\n\n\n\nHighest ranked IMDB films not appearing on BFI\n\n\nName\nYear\nRank (IMDB)\n\n\n\n\nThe Shawshank Redemption\n1994\n1\n\n\nThe Dark Knight\n2008\n3\n\n\n12 Angry Men\n1957\n5\n\n\nSchindler's List\n1993\n6\n\n\nThe Lord of the Rings: The Return of the King\n2003\n7\n\n\nThe Lord of the Rings: The Fellowship of the Ring\n2001\n9\n\n\nForrest Gump\n1994\n11\n\n\nThe Lord of the Rings: The Two Towers\n2002\n12\n\n\nFight Club\n1999\n13\n\n\nInception\n2010\n14\n\n\nStar Wars: Episode V - The Empire Strikes Back\n1980\n15\n\n\nOne Flew Over the Cuckoo's Nest\n1975\n18\n\n\nSe7en\n1995\n19\n\n\nInterstellar\n2014\n20\n\n\nDune: Part Two\n2024\n21\n\n\n\n\n\n\n\n\n\n\n\nGathering data from other sources reveals more curious patterns. The plot below aggregates data from 8 different rankings: four of them voted by critics and four of them voted by the public1. The latter are online polls voted by users and for simplicity are all shown in yellow; the former are polls voted by experts and critics and are shown in blue.\n\n\n\n\n\n\n\n\n\nAll four lists organized by critics present similar characteristics. They are shaped almost like a triangle with its vertex somewhere around the 1960’s with a small outlier peak in the late 1990’s. AFI’s ranking differs slightly from this pattern exhibiting a relatively larger peak in the 1940’s.\nThe rankings voted by the public are much more scattered. Criticker’s ranking is the most similar to the critics’. It has a large share of films in the late 1940’s and 1950’s, but fewer in the 1960’s and 70’s relative to the critics.\nIMDB’s ranking, as seen previously, is heavily skewed to right, with several films from the 1990’s and 2000’s. Empire’s ranking is similar in shape to IMDB’s, almost resembling a ladder that peaks in 1999.\nFinally, Letterboxd’s list seems to be a mixture of Criticker’s 1950’s bias with IMDB’s 1990’s bias.\n\n\nSimilarly to the BFI x IMDB comparison, these lists aren’t directly comparable. Most importantly, they were (1) published at different times2; and (2) have different sample sizes3.\nA simple way to compare the similarity between these lists is to count the overlaps in films. That is, how many films in list A are present in list B. To overcome the differences in sample size, I truncate the larger list by the smaller list size. When comparing, for instance, the Variety Top 100 with the IMDB Top 250, I simply truncate the IMDB list at 100.\nThis should work well for most comparisons except for AFI and Empire. AFI’s ranking lists only American movies up to 2000; and Empire ranks movies only up until 2008.\nThe plot below shows this simple measure of overlap where the number represents the percentage of overlap between a pair of lists. This means that 64.9% of the films listed on Criticker also appear on Letterboxd’s Top 250.\n\n\n\n\n\n\n\n\n\nOverall, the public rankings are most similar to other public rankings and dissimilar to critic’s rankings. Letterboxd has a high similarity with Criticker and IMDB and a low similarity with Variety and Timeout. Likewise, Variety’s list is most similar to AFI, BFI, and Timeout."
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#rankings",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#rankings",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "There are several “greatest movies of all time” lists. Comparing them directly is often impossible due to conflicting methodologies. For simplicity, I’ll delve mostly into the Top 250 IMDB and the BFI Greatest Films of All Time. IMDB’s ranking compiles the largest number of online votes and has a relatively sturdy methodology for both its ratings and its rankings. The BFI list compiles votes from academics, critics, curators, archivists, and programmers. Its most recent edition was published in 2022.\nThe lists are relatively comparable in size: IMDB’s has (obviously) 250 films while BFI’s has 264. While BFI’s list has ties, I ignore these to make comparisons simpler.\nIn a latter part of the post I extend the analysis to other lists published by media outlets such as Variety and TimeOut and also to other online sites such as Letterboxd.\n\n\nGetting all the data involves a lot of webscraping. More information on the data collection process can be found on my Github."
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-vs-public",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-vs-public",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "Looking at the data by decade seems to reveal a clear pattern. BFI’s list peaks around the 1960’s, while IMDB’s list peaks around the 2000’s. There’s a small bump in the 1950’s but nothing noteworthy.\n\n\n\n\n\n\n\n\n\nWhile one could argue that IMDB’s ranking suggests some level of recency bias among the public, the numbers might also be a reflection of (1) the age demographic of online voters; (2) the (lack of) availability of older films, specially in mainstream streaming services.\n\n\n\nGoing into greater details reveals some curious features about how these lists compare. Some films appear to be appreciated by both the public and the critics. Films like Kurosawa’s Seven Samurai, Hitchcock’s Psycho, and Coppola’s The Godfather all rank highly in both lists.\nThere are, of course, divergences: some films, like Metropolis (1927) and The Apartment (1960) are slightly favored by the BFI’s critics; meanwhile, Ran (1985) and Modern Times (1936) are favored by the public. These divergences don’t seem to correlate directly with the year of release of each movie or even with its director.\nIts important to note that rankings work in a counter-intuitive fashion to most graphics, since the best (highest-ranking) films are associated with lower numbers and appear on the left. To make reading the graphics easier, colors and symbols were used to identify which films ranked higher on each list.\n\n\n\n\n\n\n\n\n\n\n\n\nLooking even deeper into the discrepancies between the lists reveals an interesting trend. The movies on the top of the plot are overwhelmingly favored by the critics: these include Tokyo Story, Citizen Kane, Barry Lyndon, and The Third Man. Movies more to the bottom of the list are favorites of the public, including: Raiders of Lost Ark, The Matrix, Star Wars - New Hope, and Pulp Fiction.\nWhile year of release seems to play a significant role, one can also theorize that the content of these films plays a much larger one.\n\n\n\n\n\n\n\n\n\nFinally, it’s also important to note movies that appear exclusively in one list. These last two tables more clearly reflect the difference between critically appraised movies and favorites of the public.\nThe best ranking films in BFI’s list that don’t appear at all in IMDB’s list include The Rules of the Game (1939), Persona (1966), and Mulholland Drive (2001). This is definitely a very snobish list, but again, year of release doesn’t seem to matter much.\n\n\n\n\n\n\nHighest ranked BFI films not appearing on IMDB\n\n\nName\nYear\nRank (BFI)\n\n\n\n\nJeanne Dielman, 23, quai du commerce, 1080 Bruxelles\n1975\n1\n\n\nIn the Mood for Love\n2000\n5\n\n\nBeau Travail\n1999\n7\n\n\nMulholland Drive\n2001\n8\n\n\nMan with a Movie Camera\n1929\n9\n\n\nSunrise\n1927\n11\n\n\nThe Rules of the Game\n1939\n13\n\n\nCléo from 5 to 7\n1962\n14\n\n\nThe Searchers\n1956\n15\n\n\nMeshes of the Afternoon\n1943\n16\n\n\nClose-Up\n1990\n17\n\n\nPersona\n1966\n18\n\n\nLate Spring\n1949\n22\n\n\nPlaytime\n1967\n23\n\n\nDo the Right Thing\n1989\n24\n\n\n\n\n\n\n\nThe best ranking films in IMDB’s list that don’t appear at all in BFI’s list include The Lord of the Rings trilogy (2001-2003), The Shawshank Redemption (1994), and several Christopher Nolan movies. Outside of Lumet’s 12 Angry Men (1957) most of this list heavily skewed to the 1990’s onwards.\n\n\n\n\n\n\nHighest ranked IMDB films not appearing on BFI\n\n\nName\nYear\nRank (IMDB)\n\n\n\n\nThe Shawshank Redemption\n1994\n1\n\n\nThe Dark Knight\n2008\n3\n\n\n12 Angry Men\n1957\n5\n\n\nSchindler's List\n1993\n6\n\n\nThe Lord of the Rings: The Return of the King\n2003\n7\n\n\nThe Lord of the Rings: The Fellowship of the Ring\n2001\n9\n\n\nForrest Gump\n1994\n11\n\n\nThe Lord of the Rings: The Two Towers\n2002\n12\n\n\nFight Club\n1999\n13\n\n\nInception\n2010\n14\n\n\nStar Wars: Episode V - The Empire Strikes Back\n1980\n15\n\n\nOne Flew Over the Cuckoo's Nest\n1975\n18\n\n\nSe7en\n1995\n19\n\n\nInterstellar\n2014\n20\n\n\nDune: Part Two\n2024\n21"
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-x-public---2",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-x-public---2",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "Gathering data from other sources reveals more curious patterns. The plot below aggregates data from 8 different rankings: four of them voted by critics and four of them voted by the public1. The latter are online polls voted by users and for simplicity are all shown in yellow; the former are polls voted by experts and critics and are shown in blue.\n\n\n\n\n\n\n\n\n\nAll four lists organized by critics present similar characteristics. They are shaped almost like a triangle with its vertex somewhere around the 1960’s with a small outlier peak in the late 1990’s. AFI’s ranking differs slightly from this pattern exhibiting a relatively larger peak in the 1940’s.\nThe rankings voted by the public are much more scattered. Criticker’s ranking is the most similar to the critics’. It has a large share of films in the late 1940’s and 1950’s, but fewer in the 1960’s and 70’s relative to the critics.\nIMDB’s ranking, as seen previously, is heavily skewed to right, with several films from the 1990’s and 2000’s. Empire’s ranking is similar in shape to IMDB’s, almost resembling a ladder that peaks in 1999.\nFinally, Letterboxd’s list seems to be a mixture of Criticker’s 1950’s bias with IMDB’s 1990’s bias.\n\n\nSimilarly to the BFI x IMDB comparison, these lists aren’t directly comparable. Most importantly, they were (1) published at different times2; and (2) have different sample sizes3.\nA simple way to compare the similarity between these lists is to count the overlaps in films. That is, how many films in list A are present in list B. To overcome the differences in sample size, I truncate the larger list by the smaller list size. When comparing, for instance, the Variety Top 100 with the IMDB Top 250, I simply truncate the IMDB list at 100.\nThis should work well for most comparisons except for AFI and Empire. AFI’s ranking lists only American movies up to 2000; and Empire ranks movies only up until 2008.\nThe plot below shows this simple measure of overlap where the number represents the percentage of overlap between a pair of lists. This means that 64.9% of the films listed on Criticker also appear on Letterboxd’s Top 250.\n\n\n\n\n\n\n\n\n\nOverall, the public rankings are most similar to other public rankings and dissimilar to critic’s rankings. Letterboxd has a high similarity with Criticker and IMDB and a low similarity with Variety and Timeout. Likewise, Variety’s list is most similar to AFI, BFI, and Timeout."
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#footnotes",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#footnotes",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be fair, Empire’s list combines both critics and general public. The numbers however, skew it toward the public.↩︎\nThe Criticker (2022), Letterboxd (2022), IMDB (2024), TimeOut (2024), BFI (2022), and Variety (2022) lists are relatively comparable among themselves. Empire’s ranking was published in 2008. AFI’s ranking is more recent but doesn’t include films released after the 2000’s.↩︎\nFor some reason 250, and its multiples, is a favorite among the public and 100 is preferred by the critics.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html",
    "href": "posts/general-posts/2023-12-carros-renda/index.html",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Este post investiga a relação entre a “demanda por automóveis”, entendida simplesmente como a posse (ou não) de um automóvel. Usando dados da Pesquisa Origem e Destino do Metrô de 2017, estima-se um modelo de escolha discreta para investigar o efeito da renda domiciliar sobre a escolha de ter um carro. Encontrou-se um efeito forte e significante da renda sobre a “demanda” por carros. Idade, educação e filhos também são fatores importantes e que aumentam a probabilidade do domicílio ter um automóvel. O único fator encontrado que reduz a probabilidade do domicílio ter um automóvel é ele ser chefiado por uma mulher.\nVale notar que não se pode falar propriamente em “demanda” por automóveis já que temos somente dados de um único momento no tempo. Eventualmente, quando os dados da POD 2022 forem liberados, será possível construir uma curva de demanda por automóveis.\n\n\nA Região Metropolitana de São Paulo abriga aproximadamente 20 milhões de pessoas e gera mais de 40 milhões de viagens todos os dias. Segundo Relatório do Metrô as viagens na Região Metropolitana de São Paulo dividem-se da seguinte maneira: um terço das viagens é feita por modos não-motorizados (a pé ou de bicicleta), enquanto dois terços das viagens é feito por modos motorizados. Neste último grupo, 55% das viagens são feitas com modais coletivos (ônibus, metrô, trem) e 45% das viagens são feitas com modais individuais (carro, táxi, moto). Ao todo, as viagens em automóveis particulares (excluindo táxis) representam cerca de 27% de todas as viagens diárias; ou seja, cerca de 1 em cada 4 viagens na RMSP é feita com carro particular.\nO diagrama abaixo esquematiza todas as viagens diárias realizadas na RMSP. Vale notar que os dados são de 2017, então muitas das estações das linhas 4-amarela e 5-coral ainda estavam sob construção. Similarmente, as linhas 13-Jade, 15-Prata (monotrilho) ainda não estavam operando. Por simplicidade considerei o universo de todas as viagens: ainda que os deslocamentos casa-trabalho componham a maior parte deste conjunto, inclui-se todo tipo de deslocamento.\n\n\n\n\n\n\n\n\n\nPouco mais da metade dos domicílios na RMSP possui algum automóvel: 53% dos domicílios possui ao menos um carro, enquanto 47% dos domicílios não possui carro particular. Mais especificamente, 43,8% possui somente um carro. Domicílios com 2 automóveis são apenas 8% do total e domicílios com 3 automóveis ou mais são pouco mais de 1% do total.\n\n\n\n\n\n\n\n\n\nNa média da RMSP, há 0,635 carros por domicílios. Na capital, o número é similar, 0,6. Olhando para os distritos de São Paulo, vê-se que os menos “carro-dependentes” são distritos centrais como República e Sé. Distritos de baixa renda média como Parelheiros e Cidade Tiradentes também tem uma baixa razão de carros por domicílio.\nOs distritos mais carro-dependentes têm rendas elevadas e estão dentro ou próximos do Centro Expandido da cidade. É curioso notar que bairros verticalizados como Moema e Morumbi aparecem junto com o distrito de Alto de Pinheiros que é majoritariamente composto por residências horizontais. Isto sugere que a verticalização não acompanha menor dependência do automóvel particular.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a taxa de motorização, a razão de carros por domicílios, nos distritos da RMSP. Vê-se um padrão espacial onde as regiões com maiores taxas de motorização concentram-se no quadrante sudoeste do Centro Expandido da capital. No começo da zona oeste, na região do Tatuapé, também vê-se taxas mais elevadas. Fora da capital, vale notar que a cidade de Santana de Parnaíba tem elevada taxa de motorização, comparável aos distritos mais carro-dependentes de São Paulo.\n\n\n\n\n\n\nO share de domicílios sem carro segue um padrão espacial similar. O mapa detalha as zonas OD da cidade de São Paulo. Em outro post, discuti a distribuição desta métrica na cidade.\n\n\n\n\n\n\n\n\n\n\n\n\nA demanda por automóveis tende a aumentar junto com a renda das famílias. Mesmo famílias que moram em regiões centrais, próximas de polos de empregos e com boa oferta de infraestrutura urbana, preferem ter mais automóveis. O gráfico abaixo mostra a relação entre a renda domiciliar média e a razão de carros por domicílio; os dados são agregados por zona OD. A linha de tendência mostra uma relação quadrática ponderada (usando a população total de cada zona como peso).\nÉ interessante notar as zonas que “fogem” da tendência esperada. A região do Morumbi, por exemplo, tem renda similar a do Paraíso, mas tem uma taxa de motorização consideravelmente melhor. Possivelmente, isto reflete a infraestrutura de cada bairro: o Morumbi não tem acesso fácil nem a trem e nem a metrô; já o bairro do Paraíso tem acesso fácil a duas linhas de metrô, ciclovias e corredores de ônibus.\n\n\n\n\n\n\n\n\n\n\n\nA demanda por automóveis também parece variar por região. O gráfico abaixo agrupa as zonas por regiões: Centro Expandido, São Paulo (capital) e RMSP (resto da região metropolitana). Nota-se que as zonas dentro do CE da capital tem uma tendência a ter menos carros do que regiões de renda equivalente na RMSP.\nNote que agora os gráficos agora mostram a renda domiciliar per capita em escala logarítmica e usa-se uma tendência linear. A regressão continua sendo ponderada pela população total de cada zona, mas todos os círculos no painel da direita tem igual tamanho para facilitar a visualização dos dados.\n\n\n\n\n\n\n\n\n\n\n\n\nIndo um pouco mais a fundo, pode-se fazer uma regressão linear para melhor avaliar a diferença entre os grupos. Visualmente, as zonas dentro do CE parecem ter uma proporção menor de carros por habitante do que as zonas na RMSP, relativamente à renda familiar. A tabela abaixo mostra o resultado da regressão, onde adiciona-se também algumas variáveis auxiliares como a proporção de adultos (18-64 anos) que habita na zona e a densidade populacional da zona.\nA relação de interesse aparece nas duas últimas linhas que indica a relação entre a renda domiciliar per capita e a localização (estrato) para a proporção de carros por domicílio. Tanto regiões dentro da capital como dentro do CE apresentam uma menor proporção de carros por domicílios, relativamente às zonas da RMSP.\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nProp. Adultos (%)\n-0.00298\n-0.00762, 0.00166\n0.2\n\n\nDensidade Populacional\n-0.00058\n-0.00076, -0.00040\n&lt;0.001\n\n\nProp Ensino Superior (%)\n0.00026\n-0.00229, 0.00280\n0.8\n\n\nRenda Domiciliar per capita (log)\n0.54064\n0.45712, 0.62415\n&lt;0.001\n\n\nEstrato\n\n\n\n\n\n    RMSP\n—\n—\n\n\n\n    São Paulo\n0.49744\n-0.07001, 1.06489\n0.086\n\n\n    Centro Expandido\n0.55672\n-0.34798, 1.46142\n0.2\n\n\nRenda Domiciliar per capita (log) * Estrato\n\n\n\n\n\n    Renda Domiciliar per capita (log) * São Paulo\n-0.07532\n-0.15368, 0.00303\n0.060\n\n\n    Renda Domiciliar per capita (log) * Centro Expandido\n-0.10456\n-0.22138, 0.01227\n0.079\n\n\nR²\n0.664\n\n\n\n\nAdjusted R²\n0.658\n\n\n\n\nAIC\n-464\n\n\n\n\nSigma\n25.1\n\n\n\n\nLog-likelihood\n242\n\n\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nVisualmente, pode-se verificar isto no gráfico abaixo que mostra a relação entre a renda domiciliar per capita e a “demanda por automóveis” em cada um dos estratos geográficos.\n\n\n\n\n\n\n\n\n\n\n\n\nO mapa abaixo apresenta visualmente a relação entre renda e a taxa de automóveis. A classificação das zonas é simples e segue o algoritmo de Jenks sem considerar explicitamente a dependência espacial entre as zonas. Ainda assim, é possível distinguir entre as regiões. A região da Paraíso, por exemplo, é de renda alta e taxa de motorização média; Pinheiros é de renda média e taxa de motorização média; e Jardim Europa é de renda alta e taxa alta. Algumas regiões do Centro-Sul como Brooklin e Campo Belo são médio-alto (renda média e taxa alta). O centro antigo da cidade é baixo-baixo, assim como boa parte da periferia da cidade.\n\n\n\n\n\n\nA maior parte das zonas classificadas é do tipo “low-low”, isto é, de renda baixa e de baixa razão carros/domicílios. De fato, quase metada das zonas entra no grupo de “renda baixa” (R$2000 a R$4500). Dentre as zonas de renda alta (&gt; R$ 8000), a maior parte tem uma razão alta de carros/domicílio; ainda assim, uma proporção relativamente expressiva têm razão média de carros/domicílios (36%). Apenas duas zonas (MASP e Rodrigues Alves) são classificados como renda alta e baixa razão carros/domicílios.\n\n\n\n\n\n\n\n\nRenda - Carros\nNúmero de Zonas\nProporção de zonas\n\n\n\n\nLow-Low\n175\n35.57%\n\n\nMedium-Medium\n114\n23.17%\n\n\nLow-Medium\n63\n12.80%\n\n\nHigh-High\n52\n10.57%\n\n\nMedium-Low\n42\n8.54%\n\n\nHigh-Medium\n31\n6.30%\n\n\nMedium-High\n13\n2.64%\n\n\nHigh-Low\n2\n0.41%\n\n\n\n\n\n\n\nA tabela abaixo ilustra a classificação acima trazendo alguns exemplos de cada grupo.\n\n\n\n\n\n\n\n\nRenda - Carros\nExemplos de Região\n\n\n\n\nLow-Low\nCascatas, Morro do Índio, Sacadura Cabral, Itapevi, Jardim Presidente Dutra\n\n\nLow-Medium\nEstrada do Carneiro, Riacho Grande, Taboão, Melhoramentos, Cotia\n\n\nMedium-Low\nPenha, Vila São Rafael, Quarta Parada, Jardim Primavera, Belenzinho\n\n\nMedium-Medium\nJardim Piratininga, Vila Miranda, Itaquera, Vila Esperança, Vila Zelina\n\n\nMedium-High\nVila Bertioga, Rudge Ramos, Parque da Mooca, Granja Viana, Bosque da Saúde\n\n\nHigh-Low\nMasp, Rodrigues Alves\n\n\nHigh-Medium\nBela Vista, Vila Sônia, Chácara Itaim, Alfredo Pujol, Vila Clementino\n\n\nHigh-High\nSanta Cruz, Parque Ibirapuera, Gavião Peixoto, Santo Amaro, Vila Nova Conceição\n\n\n\n\n\n\n\n\n\n\n\nAté agora este post assumiu que o deslocamento total de carro de uma família é, de alguma forma, proporcional ao número de automóveis que ela possui. Isto é, o número total de quilômetros rodados em automóvel particular é uma função da posse do automóvel.\nEsta é uma hipótese razoável, mas como viu-se acima, a maioria dos domicílios ou não têm um carro ou tem somente um. Assim, é possível fazer ainda mais uma simplificação: considerar a posse ou não de um automóvel (ou mais).\n\n\nHá dois fatores bastante intuitivos, indicados pela literatura, que estão relacionados com a posse de automóvel: renda e idade. O gráfico abaixo mostra a relação entre a posse de automóveis e a idade do chefe da família. Como se vê, há uma relação não-linear entre as variáveis: a proporção de famílias com automóvel cresce até certo ponto e depois diminui.\nOs pontos agrupam as idades em grupos de cinco anos (18-22, 23-27, etc.) e representam a média de cada grupo. A linha de ajuste é de uma regressão polinomial de segundo grau, ajustada pelo peso proporcional da cada grupo na população total.\n\n\n\n\n\n\n\n\n\nO gráfico de colunas abaixo mostra a proporção de domicílios que possui ao menos um carro por decil de renda. Abaixo dos eixos, apresenta-se o intervalo de renda considerado. As barrinhas pequenas são o intervalo de confiança de cada estimativa.\n\n\n\n\n\n\n\n\n\n\n\n\nComo agora temos uma variável binária (posse ou não-posse de automóvel) pode-se usar um modelo de escolha discreta para estudar a relação entre renda e automóveis. Na regressão abaixo, mostro os resultados de uma regressão Logística e de uma regressão Probit. A regressão é feita sobre os microdados da Pesquisa Origem Destino (identificados por domicílio) usando os fatores de expansão como pesos.\nPara simplificar a análise, restringiu-se a amostra às zonas dentro da cidade de São Paulo (capital). Além disso, removeu-se domicílios com renda muita baixa (menos de 1/4 do salário mínimo da época) e domicílios chefiados por um responsável com menos de 18 anos. Esta regressão inclui algumas variáveis adicionais relevantes:\n\nCriança: variável binária que indica se há pelo menos uma criança no domicílio (menor de 18 anos com vínculo familiar)\nEnsino superior: variável binária que indica se o responsável pelo domicílio possui ensino superior.\nEmpregado: variável binária que indica se o responsável pelo domicílio estava empregado.\nFeminino: variável binária que indica se a responsável pelo domicílio se identifica como do sexo feminino.\nCentro Exp: variável binária que indica se a zona do domicílio se encontra dentro do Centro Expandido da cidade.\nA variável de idade foi “discretizada” em grupos para melhor capturar o seu efeito não-linear.\n\nA tabela abaixo ilustra o efeito das variáveis binárias acima sobre a frequência de domicílios em relação à posse de automóveis. A tabela soma o número de domicílios com ou sem automóvel e compara as famílias com e sem crianças. Proporcionalmente, as famílias com ao menos um filho tem uma chance cerca de 1,5 maior de ter um carro (em contraste com uma família sem filhos).\n\n\n\n\n\nPara levar em consideração o efeito espacial das variáveis incluiu-se efeitos fixos de cada zona OD. Existem métodos mais sofisticados para modelar a dependências espacial entre as regiões, mas para os propósitos deste post esta abordagem é suficiente. Os resultados das duas regressões é apresentado abaixo.\nVale notar que apresento os odd-ratios ao invés dos coeficientes na regressão logística1. Assim, pode-se ver mais imediatamente o efeito das variáveis. De maneira geral, todos os fatores tem efeito de aumentar a chance que um domicílio tenha um automóvel; apenas “Feminino” e “Centro Exp.” têm valores menores do que um, indicando que domicílios chefiados por mulheres e/ou dentro do CE têm uma chance menor de ter automóvel. Um domicílio que recebe incremento de 1% na sua renda tem um aumento de 6,74% na chance de ter um carro. Similarmente, um domicílio com criança tem um aumento de 1,33 na chance de ter um carro em relação a um domicílio sem criança.\n\n\n\n\n\nA interpretação dos coeficientes de uma regressão Probit não é tão imediato. Vale notar que o sinal de todos os coeficientes corrobora o modelo Logit: com exceção de “feminino”, todas as variáveis têm efeito positivo, isto é, aumentam a probabilidade do domicílio possuir um automóvel.\nO gráfico abaixo simula a probabilidade esperada de um domicílio de Pinheiros chefiado por um indivíduo de 35 a 44 anos possuir um automóvel, condicional a alguns fatores:\n\n\n\nCriança. Se a família possui pelo menos um filho no domicílio.\nEnsino Universitário. Se a pessoa responsável pelo domicílio possui ensino supeior.\nRenda. O nível de renda domiciliar (ajustado pelo IPC-Fipe).\n\n\n\nEm todos os casos analisados, ter um filho aumenta a probabilidade de ter um automóvel, mas este efeito varia de acordo com a renda.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCornut, B., & Madre, J. L. (2017). A longitudinal perspective on car ownership and use in relation with income inequalities in the Paris metropolitan area. Transport Reviews, 37(2), 227-244.\nDargay, J. M. (2001). The effect of income on car ownership: evidence of asymmetry. Transportation Research Part A: Policy and Practice, 35(9), 807-821.\nDargay, J., & Gately, D. (1999). Income’s effect on car and vehicle ownership, worldwide: 1960–2015. Transportation Research Part A: Policy and Practice, 33(2), 101-138.\nNolan, A. (2010). A dynamic analysis of household car ownership. Transportation research part A: policy and practice, 44(6), 446-455."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#são-paulo-fatos-gerais",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#são-paulo-fatos-gerais",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "A Região Metropolitana de São Paulo abriga aproximadamente 20 milhões de pessoas e gera mais de 40 milhões de viagens todos os dias. Segundo Relatório do Metrô as viagens na Região Metropolitana de São Paulo dividem-se da seguinte maneira: um terço das viagens é feita por modos não-motorizados (a pé ou de bicicleta), enquanto dois terços das viagens é feito por modos motorizados. Neste último grupo, 55% das viagens são feitas com modais coletivos (ônibus, metrô, trem) e 45% das viagens são feitas com modais individuais (carro, táxi, moto). Ao todo, as viagens em automóveis particulares (excluindo táxis) representam cerca de 27% de todas as viagens diárias; ou seja, cerca de 1 em cada 4 viagens na RMSP é feita com carro particular.\nO diagrama abaixo esquematiza todas as viagens diárias realizadas na RMSP. Vale notar que os dados são de 2017, então muitas das estações das linhas 4-amarela e 5-coral ainda estavam sob construção. Similarmente, as linhas 13-Jade, 15-Prata (monotrilho) ainda não estavam operando. Por simplicidade considerei o universo de todas as viagens: ainda que os deslocamentos casa-trabalho componham a maior parte deste conjunto, inclui-se todo tipo de deslocamento."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Pouco mais da metade dos domicílios na RMSP possui algum automóvel: 53% dos domicílios possui ao menos um carro, enquanto 47% dos domicílios não possui carro particular. Mais especificamente, 43,8% possui somente um carro. Domicílios com 2 automóveis são apenas 8% do total e domicílios com 3 automóveis ou mais são pouco mais de 1% do total.\n\n\n\n\n\n\n\n\n\nNa média da RMSP, há 0,635 carros por domicílios. Na capital, o número é similar, 0,6. Olhando para os distritos de São Paulo, vê-se que os menos “carro-dependentes” são distritos centrais como República e Sé. Distritos de baixa renda média como Parelheiros e Cidade Tiradentes também tem uma baixa razão de carros por domicílio.\nOs distritos mais carro-dependentes têm rendas elevadas e estão dentro ou próximos do Centro Expandido da cidade. É curioso notar que bairros verticalizados como Moema e Morumbi aparecem junto com o distrito de Alto de Pinheiros que é majoritariamente composto por residências horizontais. Isto sugere que a verticalização não acompanha menor dependência do automóvel particular.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a taxa de motorização, a razão de carros por domicílios, nos distritos da RMSP. Vê-se um padrão espacial onde as regiões com maiores taxas de motorização concentram-se no quadrante sudoeste do Centro Expandido da capital. No começo da zona oeste, na região do Tatuapé, também vê-se taxas mais elevadas. Fora da capital, vale notar que a cidade de Santana de Parnaíba tem elevada taxa de motorização, comparável aos distritos mais carro-dependentes de São Paulo.\n\n\n\n\n\n\nO share de domicílios sem carro segue um padrão espacial similar. O mapa detalha as zonas OD da cidade de São Paulo. Em outro post, discuti a distribuição desta métrica na cidade."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio-e-renda",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio-e-renda",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "A demanda por automóveis tende a aumentar junto com a renda das famílias. Mesmo famílias que moram em regiões centrais, próximas de polos de empregos e com boa oferta de infraestrutura urbana, preferem ter mais automóveis. O gráfico abaixo mostra a relação entre a renda domiciliar média e a razão de carros por domicílio; os dados são agregados por zona OD. A linha de tendência mostra uma relação quadrática ponderada (usando a população total de cada zona como peso).\nÉ interessante notar as zonas que “fogem” da tendência esperada. A região do Morumbi, por exemplo, tem renda similar a do Paraíso, mas tem uma taxa de motorização consideravelmente melhor. Possivelmente, isto reflete a infraestrutura de cada bairro: o Morumbi não tem acesso fácil nem a trem e nem a metrô; já o bairro do Paraíso tem acesso fácil a duas linhas de metrô, ciclovias e corredores de ônibus.\n\n\n\n\n\n\n\n\n\n\n\nA demanda por automóveis também parece variar por região. O gráfico abaixo agrupa as zonas por regiões: Centro Expandido, São Paulo (capital) e RMSP (resto da região metropolitana). Nota-se que as zonas dentro do CE da capital tem uma tendência a ter menos carros do que regiões de renda equivalente na RMSP.\nNote que agora os gráficos agora mostram a renda domiciliar per capita em escala logarítmica e usa-se uma tendência linear. A regressão continua sendo ponderada pela população total de cada zona, mas todos os círculos no painel da direita tem igual tamanho para facilitar a visualização dos dados.\n\n\n\n\n\n\n\n\n\n\n\n\nIndo um pouco mais a fundo, pode-se fazer uma regressão linear para melhor avaliar a diferença entre os grupos. Visualmente, as zonas dentro do CE parecem ter uma proporção menor de carros por habitante do que as zonas na RMSP, relativamente à renda familiar. A tabela abaixo mostra o resultado da regressão, onde adiciona-se também algumas variáveis auxiliares como a proporção de adultos (18-64 anos) que habita na zona e a densidade populacional da zona.\nA relação de interesse aparece nas duas últimas linhas que indica a relação entre a renda domiciliar per capita e a localização (estrato) para a proporção de carros por domicílio. Tanto regiões dentro da capital como dentro do CE apresentam uma menor proporção de carros por domicílios, relativamente às zonas da RMSP.\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nProp. Adultos (%)\n-0.00298\n-0.00762, 0.00166\n0.2\n\n\nDensidade Populacional\n-0.00058\n-0.00076, -0.00040\n&lt;0.001\n\n\nProp Ensino Superior (%)\n0.00026\n-0.00229, 0.00280\n0.8\n\n\nRenda Domiciliar per capita (log)\n0.54064\n0.45712, 0.62415\n&lt;0.001\n\n\nEstrato\n\n\n\n\n\n    RMSP\n—\n—\n\n\n\n    São Paulo\n0.49744\n-0.07001, 1.06489\n0.086\n\n\n    Centro Expandido\n0.55672\n-0.34798, 1.46142\n0.2\n\n\nRenda Domiciliar per capita (log) * Estrato\n\n\n\n\n\n    Renda Domiciliar per capita (log) * São Paulo\n-0.07532\n-0.15368, 0.00303\n0.060\n\n\n    Renda Domiciliar per capita (log) * Centro Expandido\n-0.10456\n-0.22138, 0.01227\n0.079\n\n\nR²\n0.664\n\n\n\n\nAdjusted R²\n0.658\n\n\n\n\nAIC\n-464\n\n\n\n\nSigma\n25.1\n\n\n\n\nLog-likelihood\n242\n\n\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nVisualmente, pode-se verificar isto no gráfico abaixo que mostra a relação entre a renda domiciliar per capita e a “demanda por automóveis” em cada um dos estratos geográficos.\n\n\n\n\n\n\n\n\n\n\n\n\nO mapa abaixo apresenta visualmente a relação entre renda e a taxa de automóveis. A classificação das zonas é simples e segue o algoritmo de Jenks sem considerar explicitamente a dependência espacial entre as zonas. Ainda assim, é possível distinguir entre as regiões. A região da Paraíso, por exemplo, é de renda alta e taxa de motorização média; Pinheiros é de renda média e taxa de motorização média; e Jardim Europa é de renda alta e taxa alta. Algumas regiões do Centro-Sul como Brooklin e Campo Belo são médio-alto (renda média e taxa alta). O centro antigo da cidade é baixo-baixo, assim como boa parte da periferia da cidade.\n\n\n\n\n\n\nA maior parte das zonas classificadas é do tipo “low-low”, isto é, de renda baixa e de baixa razão carros/domicílios. De fato, quase metada das zonas entra no grupo de “renda baixa” (R$2000 a R$4500). Dentre as zonas de renda alta (&gt; R$ 8000), a maior parte tem uma razão alta de carros/domicílio; ainda assim, uma proporção relativamente expressiva têm razão média de carros/domicílios (36%). Apenas duas zonas (MASP e Rodrigues Alves) são classificados como renda alta e baixa razão carros/domicílios.\n\n\n\n\n\n\n\n\nRenda - Carros\nNúmero de Zonas\nProporção de zonas\n\n\n\n\nLow-Low\n175\n35.57%\n\n\nMedium-Medium\n114\n23.17%\n\n\nLow-Medium\n63\n12.80%\n\n\nHigh-High\n52\n10.57%\n\n\nMedium-Low\n42\n8.54%\n\n\nHigh-Medium\n31\n6.30%\n\n\nMedium-High\n13\n2.64%\n\n\nHigh-Low\n2\n0.41%\n\n\n\n\n\n\n\nA tabela abaixo ilustra a classificação acima trazendo alguns exemplos de cada grupo.\n\n\n\n\n\n\n\n\nRenda - Carros\nExemplos de Região\n\n\n\n\nLow-Low\nCascatas, Morro do Índio, Sacadura Cabral, Itapevi, Jardim Presidente Dutra\n\n\nLow-Medium\nEstrada do Carneiro, Riacho Grande, Taboão, Melhoramentos, Cotia\n\n\nMedium-Low\nPenha, Vila São Rafael, Quarta Parada, Jardim Primavera, Belenzinho\n\n\nMedium-Medium\nJardim Piratininga, Vila Miranda, Itaquera, Vila Esperança, Vila Zelina\n\n\nMedium-High\nVila Bertioga, Rudge Ramos, Parque da Mooca, Granja Viana, Bosque da Saúde\n\n\nHigh-Low\nMasp, Rodrigues Alves\n\n\nHigh-Medium\nBela Vista, Vila Sônia, Chácara Itaim, Alfredo Pujol, Vila Clementino\n\n\nHigh-High\nSanta Cruz, Parque Ibirapuera, Gavião Peixoto, Santo Amaro, Vila Nova Conceição"
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#carros-e-renda-analisando-a-relação",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#carros-e-renda-analisando-a-relação",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Até agora este post assumiu que o deslocamento total de carro de uma família é, de alguma forma, proporcional ao número de automóveis que ela possui. Isto é, o número total de quilômetros rodados em automóvel particular é uma função da posse do automóvel.\nEsta é uma hipótese razoável, mas como viu-se acima, a maioria dos domicílios ou não têm um carro ou tem somente um. Assim, é possível fazer ainda mais uma simplificação: considerar a posse ou não de um automóvel (ou mais).\n\n\nHá dois fatores bastante intuitivos, indicados pela literatura, que estão relacionados com a posse de automóvel: renda e idade. O gráfico abaixo mostra a relação entre a posse de automóveis e a idade do chefe da família. Como se vê, há uma relação não-linear entre as variáveis: a proporção de famílias com automóvel cresce até certo ponto e depois diminui.\nOs pontos agrupam as idades em grupos de cinco anos (18-22, 23-27, etc.) e representam a média de cada grupo. A linha de ajuste é de uma regressão polinomial de segundo grau, ajustada pelo peso proporcional da cada grupo na população total.\n\n\n\n\n\n\n\n\n\nO gráfico de colunas abaixo mostra a proporção de domicílios que possui ao menos um carro por decil de renda. Abaixo dos eixos, apresenta-se o intervalo de renda considerado. As barrinhas pequenas são o intervalo de confiança de cada estimativa.\n\n\n\n\n\n\n\n\n\n\n\n\nComo agora temos uma variável binária (posse ou não-posse de automóvel) pode-se usar um modelo de escolha discreta para estudar a relação entre renda e automóveis. Na regressão abaixo, mostro os resultados de uma regressão Logística e de uma regressão Probit. A regressão é feita sobre os microdados da Pesquisa Origem Destino (identificados por domicílio) usando os fatores de expansão como pesos.\nPara simplificar a análise, restringiu-se a amostra às zonas dentro da cidade de São Paulo (capital). Além disso, removeu-se domicílios com renda muita baixa (menos de 1/4 do salário mínimo da época) e domicílios chefiados por um responsável com menos de 18 anos. Esta regressão inclui algumas variáveis adicionais relevantes:\n\nCriança: variável binária que indica se há pelo menos uma criança no domicílio (menor de 18 anos com vínculo familiar)\nEnsino superior: variável binária que indica se o responsável pelo domicílio possui ensino superior.\nEmpregado: variável binária que indica se o responsável pelo domicílio estava empregado.\nFeminino: variável binária que indica se a responsável pelo domicílio se identifica como do sexo feminino.\nCentro Exp: variável binária que indica se a zona do domicílio se encontra dentro do Centro Expandido da cidade.\nA variável de idade foi “discretizada” em grupos para melhor capturar o seu efeito não-linear.\n\nA tabela abaixo ilustra o efeito das variáveis binárias acima sobre a frequência de domicílios em relação à posse de automóveis. A tabela soma o número de domicílios com ou sem automóvel e compara as famílias com e sem crianças. Proporcionalmente, as famílias com ao menos um filho tem uma chance cerca de 1,5 maior de ter um carro (em contraste com uma família sem filhos).\n\n\n\n\n\nPara levar em consideração o efeito espacial das variáveis incluiu-se efeitos fixos de cada zona OD. Existem métodos mais sofisticados para modelar a dependências espacial entre as regiões, mas para os propósitos deste post esta abordagem é suficiente. Os resultados das duas regressões é apresentado abaixo.\nVale notar que apresento os odd-ratios ao invés dos coeficientes na regressão logística1. Assim, pode-se ver mais imediatamente o efeito das variáveis. De maneira geral, todos os fatores tem efeito de aumentar a chance que um domicílio tenha um automóvel; apenas “Feminino” e “Centro Exp.” têm valores menores do que um, indicando que domicílios chefiados por mulheres e/ou dentro do CE têm uma chance menor de ter automóvel. Um domicílio que recebe incremento de 1% na sua renda tem um aumento de 6,74% na chance de ter um carro. Similarmente, um domicílio com criança tem um aumento de 1,33 na chance de ter um carro em relação a um domicílio sem criança.\n\n\n\n\n\nA interpretação dos coeficientes de uma regressão Probit não é tão imediato. Vale notar que o sinal de todos os coeficientes corrobora o modelo Logit: com exceção de “feminino”, todas as variáveis têm efeito positivo, isto é, aumentam a probabilidade do domicílio possuir um automóvel.\nO gráfico abaixo simula a probabilidade esperada de um domicílio de Pinheiros chefiado por um indivíduo de 35 a 44 anos possuir um automóvel, condicional a alguns fatores:\n\n\n\nCriança. Se a família possui pelo menos um filho no domicílio.\nEnsino Universitário. Se a pessoa responsável pelo domicílio possui ensino supeior.\nRenda. O nível de renda domiciliar (ajustado pelo IPC-Fipe).\n\n\n\nEm todos os casos analisados, ter um filho aumenta a probabilidade de ter um automóvel, mas este efeito varia de acordo com a renda."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#referências",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#referências",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Cornut, B., & Madre, J. L. (2017). A longitudinal perspective on car ownership and use in relation with income inequalities in the Paris metropolitan area. Transport Reviews, 37(2), 227-244.\nDargay, J. M. (2001). The effect of income on car ownership: evidence of asymmetry. Transportation Research Part A: Policy and Practice, 35(9), 807-821.\nDargay, J., & Gately, D. (1999). Income’s effect on car and vehicle ownership, worldwide: 1960–2015. Transportation Research Part A: Policy and Practice, 33(2), 101-138.\nNolan, A. (2010). A dynamic analysis of household car ownership. Transportation research part A: policy and practice, 44(6), 446-455."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#footnotes",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#footnotes",
    "title": "Carros e Renda em Sao Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIsto é equivalente a tomar a exponencial do coeficiente. Isto é, ao invés de mostrar \\(\\beta\\) mostra-se \\(exp(\\beta)\\).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html",
    "title": "Tendência e Sazonalidade",
    "section": "",
    "text": "Séries de tempo costumam exibir alguns padrões similares. Uma abordagem particularmente útil é de decompor uma série de tempo em componentes, que representam características específicas. Pode-se pensar numa série de tempo como uma conjunção de três componentes:\n\nTendência\nSazonalidade\nResíduo (resto, ruído, etc.)\n\nEm geral, a tendência varia pouco no tempo, segue algum ciclo longo, como a tendência de crescimento do PIB de um país. O movimento sazonal de uma série reflete algum tipo de variação períodica. Muitas séries possuem sazonalidade, como nfa demanda por energia elétrica ao longo dos trimestres, o número de nascimentos a cada mês, o número diário de acidentes de trânsito a cada semana, a temperatura média ao longo do dia, etc.\nEntender a sazonalidade de um fenômeno é essencial para a sua modelagem estatística. Vale notar que, usualmente, o interesse de economistas e econometristas não está na sazonalidade em si, mas sim na série dessazonalizada, isto é, livre de qualquer sazonalidade. Esta abordagem enfatiza mais a busca pela tendência “limpa” da série do que pelo efeito sazonal.\nA sazonalidade se expressa em várias frequências, mas a ênfase deste post será em sazonalidades mensais e trimestrais. Sazonalidades complexas (mistas, por exemplo) ou de alta frequência (intradiária, diária, semanal), em geral, exigem um pouco mais de esforço no setup e pacotes específicos1."
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#stl",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#stl",
    "title": "Tendência e Sazonalidade",
    "section": "STL",
    "text": "STL\nA decomposição STL é mais sofisticada do que a decomposição clássica. A metodologia STL foi apresentada no influente artigo STL: A Seasonal-Trend Decomposition Procedure Based on Loess dos autores Robert Cleveland, William Cleveland, Jean McRae e Irma Terpenning. Assim como a decomposição clássica, a decomposição STL divide uma série em três componentes: um componente de tendência (trend), uma componente sazonal (seasonal) e um componente aleatório (remainder).\nO STL foi feito para ser um método versátil, resistente a outliers e eficiente (do ponto de vista computacional). Tipicamente, a decomposição STL funciona com qualquer série (mesmo quando há observações ausentes) independentemente da sua frequência. Para modelar a tendência e sazonalidade da série, o STL usa uma regressão LOESS. Além de ser mais flexível do que a média móvel, que vimos acima, a regressão LOESS não perde observações da série.\nO código abaixo mostra como calcular a decomposição STL e apresenta os resultados visualmente usando a série co2 que vem pré-carregada no R. Esta série é similar a uma das utilizadas no artigo original.\n\nstl_13 &lt;- stl(co2, s.window = 13)\n\nautoplot(stl_13) + theme_series\n\n\n\n\n\n\n\n\nA função stl tem diversos parâmetros de “suaviazação”, que servem para escolher o tamanho e intensidade dos ciclos de tendência e sazonalidade. Via de regra quanto maiores os valores dos argumentos x.window mais suave será o ajuste final. Outro argumento potencialmente útil é definir robust = TRUE quando a série possui outliers.\nNo presente contexto, o argumento mais relevante do comando stl é o s.window ou \\(n_{(s)}\\) na nomenclatura do artigo original. Este parâmetro, em linhas gerais, define o grau de suaviazação da tendência sazonal. Ele deve ser um número ímpar e pelo menos igual a 7. O artigo original sugere uma ferramenta visual para escolher o parâmetro, mas concede que a escolha final é arbitrária e depende da sensibilidade do usuário.\nO gráfico apresentado é o seasonal-diagnostic plot. As linhas mostram a “intensidade” do efeito sazonal e os pontos indicam como este efeito sazonal varia com o ruído aleatório. Note como as variações da curva coincidem com as variações dos pontos, sugerindo que as oscilações sazonais estão sendo afetadas pelo ruído da série.\n\n\nCode\ncomponents &lt;- stl_13$time.series\n\ndat = data.frame(\n  date = as.Date.ts(components),\n  coredata(components)\n)\n\ndat$month = lubridate::month(dat$date, label = TRUE, locale = \"pt_BR\")\ndat$year = lubridate::year(dat$date)\n\ndat &lt;- dat |&gt;\n  mutate(s_avg = mean(seasonal), .by = \"month\") |&gt;\n  mutate(s1 = seasonal - s_avg, s2 = seasonal - s_avg + remainder)\n\nggplot(dat) +\n  geom_line(aes(x = year, y = s1), lwd = 0.8) +\n  geom_point(aes(x = year, y = s2), size = 0.8, shape = 21) +\n  facet_wrap(vars(month)) +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico abaixo repete o mesmo exercício mas utilizando s.window = 35 para suavizar a série original. Note como as curvas estão mais suaves.\n\n\nCode\nstl_35 &lt;- stl(co2, s.window = 35)\ncomponents &lt;- stl_35$time.series\n\ndat = data.frame(\n  date = as.Date.ts(components),\n  coredata(components)\n)\n\ndat$month = lubridate::month(dat$date, label = TRUE, locale = \"pt_BR\")\ndat$year = lubridate::year(dat$date)\n\ndat &lt;- dat |&gt;\n  mutate(s_avg = mean(seasonal), .by = \"month\") |&gt;\n  mutate(s1 = seasonal - s_avg, s2 = seasonal - s_avg + remainder)\n\nggplot(dat) +\n  geom_line(aes(x = year, y = s1), lwd = 0.8) +\n  geom_point(aes(x = year, y = s2), size = 0.8, shape = 21) +\n  facet_wrap(vars(month)) +\n  theme_series\n\n\n\n\n\n\n\n\n\nA função forecast::mstl oferece uma opção menos manual do ajuste STL. Esta função executa seis janelas distintas para s.window iterativamente. Os resultados costumam ser satisfatórios, mas como de costume, é necessário revisar o ajuste final.\n\nstl_auto = mstl(co2, lambda = \"auto\")\nautoplot(stl_auto)"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#sarima",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#sarima",
    "title": "Tendência e Sazonalidade",
    "section": "SARIMA",
    "text": "SARIMA\nOs modelos SARIMA incluem um componente de “sazonalidade estocástica” nos modelos ARIMA. Já escrevi um post onde detalho melhor alguns aspectos teóricos deste tipo de modelo. Vale notar o modelo SARIMA não realiza uma decomposição de tendência e sazonalidade com visto acima. Quando se aplica um SARIMA, implicitamente, supõe-se que a série possui uma tendência sazonal estocástica (i.e. raiz unitária sazonal) e não uma tendência sazonal determinística5.\nComo último exemplo vamos analisar a demanda mensal por gasolina. A série é da ANP e registra o total de vendas de gasolina C no Brasil em metros cúbicos.\n\ngasolina = read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/refs/heads/main/static/data/br_anp_gasolina.csv\"\n)\n\n\ngas = ts(na.omit(gasolina$demand), start = c(2012, 1), frequency = 12)\nautoplot(gas) + theme_series\n\n\n\n\n\n\n\n\nA série parece exibir algum componente de sazonalidade. Parece haver um pico de consumo todo mês de dezembro e uma queda todo mês de fevereiro.\n\nggseasonplot(gas) +\n  scale_color_viridis_d() +\n  theme_series\n\n\n\n\n\n\n\n\nA escolha da ordem do modelo SARIMA exige a inspeção visual do correlograma da série. Para mais detalhes consulte meu post mais detalhado sobre o assunto. Aqui, por simplicidade, uso um modelo simples, que costuma funcionar bem para série em geral.\n\nsarima_model = Arima(log(gas), order = c(0, 1, 1), seasonal = c(0, 1, 1))\n\nsarima_model\n\nSeries: log(gas) \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.2719  -0.7227\ns.e.   0.0973   0.0972\n\nsigma^2 = 0.003158:  log likelihood = 179.5\nAIC=-353.01   AICc=-352.81   BIC=-344.52\n\n\nUm dos pontos fortes dos modelos SARIMA é de gerar boas previsões de curto prazo com grande facilidade. Como comentei anteriormente, não há uma interpretação simples para a sazonalidade neste tipo de abordagem.\n\nautoplot(forecast(sarima_model), include = 48) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#x13-arima",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#x13-arima",
    "title": "Tendência e Sazonalidade",
    "section": "X13-ARIMA",
    "text": "X13-ARIMA\nO X13-ARIMA ou X13-ARIMA-SEATS é a versão mais recente do X11-ARIMA, uma metodologia desenvolvida pelo US Census Bureau para lidar com a sazonalidade de séries de tempo. Este método foi pensado para lidar com o tipo de sazonalidade comumemente encontrada em séries econômicas. Os métodos de ajuste são implementados no R via o pacote seasonal.\nA função principal do pacote seas realiza um ajuste sazonal automático. Para verificar os principais resultados do ajuste usa-se a função summary. O ajuste automático costuma ser bom, mas é sempre necessário revisar o ajuste.\n\n#&gt; Consumo mensal de energia elétrica - Residencial\nenerg = rbcb::get_series(1403, as = \"ts\", start_date = as.Date(\"2002-01-01\"))\n#&gt; Executa a rotina do X13-ARIMA\nsenerg = seas(energ)\n#&gt; Resumo dos resultados\nsummary(senerg)\n\n\nCall:\nseas(x = energ)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \nMon               -0.0044594  0.0024144  -1.847   0.0647 .  \nTue               -0.0067071  0.0024161  -2.776   0.0055 ** \nWed                0.0015360  0.0023804   0.645   0.5188    \nThu                0.0060074  0.0023645   2.541   0.0111 *  \nFri                0.0043871  0.0023727   1.849   0.0645 .  \nSat               -0.0008662  0.0023711  -0.365   0.7149    \nEaster[15]        -0.0287199  0.0050182  -5.723 1.05e-08 ***\nLS2002.Apr         0.0752851  0.0172641   4.361 1.30e-05 ***\nAO2014.Feb         0.0810754  0.0164747   4.921 8.60e-07 ***\nLS2015.Mar        -0.0788268  0.0156131  -5.049 4.45e-07 ***\nMA-Nonseasonal-01  0.5532245  0.0501164  11.039  &lt; 2e-16 ***\nMA-Seasonal-12     0.7247917  0.0470988  15.389  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 282  Transform: log\nAICc:  3649, BIC:  3695  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 15.76   Shapiro (normality): 0.9952  \n\n\nO gráfico abaixo mostra a série original e a série sazonalmente ajustada em vermelho. Para extrair a série ajustada usa-se a função final().\n\nautoplot(energ) +\n  autolayer(final(senerg)) +\n  guides(color = \"none\") +\n  theme_series\n\n\n\n\n\n\n\n\nO pacote seasonal é muito bem documentado e inclui um artigo de apresentação com exemplos, um texto mostrando como rodar os exemplos oficiais do X13 dentro do R e até uma ferramenta interativa. Neste post não vou explorar todas as nuances do pacote.\nPara conseguir um ajuste mais adequado é preciso explorar os argumentos adicionais da função seas além de adaptar as variáveis de calendário. Na sua configuração padrão, a função seas não considera o efeito do carnaval, por exemplo. É possível criar eventos de calendário usando a função genhol (de “generate holiday”).\nUm recurso bastante útil para conseguir as datas das festividades brasileiras é o site do prof. Roberto Cabral de Mello Borges. O código abaixo extrai a tabela com as datas da Páscoa, Carnaval e Corpus Christi de 1951-2078.\n\n\nCode\nlibrary(rvest)\n\nurl = \"https://www.inf.ufrgs.br/~cabral/tabela_pascoa.html\"\n\ntabela = url |&gt;\n  read_html() |&gt;\n  html_table()\n\ntab = tabela[[1]]\n\nferiados_bra = data.frame(tab[-1, ])\nnames(feriados_bra) = as.character(tab[1, ])\nhead(feriados_bra)\n\n\n   Ano  a b c  d e d+e Dia   Mês      Páscoa    Carnaval Corpus Christi\n1 1951 13 3 5  1 2   3  25 Março 25/mar/1951 06/fev/1951    24/mai/1951\n2 1952 14 0 6 20 2  22  13 Abril 13/abr/1952 26/fev/1952    12/jun/1952\n3 1953 15 1 0  9 5  14   5 Abril 05/abr/1953 17/fev/1953    04/jun/1953\n4 1954 16 2 1 28 6  34  18 Abril 18/abr/1954 02/mar/1954    17/jun/1954\n5 1955 17 3 2 17 2  19  10 Abril 10/abr/1955 22/fev/1955    09/jun/1955\n6 1956 18 0 3  6 4  10   1 Abril 01/abr/1956 14/fev/1956    31/mai/1956\n\n\nPara utilizar este dado no X13-ARIMA é necessário converter a coluna Carnaval num tipo Date e então utilizar a função genhol. Para facilitar a leitura das datas uso o pacote readr.\n\nlibrary(readr)\n\nferiados_bra$date_carnaval = parse_date(\n  feriados_bra$Carnaval,\n  format = \"%d/%b/%Y\",\n  locale = locale(\"pt\")\n)\n\ncarnaval = genhol(\n  feriados_bra$date_carnaval,\n  start = -3,\n  end = 1,\n  frequency = 12\n)\n\nAs datas de carnaval são inseridas dentro do X13-ARIMA via o argumento xreg.\n\nsenerg = seas(\n  energ,\n  xreg = carnaval,\n  regression.variables = \"td1coef\",\n  arima.model = c(0, 1, 1, 0, 1, 1)\n)\n\nsummary(senerg)\n\n\nCall:\nseas(x = energ, xreg = carnaval, regression.variables = \"td1coef\", \n    arima.model = c(0, 1, 1, 0, 1, 1))\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \nxreg              -0.026183   0.006325  -4.139 3.48e-05 ***\nEaster[15]        -0.041452   0.006007  -6.900 5.19e-12 ***\nLS2015.Mar        -0.079472   0.016788  -4.734 2.20e-06 ***\nMA-Nonseasonal-01  0.596372   0.046563  12.808  &lt; 2e-16 ***\nMA-Seasonal-12     0.715057   0.044903  15.924  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 282  Transform: log\nAICc:  3695, BIC:  3717  QS (no seasonality in final):4.187  \nBox-Ljung (no autocorr.): 114.7 *** Shapiro (normality): 0.9931  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nO gráfico abaixo mostra o resulado do ajuste com as datas de feriado modificadas.\n\nautoplot(energ) +\n  autolayer(final(senerg)) +\n  guides(color = \"none\") +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#footnotes",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#footnotes",
    "title": "Tendência e Sazonalidade",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara uma referência para séries com sazonalidade complexa no R veja Hyndman e Athansopoulos (2021) Forecasting: Principles and Practice.↩︎\nPode-se usar um polinômio de qualquer grau, mas polinômios de ordens muito elevadas costumam se ajustar “perfeitamente” aos dados e vão absorver toda a sazonalidade da série.↩︎\nSempre coloca-se uma variável binária a menos do que períodos sazonais pela questão do posto da matriz de regressores. Na prática, se houvesse uma dummy para cada período sazonal a matriz de regressão seria uma matriz identidade.↩︎\nÉ comum ver esta expressão nos textos de séries de tempo; em geral o termo é utilizado em contraste com modelos SARIMA onde a sazonalidade é estocástica, mas o termo “determinístico” não tem implicação causal. Na prática, quer dizer que a sazonalidade não varia no tempo e é sempre a mesma o que pode gerar previsões ruins a depender do caso.↩︎\nPara uma boa apresentação sobre raiz unitária e sobre tendências determinísticas veja Enders (2009) Applied Econometric Time Series.↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html",
    "href": "posts/general-posts/repost-arima-no-r/index.html",
    "title": "Séries de Tempo no R",
    "section": "",
    "text": "Neste post vou explorar um pouco das funções base do R para montar um modelo SARIMA. O R vem “pré-equipado” com um robusto conjunto de funções para lidar com séries de tempo. Inclusive, como se verá, existe uma class específica de objeto para trabalhar com séries de tempo."
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html#modelagem-sarima",
    "href": "posts/general-posts/repost-arima-no-r/index.html#modelagem-sarima",
    "title": "Séries de Tempo no R",
    "section": "Modelagem SARIMA",
    "text": "Modelagem SARIMA\nAqui a ideia é experimentar com alguns modelos simples. Em especial, o modelo que Box & Jenkins sugerem para a série é de um SARIMA (0, 1, 1)(0, 1, 1)[12] da forma\n\\[\n(1 - \\Delta)(1 - \\Delta^{12})y_{t} = \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t_1} + \\Theta_{1}\\varepsilon_{t-12}\n\\]\nA metodologia correta para a análise seria primeiro fazer testes de raiz unitária para avaliar a estacionaridade da série. Mas só de olhar para as funções de autocorrelação e autocorrelação parcial, fica claro que há algum componente sazonal e que a série não é estacionária.\n\n\n\n\n\n\n\n\n\n\nTeste de raiz unitária\nApenas a título de exemplo, faço um teste Dickey-Fuller (ADF), bastante geral, com constante e tendência temporal linear. Para uma boa revisão metodológica de como aplicar testes de raiz unitária, em partiular o teste ADF, consulte o capítulo de séries não-estacionárias do livro Applied Econometric Time Series do Enders\nAqui, a escolha ótima do lag é feita usando o critério BIC (também conhecido como Critério de Schwarz). Não existe uma função que aplica o teste ADF no pacote base o R. A implementação é feita na função ur.df do pacote urca.\nA estatísitica de teste mais relevante é a tau3 e vê-se, surpreendentemente, que se rejeita a hipótese nula de raiz unitária. As estatísticas phi2 e phi3 são testes-F da significânica conjunta dos termos de constante e de tendência temporal. As estatísticas de teste são convencionais e seguem a notação do livro do Enders citado acima e também do clássico livro do Hamilton.\n\nlibrary(urca)\nadf_test &lt;- ur.df(y, type = \"trend\", selectlags = \"BIC\", lags = 13)\nsummary(adf_test)\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090139 -0.022382 -0.002417  0.021008  0.110003 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.8968049  0.3999031   2.243 0.026859 *  \nz.lag.1      -0.1809364  0.0842729  -2.147 0.033909 *  \ntt            0.0016886  0.0008609   1.961 0.052263 .  \nz.diff.lag1  -0.2588927  0.1134142  -2.283 0.024301 *  \nz.diff.lag2  -0.0986455  0.1070332  -0.922 0.358665    \nz.diff.lag3  -0.0379799  0.1045583  -0.363 0.717097    \nz.diff.lag4  -0.1392651  0.0981271  -1.419 0.158560    \nz.diff.lag5  -0.0283998  0.0963368  -0.295 0.768686    \nz.diff.lag6  -0.1326313  0.0889223  -1.492 0.138581    \nz.diff.lag7  -0.1096365  0.0865862  -1.266 0.208019    \nz.diff.lag8  -0.2348880  0.0829892  -2.830 0.005497 ** \nz.diff.lag9  -0.0926604  0.0843594  -1.098 0.274344    \nz.diff.lag10 -0.2053937  0.0789245  -2.602 0.010487 *  \nz.diff.lag11 -0.1081091  0.0786801  -1.374 0.172127    \nz.diff.lag12  0.6633101  0.0752086   8.820 1.54e-14 ***\nz.diff.lag13  0.3197783  0.0883636   3.619 0.000443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04011 on 114 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8621 \nF-statistic: 54.75 on 15 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -2.147 4.9781 3.4342 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n\n\nPela análise do correlograma do resíduo da regressão, fica claro que ainda há autocorrelação. Novamente, o mais correto seria aplicar o teste Ljung-Box sobre os resíduos para verificar a presença de autocorrelação conjunta nas primeiras k defasagens, mas este não é o foco deste post. Esta pequena digressão exemplifica como a aplicação destes testes em séries de tempo pode não ser tão direto/simples.\n\n\n\n\n\n\n\n\n\nOs gráficos abaixo mostram o correlograma da série após tirarmos a primeira diferença e a primeira diferença sazonal. A partir da análise destes correlogramas poderíamos inferir algumas especificações alternativas para modelos SARIMA e aí, poderíamos escolher o melhor modelo usando algum critério de informação.\nA metodologia Box & Jenkins de análise de séries de tempo tem, certamente, um pouco de arte e feeling. Não é tão imediato entender como devemos proceder e, na prática, faz sentido experimentar com vários modelos alternativos de ordem baixa como SARIMA(1, 1, 1)(0, 1, 1), SARIMA(2, 1, 0)(0, 1, 1), etc.\n\n\n\n\n\n\n\n\n\n\n\nOs modelos\nPara não perder muito tempo experimentando com vários modelos vou me ater a três modelos diferentes. Uma função bastante útil é a auto.arima do pacote forecast que faz a seleção automática do melhor modelo da classe SARIMA/ARIMA/ARMA.\nEu sei que o Schumway/Stoffer, autores do ótimo Time Series Analysis and Its Applications, tem um post crítico ao uso do auto.arima. Ainda assim, acho que a função tem seu mérito e costuma ser um bom ponto de partida para a sua análise. Quando temos poucas séries de tempo para analisar, podemos nos dar ao luxo de fazer a modelagem manualmente, mas quando há centenas de séries, é muito conveniente poder contar com o auto.arima.\nComo o auto.arima escolhe o mesmo modelo do Box & Jenkins eu experimento com uma especificação diferente. Novamente, a título de exemplo eu comparo ambos os modelos SARIMA com uma regressão linear simples que considera uma tendência temporal linear e uma série de dummies sazonais. O modelo é algo da forma\n\\[\ny_{t} = \\alpha_{0} + \\alpha_{1}t + \\sum_{i = 1}^{11}\\beta_{i}s_{i} + \\varepsilon_{t}\n\\]\nOnde \\(s_{i}\\) é uma variável indicadora igual a 1 se \\(t\\) corresponder ao mês \\(i\\) e igual a 0 caso contrário. Vale notar que não podemos ter uma dummy para todos os meses se não teríamos uma matriz de regressores com colinearidade perfeita!\nAqui vou contradizer um pouco o espírito do post novamente para usar o forecast. O ganho de conveniência vem na hora de fazer as previsões. Ainda assim, indico como estimar os mesmos modelos usando apenas funções base do R.\n\n# Usando o forecast\nlibrary(forecast)\nmodel1 &lt;- auto.arima(train)\nmodel2 &lt;- Arima(train, order = c(1, 1, 1), seasonal = c(1, 1, 0))\nmodel3 &lt;- tslm(train ~ trend + season)\n\n\n# Usando apenas funções base\nmodel2 &lt;- arima(\n  trains,\n  order = c(1, 1, 1),\n  seasonal = list(order = c(1, 1, 1), period = 12)\n)\n\n# Extrai uma tendência temporal linear \ntrend &lt;- time(train)\n# Cria variáveis dummies mensais\nseason &lt;- cycle(train)\nmodel3 &lt;- lm(train ~ trend + season)\n\nA saída dos modelos segue abaixo. As saídas dos modelos SARIMA não são muito interessantes. Em geral, não é muito comum avaliar nem a significância e nem o sinal dos coeficientes, já que eles não têm muito valor interpretativo. Uma coisa que fica evidente dos dois modelos abaixo é que o primeiro parece melhor ajustado aos dados pois tem valores menores em todos os critérios de informação considerados.\n\nmodel1\n\nSeries: train \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.3424  -0.5405\ns.e.   0.1009   0.0877\n\nsigma^2 = 0.001432:  log likelihood = 197.51\nAIC=-389.02   AICc=-388.78   BIC=-381\n\n\n\nmodel2\n\nSeries: train \nARIMA(1,1,1)(1,1,0)[12] \n\nCoefficients:\n         ar1      ma1     sar1\n      0.0668  -0.4518  -0.4426\ns.e.  0.3046   0.2825   0.0875\n\nsigma^2 = 0.001532:  log likelihood = 195.14\nAIC=-382.28   AICc=-381.89   BIC=-371.59\n\n\nJá o modelo de regressão linear tem uma saída mais interessante. Note que, por padrão, o primeiro mês foi omitido e seu efeito aparece no termo constante. Na tabela abaixo, vemos que há um efeito positivo e significativo, por exemplo, nos meses 6-8 (junho a agosto), que coincidem com o período de férias de verão no hemisfério norte. Já, em novembro (mês 11) parece haver uma queda na demanda por passagens aéreas.\nNote que o R quadrado da regressão é extremamente elevado e isso é um indício de que algo está errado. Isto, muito provavelmente é resultado da não-estacionaridade da série.\n\nbroom::tidy(model3)\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic   p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  4.71     0.0191    247.     2.81e-149\n 2 trend        0.0106   0.000145   73.2    3.09e- 93\n 3 season2     -0.0134   0.0245     -0.549  5.84e-  1\n 4 season3      0.120    0.0245      4.91   3.32e-  6\n 5 season4      0.0771   0.0245      3.15   2.14e-  3\n 6 season5      0.0675   0.0245      2.75   6.93e-  3\n 7 season6      0.191    0.0245      7.81   4.23e- 12\n 8 season7      0.288    0.0245     11.7    6.32e- 21\n 9 season8      0.278    0.0245     11.4    4.36e- 20\n10 season9      0.143    0.0245      5.82   6.13e-  8\n11 season10     0.00108  0.0245      0.0441 9.65e-  1\n12 season11    -0.141    0.0245     -5.77   7.97e-  8\n13 season12    -0.0248   0.0245     -1.01   3.14e-  1\n\n\nDe fato, olhando para a função de autocorrelação do resíduo do modelo de regressão linear, fica evidente que há autocorrelação. Uma forma de contornar isso seria de incluir um termo ARMA no termo de erro. Novamente, este não é o foco do post e vamos seguir normalmente.\n\nacf(resid(model3))"
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html#comparando-as-previsões",
    "href": "posts/general-posts/repost-arima-no-r/index.html#comparando-as-previsões",
    "title": "Séries de Tempo no R",
    "section": "Comparando as previsões",
    "text": "Comparando as previsões\nA maneira mais prática de trabalhar com vários modelos ao mesmo tempo é agregando eles em listas e aplicando funções nessas listas.\nAbaixo eu aplico a função forecast para gerar as previsões 24 períodos a frente nos três modelos. Depois, eu extraio somente a estimativa pontual de cada previsão.\n\nmodels &lt;- list(model1, model2, model3)\nyhat &lt;- lapply(models, forecast, h = 24)\nyhat_mean &lt;- lapply(yhat, function(x) x$mean)\n\nComparamos a performance de modelos de duas formas: (1) olhando para medidas de erro (o quão bem o modelo prevê os dados do test) e (2) olhando para critérios de informação (o quão bem o modelo se ajusta aos dados do train).\nOs critérios de informação têm todos uma interpretação bastante simples: quanto menor, melhor. Tipicamente, o AIC tende a escolher modelos sobreparametrizados enquanto o BIC tende a escolher modelos mais parcimoniosos.\nJá a comparação de medidas de erro não é tão simples. Pois ainda que um modelo tenha, por exemplo, um erro médio quadrático menor do que outro, não é claro se esta diferença é significante. Uma maneira de testar isso é via o teste Diebold-Mariano, que compara os erros de previsão de dois modelos. Implicitamente, contudo, ele assume que a diferença entre os erros de previsão é covariância-estacionária (também conhecido como estacionário de segunda ordem ou fracamente estacionário). Dependendo do contexto, esta pode ser uma hipótese mais ou menos razoável.\n\ncompute_error &lt;- function(model, test) {\n  y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    y - yhat\n}\n\ncompute_error_metrics &lt;- function(model, test) {\n\n    y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    error &lt;- y - yhat\n    \n    # Raiz do erro quadrado medio\n    rmse &lt;- sqrt(mean(error^2))\n    # Erro medio absoluto\n    mae &lt;- mean(abs(error))\n    # Erro medio percentual\n    mape &lt;- mean(abs(100 * error / y))\n    # Root mean squared scaled error\n    rmsse &lt;- sqrt(mean(error^2 / snaive(train)$mean))\n\n    # Devolve os resultados num list\n    list(rmse = rmse, mae = mae, mape = mape, rmsse = rmsse)\n    \n\n}\n\ncompute_ics &lt;- function(model) {\n\n    # Extrai criterios de informacao\n    aic  &lt;- AIC(model)\n    bic  &lt;- BIC(model)\n\n    # Devolve os resultados num list\n    list(aic = aic, bic = bic)\n\n} \n\nfcomparison &lt;- lapply(yhat, function(yhat) compute_error_metrics(yhat, test))\nicc &lt;- lapply(models, compute_ics)\n\ncomp_error &lt;- do.call(rbind.data.frame, fcomparison)\nrownames(comp_error) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\ncomp_ics &lt;- do.call(rbind.data.frame, icc)\nrownames(comp_ics) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\n\nA tabela abaixo mostra os critérios AIC e BIC para os três modelos. Em ambos os casos, o SARIMA(0, 1, 1)(0, 1, 1)[12] parece ser o escolhido. Este tipo de feliz coincidência não costuma acontecer frequentemente na prática, mas neste caso ambos os critérios apontam para o mesmo modelo.\n\ncomp_ics\n\n                    aic       bic\nAutoArima     -389.0155 -380.9970\nManual SARIMA -382.2807 -371.5894\nOLS           -342.2930 -303.2681\n\n\nNa comparação de medidas de erro, o SARIMA(0, 1, 1)(0, 1, 1)[12] realmente tem uma melhor performance, seguido pelo OLS e pelo SARIMA(1, 1, 1)(1, 1, 0)[12].\n\ncomp_error\n\n                    rmse        mae     mape      rmsse\nAutoArima     0.09593236 0.08959921 1.463477 0.03939512\nManual SARIMA 0.11688549 0.10780460 1.762201 0.04806577\nOLS           0.10333715 0.09384411 1.549261 0.04266214\n\n\nSerá que esta diferença é significante? Vamos comparar os modelos SARIMA. Pelo teste DM ela é sim. Lembre-se que o teste DM é, essencialmente, um teste Z de que \\(e_{1} - e_{2} = 0\\) ou \\(e_{1} = e_{2}\\), onde os valores são a média dos erros de previsão dos modelos.\n\nerrors &lt;- lapply(yhat, function(yhat) compute_error(yhat, test))\ne1 &lt;- errors[[1]]\ne2 &lt;- errors[[2]]\n\ndm.test(e1, e2, power = 2)\n\n\n    Diebold-Mariano Test\n\ndata:  e1e2\nDM = -4.4826, Forecast horizon = 1, Loss function power = 2, p-value =\n0.0001691\nalternative hypothesis: two.sided\n\n\nVale notar que o teste DM serve para comparar os erros de previsão de quaisquer modelos. Como o teste não faz qualquer hipótese sobre “de onde vem” os erros de previsão, ele pode ser utilizado livremente. Vale lembrar também que este teste não deve ser utilizado para escolher o melhor modelo, já que ele compara apenas a capacidade preditiva de dois modelos alternativos.\nOutro ponto, também complicado, é de qual a medida de erro que se deve escolher. O teste DM implicitamente usa o erro médio quadrático, mas há várias outras alternativas. Uma breve discussão pode ser vista aqui.\nPor fim, o gráfico abaixo mostra as previsões dos modelos alternativos contra a série real.\n\nplot(test, ylim = c(5.8, 6.5), col = \"#8ecae6\", lwd = 2, type = \"o\")\nlines(yhat_mean[[1]], col = \"#ffb703\")\nlines(yhat_mean[[2]], col = \"#fb8500\")\nlines(yhat_mean[[3]], col = \"#B86200\")\ngrid()\nlegend(\"topleft\", lty = 1,\n       legend = c(\"Test\", \"AutoArima\", \"Arima\", \"OLS\"),\n       col = c(\"#8ecae6\", \"#ffb703\", \"#fb8500\", \"#B86200\"))"
  },
  {
    "objectID": "posts/general-posts/2024-03-ciclos-economicos/index.html",
    "href": "posts/general-posts/2024-03-ciclos-economicos/index.html",
    "title": "Recessões no Brasil",
    "section": "",
    "text": "Ciclos econômicos são flutuações recorrentes observadas empiricamente na atividade econômica de países. Estes ciclos são caracterizados por variações numa série de índices de atividade, emprego, investimento, etc. Estas flutuações geralmente seguem um padrão de expansão seguido por contração/recessão, formando o que é conhecido como um ciclo. Os ciclos econômicos são influenciados por uma variedade de fatores, incluindo mudanças na demanda do consumidor, políticas governamentais, inovações tecnológicas e choques externos (e.g. quebra de safra, pandemia, etc.).\nNo Brasil, a datação dos ciclos econômicos é feita pela CODACE1. Desde 1996, houve 6 recessões. A maioria das recessões foi relativamente curta, durando de 2 a 5 trimestres. A recessão mais longa foi a da Crise Econômica no segundo governo Dilma; oficialmente, a recessão começou no segundo trimestre de 2014 e foi até o quarto trimestre de 2016.\nO gráfico abaixo mostra a série trimestral do PIB, dessazonalizada, em número índice. A base do índice é a média de 1996. Nota-se que os períodos de expansão são mais frequentes do que os períodos de recessão: no período da amostra houve 87 trimestres de expansão contra 25 trimestres de recessão. Grosso modo, 1 a cada quatro trimestres apresentou uma recessão.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo normaliza o valor do PIB em cada uma das recessões ao valor pré-crise, isto é, ao valor observado no trimestre imediatamente anterior ao do início da recessão. Comparativamente, a maior queda relativa registrada aconteceu na Crise do Covid, na primeira metade de 2020. Ao todo a queda foi de quase 11%.\n\n\n\n\n\n\n\n\n\nO período de recessão termina, grosso modo, quando a economia para de diminuir. Isto é, o final da recessão não implica que a economia já está operando no mesmo nível que estava antes da crise. É preciso alguns trimestres a mais de crescimento simplesmente para recuperar o que foi perdido, em termos de PIB.\nO segundo gráfico mostra justamente o esforço necessário para voltar ao patamar pré-crise. Na maioria dos casos, foi necessário mais 2-3 trimestres para que a economia recuperasse o que foi perdido na recessão. A exceção notável foi a crise de 2014-16: foi necessário mais de 5 anos, após o final da recessão, para que a economia voltasse ao mesmo nível que estava antes da crise. Isto é, apenas no segundo trimestre de 2022 a economia brasileira apresentou PIB acima do observado no primeiro trimestre de 2014.\n\n\n\n\n\n\n\n\n\nA tabela final abaixo resume alguns fatos sobre as recessões econômicas mais recentes da história econômica brasileira.\n\n\n\n\n\n\n\n\nPeríodo\nNome\nInício\nFim\nRecuperação\nDuração Recessão\nTempo até Recuperação*\nCrescimento Médio**\nPerda Total\n\n\n\n\n1998Q1-1999Q1\nFHC-1\nQ1 1998\nQ1 1999\nQ4 1999\n5\n8\n−1.02%\n−1.31%\n\n\n2001Q2-Q4\nFHC-2\nQ2 2001\nQ4 2001\nQ1 2002\n3\n4\n−1.93%\n−1.29%\n\n\n2003Q1-2003Q2\nLULA\nQ1 2003\nQ2 2003\nQ4 2003\n2\n4\n−2.67%\n−1.15%\n\n\n2008Q4-2009Q1\nGFR\nQ4 2008\nQ1 2009\nQ4 2009\n2\n5\n−14.10%\n−4.99%\n\n\n2014Q2-2016Q4\nDILMA\nQ2 2014\nQ4 2016\nQ2 2022\n11\n33\n−4.94%\n−8.00%\n\n\n2020Q1-2020Q2\nCOVID\nQ1 2020\nQ2 2020\nQ1 2021\n2\n5\n−30.72%\n−10.78%\n\n\n\n(*) - Número de trimestres até a economia voltar ao patamar pré-crise\n\n\n(**) - Crescimento trimestral médio anualizado"
  },
  {
    "objectID": "posts/general-posts/2024-03-ciclos-economicos/index.html#footnotes",
    "href": "posts/general-posts/2024-03-ciclos-economicos/index.html#footnotes",
    "title": "Recessões no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO Comitê de Datação de Ciclos Econômicos (CODACE) organizado pela Fundação Getúlio Vargas (FGV) se reúne periodicamente para datar os ciclos econômicos brasileiros. Para mais informações veja o site.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-expectativa/index.html",
    "href": "posts/general-posts/2023-11-wz-expectativa/index.html",
    "title": "Expectativa de Vida em São Paulo",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(tidypod)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(readxl)\nlibrary(here)\nlibrary(sf)\nlibrary(patchwork)\n\nfont_add_google(\"Roboto Mono\", \"Roboto Mono\")\nshowtext_auto()\n\n# Import districts shapefile\ndstr &lt;- tidypod::districts\ndstr &lt;- filter(dstr, code_muni == 36)\n\n# Import Mapa Desigualdade Data\n# https://www.nossasaopaulo.org.br/campanhas/#13\n\nurl &lt;- \"https://www.nossasaopaulo.org.br/wp-content/uploads/2023/11/mapa_da_desigualdade_2023_dados.xlsx\"\ntf &lt;- tempfile(fileext = \"xlsx\")\ndownload.file(url, tf, quiet = TRUE)\n\nsp &lt;- read_excel(\n  tf,\n  sheet = 4,\n  .name_repair = janitor::make_clean_names\n)\n\nsp &lt;- sp |&gt; \n  rename(name_district = distritos) |&gt; \n  mutate(name_district = if_else(name_district == \"Moóca\", \"Mooca\", name_district))\n\n\nmapasp &lt;- dstr |&gt; \n  left_join(sp, by = \"name_district\")\n\npmap &lt;- ggplot(mapasp, aes(fill = idade_media_ao_morrer)) +\n  geom_sf() +\n  scale_fill_fermenter(\n    name = \"\",\n    palette = \"RdBu\",\n    breaks = seq(55, 85, 5),\n    direction = 1) +\n  labs(\n    title = \"Expectativa de Vida\",\n    subtitle = \"Idade média ao morrer por distrito em São Paulo\",\n    caption = \"Fonte: Nossa São Paulo, Mapa da Desigualdade (2023)\"\n    ) +\n  ggthemes::theme_map(base_family = \"Roboto Mono\") +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.width = unit(1.25, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 28, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n  )\n\ncount_group &lt;- mapasp |&gt; \n  st_drop_geometry() |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = findInterval(idade_media_ao_morrer, seq(55, 85, 5))\n  ) |&gt; \n  summarise(\n    count = n(),\n    min = min(idade_media_ao_morrer),\n    pop = sum(populacao_total),\n    .by = \"group\"\n  ) |&gt; \n  mutate(share = pop / sum(pop) * 100) |&gt; \n  arrange(group)\n\npcol &lt;- ggplot(count_group, aes(x = group, y = share, fill = as.factor(group))) +\n  geom_col() +\n  geom_text(\n    family = \"Roboto Mono\",\n    size = 5,\n    aes(x = group, y = share + 2, label = round(share, 1))\n    ) +\n  labs(title = stringr::str_wrap(\"Percentual da população dentro de cada grupo\", 25)) +\n  scale_fill_brewer(palette = \"RdBu\") +\n  guides(fill = 'none') +\n  theme_void(base_family = \"Roboto Mono\") +\n  theme(plot.title = element_text(size = 10, hjust = 0))\n\nmap_expectativa_de_vida &lt;- \n  pmap + inset_element(pcol, left = 0.55, bottom = 0.05, right = 1, top = 0.4)\n\n\n\nExpectativa de vida\nSão Paulo é uma cidada marcada por desigualdades. Talvez uma das mais surpreendentes seja a da expectativa de vida. Do Jardim Paulista até Anhanguera são 23 anos de diferença na idade média ao morrer. Enquanto no Jardim Paulista espera-se viver até 82 anos, em Anhanguera a expectativa de vida não chega aos 60 anos.\n\n\n\n\n\n\n\n\n\n\nDados: Nossa São Paulo (Mapa da Desigualdade, 2023)\nPaleta: RdBu (ColorBrewer)\nTipografia: Roboto Mono"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html",
    "href": "posts/general-posts/2023-10-realestatebr/index.html",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "Consumir os dados do mercado imobiliário brasileiro não é tarefa fácil. Pensando em simplificar este processo eu criei um pacote chamado realestatebr que importa e limpa diversas bases de dados relacionadas ao mercado imobiliário. Atualmente, o pacote está majoritariamente focado no mercado residencial.\nPara instalar o pacote é preciso ter o devtools ou remotes instalado.\n\n# Instala (se necessário)\ninstall.packages(\"remotes\")\n# Instala o pacote realestatebr\nremotes::install_github(\"viniciusoike/realestatebr\")\n\nO pacote é estruturado em torno de funções get_* que importam bases de dados. O pacote cobre:\n\nAbrainc: Indicadores\nAbrainc: IAMI\nAbrainc: Radar\nAbrainc: Relatórios MCMV e MAP\nAbecip: IGMI-R\nAbecip: Indicadores de crédito\nBIS: Residential Property Price Indices\nBanco Central do Brasil: Estatísticas do Mercado Imobiliário\nBanco Central do Brasil: Séries macroeconômicas\nB3: Ações de empresas relacionadas ao mercado imobiliário\nFipeZap: Índice FipeZap\nFGV-Ibre: Séries macroeconômicas, IVAR, INCC, etc.\nQuintoAndar: Índice QuintoAndar de Aluguel\nRegistro de Imóveis\nSECOVI-SP: Panorama do Mercado Imobiliário\n\nPara mais detalhes sobre o pacote consulte o repositório no GitHub.\n\n\nOs exemplos abaixo demonstram alguns dos usos do pacote.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RcppRoll)\nlibrary(realestatebr)\n\n\n\n\nO Brasil tem diversos índices de preços imobiliários. A função get_rppi permite baixar tanto os índices de aluguel como de compra/venda. O código abaixo importa as séries do FipeZap, IGMI-R (FGV e Abecip) e IVG-R (BCB).\n\nsale &lt;- get_rppi(\"sale\", stack = TRUE)\n\nOs dados são importados em formato longitudinal identificados pela coluna source. A coluna chg traz a variação mensal do índice enquanto a coluna acum12m traz a variação acumulada em 12 meses.\n\n\n\n\n\n\nO código abaixo mostra os três índices desde 2018. Note que há um descolamento interessante entre o IGMI-R e o IVG-R. O Índice FipeZap e o IVG-R, que historicamente costumam convergir, também apresentam comportamento divergente nesta janela de tempo.\n\n# Filter only Brazil\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2018-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra as séries num horizonte maior de tempo.\n\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2009-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nPode-se importar facilmente os dados da Abecip sobre financiamentos imobiliários. A função get_abecip_indicators é uma lista que retorna dados sobre o SBPE e o CGI.\n\n# Import data from Abecip\nabecip &lt;- get_abecip_indicators()\n\nnames(abecip)\n\n[1] \"sbpe\"  \"units\" \"cgi\"  \n\n\nO gráfico abaixo mostra o total de unidades financiadas a cada ano.\n\nunidades &lt;- abecip$units\n\ntbl_unidades_ano &lt;- unidades |&gt; \n  mutate(ano = lubridate::year(date)) |&gt; \n  summarise(total_ano = sum(units_total), .by = \"ano\")\n\nggplot(tbl_unidades_ano, aes(x = ano, y = total_ano)) +\n  geom_col(fill = \"#264653\") +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = 2001:2023) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  labs(\n    title = \"Unidades Financiadas SBPE\",\n    x = NULL,\n    y = \"Unidades\",\n    caption = \"Unidades de 2023 acumuladas até agosto.\"\n    ) +\n  theme_light() +\n  theme(\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\n\n\n\n\n\n\n\nO Banco Central disponibiliza uma enorme variedade de séries relacionadas ao mercado imobiliário. A função get_bcb_realestate retorna mais de 3500 séries. Para agilizar o download destas bases, todas as funções oferecem a opção cached = TRUE. Usando esta opção o download é feito via GitHub e tende a ser mais rápido; o dado, contudo, corre o risco de estar levemente desatualizado.\n\nbcb &lt;- get_bcb_realestate(category = \"all\", cached = TRUE)\n\nlength(unique(bcb$series_info))\n\n[1] 3656\n\n\nInfelizmente, não é trivial usar esta base de dados. As colunas category, type e v1 a v5 tentam facilitar o trabalho de filtrar as linhas. Há seis grandes categorias.\n\ncount(bcb, category)\n\n# A tibble: 6 × 2\n  category            n\n  &lt;fct&gt;           &lt;int&gt;\n1 contabil          303\n2 credito        285853\n3 direcionamento   5223\n4 fontes            604\n5 indices           555\n6 imoveis         25353\n\n\nCada série é identificada pela coluna series_info. Quebrando esta coluna em outras, fica um pouco mais fácil de encontrar as séries desejadas.\n\nbcb |&gt; \n  filter(category == \"contabil\") |&gt; \n  count(series_info, type, v1)\n\n# A tibble: 3 × 4\n  series_info                           type          v1              n\n  &lt;chr&gt;                                 &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt;\n1 contabil_bndu_imobiliario_br          bndu          imobiliario    81\n2 contabil_financiamento_comercial_br   financiamento comercial     111\n3 contabil_financiamento_residencial_br financiamento residencial   111\n\n\nO gráfico abaixo mostra o preço mediano do imóvel financiado nos estados da região Sul. Note que estas séries possuem um comportamento irregular em 2019.\n\ntbl_imoveis_sul &lt;- bcb |&gt; \n  filter(\n    category == \"imoveis\",\n    type == \"valor\",\n    v1 == \"compra\",\n    abbrev_state %in% c(\"RS\", \"SC\", \"PR\")\n    )\n\nggplot(tbl_imoveis_sul, aes(x = date, y = value, color = abbrev_state)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Preço de financiamento\",\n    x = NULL,\n    y = \"R$\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nA função get_bcb_series facilita a importação de séries macroeconômicas potencialmente relacionadas com o mercado imobiliário. Como há muitas séries, o argumento category facilita a seleção de temas específicos. A categoria de preços inlcui os principais índices de preço relacionados ao mercado, como o INCC, IPCA e IGPM.\n\nmacro &lt;- get_bcb_series(category = \"price\")\n\nunique(macro$name_pt)\n\n[1] \"Índice geral de preços do mercado (IGP-M)\"                                         \n[2] \"Índice geral de preços-disponibilidade interna (IGP-DI)\"                           \n[3] \"Índice nacional de custo da construção (INCC)\"                                     \n[4] \"Índice nacional de preços ao consumidor-amplo (IPCA)\"                              \n[5] \"Índice nacional de preços ao consumidor-Amplo (IPCA) - Habitação\"                  \n[6] \"Índice nacional de preços ao consumidor (INPC) - Habitação\"                        \n[7] \"Meios de pagamento - M1 (saldo em final de período) - Novo - sazonalmente ajustado\"\n\n\nO gráfico abaixo combina a série de preços do IGMI-R, importada anteriormente, com o IPCA. Os valores são acumulados em 12 meses e apresentados desde 2018. Nota-se como o preço dos imóveis cresceu acima da inflação a partir de 2020.\n\nseries_ipca &lt;- macro |&gt; \n  filter(name_simplified == \"ipca\") |&gt; \n  mutate(\n    acum12m = roll_prodr(1 + value / 100, n = 12) - 1,\n    series = \"ipca\"\n    ) |&gt; \n  select(date, series, acum12m)\n\nseries_igmi &lt;- sale |&gt; \n  filter(\n    source == \"IGMI-R\",\n    name_muni %in% c(\"Porto Alegre\", \"Brazil\")\n    ) |&gt; \n  select(date, series = name_muni, acum12m)\n\nseries_macro &lt;- rbind(series_ipca, series_igmi)\n\nseries_macro &lt;- series_macro |&gt; \n  filter(date &gt;= as.Date(\"2018-01-01\"), date &lt;= as.Date(\"2023-07-01\")) |&gt; \n  mutate(\n    series = factor(series, levels = c(\"Brazil\", \"Porto Alegre\", \"ipca\"))\n  )\n\nggplot(series_macro, aes(x = date, y = acum12m, color = series)) +\n  geom_line(linewidth = 0.8) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\"),\n    labels = c(\"Brasil (geral)\", \"Porto Alegre\", \"IPCA\")\n    ) +\n  labs(\n    title = \"Preços de imóveis crescem acima da inflação\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nO desenvolvimento futuro do pacote será guiado pela incorporação de mais bases de dados e relatórios de mercado."
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#usando-o-pacote",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#usando-o-pacote",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "Os exemplos abaixo demonstram alguns dos usos do pacote.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RcppRoll)\nlibrary(realestatebr)"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#índices-de-preços",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#índices-de-preços",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O Brasil tem diversos índices de preços imobiliários. A função get_rppi permite baixar tanto os índices de aluguel como de compra/venda. O código abaixo importa as séries do FipeZap, IGMI-R (FGV e Abecip) e IVG-R (BCB).\n\nsale &lt;- get_rppi(\"sale\", stack = TRUE)\n\nOs dados são importados em formato longitudinal identificados pela coluna source. A coluna chg traz a variação mensal do índice enquanto a coluna acum12m traz a variação acumulada em 12 meses.\n\n\n\n\n\n\nO código abaixo mostra os três índices desde 2018. Note que há um descolamento interessante entre o IGMI-R e o IVG-R. O Índice FipeZap e o IVG-R, que historicamente costumam convergir, também apresentam comportamento divergente nesta janela de tempo.\n\n# Filter only Brazil\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2018-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra as séries num horizonte maior de tempo.\n\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2009-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#financiamento-de-imóveis",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#financiamento-de-imóveis",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "Pode-se importar facilmente os dados da Abecip sobre financiamentos imobiliários. A função get_abecip_indicators é uma lista que retorna dados sobre o SBPE e o CGI.\n\n# Import data from Abecip\nabecip &lt;- get_abecip_indicators()\n\nnames(abecip)\n\n[1] \"sbpe\"  \"units\" \"cgi\"  \n\n\nO gráfico abaixo mostra o total de unidades financiadas a cada ano.\n\nunidades &lt;- abecip$units\n\ntbl_unidades_ano &lt;- unidades |&gt; \n  mutate(ano = lubridate::year(date)) |&gt; \n  summarise(total_ano = sum(units_total), .by = \"ano\")\n\nggplot(tbl_unidades_ano, aes(x = ano, y = total_ano)) +\n  geom_col(fill = \"#264653\") +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = 2001:2023) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  labs(\n    title = \"Unidades Financiadas SBPE\",\n    x = NULL,\n    y = \"Unidades\",\n    caption = \"Unidades de 2023 acumuladas até agosto.\"\n    ) +\n  theme_light() +\n  theme(\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#preços-de-imóveis",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#preços-de-imóveis",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O Banco Central disponibiliza uma enorme variedade de séries relacionadas ao mercado imobiliário. A função get_bcb_realestate retorna mais de 3500 séries. Para agilizar o download destas bases, todas as funções oferecem a opção cached = TRUE. Usando esta opção o download é feito via GitHub e tende a ser mais rápido; o dado, contudo, corre o risco de estar levemente desatualizado.\n\nbcb &lt;- get_bcb_realestate(category = \"all\", cached = TRUE)\n\nlength(unique(bcb$series_info))\n\n[1] 3656\n\n\nInfelizmente, não é trivial usar esta base de dados. As colunas category, type e v1 a v5 tentam facilitar o trabalho de filtrar as linhas. Há seis grandes categorias.\n\ncount(bcb, category)\n\n# A tibble: 6 × 2\n  category            n\n  &lt;fct&gt;           &lt;int&gt;\n1 contabil          303\n2 credito        285853\n3 direcionamento   5223\n4 fontes            604\n5 indices           555\n6 imoveis         25353\n\n\nCada série é identificada pela coluna series_info. Quebrando esta coluna em outras, fica um pouco mais fácil de encontrar as séries desejadas.\n\nbcb |&gt; \n  filter(category == \"contabil\") |&gt; \n  count(series_info, type, v1)\n\n# A tibble: 3 × 4\n  series_info                           type          v1              n\n  &lt;chr&gt;                                 &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt;\n1 contabil_bndu_imobiliario_br          bndu          imobiliario    81\n2 contabil_financiamento_comercial_br   financiamento comercial     111\n3 contabil_financiamento_residencial_br financiamento residencial   111\n\n\nO gráfico abaixo mostra o preço mediano do imóvel financiado nos estados da região Sul. Note que estas séries possuem um comportamento irregular em 2019.\n\ntbl_imoveis_sul &lt;- bcb |&gt; \n  filter(\n    category == \"imoveis\",\n    type == \"valor\",\n    v1 == \"compra\",\n    abbrev_state %in% c(\"RS\", \"SC\", \"PR\")\n    )\n\nggplot(tbl_imoveis_sul, aes(x = date, y = value, color = abbrev_state)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Preço de financiamento\",\n    x = NULL,\n    y = \"R$\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#séries-macroeconômicas",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#séries-macroeconômicas",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "A função get_bcb_series facilita a importação de séries macroeconômicas potencialmente relacionadas com o mercado imobiliário. Como há muitas séries, o argumento category facilita a seleção de temas específicos. A categoria de preços inlcui os principais índices de preço relacionados ao mercado, como o INCC, IPCA e IGPM.\n\nmacro &lt;- get_bcb_series(category = \"price\")\n\nunique(macro$name_pt)\n\n[1] \"Índice geral de preços do mercado (IGP-M)\"                                         \n[2] \"Índice geral de preços-disponibilidade interna (IGP-DI)\"                           \n[3] \"Índice nacional de custo da construção (INCC)\"                                     \n[4] \"Índice nacional de preços ao consumidor-amplo (IPCA)\"                              \n[5] \"Índice nacional de preços ao consumidor-Amplo (IPCA) - Habitação\"                  \n[6] \"Índice nacional de preços ao consumidor (INPC) - Habitação\"                        \n[7] \"Meios de pagamento - M1 (saldo em final de período) - Novo - sazonalmente ajustado\"\n\n\nO gráfico abaixo combina a série de preços do IGMI-R, importada anteriormente, com o IPCA. Os valores são acumulados em 12 meses e apresentados desde 2018. Nota-se como o preço dos imóveis cresceu acima da inflação a partir de 2020.\n\nseries_ipca &lt;- macro |&gt; \n  filter(name_simplified == \"ipca\") |&gt; \n  mutate(\n    acum12m = roll_prodr(1 + value / 100, n = 12) - 1,\n    series = \"ipca\"\n    ) |&gt; \n  select(date, series, acum12m)\n\nseries_igmi &lt;- sale |&gt; \n  filter(\n    source == \"IGMI-R\",\n    name_muni %in% c(\"Porto Alegre\", \"Brazil\")\n    ) |&gt; \n  select(date, series = name_muni, acum12m)\n\nseries_macro &lt;- rbind(series_ipca, series_igmi)\n\nseries_macro &lt;- series_macro |&gt; \n  filter(date &gt;= as.Date(\"2018-01-01\"), date &lt;= as.Date(\"2023-07-01\")) |&gt; \n  mutate(\n    series = factor(series, levels = c(\"Brazil\", \"Porto Alegre\", \"ipca\"))\n  )\n\nggplot(series_macro, aes(x = date, y = acum12m, color = series)) +\n  geom_line(linewidth = 0.8) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\"),\n    labels = c(\"Brasil (geral)\", \"Porto Alegre\", \"IPCA\")\n    ) +\n  labs(\n    title = \"Preços de imóveis crescem acima da inflação\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#caminhos-futuros",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#caminhos-futuros",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O desenvolvimento futuro do pacote será guiado pela incorporação de mais bases de dados e relatórios de mercado."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-sp-college/index.html",
    "href": "posts/general-posts/2023-11-wz-sp-college/index.html",
    "title": "Ensino Superior em São Paulo",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(tidypod)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(sf)\nlibrary(stringr)\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nshowtext_opts(dpi = 72)\nshowtext_auto()\n\n# Import POD data\npod &lt;- tidypod::import_pod_tables(geo = TRUE)\npod &lt;- filter(pod, code_muni == 36)\n\n\n# Ensino Superior ---------------------------------------------------------\n\njenks &lt;- BAMMtools::getJenksBreaks(pod$share_educ_superior, 7)\n\nmap_ensino_superior &lt;- ggplot(pod) +\n  geom_sf(aes(fill = share_educ_superior), lwd = 0.1, color = \"white\") +\n  scale_fill_fermenter(\n    name = \"\",\n    palette = \"BrBG\",\n    direction = 1,\n    breaks = jenks,\n    labels = round(jenks, 1)) +\n  coord_sf(ylim = c(-24, -23.38), xlim = c(-46.81, -46.38)) +\n  ggtitle(\"Percentual com Ensino Superior (%)\") +\n  labs(\n    subtitle = \"Percentual da população adulta com diploma de ensino superior por Zona OD.\",\n    caption = \"Fonte: Pesquisa Origem e Destino (2017). @viniciusoike.\") +\n  ggthemes::theme_map(base_family = \"Playfair Display\") +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.75, \"cm\"),\n    legend.text = element_text(size = 16),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 28, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n  )\n\n\n\nEnsino Superior\nO acesso ao ensino superior no Brasil historicamente apresenta grandes desigualdades. Na cidade de São Paulo não é diferente. No mapa abaixo, apresenta-se o percentual da população adulta com ensino superior por Zona OD. Vê-se nos dados o conhecido padrão da cidade: indicadores de ensino superior altos dentro do Centro Expandido da cidade, que vão decaindo gradativamente à medida que se aproxima da periferia.\n\n\n\n\n\n\n\n\n\n\nDados: Metrô (Pesquisa Origem e Destino, 2017)\nTipografia: Playfair Display\nPaleta: BrBG (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html",
    "href": "posts/general-posts/2023-12-bump-plots/index.html",
    "title": "Bump Plots",
    "section": "",
    "text": "Existe um pacote auxiliar específico para criar “bump charts” chamado {ggbump}. O pacote está disponível tanto no CRAN como no Github do autor.\nNeste tipo de gráfico, quer-se comparar o valor de uma variável em diferentes contextos. Podemos ter uma comparação entre os mesmos grupos ao longo do tempo ou os mesmos grupos ao longo de variáveis distintas. Em geral, estes gráficos são organizados em forma de rankings e têm como objetivo facilitar a comparação entre grupos.\nAlguns exemplos de aplicação incluem:\n\nNúmero de medalhas de ouro nas olimpíadas de uma subamostra de países.\nGênero mais escutado de música de um usuário ao longo dos anos.\nPaíses mais populosos do mundo ao longo de décadas.\nRanking de imóveis em vários critérios.\nRanking de times num campeonato a cada semana.\nRanking de países mais ricos segundo diferentes critérios de riqueza\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggbump)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nO primeiro exemplo é tomado emprestado da página de apresentação do pacote e ilustra o básico da função geom_bump. Os dados tem de estar em formato ‘tidy’ (longitudinal) onde a posição dos “pontos” é informada pelos argumentos x e y e os grupos identificados via group. Isto é, essencialmente, temos um grid de pontos, que ocupa 100% do espaço do gráfico, conectados via uma coluna “group”.\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nplayer &lt;- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf &lt;- data.frame(x = year,\n                 y = position,\n                 group = player)\n\n\n\n\n\n\nx\ny\ngroup\n\n\n\n\n2019\n4\nA\n\n\n2020\n2\nA\n\n\n2021\n2\nA\n\n\n2019\n3\nB\n\n\n2020\n1\nB\n\n\n2021\n4\nB\n\n\n2019\n2\nC\n\n\n2020\n3\nC\n\n\n2021\n1\nC\n\n\n2019\n1\nD\n\n\n2020\n4\nD\n\n\n2021\n3\nD\n\n\n\n\n\nVale tirar um tempo para comparar, com calma, as entradas na tabela acima e o resultado no gráfico abaixo.\n\nggplot(df, aes(year, position, color = player)) +\n  geom_bump()\n\n\n\n\n\n\n\n\n\n\n\nPodemos analisar as cidades com maior número de vendas ao longo dos anos usando a já conhecida txhousing. Quem acompanha meus tutoriais de ggplot2 já deve estar cansado de ver esta base sendo utilizada.\nQueremos montar um gráfico que mostra o ranking de vendas de imóveis ao longo dos anos. Removo o ano de 2015, pois este ano não está completo na amostra. Como há mais de quarenta cidades na amostra eu crio uma subamostra que contém apenas as cidades com maior número de vendas em 2014. O código abaixo mostra o passo-a-passo da manipulação de dados.\n\n#&gt; Encontra as top-15 cidades com maior número de vendas em 2014\ntop_cities &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas o ano de 2014\n  filter(year == 2014) |&gt; \n  #&gt; Calcula o total de vendas em cada cidade\n  summarise(total = sum(listings, na.rm = TRUE), .by = \"city\") |&gt; \n  #&gt; Seleciona o top-15\n  slice_max(total, n = 15) |&gt; \n  pull(city)\n\n#&gt; Calcula o total de vendas anuais na subamostra de cidades e faz o ranking anual\n\nrank_housing &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas cidades dentro da subamostra\n  filter(city %in% top_cities, year &gt; 2005, year &lt; 2015) |&gt; \n  #&gt; Calcula o total de vendas a cada ano\n  summarise(\n    listing_year = sum(listings, na.rm = TRUE),\n    .by = c(\"city\", \"year\")\n    ) |&gt; \n  #&gt; Faz o ranking das cidades dentro de cada ano\n  mutate(rank = rank(-listing_year, \"first\"), .by = \"year\")\n\nNo gráfico abaixo, cada cidade tem uma cor diferente, mas omito a legenda de cores. Note o uso de scale_y_reverse já que, tipicamente, queremos mostrar os menores valores na parte superior do gráfico.\n\nggplot(rank_housing, aes(year, rank, group = city, color = city)) +\n  geom_bump() +\n  scale_y_reverse() +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nPara melhorar a legibilidade do gráfico podemos colocar o nome das cidades ao lado das linhas usando geom_text.\n\nggplot() +\n  geom_bump(\n    data = rank_housing,\n    aes(year, rank, group = city, color = city)\n    ) +\n  geom_text(\n    data = filter(rank_housing, year == max(year)),\n    aes(year, rank, label = city),\n    nudge_x = 0.1,\n    hjust = 0\n  ) +\n  scale_y_reverse() +\n  scale_x_continuous(limits = c(NA, 2017)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nMesmo com o nome das cidades, há muitas linhas para acompanhar no gráfico. Imagine, por exemplo, que queremos destacar apenas algumas das cidades selecionadas. Não parece fazer muito sentido destacar as cidades do top 4 (Houston, Dallas, San Antonio e Austin), já que elas praticamente não se alternam no ranking durante todo o período. Vamos, então, destacar as cidades de Bay Area, El Paso, Corpus Christi e Tyler.\n\nsel_cities &lt;- c(\"Bay Area\", \"El Paso\", \"Corpus Christi\", \"Tyler\")\n\nrank_housing &lt;- rank_housing |&gt; \n  mutate(\n    highlight = if_else(city %in% sel_cities, city, \"\"),\n    is_highlight = factor(if_else(city %in% sel_cities, 1L, 0L))\n  )\n\nO gráfico abaixo exige um código considervalmente mais longo, mas melhora o gráfico original em vários aspectos. Agora temos um maior destaque para as cidades de interesse. Os eixos estão melhor delimitados e as linhas de fundo redundantes foram removidas.\n\n\nCode\nggplot() +\n  #&gt; Linhas em cinza (sem destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 0),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 0.8,\n    smooth = 8\n    ) +\n  #&gt; Linhas coloridas (com destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 1),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 2,\n    smooth = 8\n  ) +\n  #&gt; Pontos \n  geom_point(\n    data = rank_housing,\n    aes(year, rank, color = highlight),\n    size = 4\n  ) +\n  #&gt; Nomes sem destaque\n  geom_text(\n    data = filter(rank_housing, year == max(year), !(city %in% sel_cities)),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    color = \"gray20\"\n  ) +\n  #&gt; Nome com destaque (em negrito)\n  geom_text(\n    data = filter(rank_housing, year == max(year), city %in% sel_cities),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    fontface = \"bold\"\n  ) +\n  #&gt; Adiciona os eixos para melhorar leitura do gráfico\n  scale_y_reverse(breaks = 1:15) +\n  scale_x_continuous(limits = c(NA, 2017), breaks = 2006:2014) +\n  #&gt; Cores\n  scale_color_manual(\n    values = c(\"gray70\", \"#2f4858\", \"#86bbd8\", \"#f6ae2d\", \"#f26419\")\n  ) +\n  #&gt; Elementos temáticos\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#pacotes",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#pacotes",
    "title": "Bump Plots",
    "section": "",
    "text": "library(ggplot2)\nlibrary(ggbump)\nlibrary(dplyr)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#exemplo-simples",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#exemplo-simples",
    "title": "Bump Plots",
    "section": "",
    "text": "O primeiro exemplo é tomado emprestado da página de apresentação do pacote e ilustra o básico da função geom_bump. Os dados tem de estar em formato ‘tidy’ (longitudinal) onde a posição dos “pontos” é informada pelos argumentos x e y e os grupos identificados via group. Isto é, essencialmente, temos um grid de pontos, que ocupa 100% do espaço do gráfico, conectados via uma coluna “group”.\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nplayer &lt;- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf &lt;- data.frame(x = year,\n                 y = position,\n                 group = player)\n\n\n\n\n\n\nx\ny\ngroup\n\n\n\n\n2019\n4\nA\n\n\n2020\n2\nA\n\n\n2021\n2\nA\n\n\n2019\n3\nB\n\n\n2020\n1\nB\n\n\n2021\n4\nB\n\n\n2019\n2\nC\n\n\n2020\n3\nC\n\n\n2021\n1\nC\n\n\n2019\n1\nD\n\n\n2020\n4\nD\n\n\n2021\n3\nD\n\n\n\n\n\nVale tirar um tempo para comparar, com calma, as entradas na tabela acima e o resultado no gráfico abaixo.\n\nggplot(df, aes(year, position, color = player)) +\n  geom_bump()"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#venda-de-imóveis",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#venda-de-imóveis",
    "title": "Bump Plots",
    "section": "",
    "text": "Podemos analisar as cidades com maior número de vendas ao longo dos anos usando a já conhecida txhousing. Quem acompanha meus tutoriais de ggplot2 já deve estar cansado de ver esta base sendo utilizada.\nQueremos montar um gráfico que mostra o ranking de vendas de imóveis ao longo dos anos. Removo o ano de 2015, pois este ano não está completo na amostra. Como há mais de quarenta cidades na amostra eu crio uma subamostra que contém apenas as cidades com maior número de vendas em 2014. O código abaixo mostra o passo-a-passo da manipulação de dados.\n\n#&gt; Encontra as top-15 cidades com maior número de vendas em 2014\ntop_cities &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas o ano de 2014\n  filter(year == 2014) |&gt; \n  #&gt; Calcula o total de vendas em cada cidade\n  summarise(total = sum(listings, na.rm = TRUE), .by = \"city\") |&gt; \n  #&gt; Seleciona o top-15\n  slice_max(total, n = 15) |&gt; \n  pull(city)\n\n#&gt; Calcula o total de vendas anuais na subamostra de cidades e faz o ranking anual\n\nrank_housing &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas cidades dentro da subamostra\n  filter(city %in% top_cities, year &gt; 2005, year &lt; 2015) |&gt; \n  #&gt; Calcula o total de vendas a cada ano\n  summarise(\n    listing_year = sum(listings, na.rm = TRUE),\n    .by = c(\"city\", \"year\")\n    ) |&gt; \n  #&gt; Faz o ranking das cidades dentro de cada ano\n  mutate(rank = rank(-listing_year, \"first\"), .by = \"year\")\n\nNo gráfico abaixo, cada cidade tem uma cor diferente, mas omito a legenda de cores. Note o uso de scale_y_reverse já que, tipicamente, queremos mostrar os menores valores na parte superior do gráfico.\n\nggplot(rank_housing, aes(year, rank, group = city, color = city)) +\n  geom_bump() +\n  scale_y_reverse() +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nPara melhorar a legibilidade do gráfico podemos colocar o nome das cidades ao lado das linhas usando geom_text.\n\nggplot() +\n  geom_bump(\n    data = rank_housing,\n    aes(year, rank, group = city, color = city)\n    ) +\n  geom_text(\n    data = filter(rank_housing, year == max(year)),\n    aes(year, rank, label = city),\n    nudge_x = 0.1,\n    hjust = 0\n  ) +\n  scale_y_reverse() +\n  scale_x_continuous(limits = c(NA, 2017)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nMesmo com o nome das cidades, há muitas linhas para acompanhar no gráfico. Imagine, por exemplo, que queremos destacar apenas algumas das cidades selecionadas. Não parece fazer muito sentido destacar as cidades do top 4 (Houston, Dallas, San Antonio e Austin), já que elas praticamente não se alternam no ranking durante todo o período. Vamos, então, destacar as cidades de Bay Area, El Paso, Corpus Christi e Tyler.\n\nsel_cities &lt;- c(\"Bay Area\", \"El Paso\", \"Corpus Christi\", \"Tyler\")\n\nrank_housing &lt;- rank_housing |&gt; \n  mutate(\n    highlight = if_else(city %in% sel_cities, city, \"\"),\n    is_highlight = factor(if_else(city %in% sel_cities, 1L, 0L))\n  )\n\nO gráfico abaixo exige um código considervalmente mais longo, mas melhora o gráfico original em vários aspectos. Agora temos um maior destaque para as cidades de interesse. Os eixos estão melhor delimitados e as linhas de fundo redundantes foram removidas.\n\n\nCode\nggplot() +\n  #&gt; Linhas em cinza (sem destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 0),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 0.8,\n    smooth = 8\n    ) +\n  #&gt; Linhas coloridas (com destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 1),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 2,\n    smooth = 8\n  ) +\n  #&gt; Pontos \n  geom_point(\n    data = rank_housing,\n    aes(year, rank, color = highlight),\n    size = 4\n  ) +\n  #&gt; Nomes sem destaque\n  geom_text(\n    data = filter(rank_housing, year == max(year), !(city %in% sel_cities)),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    color = \"gray20\"\n  ) +\n  #&gt; Nome com destaque (em negrito)\n  geom_text(\n    data = filter(rank_housing, year == max(year), city %in% sel_cities),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    fontface = \"bold\"\n  ) +\n  #&gt; Adiciona os eixos para melhorar leitura do gráfico\n  scale_y_reverse(breaks = 1:15) +\n  scale_x_continuous(limits = c(NA, 2017), breaks = 2006:2014) +\n  #&gt; Cores\n  scale_color_manual(\n    values = c(\"gray70\", \"#2f4858\", \"#86bbd8\", \"#f6ae2d\", \"#f26419\")\n  ) +\n  #&gt; Elementos temáticos\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#dados",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#dados",
    "title": "Bump Plots",
    "section": "Dados",
    "text": "Dados\nPrimeiro defino alguns objetos úteis como o nome dos países que serão destacados e o nome das colunas que contém as variáveis de PIB. Além disso, crio uma tabela menor que contém apenas as colunas necessárias para a visualização.\n\ncountries_sel &lt;- c(\"Norway\", \"Belgium\", \"Austria\", \"United States\", \"Germany\")\n\nmeasures &lt;- c(\"gdp_over_pop\", \"gdp_ppp_over_pop\", \"gdp_ppp_over_k_hours_worked\")\n\nsub &lt;- dat |&gt; \n  select(country, year, all_of(measures)) |&gt; \n  na.omit()\n\nA transformação essencial é converter os dados em formato tidy e ranquear as observações dentro de cada métrica de PIB.\n\nranking &lt;- sub |&gt; \n  filter(year == max(year)) |&gt; \n  pivot_longer(cols = -c(country, year), names_to = \"measure\") |&gt; \n  mutate(rank = rank(-value), .by = \"measure\")\n\nAgora, mais por conveniência, eu crio algumas variáveis auxiliares que serão úteis para mapear os diferentes elementos estéticos.\n\nranking &lt;- ranking |&gt; \n  mutate(\n    highlight = if_else(country %in% countries_sel, country, \"\"),\n    highlight = factor(highlight, levels = c(countries_sel, \"\")),\n    is_highlight = factor(if_else(country %in% countries_sel, 1L, 0L)),\n    rank_labels = if_else(rank %in% c(1, 5, 10, 15, 20), rank, NA),\n    rank_labels = stringr::str_replace(rank_labels, \"^1$\", \"1st\"),\n    measure = factor(measure, levels = measures)\n    )\n\nPor fim, eu defino as cores das linhas e crio uma tabela auxiliar que contém apenas o texto que vai em cima do gráfico.\n\ncores &lt;- c(\"#101010\", \"#f7443e\", \"#8db0cc\", \"#fa9494\", \"#225d9f\", \"#c7c7c7\")\n\ndf_gdp &lt;- tibble(\n  measure = measures,\n  measure_label = c(\n    \"GDP per person at market rates\",\n    \"Adjusted for cost differences*\",\n    \"Adjusted for costs and hours worked\"\n  ),\n  position = -1.2\n)\n\ndf_gdp &lt;- df_gdp |&gt; \n  mutate(\n    measure = factor(measure, levels = measures),\n    measure_label = stringr::str_wrap(measure_label, width = 12),\n    measure_label = paste0(\"  \", measure_label)\n    )\n\nA versão simplificada do gráfico está resumida no código abaixo. Vale notar o uso da coord_cartesian para “cortar o gráfico” sem perder informação. Não é muito usual utilizar linewidth como um elemento estético dentro de aes mas pode-se ver como isto é bastante simples e como isto economiza algumas linhas de código, quando comparado com o gráfico anterior.\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]]),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2))\n\n\n\n\n\n\n\n\nNote que o gráfico acima inclui alguns países que não aparecem na visualização original como Irlanda e Luxemburgo. Existe um certo debate sobre a inflação do PIB per capita da Irlanda (devido ao grande número de empresas estrangeiras que mantêm suas sedes no país, em função dos baixos impostos da Irlanda). Um argumento similar pode ser feito sobre Luxemburgo. Ainda assim, decidi manter os dois países no gráfico final.\nO código final, como de costume, é bastante extenso. De maneira geral, o resultado é bastante satisfatório.\n\n\nCode\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  #&gt; Nome dos páises sem destaque\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight != 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  #&gt; Nome dos páises com destaque (em negrito)\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight == 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\"\n  ) +\n  #&gt; \"Eixo\" na esquerda (1st, 5, 10, 15, 20)\n  geom_text(\n    data = filter(ranking, measure == measures[[1]]),\n    aes(x = measure, y = rank, label = rank_labels),\n    nudge_x = -0.15,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  #&gt; Texto descritivo acima do gráfico\n  geom_text(\n    data = df_gdp,\n    aes(x = measure, y = position, label = measure_label),\n    inherit.aes = FALSE,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\"\n  ) +\n  #&gt; Posiciona as flechas apontando para baixo\n  annotate(\"text\", x = 1, y = -2.2, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 2, y = -2.2, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 3, y = -2.2, label = expression(\"\\u2193\")) +\n  #&gt; Corta o gráfico\n  coord_cartesian(ylim = c(21, -2)) +\n  #&gt; Cores\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  #&gt; Espessura das linhas\n  scale_linewidth_manual(values = c(0.5, 1.2)) +\n  #&gt; Elementos temáticos\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nO objetivo destes posts é de sempre fazer o máximo possível usando ggplot2 mas, na prática, as caixas de texto acima do gráfico podem ser feitas num software externo. Não é muito fácil usar caracteres especiais (neste caso, flechas) e a própria fonte (Lato) não inclui flechas em unicode. Pode-se melhorar a ordem da sobreposição das linhas usando geom_bump duas vezes como fizemos no gráfico dos imóveis, mas isto exigiria várias linhas adicionais de código."
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html",
    "href": "posts/general-posts/2023-09-happiness/index.html",
    "title": "Life Satisfaction and GDP per capita",
    "section": "",
    "text": "In this tutorial post I will replicate this graph, from OurWorldInData (OWID) using ggplot2. The original graph is available at OWID website. The plot shows the correlation between GDP per capita and self-reported happiness. The income data comes from the World Bank and is in 2017 constant PPP dollars: this ensures the data is comparable across countries since it accounts for both inflation and different costs of living. The happiness data comes from the World Happiness Report.\n\nThis plot has several attractive features including how colors are used to represent continents and how size is used to show the population of each country.\nTo follow this tutorial make sure all of the packages below are installed.\n\n#&gt; Packages needed to replicate the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(showtext)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n\n\n\nThe first step is acquiring the data. Luckily, the csv data is readily available at the OWID website. For convenience I stored a smaller version of the dataset in my Github. The code below imports the data directly into the R session.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nThe data shows both the GDP per capita and the Happiness Indicator for several countries across many years. The code below selects only the most recent that is available for each country.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#the-data",
    "href": "posts/general-posts/2023-09-happiness/index.html#the-data",
    "title": "Life Satisfaction and GDP per capita",
    "section": "",
    "text": "The first step is acquiring the data. Luckily, the csv data is readily available at the OWID website. For convenience I stored a smaller version of the dataset in my Github. The code below imports the data directly into the R session.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nThe data shows both the GDP per capita and the Happiness Indicator for several countries across many years. The code below selects only the most recent that is available for each country.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#bubbles",
    "href": "posts/general-posts/2023-09-happiness/index.html#bubbles",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Bubbles",
    "text": "Bubbles\nThe most essential aspect of this plot is summarized in the code below. The plot shows each country as bubble, where the size of the bubble is proportional to its population. The position of each bubble shows the country’s GDP per capita (on the horizontal axis) and average life satisfaction (on the vertical axis). Finally, the color of each bubble corresponds to the continent of the country. Since many of the observations overlap the original plot uses a bit of transparency.\nNote that I use shape = 21 to get a special circle with two colors. The color argument controls the circle’s border while the fill argument controls the circle’s interior color.\n\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#highlighting-the-countries",
    "href": "posts/general-posts/2023-09-happiness/index.html#highlighting-the-countries",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Highlighting the countries",
    "text": "Highlighting the countries\nThis part is mostly manual labor. I create a simple vector with the names of all countries highlighted in the original plot. This vector allows me to create a dummy variable that indicates whether the name of the country should be plotted or not. For increased flexibility I store this as an auxiliar tibble called dftext. In the end, this didn’t make much of a difference but it can be helpful in cases where one needs finer control over the text that is plotted.\nI ggrepel to avoid overlapping the text labels.\n\n#&gt; Countries to highlight\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\n#&gt; Auxiliar tibble with names of countries to highlight\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\n#&gt; Creates a base plot with the bubbles plus the text labels\nbase_plot &lt;- ggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_text_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#scales-and-colors",
    "href": "posts/general-posts/2023-09-happiness/index.html#scales-and-colors",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Scales and colors",
    "text": "Scales and colors\nThe x-axis of the original plot is in a logarithmic scale and its labels highlight specific values (1000, 2000, 5000, …, 100000). The numbers are formatted with a comma and dollar sign. The y-axis is much more straightforward since the numbers are simple integers ranging from 3 to 7.\nThe default size of the bubbles in ggplot is small so I use scale_size_continuous to increase them. I have no idea how to emulate the original size legend (the circle within a circle) so I omit it.\nBoth the interior color of the bubbles and the text follow a particular color scheme. I got the exact colors of the original plot by exporting it to SVG. By default, the legend key inherits its colors. So the legend shows slightly transparent round circles. To get solid colored squares I override this default behavior.\n\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\nbase_plot &lt;- base_plot +\n  #&gt; Adds labels to the log scale\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  #&gt; Adds labels to the y-axis scale\n  scale_y_continuous(breaks = 3:7) +\n  #&gt; Increases the size of the bubbles\n  scale_size_continuous(range = c(1, 15)) +\n  #&gt; Adds colors to the bubbles and text labels\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  #&gt; Removes the size and color legend\n  guides(\n    color = \"none\",\n    size = \"none\",\n    #&gt; Override default behaviour to get solid colors\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#text-elements",
    "href": "posts/general-posts/2023-09-happiness/index.html#text-elements",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Text elements",
    "text": "Text elements\nThe code below inserts the other textual elements of the plot.\n\n#&gt; Caption\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n#&gt; Subtitle\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\n#&gt; Adds textual elements to base plot\nbase_plot &lt;- base_plot +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#font",
    "href": "posts/general-posts/2023-09-happiness/index.html#font",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Font",
    "text": "Font\nLooking at the source code of the page it seems that most of the text is displayed in Lato while the titles are displayed in Playfair Display. I import both fonts using font_add_google and load them using showtext.\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nfont_add_google(\"Lato\", \"Lato\")\n\nshowtext::showtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#theme",
    "href": "posts/general-posts/2023-09-happiness/index.html#theme",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Theme",
    "text": "Theme\nThis step really does the magic.\nFor the theme, I use theme_minimal as a template, since its fairly similar to the OWID graphic. I start by removing the minor panel grids from the background and changing the major panel grids. Then, I alter the textual elements of the graphic and make minor tweaks to the legend and plot margins.\nAll of the text is in different shades of gray with exception of the title, the axis titles, and the legend text which are all in plain black. Figuring out the sizes of the text is mostly a trial and error process. Almost all of the text is small except the main title and the text on the axes.\n\nbase_plot +\n  theme_minimal() +\n  theme(\n    #&gt; Background grid\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(linetype = 3, color = \"#DDDDDD\"),\n    \n    #&gt; Text elements (change font)\n    text = element_text(family = \"Lato\"),\n    title = element_text(family = \"Lato\"),\n    #&gt; Caption, title, and subtitle\n    plot.caption = element_text(color = \"#777777\", hjust = 0, size = 8),\n    plot.title = element_text(\n      color = \"#444444\",\n      family = \"Playfair Display\",\n      size = 18),\n    plot.subtitle = element_text(color = \"#666666\", size = 11),\n    #&gt; Axis text\n    axis.title = element_text(color = \"#000000\", size = 9),\n    axis.text = element_text(color = \"#666666\", size = 12),\n    #&gt; Legend\n    legend.key.size = unit(5, \"pt\"),\n    legend.position = \"right\",\n    legend.text = element_text(size = 10),\n    #&gt; Margin\n    plot.margin = margin(rep(10, 4))\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-pib-municipios/index.html",
    "href": "posts/general-posts/2024-04-wz-pib-municipios/index.html",
    "title": "GDP in Brazil",
    "section": "",
    "text": "Code\n# Libraries\nlibrary(geobr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(MetBrewer)\nlibrary(showtext)\n\n# Color palette\ncores = met.brewer(\"Hiroshige\", 20)[c(1, 8, 15, 20)]\n# Import Gill Sans locally\nfont_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext_auto()\n\n# Data ------------------------------------------------------------------\n\n# Shapes\n\n# Import city shapefile\nmunis &lt;- read_municipality(year = 2022, showProgress = FALSE)\n# Import state shapefile\ngeostate &lt;- read_state(showProgress = FALSE)\n\n# Import city data from Brazil (see data/raw/R/painel_municipios.R)\npanel &lt;- readr::read_csv(here::here(\"static/data\", \"cities_brazil.csv\"))\n\n## Data cleaning --------------------------------------------------------\n\n# Vector to select GDP columns\npib_cols = c(\"pib_agriculture\", \"pib_industrial\", \"pib_services\",\n             \"pib_govmt_services\")\n\n# Compute GDP share by city\npib_share = panel |&gt; \n  select(all_of(c(\"code_muni\", \"pib\", pib_cols))) |&gt; \n  pivot_longer(cols = 3:last_col(), names_to = \"sector\", values_to = \"total\") |&gt; \n  mutate(\n    share = total / pib * 100,\n    # Improve text labels\n    sector_label = str_remove(sector, \"pib_\"),\n    sector_label = str_to_title(sector_label),\n    sector_label = str_replace(sector_label, \"Govmt_services\", \"Government\"),\n    sector_label = str_replace(sector_label, \"Industrial\", \"Industry\")\n    )\n\n# Convert to wider\ntab_pib_share = pib_share |&gt; \n  pivot_wider(\n    id_cols = \"code_muni\",\n    names_from = \"sector\",\n    values_from = \"share\"\n    )\n\n# Find the most relevant economic sector by city\ntop_pib_share = pib_share |&gt; \n  filter(share == max(share), .by = \"code_muni\") |&gt; \n  select(code_muni, top_sector = sector_label)\n\n# Join tabular data with city shapefile\nmap_share = munis |&gt; \n  left_join(tab_pib_share, by = \"code_muni\") |&gt; \n  left_join(top_pib_share, by = \"code_muni\")\n\n# Map -------------------------------------------------------------------\n\nm1 = ggplot() +\n  # State borders\n  geom_sf(data = geostate, lwd = 0.5, color = \"gray30\", fill = \"white\") +\n  # City colors\n  geom_sf(\n    data = na.omit(map_share),\n    aes(fill = top_sector, color = top_sector),\n    lwd = 0.01\n    ) +\n  # Remove junk from the east coast\n  coord_sf(xlim = c(NA, -35)) +\n  # Use Hiroshige colors\n  scale_fill_manual(name = \"\", values = cores) +\n  scale_color_manual(name = \"\", values = cores) +\n  labs(\n    title = \"Top GDP contribution by City (2021)\",\n    subtitle = \"Colors represent the most relevant economic contributing group in each city\",\n    caption = \"Source: National Accounts, IBGE (2023)\") +\n  ggthemes::theme_map(base_family = \"Gill Sans\") +\n  # Thematic elements\n  theme(\n    # Background\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    # Legend\n    legend.position = \"left\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.5, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    # Labels (title, subtitle)\n    plot.title = element_text(size = 38, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5),\n    # Plot margins\n    plot.margin = margin(10, 5, 5, 10)\n  )\n\n# Histogram -------------------------------------------------------------\n\npib_bars = top_pib_share |&gt; \n  count(top_sector) |&gt; \n  mutate(top_sector = forcats::fct_rev(top_sector))\n\np_hist = ggplot(pib_bars, aes(top_sector, n)) +\n  geom_col(aes(fill = top_sector)) +\n  # Number labels\n  geom_text(\n    aes(y = n, label = n, color = factor(c(0, 0, 0, 1))),\n    size = 4,\n    family = \"Gill Sans\",\n    nudge_y = c(-200, -200, 200, -200)\n    ) +\n  # Text labels\n  geom_text(\n    aes(y = 50, label = top_sector, color = factor(c(0, 0, 0, 1))),\n    family = \"Gill Sans\",\n    size = 4,\n    hjust = 0\n  ) +\n  # Flip axis\n  coord_flip() +\n  scale_fill_manual(values = rev(cores)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  labs(x = NULL, y = NULL, title = \"Number of cities\") +\n  guides(fill = \"none\", color = \"none\") +\n  theme_minimal(base_family = \"Gill Sans\") +\n  theme(\n    plot.title = element_text(size = 14, hjust = 0.5),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text = element_blank()\n    )\n\n\n\nGDP Contribution by city\n\nThe number of cities where Agriculture is the main driver of the local economy rose from 1049 in 2020 to 1272 in 2021. Agriculture is most prevalent in the Midwest cities.\nPublic Administration is the most prevalent economic activity among cities in the North and Northeast of the country. General services are most prevalent in the Southeast.\nIndustrial activities constitute the primary economic driver for fewer than 8% of cities. The few industrial cities are spatially disperse across the country, exhibiting no clear pattern.\n\n\n\n\n\n\n\n\n\n\n\nDados: IBGE, Contas Nacionais, Produto Interno Bruto dos Municípios (2023)\nTipografia: Gill Sans\nPaleta: Hiroshige (MetBrewer)"
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html",
    "title": "Preços de Imóveis e Demografia",
    "section": "",
    "text": "Já vi em alguns lugares uma suposta ligação entre fatores demográficos e tendências de longo prazo no mercado imobiliário. Intuitivamente, alguns dos principais motivadores para comprar ou vender um imóvel estão ligados a fatores demográficos: nascimentos, casamentos, divórcios ou óbitos.\nEste tipo de análise omite fatores importantes como renda, condições de financiamento e o contexto geral da economia. Ainda assim, fiquei curioso para ver se havia algum padrão entre tendências demográficas mais simples e o comportamento dos preços."
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html#todos-os-países",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html#todos-os-países",
    "title": "Preços de Imóveis e Demografia",
    "section": "Todos os países",
    "text": "Todos os países\nEsta análise é ainda bastante preliminar. Ainda que a demografia seja um motor para a demanda imobiliária, outros fatores como oferta de moradia e condições de crédito são importantes demais parecem serem omitidos.\nO gráfico abaixo compara a população em 2010/2020 com os preços em 2010/2020. Os países ao lado direito do gráfico, são os países onde houve crescimento populacional. Os países na parte de cima do gráfico são os países onde houve crescimento real do preço dos imóveis. Na média da amostra, destacada como WLD, houve crescimento de ambos.\nPaíses que estão muito para cima como Chile (CHL), Índia (IND) e Estônia (EDT) estão com imóveis muito “caros”. Já em países com França e Finlândia tanto a população como o nível de preço dos imóveis cresceram muito pouco. No caso da Grécia, tanto a população como o preço dos imóveis diminuiu nos últimos dez anos.\nO gráfico não me surpreendeu muito, mas esperava que os EUA estivessem mais para cima no gráfico e que o Brasil estivesse ao menos do lado positivo do eixo-x. Isso me sugere que a impressão de que os imóveis no Brasil são ou estão caros tem muito mais a ver com a baixa renda da população.\n\n\nCode\np5 &lt;- \n  ggplot(\n    data = na.omit(tbl_wide),\n    aes(x = index_pop, y = index_house)\n    ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, color = colors[1]) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(aes(label = iso3c, color = highlight)) +\n  scale_x_continuous(breaks = seq(-10, 30, 5)) +\n  scale_y_continuous(breaks = seq(-30, 90, 15)) +\n  scale_color_manual(values = c(\"black\", colors[1])) +\n  guides(color = \"none\") +\n  labs(\n    title = \"Real House Prices x Population (2010/20)\",\n    x = \"Population (2010/2020)\",\n    y = \"RPPI (2010/2020)\",\n    caption = \"Source: Real House Price Indexes (BIS), Population (UN).\") +\n  theme_vini\n\np5"
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html#footnotes",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html#footnotes",
    "title": "Preços de Imóveis e Demografia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHong Kong: https://exame.com/economia/as-raizes-economicas-dos-protestos-de-hong-kong/↩︎"
  },
  {
    "objectID": "posts/general-posts/2025-07-demografia-brasil/index.html",
    "href": "posts/general-posts/2025-07-demografia-brasil/index.html",
    "title": "O Novo Perfil Demográfico do Brasil",
    "section": "",
    "text": "O Brasil tem a maior geração de jovens (12-43 anos) da sua história. Conhecidas como Geração-Z e Millenialls, estes jovens e jovens-adultos somam quase 100 milhões de habitantes.\nA população brasileira já é idosa e vai ficar ainda mais nas próximas décadas. A proporção de idosos em relação aos jovens cresceu 300% desde 1991. Atualmente, há 30 milhões de idosos (60 anos ou mais) e este número deve saltar para 50 milhões até 2050.\nSul e Sudeste lideram o envelhecimento populacional. Essas regiões são as mais envelhecidas do país, trazendo desafios econômicos. Serão necessárias adaptações no mercado de trabalho, saúde pública, habitação e infraestrutura urbana para atender esta nova realidade demográfica.\nA região Norte ainda tem um bônus demográfico significativo e será a última a convergir para taxas de dependência mais elevadas. O Centro-Oeste e Nordeste apresentam estados em transição com sinais claros de envelhecimento.\n\n\n\n\nO envelhecimento populacional do Brasil parece ter entrado apenas recentemente no debate público. A discussão em torna de reforma previdenciária de 2017-18 trouxe à tona a trajetória de envelhecimento da população; o debate, contudo, acabou sendo excessivamente politizado e marcado por erros conceituais simples, como a confusão entre expectativa de vida e expectativa de sobrevida.\nMais recentemente, o debate demográfico voltou à discussão após a surpreendente redução da população observada no Censo de 2022. Também houve um significativo aumento do índice de envelhecimento da população. Contudo, os sinais da desaceleração demográfica brasileira já estavam presentes há mais tempo. A taxa de fecundidade brasileira, que era de 6,3 filhos/mulher em 1960, caiu para 2,4 já em 2000 e atualmente está em 1,6 — valor bastante abaixo da taxa de resposição e próximo de países como França e Estados Unidos.\n\n\n\nO Brasil atravessa um momento único em sua história demográfica, experimentando simultaneamente o auge de sua população jovem e o início de um envelhecimento populacional acelerado. Com quase 100 milhões de habitantes entre 12 e 43 anos - as gerações Z e Millennials - o país dispõe da maior concentração de jovens de sua história, representando quase metade da população nacional. Paradoxalmente, as baixas taxas de fecundidade e o aumento de casais sem filhos indicam que esta será também a última grande geração jovem brasileira. Os números mais recentes da ONU projetam que a população idosa do Brasil deve crescer de 30 milhões para 50 milhões até 2050.\nEsta transição demográfica revela profundas disparidades regionais que espelham diferentes estágios de desenvolvimento socioeconômico. Enquanto o Norte mantém altas taxas de natalidade e baixa mortalidade, caracterizando uma população jovem em expansão, o Sul e Sudeste já apresentam baixas taxas de natalidade combinadas com alta mortalidade, sinalizando sociedades envelhecidas com transição demográfica avançada.\nO Índice de Envelhecimento nacional saltou de 13,9 para 55,2 entre 1991 e 2022, mas com variações regionais. O Rio Grande do Sul, em particular, enfrenta os maiores desafios: quase metade dos seus municípios já tem mais idosos que jovens. Esta heterogeneidade demográfica representa tanto uma janela de oportunidade histórica quanto um desafio de planejamento que exigirá políticas públicas regionalizadas e coordenadas para aproveitar o bônus demográfico atual e preparar o país para uma sociedade crescentemente envelhecida.\n\n\n\n\nO Brasil está num momento singular na sua transição demográfica. Ao mesmo tempo em que a população está envelhecendo e menos pessoas estão nascendo, temos a maior geração de jovens da nossa história.\n\n\nQuase metade da população brasileira, atualmente, tem entre 12 e 43 anos. Conhecidas como Geração-Z e Millenialls, estes jovens e jovens-adultos somam quase 100 milhões de habitantes.\nA baixa taxa de fecundidade e o aumento de casais sem filhos, contudo, indica que haverá menos jovens no futuro. Os maiores de 60 anos representam cerca de 30 milhões (15% da população), mas devem somar 50 milhões já em 2050.\n\n\n\n\n\n\n\n\n\n\n\nDefinindo Gerações  As gerações no gráfico seguem a definição do Beresford Research. Segundo o estudo, os Millenials (Geração Y) são aqueles nascidos entre 1981-96 enquanto a Geração-Z são aqueles nascidos entre 1997-2012.\n\n\nO envelhecimento da população é uma realidade nacional, como mostra a evolução do Índice de Envelhecimento (IE) de 13,9 em 1991 para 55,2 em 2022 - um crescimento de quase 300% em três décadas.\nA distribuição espacial revela forte contraste regional: o Norte mantém-se predominantemente jovem com IE entre 25-50, enquanto o Sul e o Sudeste apresentam índices mais elevados. O Nordeste exibe padrão heterogêneo - o interior ainda apresenta população mais jovem, mas a faixa litorânea já demonstra envelhecimento.\n\n\n\n\n\n\n\n\n\n\n\nDefinição: IE  O Índice de Envelhecimento (IE) é definido como a razão entre idosos e jovens da seguinte forma:\n\\[\n\\text{Ind} = 100\\times\\frac{\\text{P}_{\\text{jovem}}}{\\text{P}_{\\text{idosa}}}\n\\]\nonde \\(\\text{P}_{\\text{jovem}}\\) é o número de habitantes com até 14 anos de idade e \\(\\text{P}_{\\text{idosa}}\\) é o número de habitantes com 65 anos ou mais. Valores menores que 100 indicam que há mais jovens do que idosos e vice-versa.\nComo se vê no mapa, há municípios onde o IE ultrapassa 100, indicando que ali há mais idosos do que jovens.\nSanta Catarina destaca-se como exceção no Sul, mantendo índices moderados devido ao intenso fluxo migratório de adultos em idade produtiva atraídos pelas oportunidades econômicas do estado. No Centro-Oeste, observa-se um padrão misto, com o Distrito Federal e áreas urbanas apresentando maior envelhecimento.\nO mapa das taxas brutas de natalidade (TBN) e mortalidade (TBM) por mil habitantes revela dois padrões demográficos distintos no Brasil.\n\n\nA região Norte (tons azuis) apresenta alta natalidade e baixa mortalidade, caracterizando uma população jovem em expansão. O Sul e partes do Sudeste exibem o padrão oposto - baixa natalidade e alta mortalidade - indicando regiões com transição demográfica avançada e população envelhecida. No Rio Grande do Sul, diversos municípios registram mais óbitos do que nascimentos.\n\n\n\n\n\n\n\n\n\n\n\nDefinições: TBN e TBM  A Taxa Bruta de Natalidade (TBN) é o número de nascidos vivos em relação à população total. Formalmente, \\[\n\\text{TBN} = \\frac{\\text{N}}{\\text{P}} \\times 1000\n\\] onde N é o número de nascimentos em um ano e P é a população total no meio do período. Analogamente, define-se a Taxa Bruta de Mortalidade (TBM) como: \\[\n\\text{TBM} = \\frac{\\text{O}}{\\text{P}} \\times 1000\n\\] onde O é o número de óbitos. Ambas as taxas são expressas por mil habitantes.\nEsta janela demográfica representa tanto uma oportunidade quanto um desafio de planejamento. Com quase 100 milhões de jovens atualmente no país, o Brasil dispõe de um potencial produtivo e inovador sem precedentes, mas que não se repetirá nas próximas décadas.\nO contraste regional - com o Norte ainda experimentando crescimento populacional enquanto o Sul e Sudeste aceleram o envelhecimento - exige políticas públicas diferenciadas e coordenadas. O sucesso em aproveitar este momento único dependerá da capacidade de investir na qualificação desta geração, criar oportunidades de trabalho e renda, e simultaneamente preparar o país para uma sociedade crescentemente envelhecida.\n\n\n\n\nA razão de dependência mede a proporção da população que depende, num sentido amplo, do trabalho da população economicamente ativa. No Brasil, o IBGE define a Razão de Dependência Total (RDT), como a razão entre o número de jovens e idosos e o número de adultos (em idade de trabalhar).\n\n\nDefinições: RDT, RDI e RDJ  Formalmente, define-se a Razão de Dependência Total (RDT) como:\n\\[\n\\text{RDT} = \\frac{\\text{P}_{\\text{jovem}} + \\text{P}_{\\text{idosa}}}{\\text{PEA}}\n\\]\nonde PEA é a população economicamente ativa. Como critério de corte, define-se jovem a pessoa com até 14 anos e idoso a pessoa com 65 anos ou mais.\nAnalogamente, define-se a Razão de Dependência Jovem (RDJ) e a Razão de Dependência Idosa (RDI) como:\n\\[\n\\begin{align}\n\\text{RDJ} & = \\frac{\\text{P}_{\\text{jovem}}}{\\text{PEA}} \\\\\n\\text{RDI} & = \\frac{\\text{P}_{\\text{idosa}}}{\\text{PEA}}\n\\end{align}\n\\]\nA elevação da RDT representa um desafio nacional, porém com origens regionais distintas: enquanto algumas regiões enfrentam alta dependência devido ao excesso de jovens, outras lidam com o peso crescente da população idosa.\nGrandes Regiões \n\n\nAs regiões Sul e Norte ilustram os contrastes demográficos do Brasil. Apesar de estarem atualmente em situações demográficas opostas, ambas as regiões convergem para padrões similares em 2060, com RDI elevada e superior à RDJ.\nO Sul vai liderar o envelhecimento nacional, com sua RDI crescendo de 15 para 45 entre 2000-2060. A região já experimenta o ponto de inflexão demográfico por volta de 2025-2030, quando terá mais idosos dependentes que jovens – reflexo de décadas de baixa fecundidade e maior longevidade.\nO Norte parte da situação oposta: alta dependência jovem (RDJ de 50) e baixa dependência idosa (RDI de 8). Contudo, a região enfrenta a transformação mais dramática do país: a RDJ despenca para 25 enquanto a RDI quintuplica para 40, com o ponto de inflexão ocorrendo em 2040-2045.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRDT no Brasil por Regiões\n\n\nRegião\n2010\n2015\n2020\n2025\n2030\n2035\n2040\n2045\n2050\n2055\n2060\n\n\n\n\nNorte\n57.4\n51.1\n47.3\n45.4\n44.8\n45.0\n45.7\n47.8\n50.7\n53.8\n57.0\n\n\nNordeste\n52.1\n47.2\n45.0\n44.8\n46.0\n47.6\n49.2\n52.5\n57.0\n61.8\n66.3\n\n\nSudeste\n43.7\n42.2\n43.6\n46.6\n49.5\n51.8\n54.2\n57.9\n62.5\n66.8\n69.9\n\n\nSul\n43.6\n42.2\n44.2\n48.1\n52.0\n54.8\n56.9\n59.9\n64.2\n68.5\n72.7\n\n\nCentro Oeste\n44.7\n42.2\n42.4\n43.9\n45.7\n47.4\n49.5\n52.7\n56.4\n59.6\n62.1\n\n\n\nFonte: IBGE (Censo 2022)\n\n\n\n\n\n\n\n\n\n\nTodas as regiões brasileiras enfrentarão desafios similares do envelhecimento populacional, exigindo adaptações coordenadas nos sistemas de saúde, previdência e políticas públicas. A partir de 2050, a RDT passa de 50 em todas as regiões\n\n\n\n\nHá três perfis demográficos regionais bem definidos no Brasil: estados jovens, estados em transição e estados envelhecidos.\n\n\nOs estados do Norte do Brasil apresentam RDJ elevado e RDI baixo (azul-claro), indicando sociedades jovens onde a pressão sobre a PEA vem de crianças e adolescentes. Estados como São Paulo e Paraná mostram o padrão oposto (vermelho-escuro) caracterizando populações envelhecidas.\nO Nordeste e o Centro-Oeste exibem combinações variadas, com estados como Piauí e Minas Gerais apresentando RDJ e RDI equilibradas, indicando populações em plena transição demográfica. O Distrito Federal destaca-se como exceção regional, mantendo baixas dependências em ambas as categorias devido ao perfil migratório seletivo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs pirâmides etárias revelam perfis demográficos contrastantes entre os estados brasileiros. O Amazonas tipifica a região Norte com estrutura jovem – baixa RDI (8,9) e alta RDJ (40,9) – enquanto o Rio Grande do Sul representa o extremo oposto, liderando o envelhecimento nacional com RDI de 20,6 e RDJ de apenas 25,6.\nSanta Catarina se destaca como exceção, mantendo a menor RDI entre os estados do Sul e Sudeste devido ao influxo migratório de adultos em idade produtiva. O Piauí ilustra o Nordeste em transição, com pirâmide já apresentando base estreitada e topo expandido, sinalizando o início da mudança demográfica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Lendo o gráfico  A pirâmide demográfica mostra o percentual da população dentro de cada grupo de idade. Por convenção indivíduos do sexo masculino são apresentados à esquerda (verde) e indivíduos do sexo feminino à direita (roxo).\n\n\n\n\nO Brasil deve enfrentar diversos desafios relacionados à sua dinâmica demográfica nas próximas décadas. O fim do bônus demográfico no Brasil não é apenas uma questão estatística, mas uma transformação profunda que exigirá planejamento estratégico, investimentos direcionados e políticas públicas eficazes. O sucesso na gestão dessa transição determinará se o país conseguirá manter seu desenvolvimento e qualidade de vida nas próximas décadas.\n\n\n\n\nEnvelhecimento no Brasil\nNascimentos e Óbitos no Brasil"
  },
  {
    "objectID": "posts/general-posts/2025-07-demografia-brasil/index.html#resumo",
    "href": "posts/general-posts/2025-07-demografia-brasil/index.html#resumo",
    "title": "O Novo Perfil Demográfico do Brasil",
    "section": "",
    "text": "O Brasil tem a maior geração de jovens (12-43 anos) da sua história. Conhecidas como Geração-Z e Millenialls, estes jovens e jovens-adultos somam quase 100 milhões de habitantes.\nA população brasileira já é idosa e vai ficar ainda mais nas próximas décadas. A proporção de idosos em relação aos jovens cresceu 300% desde 1991. Atualmente, há 30 milhões de idosos (60 anos ou mais) e este número deve saltar para 50 milhões até 2050.\nSul e Sudeste lideram o envelhecimento populacional. Essas regiões são as mais envelhecidas do país, trazendo desafios econômicos. Serão necessárias adaptações no mercado de trabalho, saúde pública, habitação e infraestrutura urbana para atender esta nova realidade demográfica.\nA região Norte ainda tem um bônus demográfico significativo e será a última a convergir para taxas de dependência mais elevadas. O Centro-Oeste e Nordeste apresentam estados em transição com sinais claros de envelhecimento.\n\n\n\n\nO envelhecimento populacional do Brasil parece ter entrado apenas recentemente no debate público. A discussão em torna de reforma previdenciária de 2017-18 trouxe à tona a trajetória de envelhecimento da população; o debate, contudo, acabou sendo excessivamente politizado e marcado por erros conceituais simples, como a confusão entre expectativa de vida e expectativa de sobrevida.\nMais recentemente, o debate demográfico voltou à discussão após a surpreendente redução da população observada no Censo de 2022. Também houve um significativo aumento do índice de envelhecimento da população. Contudo, os sinais da desaceleração demográfica brasileira já estavam presentes há mais tempo. A taxa de fecundidade brasileira, que era de 6,3 filhos/mulher em 1960, caiu para 2,4 já em 2000 e atualmente está em 1,6 — valor bastante abaixo da taxa de resposição e próximo de países como França e Estados Unidos.\n\n\n\nO Brasil atravessa um momento único em sua história demográfica, experimentando simultaneamente o auge de sua população jovem e o início de um envelhecimento populacional acelerado. Com quase 100 milhões de habitantes entre 12 e 43 anos - as gerações Z e Millennials - o país dispõe da maior concentração de jovens de sua história, representando quase metade da população nacional. Paradoxalmente, as baixas taxas de fecundidade e o aumento de casais sem filhos indicam que esta será também a última grande geração jovem brasileira. Os números mais recentes da ONU projetam que a população idosa do Brasil deve crescer de 30 milhões para 50 milhões até 2050.\nEsta transição demográfica revela profundas disparidades regionais que espelham diferentes estágios de desenvolvimento socioeconômico. Enquanto o Norte mantém altas taxas de natalidade e baixa mortalidade, caracterizando uma população jovem em expansão, o Sul e Sudeste já apresentam baixas taxas de natalidade combinadas com alta mortalidade, sinalizando sociedades envelhecidas com transição demográfica avançada.\nO Índice de Envelhecimento nacional saltou de 13,9 para 55,2 entre 1991 e 2022, mas com variações regionais. O Rio Grande do Sul, em particular, enfrenta os maiores desafios: quase metade dos seus municípios já tem mais idosos que jovens. Esta heterogeneidade demográfica representa tanto uma janela de oportunidade histórica quanto um desafio de planejamento que exigirá políticas públicas regionalizadas e coordenadas para aproveitar o bônus demográfico atual e preparar o país para uma sociedade crescentemente envelhecida.\n\n\n\n\nO Brasil está num momento singular na sua transição demográfica. Ao mesmo tempo em que a população está envelhecendo e menos pessoas estão nascendo, temos a maior geração de jovens da nossa história.\n\n\nQuase metade da população brasileira, atualmente, tem entre 12 e 43 anos. Conhecidas como Geração-Z e Millenialls, estes jovens e jovens-adultos somam quase 100 milhões de habitantes.\nA baixa taxa de fecundidade e o aumento de casais sem filhos, contudo, indica que haverá menos jovens no futuro. Os maiores de 60 anos representam cerca de 30 milhões (15% da população), mas devem somar 50 milhões já em 2050.\n\n\n\n\n\n\n\n\n\n\n\nDefinindo Gerações  As gerações no gráfico seguem a definição do Beresford Research. Segundo o estudo, os Millenials (Geração Y) são aqueles nascidos entre 1981-96 enquanto a Geração-Z são aqueles nascidos entre 1997-2012.\n\n\nO envelhecimento da população é uma realidade nacional, como mostra a evolução do Índice de Envelhecimento (IE) de 13,9 em 1991 para 55,2 em 2022 - um crescimento de quase 300% em três décadas.\nA distribuição espacial revela forte contraste regional: o Norte mantém-se predominantemente jovem com IE entre 25-50, enquanto o Sul e o Sudeste apresentam índices mais elevados. O Nordeste exibe padrão heterogêneo - o interior ainda apresenta população mais jovem, mas a faixa litorânea já demonstra envelhecimento.\n\n\n\n\n\n\n\n\n\n\n\nDefinição: IE  O Índice de Envelhecimento (IE) é definido como a razão entre idosos e jovens da seguinte forma:\n\\[\n\\text{Ind} = 100\\times\\frac{\\text{P}_{\\text{jovem}}}{\\text{P}_{\\text{idosa}}}\n\\]\nonde \\(\\text{P}_{\\text{jovem}}\\) é o número de habitantes com até 14 anos de idade e \\(\\text{P}_{\\text{idosa}}\\) é o número de habitantes com 65 anos ou mais. Valores menores que 100 indicam que há mais jovens do que idosos e vice-versa.\nComo se vê no mapa, há municípios onde o IE ultrapassa 100, indicando que ali há mais idosos do que jovens.\nSanta Catarina destaca-se como exceção no Sul, mantendo índices moderados devido ao intenso fluxo migratório de adultos em idade produtiva atraídos pelas oportunidades econômicas do estado. No Centro-Oeste, observa-se um padrão misto, com o Distrito Federal e áreas urbanas apresentando maior envelhecimento.\nO mapa das taxas brutas de natalidade (TBN) e mortalidade (TBM) por mil habitantes revela dois padrões demográficos distintos no Brasil.\n\n\nA região Norte (tons azuis) apresenta alta natalidade e baixa mortalidade, caracterizando uma população jovem em expansão. O Sul e partes do Sudeste exibem o padrão oposto - baixa natalidade e alta mortalidade - indicando regiões com transição demográfica avançada e população envelhecida. No Rio Grande do Sul, diversos municípios registram mais óbitos do que nascimentos.\n\n\n\n\n\n\n\n\n\n\n\nDefinições: TBN e TBM  A Taxa Bruta de Natalidade (TBN) é o número de nascidos vivos em relação à população total. Formalmente, \\[\n\\text{TBN} = \\frac{\\text{N}}{\\text{P}} \\times 1000\n\\] onde N é o número de nascimentos em um ano e P é a população total no meio do período. Analogamente, define-se a Taxa Bruta de Mortalidade (TBM) como: \\[\n\\text{TBM} = \\frac{\\text{O}}{\\text{P}} \\times 1000\n\\] onde O é o número de óbitos. Ambas as taxas são expressas por mil habitantes.\nEsta janela demográfica representa tanto uma oportunidade quanto um desafio de planejamento. Com quase 100 milhões de jovens atualmente no país, o Brasil dispõe de um potencial produtivo e inovador sem precedentes, mas que não se repetirá nas próximas décadas.\nO contraste regional - com o Norte ainda experimentando crescimento populacional enquanto o Sul e Sudeste aceleram o envelhecimento - exige políticas públicas diferenciadas e coordenadas. O sucesso em aproveitar este momento único dependerá da capacidade de investir na qualificação desta geração, criar oportunidades de trabalho e renda, e simultaneamente preparar o país para uma sociedade crescentemente envelhecida.\n\n\n\n\nA razão de dependência mede a proporção da população que depende, num sentido amplo, do trabalho da população economicamente ativa. No Brasil, o IBGE define a Razão de Dependência Total (RDT), como a razão entre o número de jovens e idosos e o número de adultos (em idade de trabalhar).\n\n\nDefinições: RDT, RDI e RDJ  Formalmente, define-se a Razão de Dependência Total (RDT) como:\n\\[\n\\text{RDT} = \\frac{\\text{P}_{\\text{jovem}} + \\text{P}_{\\text{idosa}}}{\\text{PEA}}\n\\]\nonde PEA é a população economicamente ativa. Como critério de corte, define-se jovem a pessoa com até 14 anos e idoso a pessoa com 65 anos ou mais.\nAnalogamente, define-se a Razão de Dependência Jovem (RDJ) e a Razão de Dependência Idosa (RDI) como:\n\\[\n\\begin{align}\n\\text{RDJ} & = \\frac{\\text{P}_{\\text{jovem}}}{\\text{PEA}} \\\\\n\\text{RDI} & = \\frac{\\text{P}_{\\text{idosa}}}{\\text{PEA}}\n\\end{align}\n\\]\nA elevação da RDT representa um desafio nacional, porém com origens regionais distintas: enquanto algumas regiões enfrentam alta dependência devido ao excesso de jovens, outras lidam com o peso crescente da população idosa.\nGrandes Regiões \n\n\nAs regiões Sul e Norte ilustram os contrastes demográficos do Brasil. Apesar de estarem atualmente em situações demográficas opostas, ambas as regiões convergem para padrões similares em 2060, com RDI elevada e superior à RDJ.\nO Sul vai liderar o envelhecimento nacional, com sua RDI crescendo de 15 para 45 entre 2000-2060. A região já experimenta o ponto de inflexão demográfico por volta de 2025-2030, quando terá mais idosos dependentes que jovens – reflexo de décadas de baixa fecundidade e maior longevidade.\nO Norte parte da situação oposta: alta dependência jovem (RDJ de 50) e baixa dependência idosa (RDI de 8). Contudo, a região enfrenta a transformação mais dramática do país: a RDJ despenca para 25 enquanto a RDI quintuplica para 40, com o ponto de inflexão ocorrendo em 2040-2045.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRDT no Brasil por Regiões\n\n\nRegião\n2010\n2015\n2020\n2025\n2030\n2035\n2040\n2045\n2050\n2055\n2060\n\n\n\n\nNorte\n57.4\n51.1\n47.3\n45.4\n44.8\n45.0\n45.7\n47.8\n50.7\n53.8\n57.0\n\n\nNordeste\n52.1\n47.2\n45.0\n44.8\n46.0\n47.6\n49.2\n52.5\n57.0\n61.8\n66.3\n\n\nSudeste\n43.7\n42.2\n43.6\n46.6\n49.5\n51.8\n54.2\n57.9\n62.5\n66.8\n69.9\n\n\nSul\n43.6\n42.2\n44.2\n48.1\n52.0\n54.8\n56.9\n59.9\n64.2\n68.5\n72.7\n\n\nCentro Oeste\n44.7\n42.2\n42.4\n43.9\n45.7\n47.4\n49.5\n52.7\n56.4\n59.6\n62.1\n\n\n\nFonte: IBGE (Censo 2022)\n\n\n\n\n\n\n\n\n\n\nTodas as regiões brasileiras enfrentarão desafios similares do envelhecimento populacional, exigindo adaptações coordenadas nos sistemas de saúde, previdência e políticas públicas. A partir de 2050, a RDT passa de 50 em todas as regiões\n\n\n\n\nHá três perfis demográficos regionais bem definidos no Brasil: estados jovens, estados em transição e estados envelhecidos.\n\n\nOs estados do Norte do Brasil apresentam RDJ elevado e RDI baixo (azul-claro), indicando sociedades jovens onde a pressão sobre a PEA vem de crianças e adolescentes. Estados como São Paulo e Paraná mostram o padrão oposto (vermelho-escuro) caracterizando populações envelhecidas.\nO Nordeste e o Centro-Oeste exibem combinações variadas, com estados como Piauí e Minas Gerais apresentando RDJ e RDI equilibradas, indicando populações em plena transição demográfica. O Distrito Federal destaca-se como exceção regional, mantendo baixas dependências em ambas as categorias devido ao perfil migratório seletivo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs pirâmides etárias revelam perfis demográficos contrastantes entre os estados brasileiros. O Amazonas tipifica a região Norte com estrutura jovem – baixa RDI (8,9) e alta RDJ (40,9) – enquanto o Rio Grande do Sul representa o extremo oposto, liderando o envelhecimento nacional com RDI de 20,6 e RDJ de apenas 25,6.\nSanta Catarina se destaca como exceção, mantendo a menor RDI entre os estados do Sul e Sudeste devido ao influxo migratório de adultos em idade produtiva. O Piauí ilustra o Nordeste em transição, com pirâmide já apresentando base estreitada e topo expandido, sinalizando o início da mudança demográfica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n     Lendo o gráfico  A pirâmide demográfica mostra o percentual da população dentro de cada grupo de idade. Por convenção indivíduos do sexo masculino são apresentados à esquerda (verde) e indivíduos do sexo feminino à direita (roxo)."
  },
  {
    "objectID": "posts/general-posts/2025-07-demografia-brasil/index.html#desafios-do-envelhecimento",
    "href": "posts/general-posts/2025-07-demografia-brasil/index.html#desafios-do-envelhecimento",
    "title": "O Novo Perfil Demográfico do Brasil",
    "section": "",
    "text": "O Brasil deve enfrentar diversos desafios relacionados à sua dinâmica demográfica nas próximas décadas. O fim do bônus demográfico no Brasil não é apenas uma questão estatística, mas uma transformação profunda que exigirá planejamento estratégico, investimentos direcionados e políticas públicas eficazes. O sucesso na gestão dessa transição determinará se o país conseguirá manter seu desenvolvimento e qualidade de vida nas próximas décadas."
  },
  {
    "objectID": "posts/general-posts/2025-07-demografia-brasil/index.html#posts-relacionados",
    "href": "posts/general-posts/2025-07-demografia-brasil/index.html#posts-relacionados",
    "title": "O Novo Perfil Demográfico do Brasil",
    "section": "",
    "text": "Envelhecimento no Brasil\nNascimentos e Óbitos no Brasil"
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html",
    "title": "Analfabetismo no Brasil",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sidrar)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(showtext)\nlibrary(patchwork)\n\nfont_add_google(\"Fira Mono\", \"Fira Mono\")\nfont_add_google(\"Open Sans\", \"Open Sans\")\nshowtext_auto()\n\nmuni = geobr::read_municipality(year = 2020, showProgress = FALSE)\ndim_muni = as_tibble(st_drop_geometry(muni))\n\nanalf = get_sidra(9543, geo = \"City\", classific = \"c2\")\n\ntbl_gender = analf |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble() |&gt; \n  filter(sexo != \"Total\") |&gt; \n  select(code_muni = municipio_codigo, sex = sexo, rate = valor) |&gt; \n  mutate(code_muni = as.numeric(code_muni))\n\nanalf_gender = tbl_gender |&gt; \n  mutate(sex = factor(sex)) |&gt; \n  pivot_wider(id_cols = \"code_muni\", names_from = \"sex\", values_from = \"rate\") |&gt; \n  rename_with(tolower) |&gt; \n  mutate(gender_gap = homens - mulheres) |&gt; \n  arrange(gender_gap)\n\nanalf_gender = analf_gender |&gt; \n  left_join(dim_muni) |&gt; \n  mutate(\n    is_nordeste = factor(if_else(code_region == 2, 1L, 0L))\n  )\n\ntbl_analf = analf |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble() |&gt; \n  filter(sexo == \"Total\") |&gt; \n  select(code_muni = municipio_codigo, rate = valor) |&gt; \n  mutate(code_muni = as.numeric(code_muni))\n\nanalf_city = left_join(muni, tbl_analf, by = \"code_muni\")\n\nanalf_city = analf_city |&gt; \n  mutate(analf_rate = 100 - rate)\n\nbreaks_jenks = BAMMtools::getJenksBreaks(analf_city$analf_rate, k = 9)[-1]\nbreaks_jenks = ceiling(breaks_jenks)\n\nlabels = c(\"&lt;5%\", \"5-8%\", \"8-11%\", \"11-15%\", \"15-19%\", \"19-23%\", \"23-27%\", \"27-37%\")\n\nanalf_city = analf_city |&gt; \n  mutate(analf_group = factor(findInterval(analf_rate, breaks_jenks, left.open = TRUE)))\n\np1 = ggplot(analf_city) +\n  geom_sf(aes(fill = analf_group, color = analf_group), lwd = 0.15) +\n  scale_fill_brewer(\n    name = \"\",\n    type = \"div\",\n    direction = -1,\n    labels = labels\n    ) +\n  scale_color_brewer(\n    name = \"\",\n    type = \"div\",\n    direction = -1,\n    labels = labels\n    ) +\n  labs(\n    title = \"Taxa de Analfabetismo\",\n    subtitle = \"Taxa de analfabetismo total por município\"\n  ) +\n  coord_sf(xlim = c(NA, -35)) +\n  ggthemes::theme_map(base_family = \"Open Sans\") +\n  theme(\n    legend.position.inside = c(0.1, 0.1),\n    plot.title = element_text(\n      hjust = 0.5,\n      size = 22,\n      margin = margin(5, 0, 5, 0)\n      ),\n    plot.subtitle = element_text(\n      hjust = 0.5,\n      margin = margin(2.5, 0, 0, 0)\n      ),\n    plot.margin = margin(0, 0, 0, 0)\n  )\n\nanalf_city = analf_city |&gt; \n  mutate(is_nordeste = factor(if_else(code_region == 2, 1L, 0L)))\n\ntbl_summary = analf_city |&gt; \n  st_drop_geometry() |&gt; \n  summarise(avg = mean(analf_rate), .by = \"is_nordeste\")\n\np2 = ggplot() +\n  geom_density(\n    data = analf_city,\n    aes(x = analf_rate, fill = is_nordeste),\n    alpha = 0.6\n  ) +\n  geom_vline(\n    data = tbl_summary,\n    aes(xintercept = avg, color = is_nordeste),\n    lty = 2\n  ) +\n  geom_hline(yintercept = 0) +\n  guides(fill = \"none\", color = \"none\") +\n  scale_fill_manual(values = c(\"#01665e\", \"#8c510a\")) +\n  scale_color_manual(values = c(\"#01665e\", \"#8c510a\")) +\n  labs(\n    title = \"Taxa de analfabetismo médio é quase 3x maior entre as cidades do &lt;b&gt;&lt;span style='color:#8c510a'&gt;Nordeste&lt;/span&gt;&lt;/b&gt;, em relação ao resto do &lt;b&gt;&lt;span style='color:#01665e'&gt;Brasil&lt;/span&gt;&lt;/b&gt;.\",\n    subtitle = \"Taxa de analfabetismo total nas cidades brasileiras. Linhas tracejadas indicam taxa média entre as cidades do Nordeste e no restante do país.\",\n    x = \"Taxa de analfabetismo (%)\",\n    y = NULL\n  ) +\n  theme_minimal(base_family = \"Fira Mono\") +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_textbox_simple(\n      size = 16,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      family = \"Open Sans\"\n      ),\n    plot.subtitle = element_textbox_simple(\n      size = 8,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0))\n    )\n\nlabel_1 = \"Em 97,4% das cidades do &lt;b&gt;&lt;span style='color:#8c510a'&gt;Nordeste&lt;/span&gt;&lt;/b&gt;, as pessoas do &lt;i&gt;sexo feminino têm taxas de alfabetização menores&lt;/i&gt;. Em alguns casos, a diferença supera 10 pontos percentuais.\"\n\nlabel_2 = \"Na maior parte das cidades do &lt;b&gt;&lt;span style='color:#01665e'&gt;Brasil&lt;/span&gt;&lt;/b&gt;, não há diferença grande na taxa de alfabetização entre pessoas do sexo masculino e do sexo feminino.\"\n\ntext_labels = tibble(\n  x = c(-10, 7.5), y = c(0.2, 0.25), label = c(label_1, label_2)\n)\n\np3 = ggplot() +\n  geom_density(\n    data = analf_gender,\n    aes(x = gender_gap, fill = is_nordeste),\n    alpha = 0.7) +\n  geom_textbox(\n    data = text_labels,\n    aes(x, y, label = label),\n    family = \"Open Sans\",\n    size = 3\n  ) +\n  scale_fill_manual(values = c(\"#01665e\", \"#8c510a\")) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Gender gap na alfabetização\",\n    subtitle = \"Diferença, em pontos percentuais, da taxa de alfabetização entre pessoas do sexo masculino e pessoas do sexo feminino. Valores próximos de zero indicam diferenças pequenas. Valores negativos indicam que mulheres têm taxas de alfabetização menores do que homens.\",\n    caption = \"Fonte: IBGE (Censo, 2022). @viniciusoike\",\n    y = NULL,\n    x = \"Gender gap\"\n  ) +\n  theme_minimal(base_family = \"Fira Mono\") +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_textbox_simple(\n      size = 16,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      family = \"Open Sans\"\n    ),\n    plot.subtitle = element_textbox_simple(\n      size = 8,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0))\n  )\n\npanel = p1 | (p2 / p3)\npanel = panel + plot_layout(widths = c(0.5, 0.5))"
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html#mapa-dos-municípios",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html#mapa-dos-municípios",
    "title": "Analfabetismo no Brasil",
    "section": "Mapa dos municípios",
    "text": "Mapa dos municípios\nA taxa média de analfabetismo no Brasil é de 7%, segundo o Censo mais recente do IBGE. Isto é, os municípios em tons mais claros de verde e em tons de marrom estão todos acima da média nacional."
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html#nordeste",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html#nordeste",
    "title": "Analfabetismo no Brasil",
    "section": "Nordeste",
    "text": "Nordeste\n\nCidades no Nordeste tem taxas de analfabetismo mais elevadas\nQuando se olha especificamente para as cidades nordestinas, verifica-se taxas de analfabetismo consideravelmente elevadas. A taxa média chega a ser quase três vezes maior do que a observada no restante do país.\n\n\n\n\n\n\n\n\n\n\n\nDiscrepâncias entre homens e mulheres é elevada\nA disparidade nas taxas de alfabetização também se apresenta quando olha-se os dados relativos a pessoas do sexo masculino e pessoas do sexo feminino. O “gender gap”, diferença entre a taxa de alfabetização entre homens e entre mulheres, também é maior entre cidades do Nordeste do que no restante do país. Vale reforçar que o gráfico abaixo mostra a diferença nas taxas de alfabetização, assim valores negativos, indicam que as taxas de analfabetismo são maiores entre mulheres."
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html#painel-geral",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html#painel-geral",
    "title": "Analfabetismo no Brasil",
    "section": "Painel Geral",
    "text": "Painel Geral\n\n\n\n\n\n\n\n\n\n\n\nDados: Censo 2022 (IBGE)\nTipografia: Fira Mono e Open Sans\nPaleta: BrBG (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-01-wz-house-prices/index.html",
    "href": "posts/general-posts/2024-01-wz-house-prices/index.html",
    "title": "Preços de Imóveis no Brasil",
    "section": "",
    "text": "Preços de Imóveis no Brasil\nExiste uma percepção generalizada na população de que os preços dos imóveis no Brasil são muito caros. Isto pode ser resultado tanto de uma ignorância sobre a dinâmica do mercado como do excesso de notícias sobre recordes de preços que se vê na imprensa. Também não faltam casos de regiões ou mesmo de ruas que, após significativo processo de revitalização, apresentaram aumentos de preços muito acima da inflação. De maneira geral, contudo, os dados apontam que os preços dos imóveis no Brasil andaram de lado: em termos reais, isto é, descontando a inflação, o nível atual do preço dos imóveis está praticamente idêntico ao que se observava em 2010.\n\nPreços do Brasil x Mundo\n\n\n\n\n\n\n\n\n\n\n\nPreços do Brasil x Países Desenvolvidos\n\n\n\n\n\n\n\n\n\n\n\nPreços do Brasil x Países LATAM"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "A “punchcard” plot shows the occurrence/frequency of a pair of discrete variables. Each discrete variable is plotted onto the X-Y axis and the intensity of the frequency is represented by the size or color of point. The use of colors can also help visualize a third discrete variable.\nThere are several possible applications of a “punchcard” plot. They might include:\n\nthe number of visitors received by a chain of stores in each state (x) and each day of the week (y);\nthe aggregated rating of products (y) based on their category (x)\nthe number of students attending each class (x) based on their major (y); and many other examples.\n\nI haven’t found much formal documentation on “punchcard plots” and have myself only found out about this term while looking for a name for the plot below. This visualization shows the IMDB ratings for the top 250 films by decade. The size of each bubble shows how many films appear in each rating bin in each decade.\n\n\n\n\n\nThere are some other posts discussing this type of plot (here, and here) but overall this is definitely not a well-established term.\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nThe simplest way to make a punchcard plot is to use the geom_count function. This will count the ocurrences of the x-y pair variables selected. The example below uses the diamonds data.\n\nggplot(diamonds, aes(cut, color)) +\n  geom_count()\n\n\n\n\n\n\n\n\n\n\n\n\nThis function is a simple wrapper around geom_point(aes(size = n)). The code below makes the same plot, but first we count the number of occurrences of each of the cut and color variables.\nIt’s important to note that there’s nothing special about n. It’s simply the default column name that results from using the count function.\n\ntab &lt;- diamonds |&gt; \n  count(cut, color)\n\nggplot(tab, aes(cut, color)) +\n  geom_point(aes(size = n))\n\n\n\n\n\n\n\n\nIf our data is already aggregated we might desire to show the sum of some pair of variables. In the txhousing data we have the total number of house sales in each month and year by city. To aggregate the total number of sales, across all cities, we can use weight = sales.\nNote that both year and month are continuous variables but we can treat them as if they were discrete. To actually force R to treat them as categorical variables they must be converted to factor. Also note the use na.omit since there are missing values in the sales column.\n\ntxhousing &lt;- na.omit(txhousing)\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales))\n\n\n\n\n\n\n\n\nAgain, the same result can be achieved using count and geom_point but more code is necessary.\n\ntxhousing |&gt; \n  na.omit() |&gt; \n  count(year, month, wt = sales) |&gt; \n  ggplot(aes(month, year)) +\n  geom_point(aes(size = n))\n\n\n\nThere isn’t much customization available for punchcard plots. To change the size of each bubble we use scale_size_*.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nSince geom_count is essentially the same as geom_point we can alter its shape.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales), shape = 22) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nFinally, we can use geom_color_* and geom_fill_ to map variables as a color on the plot. In the plot below I highlight the top-selling month in each year. It becomes clear the June and July are the most active months for the housing market.\n\nagghousing &lt;- txhousing |&gt; \n  count(month, year, wt = sales) |&gt; \n  mutate(\n    year = as.factor(year),\n    month = as.factor(month),\n    highlight = factor(if_else(n == max(n), 1L, 0L)),\n    .by = \"year\"\n    )\n\nggplot(agghousing, aes(month, year)) +\n  geom_point(aes(size = n, color = highlight)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\n\n\n\n\nFor a more interesting example we can calculate the share of countries facing an economic recession by region. We use the maddison database from the homonyms package.\n\n\nCode\nimport::from(maddison, maddison)\n\ndat &lt;- maddison |&gt; \n  filter(year &gt;= 1999) |&gt; \n  mutate(\n    growth = rgdpnapc / lag(rgdpnapc) - 1,\n    is_growth = if_else(growth &gt;= 0, 1L, 0L),\n    .by = \"iso3c\"\n  )\n\nrecession_region &lt;- dat |&gt; \n  filter(year &gt;= 2000) |&gt; \n  summarise(\n    growth = sum(is_growth, na.rm = TRUE),\n    total = n(),\n    .by = c(\"region\", \"year\")\n  ) |&gt; \n  mutate(\n    share = (1 - growth / total) * 100,\n    highlight = factor(if_else(share &gt; 20, 1, 0)),\n    region = as.factor(region),\n    region = forcats::fct_rev(region)\n    )\n\n\nThe plot shows the share of countries in each region that are facing an economic recession in a given year in the 2000-2016 period. For simplicity, I define an economic recession simply as country facing negative GDP per capita growth (year on year). I highlight the years when over 20% of the countries were facing a recession.\nNote that this visualization is mostly illustrative and a better categorization of each region would likely be required. Even so, we can see how some regions such as Western Africa faced several recessions. We can also see the impact of the Great Financial Recession in 2008-09.\n\nggplot(recession_region, aes(year, region)) + \n  geom_count(aes(size = share, color = highlight)) +\n  scale_size_continuous(\n    name = \"Share of countries\\nin recession (%)\",\n    breaks = c(0, 20, 40, 60, 80, 100)\n  ) +\n  scale_y_discrete(labels = \\(x) stringr::str_wrap(x, width = 17)) +\n  scale_color_manual(values = RColorBrewer::brewer.pal(3, \"RdBu\")[c(3, 1)]) +\n  guides(color = \"none\", size = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  labs(\n    title = \"Economic Recessions across regions\",\n    subtitle = \"Red circles show that over 20% of countries in the region are in recession.\",\n    x = NULL,\n    y = NULL,\n    caption = \"Source: Maddison Project (2018)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n\n\n\nFully replicating the Nexo plot shown in the beginning of this post requires a substantial amount of effort. For the purposes of this tutorial, I’ll show how to replicate the most prevalent aspects of the visualization, ignoring the annotations, arrows, and fonts.\nFor a complete replication of the Nexo plot see my other post.\n\n\nCode\nnexo_labels &lt;- c(\"até\\n1950\", \"1960\", \"70\", \"80\", \"90\", \"00\", \"10\", \"20\", \"até\\nhoje\")\n\ncolors &lt;- c(\"#328bff\", \"#88bce4\")\n\nggplot(imdb, aes(trunc_decade, trunc_rating)) +\n  geom_count(aes(color = is_top20)) +\n  geom_hline(yintercept = 7.9) +\n  scale_x_continuous(breaks = seq(1940, 2020, 10), labels = nexo_labels) +\n  scale_y_continuous(\n    limits = c(7.9, 9.45),\n    breaks = seq(7.9, 9.3, 0.1),\n    labels = scales::label_number(decimal.mark = \",\", accuracy = 0.1),\n    expand = c(0, 0)\n  ) +\n  scale_color_manual(values = rev(colors)) +\n  scale_size_area(name = \"\", breaks = c(4, 8, 12, 16)) +\n  guides(\n    color = \"none\",\n    size = guide_legend(\n      label.position = \"bottom\",\n      override.aes = list(color = \"gray80\"))\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \"\\n\\n\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.major.y = element_line(linetype = 3, color = \"#d9d9d9\", linewidth = 0.35),\n    panel.grid.major.x = element_line(color = \"#838484\", linewidth = 0.35),\n    panel.grid.minor = element_blank(),\n    legend.position = c(0.05, 1.1),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 10),\n    \n    axis.title.y = element_text(color = \"#767676\"),\n    axis.ticks.x = element_line(color = \"#000000\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nPunchcard plots are an excellent way to visualize data distributed across two categorical dimensions, such as days of the week and hours of the day. Whether you’re analyzing time-based activity patterns, attendance trends, or any other data with two categorical dimensions, punchcard plots are a powerful and visually engaging tool.\nNow that you’ve mastered the basics, consider applying this technique to your own datasets—whether it’s analyzing call center activity, website traffic, or any other time-based data. Experiment with colors, sizes, and annotations to make your punchcard plots even more impactful. Happy plotting!"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#ggplot2",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#ggplot2",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\n\nThe simplest way to make a punchcard plot is to use the geom_count function. This will count the ocurrences of the x-y pair variables selected. The example below uses the diamonds data.\n\nggplot(diamonds, aes(cut, color)) +\n  geom_count()"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#section",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#section",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "This function is a simple wrapper around geom_point(aes(size = n)). The code below makes the same plot, but first we count the number of occurrences of each of the cut and color variables.\nIt’s important to note that there’s nothing special about n. It’s simply the default column name that results from using the count function.\n\ntab &lt;- diamonds |&gt; \n  count(cut, color)\n\nggplot(tab, aes(cut, color)) +\n  geom_point(aes(size = n))\n\n\n\n\n\n\n\n\nIf our data is already aggregated we might desire to show the sum of some pair of variables. In the txhousing data we have the total number of house sales in each month and year by city. To aggregate the total number of sales, across all cities, we can use weight = sales.\nNote that both year and month are continuous variables but we can treat them as if they were discrete. To actually force R to treat them as categorical variables they must be converted to factor. Also note the use na.omit since there are missing values in the sales column.\n\ntxhousing &lt;- na.omit(txhousing)\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales))\n\n\n\n\n\n\n\n\nAgain, the same result can be achieved using count and geom_point but more code is necessary.\n\ntxhousing |&gt; \n  na.omit() |&gt; \n  count(year, month, wt = sales) |&gt; \n  ggplot(aes(month, year)) +\n  geom_point(aes(size = n))\n\n\n\nThere isn’t much customization available for punchcard plots. To change the size of each bubble we use scale_size_*.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nSince geom_count is essentially the same as geom_point we can alter its shape.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales), shape = 22) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nFinally, we can use geom_color_* and geom_fill_ to map variables as a color on the plot. In the plot below I highlight the top-selling month in each year. It becomes clear the June and July are the most active months for the housing market.\n\nagghousing &lt;- txhousing |&gt; \n  count(month, year, wt = sales) |&gt; \n  mutate(\n    year = as.factor(year),\n    month = as.factor(month),\n    highlight = factor(if_else(n == max(n), 1L, 0L)),\n    .by = \"year\"\n    )\n\nggplot(agghousing, aes(month, year)) +\n  geom_point(aes(size = n, color = highlight)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#countries-in-recession",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#countries-in-recession",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "For a more interesting example we can calculate the share of countries facing an economic recession by region. We use the maddison database from the homonyms package.\n\n\nCode\nimport::from(maddison, maddison)\n\ndat &lt;- maddison |&gt; \n  filter(year &gt;= 1999) |&gt; \n  mutate(\n    growth = rgdpnapc / lag(rgdpnapc) - 1,\n    is_growth = if_else(growth &gt;= 0, 1L, 0L),\n    .by = \"iso3c\"\n  )\n\nrecession_region &lt;- dat |&gt; \n  filter(year &gt;= 2000) |&gt; \n  summarise(\n    growth = sum(is_growth, na.rm = TRUE),\n    total = n(),\n    .by = c(\"region\", \"year\")\n  ) |&gt; \n  mutate(\n    share = (1 - growth / total) * 100,\n    highlight = factor(if_else(share &gt; 20, 1, 0)),\n    region = as.factor(region),\n    region = forcats::fct_rev(region)\n    )\n\n\nThe plot shows the share of countries in each region that are facing an economic recession in a given year in the 2000-2016 period. For simplicity, I define an economic recession simply as country facing negative GDP per capita growth (year on year). I highlight the years when over 20% of the countries were facing a recession.\nNote that this visualization is mostly illustrative and a better categorization of each region would likely be required. Even so, we can see how some regions such as Western Africa faced several recessions. We can also see the impact of the Great Financial Recession in 2008-09.\n\nggplot(recession_region, aes(year, region)) + \n  geom_count(aes(size = share, color = highlight)) +\n  scale_size_continuous(\n    name = \"Share of countries\\nin recession (%)\",\n    breaks = c(0, 20, 40, 60, 80, 100)\n  ) +\n  scale_y_discrete(labels = \\(x) stringr::str_wrap(x, width = 17)) +\n  scale_color_manual(values = RColorBrewer::brewer.pal(3, \"RdBu\")[c(3, 1)]) +\n  guides(color = \"none\", size = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  labs(\n    title = \"Economic Recessions across regions\",\n    subtitle = \"Red circles show that over 20% of countries in the region are in recession.\",\n    x = NULL,\n    y = NULL,\n    caption = \"Source: Maddison Project (2018)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", legend.direction = \"horizontal\")"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#replicating-the-nexo-plot",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#replicating-the-nexo-plot",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "Fully replicating the Nexo plot shown in the beginning of this post requires a substantial amount of effort. For the purposes of this tutorial, I’ll show how to replicate the most prevalent aspects of the visualization, ignoring the annotations, arrows, and fonts.\nFor a complete replication of the Nexo plot see my other post.\n\n\nCode\nnexo_labels &lt;- c(\"até\\n1950\", \"1960\", \"70\", \"80\", \"90\", \"00\", \"10\", \"20\", \"até\\nhoje\")\n\ncolors &lt;- c(\"#328bff\", \"#88bce4\")\n\nggplot(imdb, aes(trunc_decade, trunc_rating)) +\n  geom_count(aes(color = is_top20)) +\n  geom_hline(yintercept = 7.9) +\n  scale_x_continuous(breaks = seq(1940, 2020, 10), labels = nexo_labels) +\n  scale_y_continuous(\n    limits = c(7.9, 9.45),\n    breaks = seq(7.9, 9.3, 0.1),\n    labels = scales::label_number(decimal.mark = \",\", accuracy = 0.1),\n    expand = c(0, 0)\n  ) +\n  scale_color_manual(values = rev(colors)) +\n  scale_size_area(name = \"\", breaks = c(4, 8, 12, 16)) +\n  guides(\n    color = \"none\",\n    size = guide_legend(\n      label.position = \"bottom\",\n      override.aes = list(color = \"gray80\"))\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \"\\n\\n\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.major.y = element_line(linetype = 3, color = \"#d9d9d9\", linewidth = 0.35),\n    panel.grid.major.x = element_line(color = \"#838484\", linewidth = 0.35),\n    panel.grid.minor = element_blank(),\n    legend.position = c(0.05, 1.1),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 10),\n    \n    axis.title.y = element_text(color = \"#767676\"),\n    axis.ticks.x = element_line(color = \"#000000\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#conclusion",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#conclusion",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "Punchcard plots are an excellent way to visualize data distributed across two categorical dimensions, such as days of the week and hours of the day. Whether you’re analyzing time-based activity patterns, attendance trends, or any other data with two categorical dimensions, punchcard plots are a powerful and visually engaging tool.\nNow that you’ve mastered the basics, consider applying this technique to your own datasets—whether it’s analyzing call center activity, website traffic, or any other time-based data. Experiment with colors, sizes, and annotations to make your punchcard plots even more impactful. Happy plotting!"
  },
  {
    "objectID": "posts/general-posts/2025-06-censo-metro-regions/index.html",
    "href": "posts/general-posts/2025-06-censo-metro-regions/index.html",
    "title": "O crescimento das Regiões Metropolitanas Brasileiras",
    "section": "",
    "text": "Os resultados do Censo 2022 trouxeram à tona um fenômeno que tem gerado intensos debates sobre o futuro demográfico do Brasil: pela primeira vez em décadas, algumas das principais regiões metropolitanas do país registraram redução populacional ou crescimento próximo de zero. Para melhor compreender esta transformação, vale analisar os dados em uma perspectiva histórica e regional."
  },
  {
    "objectID": "posts/general-posts/2025-06-censo-metro-regions/index.html#as-mudanças-demográficas-nas-regiões-metropolitanas-brasileiras",
    "href": "posts/general-posts/2025-06-censo-metro-regions/index.html#as-mudanças-demográficas-nas-regiões-metropolitanas-brasileiras",
    "title": "O crescimento das Regiões Metropolitanas Brasileiras",
    "section": "",
    "text": "Os resultados do Censo 2022 trouxeram à tona um fenômeno que tem gerado intensos debates sobre o futuro demográfico do Brasil: pela primeira vez em décadas, algumas das principais regiões metropolitanas do país registraram redução populacional ou crescimento próximo de zero. Para melhor compreender esta transformação, vale analisar os dados em uma perspectiva histórica e regional."
  },
  {
    "objectID": "posts/general-posts/2025-06-censo-metro-regions/index.html#resumo",
    "href": "posts/general-posts/2025-06-censo-metro-regions/index.html#resumo",
    "title": "O crescimento das Regiões Metropolitanas Brasileiras",
    "section": "Resumo",
    "text": "Resumo\n\nDesaceleração generalizada. Praticamente todas as grandes metrópoles brasileiras registram queda nas taxas de crescimento populacional nas últimas décadas\nCrescimento negativo em grandes centros. Rio de Janeiro (-0,13%), Salvador (-0,35%), Porto Alegre (-0,03%), AU do Sul/Rio Grande do Sul (-0,08%) e Belém (-0,11%) apresentam retração populacional\nSão Paulo em forte desaceleração. A maior metrópole do país viu seu crescimento cair de 1,47% (anos 90) para apenas 0,4% (2010-2022)\nFlorianópolis e Santa Catarina em destaque. A RM de Florianópolis registrou o maior crescimento entre as capitais estaduais de 2,28%. Outros eixos de SC também cresceram acima da média brasileira como Chapecó (1,63%), a Foz do Rio Itajaí (3%), o Vale do Itajaí (1,54%) a Região do Norte/Nordeste Catarinense (1,48%) e a Região Carbonífera (1,23%).\nCidades médias superam metrópoles: Várias RMs menores crescem mais que as tradicionais - Norte/Nordeste Catarinense (1,48%), RIDE Petrolina/Juazeiro (1,52%), AU de Jundiaí (1,46%). O processo sugere desconcentração urbana com forte crescimento em polos regionais do Sul, Nordeste interior e Centro-Oeste.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrescimento Demográfico nas Regiões Metropolitanas\n\n\nPopulação total e taxa de crescimento geométrico médio entre cada Censo Demográfico. Tabela mostra apenas as RMs com população superior a um milhão de habitantes em 2022.\n\n\nRegião Metro\n\nPopulacao (Mil)\n\n\nCrescimento (%)\n\nTCG\n\n\n2000\n2010\n2022\n1991/2000\n2000/2010\n2010/2022\n\n\n\n\nSão Paulo\n17.880\n19.684\n20.732\n1,47%\n0,88%\n0,40%\n\n\n\n   0.4\n\n\n\nRio de Janeiro\n10.967\n11.946\n11.743\n1,05%\n0,78%\n−0,13%\n\n\n\n   -0.1\n\n\n\nBelo Horizonte\n4.833\n5.430\n5.734\n2,11%\n1,06%\n0,42%\n\n\n\n   0.4\n\n\n\nRIDE DF\n3.118\n3.911\n4.484\n3,06%\n2,08%\n1,06%\n\n\n\n   1.1\n\n\n\nPorto Alegre\n3.783\n4.032\n4.019\n1,43%\n0,58%\n−0,03%\n\n\n\n   0.0\n\n\n\nFortaleza\n3.166\n3.741\n3.906\n2,19%\n1,53%\n0,33%\n\n\n\n   0.3\n\n\n\nRecife\n3.409\n3.766\n3.808\n1,34%\n0,91%\n0,09%\n\n\n\n   0.1\n\n\n\nCuritiba\n2.813\n3.224\n3.560\n2,77%\n1,25%\n0,77%\n\n\n\n   0.8\n\n\n\nSalvador\n3.120\n3.574\n3.415\n1,89%\n1,24%\n−0,35%\n\n\n\n   -0.3\n\n\n\nCampinas\n2.348\n2.809\n3.179\n2,28%\n1,64%\n0,96%\n\n\n\n   1.0\n\n\n\nGoiânia\n1.704\n2.131\n2.549\n2,92%\n2,05%\n1,39%\n\n\n\n   1.4\n\n\n\nManaus\n1.726\n2.211\n2.533\n3,17%\n2,28%\n1,05%\n\n\n\n   1.1\n\n\n\nVale do Paraíba e Litoral Norte\n1.992\n2.265\n2.506\n1,89%\n1,17%\n0,78%\n\n\n\n   0.8\n\n\n\nBelém\n1.973\n2.275\n2.244\n2,53%\n1,30%\n−0,11%\n\n\n\n   -0.1\n\n\n\nSorocaba\n1.603\n1.871\n2.175\n2,46%\n1,41%\n1,16%\n\n\n\n   1.2\n\n\n\nGrande Vitória\n1.439\n1.688\n1.881\n2,38%\n1,46%\n0,84%\n\n\n\n   0.8\n\n\n\nBaixada Santista\n1.477\n1.664\n1.806\n1,93%\n1,09%\n0,63%\n\n\n\n   0.6\n\n\n\nRibeirão Preto\n1.308\n1.511\n1.648\n1,61%\n1,32%\n0,67%\n\n\n\n   0.7\n\n\n\nGrande São Luís\n1.225\n1.492\n1.646\n2,50%\n1,81%\n0,76%\n\n\n\n   0.8\n\n\n\nNatal\n1.172\n1.409\n1.518\n2,20%\n1,69%\n0,57%\n\n\n\n   0.6\n\n\n\nNorte/Nordeste Catarinense\n1.030\n1.223\n1.481\n1,98%\n1,57%\n1,48%\n\n\n\n   1.5\n\n\n\nAU de Piracicaba-AU- Piracicaba\n1.181\n1.333\n1.467\n1,86%\n1,11%\n0,74%\n\n\n\n   0.7\n\n\n\nFlorianópolis\n816\n1.012\n1.357\n2,64%\n1,97%\n2,28%\n\n\n\n   2.3\n\n\n\nJoão Pessoa\n981\n1.156\n1.304\n1,80%\n1,51%\n0,93%\n\n\n\n   0.9\n\n\n\nMaceió\n1.054\n1.227\n1.301\n2,13%\n1,39%\n0,45%\n\n\n\n   0.4\n\n\n\nRIDE Teresina\n1.008\n1.151\n1.247\n1,68%\n1,21%\n0,62%\n\n\n\n   0.6\n\n\n\nLondrina\n897\n1.000\n1.089\n1,30%\n0,99%\n0,66%\n\n\n\n   0.7\n\n\n\nVale do Rio Cuiabá\n836\n944\n1.087\n1,81%\n1,11%\n1,09%\n\n\n\n   1.1"
  },
  {
    "objectID": "posts/general-posts/2025-06-censo-metro-regions/index.html#o-contexto-da-desaceleração-demográfica",
    "href": "posts/general-posts/2025-06-censo-metro-regions/index.html#o-contexto-da-desaceleração-demográfica",
    "title": "O crescimento das Regiões Metropolitanas Brasileiras",
    "section": "O Contexto da Desaceleração Demográfica",
    "text": "O Contexto da Desaceleração Demográfica\nOs dados mostram um cenário de clara desaceleração do crescimento populacional nas principais regiões metropolitanas brasileiras. São Paulo, que manteve taxas de crescimento de 1,47% ao ano na década de 1990, viu esse ritmo cair para 0,88% entre 2000-2010 e apenas 0,40% no período mais recente. O Rio de Janeiro apresenta situação ainda mais dramática, com crescimento negativo de -0,13% entre 2010-2022, indicando efetiva redução populacional.\n\nDiferentes Dinâmicas Regionais\nA análise dos dados revela padrões regionais distintos que refletem as transformações econômicas e sociais do país.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRMs do Sudeste\n\n\nPopulação total e taxa de crescimento geométrico médio entre cada Censo Demográfico.\n\n\nRegião Metro\nPop. 2022\n\nCrescimento (%)\n\nTCG\n\n\n1991/2000\n2000/2010\n2010/2022\n\n\n\n\nSão Paulo\n20.732.000\n1,47%\n0,88%\n0,40%\n\n\n\n   0.4\n\n\n\nRio de Janeiro\n11.743.000\n1,05%\n0,78%\n−0,13%\n\n\n\n   -0.1\n\n\n\nBelo Horizonte\n5.734.000\n2,11%\n1,06%\n0,42%\n\n\n\n   0.4\n\n\n\nCampinas\n3.179.000\n2,28%\n1,64%\n0,96%\n\n\n\n   1.0\n\n\n\nVale do Paraíba e Litoral Norte\n2.506.000\n1,89%\n1,17%\n0,78%\n\n\n\n   0.8\n\n\n\nSorocaba\n2.175.000\n2,46%\n1,41%\n1,16%\n\n\n\n   1.2\n\n\n\nGrande Vitória\n1.881.000\n2,38%\n1,46%\n0,84%\n\n\n\n   0.8\n\n\n\nBaixada Santista\n1.806.000\n1,93%\n1,09%\n0,63%\n\n\n\n   0.6\n\n\n\nRibeirão Preto\n1.648.000\n1,61%\n1,32%\n0,67%\n\n\n\n   0.7\n\n\n\nAU de Piracicaba-AU- Piracicaba\n1.467.000\n1,86%\n1,11%\n0,74%\n\n\n\n   0.7\n\n\n\nAU de Jundiaí\n844.000\n2,18%\n1,71%\n1,46%\n\n\n\n   1.5\n\n\n\nVale do Aço\n718.000\n0,59%\n0,78%\n0,03%\n\n\n\n   0.0\n\n\n\n\n\n\n\n\nNo Sudeste, observa-se uma clara desaceleração: além de São Paulo e Rio de Janeiro, mesmo regiões tradicionalmente dinâmicas como Campinas (0,96%) e Vale do Paraíba (0,78%) apresentam crescimento modesto. Belo Horizonte mantém algum dinamismo (0,42%), mas bem abaixo dos padrões históricos.\nO Sul apresenta o cenário mais heterogêneo do país. Enquanto Porto Alegre encolhe (-0,03%), regiões como Florianópolis (2,28%) lideram o crescimento nacional. Cidades médias como Chapecó (1,63%), Maringá (1,34%) e a região Carbonífera de Santa Catarina (1,23%) demonstram forte vitalidade econômica, refletindo o dinamismo do agronegócio e da indústria regional.\n\nParanáSanta CatarinaRio Grande do Sul\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulação total e taxa de crescimento geométrico médio entre cada Censo Demográfico.\n\n\nRegião Metro\nPop. 2022\n\nCrescimento (%)\n\nTCG\n\n\n1991/2000\n2000/2010\n2010/2022\n\n\n\n\nCuritiba\n3.560.000\n2,77%\n1,25%\n0,77%\n\n\n\n   0.8\n\n\n\nLondrina\n1.089.000\n1,30%\n0,99%\n0,66%\n\n\n\n   0.7\n\n\n\nMaringá\n852.000\n1,65%\n1,37%\n1,34%\n\n\n\n   1.3\n\n\n\nCascavel\n552.000\n0,94%\n0,75%\n1,07%\n\n\n\n   1.1\n\n\n\nToledo\n419.000\n0,08%\n0,98%\n1,20%\n\n\n\n   1.2\n\n\n\nCampo Mourão\n342.000\n−1,11%\n−0,33%\n0,18%\n\n\n\n   0.18\n\n\n\nUmuarama\n325.000\n−0,93%\n0,30%\n0,64%\n\n\n\n   0.64\n\n\n\nApucarana\n296.000\n−0,22%\n0,18%\n0,18%\n\n\n\n   0.18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulação total e taxa de crescimento geométrico médio entre cada Censo Demográfico.\n\n\nRegião Metro\nPop. 2022\n\nCrescimento (%)\n\nTCG\n\n\n1991/2000\n2000/2010\n2010/2022\n\n\n\n\nNorte/Nordeste Catarinense\n1.481.000\n1,98%\n1,57%\n1,48%\n\n\n\n   1.5\n\n\n\nFlorianópolis\n1.357.000\n2,64%\n1,97%\n2,28%\n\n\n\n   2.3\n\n\n\nVale do Itajaí\n842.000\n2,14%\n1,94%\n1,54%\n\n\n\n   1.5\n\n\n\nFoz do Rio Itajaí\n782.000\n3,98%\n3,23%\n3,00%\n\n\n\n   3.0\n\n\n\nCarbonífera\n645.000\n1,52%\n1,15%\n1,23%\n\n\n\n   1.2\n\n\n\nContestado\n539.000\n1,19%\n0,60%\n0,58%\n\n\n\n   0.6\n\n\n\nChapecó\n535.000\n0,79%\n1,12%\n1,63%\n\n\n\n   1.6\n\n\n\nTubarão\n405.000\n1,21%\n0,86%\n0,98%\n\n\n\n   1.0\n\n\n\nLages\n363.000\n0,70%\n0,04%\n0,26%\n\n\n\n   0.26\n\n\n\nExtremo Oeste\n351.000\n−0,34%\n0,19%\n0,49%\n\n\n\n   0.49\n\n\n\nAlto Vale do Itajaí\n310.000\n0,83%\n0,92%\n1,08%\n\n\n\n   1.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulação total e taxa de crescimento geométrico médio entre cada Censo Demográfico.\n\n\nRegião Metro\nPop. 2022\n\nCrescimento (%)\n\nTCG\n\n\n1991/2000\n2000/2010\n2010/2022\n\n\n\n\nPorto Alegre\n4.019.000\n1,43%\n0,58%\n−0,03%\n\n\n\n   0.0\n\n\n\nSerra Gaúcha\n801.000\n2,06%\n1,50%\n0,66%\n\n\n\n   0.7\n\n\n\nAU do Sul\n572.000\n1,00%\n0,33%\n−0,08%\n\n\n\n   -0.08\n\n\n\nAU do Litoral Norte\n361.000\n3,26%\n1,86%\n1,86%\n\n\n\n   1.9\n\n\n\n\n\n\n\n\n\n\n\nNo Centro-Oeste, todas as regiões metropolitanas mantêm crescimento positivo. Goiânia (1,39%) e a RIDE-DF (1,06%) se beneficiam da expansão do agronegócio e do setor público, enquanto o Vale do Rio Cuiabá (1,09%) reflete o boom do agronegócio mato-grossense.\nO Nordeste mostra sinais de consolidação urbana, com crescimento moderado na maioria das capitais – Fortaleza (0,33%), Recife (0,09%) e Natal (0,57%) - mas retração em Salvador (-0,35%). Destaca-se o crescimento de Campina Grande (0,56%) e centros regionais menores.\nA Região Norte mantém dinamismo demográfico, com Manaus (1,05%) liderando, seguida por Belém que, diferentemente das outras grandes metrópoles, apresenta retração populacional (-0,11%), sinalizando possível saturação urbana mesmo na região de maior crescimento populacional do país.\n\n\nO Crescimento das Cidades Médias e Novos Polos Econômicos\nUm dos aspectos mais notáveis dos dados é o dinamismo das cidades médias e regiões não-metropolitanas tradicionais. Foz do Rio Itajaí lidera com impressionantes 3,00% de crescimento anual.\nFlorianópolis emerge como a capital de maior crescimento (2,28%), consolidando-se como polo tecnológico e de qualidade de vida. No interior catarinense, o dinamismo se repete: Chapecó (1,63%), Vale do Itajaí (1,54%), Norte/Nordeste Catarinense (1,48%) e região Carbonífera (1,23%) demonstram a vitalidade econômica diversificada do estado.\n\nNúmero de CidadesCrescimento MédioCrescimento Total (ganho pop.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO fenômeno se estende para outras regiões: no Nordeste, a RIDE Petrolina/Juazeiro (1,52%) e o Cariri cearense (0,89%) mostram o potencial do interior nordestino. Em São Paulo, além de Sorocaba (1,16%), destaca-se a AU de Jundiaí (1,46%). No Paraná, Maringá (1,34%) e Cascavel (1,07%) refletem a força do agronegócio.\nRegiões antes periféricas ganham protagonismo: Macapá (0,79%) no Amapá, Grande São Luís (0,76%) no Maranhão, e até pequenas aglomerações como Contestado (0,58%) em Santa Catarina mostram que o crescimento populacional está se redistribuindo amplamente pelo território nacional.\nEsse padrão sugere uma reconfiguração profunda do sistema urbano brasileiro, onde fatores como qualidade de vida, custos menores, oportunidades econômicas setoriais e conectividade digital permitem que cidades médias e polos regionais compitam efetivamente com as grandes metrópoles na atração de população e investimentos.\n\n\nNuances regionais: o caso de Belo Horizonte\nA RM de Belo Horizonte ilustra um processo de redistribuição populacional. Enquanto a capital mineira registrou perda populacional de quase 60 mil habitantes, passando de 2,375 milhões para 2,316 milhões entre 2010 e 2022 (queda de -0,21%), a população da região metropolitana como um todo cresceu 0,42% ao ano no mesmo período. Em termos absolutos, a RM de Belo Horizonte ganhou pouco mais de 300 mil habitantes (crescimento de 5,6%). \n\n\n\n\n\n\n\nEsta migração intra-metropolitana, onde a população migra da capital para as periferias, pode apontar uma busca por melhor qualidade de vida, menores custos habitacionais e novas oportunidades. Este tipo de movimentação também pode sinalizar uma saturação da capital, resultado de várias causas, incluindo: aumento da violência urbana, saturação da infraestrutura viária, aumento do custo de vida, piora na qualidade e oferta de serviços públicos. No caso de BH, parte deste processo se deve aoPlano Diretor excessivamente restritivo da cidade, que dificulta a expansão imobiliária na cidade1.\nNova Lima foi a cidade que mais se beneficiou deste processo migratório. A cidade registrou crescimento explosivo de 2,71% ao ano, saltando de 80.998 para 111.697 habitantes - um aumento de quase 38% em apenas 12 anos. A cidade tem localização estratégia, próxima à região Centro-Sul da capital mineira, facilitando o deslocamento para regiões com empregos de alta qualidade. Além disso, a cidade possui código de obras mais flexível, permitindo maior verticalização e adensamento populacional.\nO padrão se repete em outros municípios da região: São José da Lapa (2,32%), Mateus Leme (2,59%), Igarapé (2,31%) e Itatiaiuçu (2,25%) apresentam crescimento robusto, evidenciando que o dinamismo metropolitano se deslocou para a periferia. Mesmo municípios menores como Florestal (1,66%) e Fortuna de Minas (1,12%) participam desse processo de redistribuição."
  },
  {
    "objectID": "posts/general-posts/2025-06-censo-metro-regions/index.html#footnotes",
    "href": "posts/general-posts/2025-06-censo-metro-regions/index.html#footnotes",
    "title": "O crescimento das Regiões Metropolitanas Brasileiras",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA perda de 60 mil habitantes no município entre 2010 e 2022, segundo dados do último Censo realizado pelo IBGE, é o resultado de uma política urbana deliberada. Leis de uso do solo restringem o potencial de construção dos terrenos, inviabilizando a ampliação do estoque imobiliário para sustentar a demanda de crescimento populacional e renda, principalmente nas áreas centrais, causando a dispersão da população e a planificação do seu perfil de densidade.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "O Gradient Descent (GD), também conhecido como steepest descent ou até como Stochastic Gradient Descent (SGD)1, é um algoritmo fundamental em otimização que desempenha um papel crucial no ajuste de parâmetros de modelos de machine learning. Basicamente, o GD utiliza a informação do gradiente de uma função, que se quer otimizar, para encontrar o “caminho” em que ela decaí mais rapidamente.\nNo caso mais típico, da minimização de uma função de perda, o GD ou SGD usa o gradiente da função para encontrar os valores dos parâmetros que minimizam esta função. O algoritmo opera iterativamente e, idealmente, aproxima-se gradualmente da solução correta.\nAssim, a utilidade geral do GD reside na sua capacidade de iterativamente encontrar o mínimo de funções usando relativamente pouca informação.\n\n\n\n\nPara tornar as coisas mais concretas vamos considerar o caso de uma regressão linear simples. Temos uma variável de resposta \\(y\\) que será “explicada” por uma variável independente (feature) \\(x\\) . A relação é modelada de maneira linear como:\n\\[\ny = \\beta_{0} + \\beta_{1}x + \\varepsilon\n\\]\nA equação acima descreve o nosso modelo, que será estimado posteriormente. Com este modelo podemos calcular os valores preditos \\(\\hat{y_{i}}\\) que serão comparados com os valores observados (reais) \\(y_{i}\\). Nosso objetivo é ajustar os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\) para minimizar a função de custo, que pode ser representada como o Erro Quadrático Médio (MSE, do inglês Mean Squared Error):\n\\[\n\\text{MSE} =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_{i}})^2= \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 X_i))^2\n\\]\nPara atualizar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) usando GD, precisamos calcular o gradiente da função de custo em relação a esses parâmetros. A primeira derivada da função de custo em relação a \\(\\beta_0\\) e \\(\\beta_1\\) nos dá as direções em que devemos ajustar esses parâmetros para minimizar a função de custo.\nA primeira derivada da função de custo em relação a \\(\\beta_0\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nE a primeira derivada da função de custo em relação a \\(\\beta_1\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_1} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nEssas derivadas nos fornecem a direção em que devemos ajustar os parâmetros para minimizar a função de custo.\n\n\n\nO algortimo de GD funciona iterativamente O algoritmo de SGD atualiza o parâmetro \\(\\beta^{t}\\) a cada iteração t, onde \\(\\beta^{0}\\) é dado, usando o gradiente \\(\\nabla_{\\beta}^t\\) da seguinte maneira:\n\\[\n\\beta^{t+1} = \\beta^{t} - \\gamma\\nabla_{\\beta}^{t}\n\\]\nonde \\(\\gamma\\) é um número real não-negativo, tipicamente próximo de 0.001, chamado “learning rate”. Quanto maior for o valor de \\(\\gamma\\) maiores serão os “passos” no processo de atualização; inversamente, quanto menor for o valor de \\(\\gamma\\) menores serão os “passos”no processo iterativo.\nPara implementar o passo-a-passo do algoritmo vamos usar a base mtcars. O primeiro passo é ajustar os dados usando a função scale. Em seguida, separamos alguns objetos úteis para facilitar a exposição.\n\n#&gt; Regularizar os dados\nmtcars_scaled &lt;- as.data.frame(scale(mtcars))\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\nVamos fazer a regressão de mpg (milhas por galão), uma medida da eficiência do veículo, contra wt (peso), o peso do veículo. Visualmente, parece haver uma relação linear decrescente entre as variáveis.\n\nplot(y ~ x)\n\n\n\n\n\n\n\n\nAntes de fazer o loop, vamos decompor o algoritmo em etapas. Primeiro, precisa-se de valores iniciais para os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\). Por simplicidade, vamos sortear números aleatórios entre 0 e 1 a partir de uma distribuição uniforme. Com estes valores será possível calcular o valor de \\(\\hat{y}_{i}^0\\), onde 0 indica que estamos na iteração de valor 0.\n\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\n(yhat &lt;- b0 + b1 * x)\n\n [1] -0.17140720  0.05168499 -0.43386860  0.34914125  0.54598730  0.56348473\n [7]  0.65972058  0.32726947  0.29227461  0.54598730  0.54598730  1.09715625\n[13]  0.79969999  0.84344356  2.12950443  2.28173204  2.21261721 -0.53885316\n[19] -1.05065289 -0.85818120 -0.30701225  0.61597701  0.54161295  0.89593584\n[25]  0.90031019 -0.77069407 -0.59134544 -1.13988977  0.30977204 -0.04017650\n[31]  0.65972058 -0.03142778\n\n\nNão é necessário, mas é instrutivo calcular a função de perda.\n\n(mse &lt;- mean((y - yhat)^2))\n\n[1] 3.240961\n\n\nAgora, calculamos o valor do gradiente neste ponto.\n\n(gb0 &lt;- sum(y - yhat) * (-2 / N))\n\n[1] 0.7022194\n\n(gb1 &lt;- sum((y - yhat) * x) * (-2 / N))\n\n[1] 3.339637\n\n\nPor fim, o valor dos parâmetros é atualizado segundo a fórmula matemática do algoritmo. Utiliza-se \\(g = 0.01\\) como valor para a learning-rate.\n\ng = 0.01\n\n(b0_new &lt;- b0 - g * gb0)\n\n[1] 0.3440875\n\n(b1_new &lt;- b1 - g * gb1)\n\n[1] 0.822628\n\n\nEste processo será repetido \\(T\\) vezes até que se atinja algum critério de convergência. Em geral, estabelece-se\n\nUm número máximo de iterações. (10.000 iterações, por exemplo).\nUm valor mínimo de mudança na estimativa dos parâmetros. Isto é, quando o valor das estimativas para de mudar significativamente, entende-se que ele convergiu para um valor satisfatório.\n\nPara deixar o loop abaixo mais simples, vou simplesmente estabelecer um número máximo de 5000 iterações. O código segue abaixo. Note que algumas partes do código acima foram repetidas por conveniência da leitura.\n\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\ng &lt;- 0.01\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nfor (i in seq_len(num_iterations)) {\n  if (i %% 100 == 0) {\n    cat(\"Iteração: \", i, \"\\n\")\n  }\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  if (i %% 100 == 0) {\n    cat(\"Valor da perda: \", as.numeric(mse), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2 / N)\n  gb1 &lt;- sum((y - yhat) * x) * (-2 / N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - g * gb0\n  b1_new &lt;- b1 - g * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n\n  if (i %% 100 == 0) {\n    cat(\"Betas: \", c(b0, b1), \"\\n\\n\")\n  }\n}\n\nIteração:  100 \nValor da perda:  0.3013997 \nBetas:  0.1179619 -0.6505911 \n\nIteração:  200 \nValor da perda:  0.2406465 \nBetas:  0.01564405 -0.8369768 \n\nIteração:  300 \nValor da perda:  0.2394667 \nBetas:  0.002074708 -0.8633224 \n\nIteração:  400 \nValor da perda:  0.2394437 \nBetas:  0.0002751468 -0.8670463 \n\nIteração:  500 \nValor da perda:  0.2394432 \nBetas:  3.648985e-05 -0.8675727 \n\nIteração:  600 \nValor da perda:  0.2394432 \nBetas:  4.839267e-06 -0.8676471 \n\nIteração:  700 \nValor da perda:  0.2394432 \nBetas:  6.417815e-07 -0.8676576 \n\nIteração:  800 \nValor da perda:  0.2394432 \nBetas:  8.511277e-08 -0.8676591 \n\nIteração:  900 \nValor da perda:  0.2394432 \nBetas:  1.128762e-08 -0.8676593 \n\nIteração:  1000 \nValor da perda:  0.2394432 \nBetas:  1.49696e-09 -0.8676594 \n\n\nPara recuperar o valor final dos betas.\n\nc(b0, b1)\n\n[1]  1.496960e-09 -8.676594e-01\n\n\nPara efeito didático, vamos comparar estas estimativas finais contra os valores estimados pela função lm. Note que os valores estão muito similares\n\nsummary(model_lm &lt;- lm(mpg ~ wt, data = mtcars_scaled))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars_scaled)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75381 -0.39236 -0.02077  0.23388  1.14033 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.040e-15  8.934e-02   0.000        1    \nwt          -8.677e-01  9.077e-02  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5054 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\n\n\nSempre que possível, é útil visualizar o funcionamento do algoritmo em gráficos. Como estamos trabalhando com um exemplo simples, pode-se plotar os resultados gradativamente num gráfico de dispersão. O código abaixo mostra como a linha de ajuste (linha de regressão) vai se alterando à medida que se aumenta o número de amostras.\n\n\nCode\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\nalpha &lt;- 0.01\n\ny &lt;- dat$mpg\nx &lt;- dat$wt\nN &lt;- nrow(dat)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nbetas &lt;- matrix(ncol = 2, nrow = num_iterations)\nfor (i in seq_len(num_iterations)) {\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2 / N)\n  gb1 &lt;- sum((y - yhat) * dat$wt) * (-2 / N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - alpha * gb0\n  b1_new &lt;- b1 - alpha * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n\n  betas[i, 1] &lt;- b0_new\n  betas[i, 2] &lt;- b1_new\n}\n\nsel &lt;- c(1:10, seq(20, 100, 10), seq(100, 1000, 50))\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tibble)\n\ntbl_betas &lt;- tibble(\n  iter = sel,\n  beta0 = betas[sel, 1],\n  beta1 = betas[sel, 2]\n)\n\nggplot() +\n  geom_point(\n    data = dat,\n    aes(x = wt, y = mpg),\n    shape = 21\n  ) +\n  geom_abline(\n    data = tbl_betas,\n    aes(intercept = beta0, slope = beta1),\n    color = \"#CB181D\",\n    lwd = 0.8\n  ) +\n  geom_text(\n    data = tbl_betas,\n    aes(x = 1.8, y = 1.8, label = paste(\"Iteration:\", iter)),\n    size = 5\n  ) +\n  transition_states(iter) +\n  enter_fade() +\n  exit_shrink() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgora vamos considerar o caso de regressão múltipla, onde temos várias variáveis independentes. A equação do modelo é generalizada para:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\n\\]\nIsto é, agora temos \\(k\\) variáveis independentes, \\(x_1, x_2, ..., x_k\\), e temos \\(k\\) coeficientes, \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k\\), a ser estimados. A função de custo torna-se:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_{1} x_{i1} + \\beta_2 x_{i2} + ... + \\beta_k X_{ik}))^2\n\\]\nAgora, as derivadas parciais da função de custo em relação a cada parâmetro \\(\\beta\\) são calculadas e utilizadas para atualizar os coeficientes durante o GD.\n\\[\n\\nabla_{\\beta} = (\\frac{\\partial\\text{MSE}}{\\partial\\beta_{0}}, \\frac{\\partial\\text{MSE}}{\\partial\\beta_{1}}, ..., \\frac{\\partial\\text{MSE}}{\\partial\\beta_{k}})\n\\]\n\n\nNa regressão múltipla, podemos representar os dados de entrada \\(X\\) e os parâmetros do modelo \\(\\beta\\) como matrizes. Esta forma de representação é mais prática quando temos muitas variáveis e permite dispensar o uso de somatórios.\nSuponha que tenhamos \\(n\\) observações e \\(k\\) variáveis independentes.\nAs observações de entrada podem ser organizadas em uma matriz \\(X\\) de dimensão \\(n \\times (k+1)\\), onde a primeira coluna é composta por \\(1\\)s para representar o intercepto do modelo. Assim, a matriz \\(X\\) é dada por:\n\\[\nX = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\]\nOs parâmetros do modelo \\(\\beta\\) podem ser representados como um vetor de coeficientes de dimensão \\((k+1) \\times 1\\). Assim, o vetor \\(\\beta\\) é dado por:\n\\[\n\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix}\n\\]\nA resposta \\(y\\) pode ser representada como um vetor de dimensão \\(n \\times 1\\).\n\\[\ny = \\begin{bmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\]\nNote que o problema de minimização acima é equivalente a minimizar a soma do erro ao quadrado do modelo. Isto acontece pois o erro, no caso mais simples, é simplesmente\n\\[\n\\varepsilon = y_i - \\hat{y_i} = y_i - \\beta_{0} - \\beta_{1}x_i\n\\]\ne o problema de minimizar:\n\\[\n\\text{min } \\varepsilon^2 = (y_i - \\hat{y_i})^2\n\\]\nEm termos matriciais temos:\n\\[\ne = y - \\hat{y} = y - X\\beta\n\\] e agora o problema de minimizar\n\\[\n\\text{min } e^te = (y - X\\beta)^t(y-XB)\n\\]\nPara encontrar o gradiente da função de custo \\(e^te\\), em relação aos parâmetros \\(\\beta\\), podemos usar cálculo matricial.\nO gradiente \\(\\nabla_{\\beta} (e'e)\\) é dado por:\n\\[\n\\nabla_{\\beta} (e'e) = -2X^T(y - X\\beta)\n\\]\nOnde \\(X^T\\) representa a transposta da matriz X. Este gradiente nos fornece a direção em que devemos ajustar os (múltiplos) parâmetros \\(\\beta\\) para minimizar a função de custo \\(e'e\\).\n\n\n\n\nDesta vez, vamos implementar tanto o gradiente como a função de perda como functions no R.\n\ngrad &lt;- function(beta) {\n\n  (2/N) * t(X) %*% (X %*% beta - y)\n\n}\n\nloss &lt;- function(beta) {\n\n  e = y - X %*% beta\n\n  t(e) %*% e\n\n}\n\n\n\nNosso modelo de regressão agora terá a forma:\n\\[\n\\text{mpg} = \\beta_{0} + \\beta_{1}\\text{wt} + \\beta_{2}\\text{qsec}+ \\beta_{3}\\text{am}\n\\]\nonde mpg e wt tem as mesmas definições dadas acima; já qsec é uma medida de velocidade do veículo e am é uma variável binária que indica se o câmbio do veículo é manual ou automático.\n\n\n\nDesta vez, o preparo dos dados será feito usando o pacote dplyr.\n\nlibrary(dplyr)\n\ndat &lt;- mtcars |&gt;\n  select(c(\"mpg\", \"wt\", \"qsec\", \"am\")) |&gt;\n  mutate(across(everything(), ~as.numeric(scale(.x))))\n\ny &lt;- dat$mpg\nX &lt;- as.matrix(dat[, c(\"wt\", \"qsec\", \"am\")])\nX &lt;- cbind(1, X)\ncolnames(X)[1] &lt;- c(\"coef\")\nN &lt;- nrow(X)\n\n\n\n\nNovamente, para ganhar um pouco de intuição vamos rodar o\n\n#&gt; Valor inicial para os betas\nbeta &lt;- runif(ncol(X))\n\n# Opcional\n#&gt; Computa a \"predição\" do modelo\nyhat &lt;- X %*% beta\n#&gt; Calcula o valor da função de perda\nl &lt;- loss(beta)\n\n#&gt; Atualiza o valor dos beta\nbeta_new &lt;- beta - alpha * grad(beta)\n\n\n\n\nO código abaixo mostra o loop completo. Fora algumas pequenas modificações, ele é exatamente igual ao loop anterior. Neste segundo exemplo, eu reduzo o valor da learning-rate e aumento o número de iterações.\n\n\nCode\nbeta &lt;- runif(ncol(X))\nnum_iterations &lt;- 10000\nalpha &lt;- 0.001\n\nfor (i in seq_len(num_iterations)) {\n\n  if (i %% 1000 == 0) cat(\"Iteração: \", i, \"\\n\")\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- X %*% beta\n\n  #&gt; Calcula a \"função de perda\"\n  vl_loss &lt;- loss(beta)\n\n  if (i %% 1000 == 0) {\n    cat(\"Valor da perda: \", as.numeric(vl_loss), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  grad_current &lt;- grad(beta)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  beta_current &lt;- beta - alpha * grad_current\n\n  beta &lt;- beta_current\n\n  if (i %% 1000 == 0) {\n    cat(\"Betas: \", beta, \"\\n\\n\")\n  }\n\n}\n\n\nIteração:  1000 \nValor da perda:  7.843567 \nBetas:  0.1035636 -0.1690523 0.5729491 0.6570115 \n\nIteração:  2000 \nValor da perda:  5.904957 \nBetas:  0.01398776 -0.3430823 0.5113749 0.53897 \n\nIteração:  3000 \nValor da perda:  5.220956 \nBetas:  0.001889251 -0.4398704 0.4639382 0.4426585 \n\nIteração:  4000 \nValor da perda:  4.914118 \nBetas:  0.0002551707 -0.5040486 0.4311833 0.377357 \n\nIteração:  5000 \nValor da perda:  4.775269 \nBetas:  3.446451e-05 -0.5471717 0.4090413 0.3334177 \n\nIteração:  6000 \nValor da perda:  4.712415 \nBetas:  4.654933e-06 -0.5761808 0.3941314 0.3038562 \n\nIteração:  7000 \nValor da perda:  4.683963 \nBetas:  6.287163e-07 -0.5956981 0.3840982 0.2839671 \n\nIteração:  8000 \nValor da perda:  4.671083 \nBetas:  8.491727e-08 -0.6088296 0.3773476 0.2705854 \n\nIteração:  9000 \nValor da perda:  4.665252 \nBetas:  1.146931e-08 -0.6176647 0.3728056 0.2615821 \n\nIteração:  10000 \nValor da perda:  4.662613 \nBetas:  1.549098e-09 -0.6236091 0.3697497 0.2555244 \n\n\nPor fim, temos o valor final dos betas estimados.\n\nbeta_current\n\n              [,1]\ncoef  1.549098e-09\nwt   -6.236091e-01\nqsec  3.697497e-01\nam    2.555244e-01\n\n\nNovamente, podemos comparar estas estimativas com aquelas calculadas pela função lm. Note que, neste caso, mesmo após 10.000 iterações ainda há algumas pequenas divergências entre os valores.\n\nsummary(model_lm &lt;- lm(mpg ~ wt + qsec + am, data = dat))\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5776 -0.2581 -0.1204  0.2341  0.7734 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.485e-15  7.212e-02   0.000 1.000000    \nwt          -6.358e-01  1.155e-01  -5.507 6.95e-06 ***\nqsec         3.635e-01  8.559e-02   4.247 0.000216 ***\nam           2.431e-01  1.168e-01   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.408 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11\n\n\n\n\n\n\nO Gradiente Descendente é uma ferramenta poderosa para otimização em aprendizado de máquina e outros campos. Neste post, explicamos a matemática por trás do GD, mostramos como derivar o gradiente tanto para regressão linear simples quanto múltipla, e como entender o funcionamento deste algoritmo fundamental."
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-simples",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-simples",
    "title": "Gradient Descent",
    "section": "",
    "text": "Para tornar as coisas mais concretas vamos considerar o caso de uma regressão linear simples. Temos uma variável de resposta \\(y\\) que será “explicada” por uma variável independente (feature) \\(x\\) . A relação é modelada de maneira linear como:\n\\[\ny = \\beta_{0} + \\beta_{1}x + \\varepsilon\n\\]\nA equação acima descreve o nosso modelo, que será estimado posteriormente. Com este modelo podemos calcular os valores preditos \\(\\hat{y_{i}}\\) que serão comparados com os valores observados (reais) \\(y_{i}\\). Nosso objetivo é ajustar os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\) para minimizar a função de custo, que pode ser representada como o Erro Quadrático Médio (MSE, do inglês Mean Squared Error):\n\\[\n\\text{MSE} =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_{i}})^2= \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 X_i))^2\n\\]\nPara atualizar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) usando GD, precisamos calcular o gradiente da função de custo em relação a esses parâmetros. A primeira derivada da função de custo em relação a \\(\\beta_0\\) e \\(\\beta_1\\) nos dá as direções em que devemos ajustar esses parâmetros para minimizar a função de custo.\nA primeira derivada da função de custo em relação a \\(\\beta_0\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nE a primeira derivada da função de custo em relação a \\(\\beta_1\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_1} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nEssas derivadas nos fornecem a direção em que devemos ajustar os parâmetros para minimizar a função de custo.\n\n\n\nO algortimo de GD funciona iterativamente O algoritmo de SGD atualiza o parâmetro \\(\\beta^{t}\\) a cada iteração t, onde \\(\\beta^{0}\\) é dado, usando o gradiente \\(\\nabla_{\\beta}^t\\) da seguinte maneira:\n\\[\n\\beta^{t+1} = \\beta^{t} - \\gamma\\nabla_{\\beta}^{t}\n\\]\nonde \\(\\gamma\\) é um número real não-negativo, tipicamente próximo de 0.001, chamado “learning rate”. Quanto maior for o valor de \\(\\gamma\\) maiores serão os “passos” no processo de atualização; inversamente, quanto menor for o valor de \\(\\gamma\\) menores serão os “passos”no processo iterativo.\nPara implementar o passo-a-passo do algoritmo vamos usar a base mtcars. O primeiro passo é ajustar os dados usando a função scale. Em seguida, separamos alguns objetos úteis para facilitar a exposição.\n\n#&gt; Regularizar os dados\nmtcars_scaled &lt;- as.data.frame(scale(mtcars))\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\nVamos fazer a regressão de mpg (milhas por galão), uma medida da eficiência do veículo, contra wt (peso), o peso do veículo. Visualmente, parece haver uma relação linear decrescente entre as variáveis.\n\nplot(y ~ x)\n\n\n\n\n\n\n\n\nAntes de fazer o loop, vamos decompor o algoritmo em etapas. Primeiro, precisa-se de valores iniciais para os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\). Por simplicidade, vamos sortear números aleatórios entre 0 e 1 a partir de uma distribuição uniforme. Com estes valores será possível calcular o valor de \\(\\hat{y}_{i}^0\\), onde 0 indica que estamos na iteração de valor 0.\n\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\n(yhat &lt;- b0 + b1 * x)\n\n [1] -0.17140720  0.05168499 -0.43386860  0.34914125  0.54598730  0.56348473\n [7]  0.65972058  0.32726947  0.29227461  0.54598730  0.54598730  1.09715625\n[13]  0.79969999  0.84344356  2.12950443  2.28173204  2.21261721 -0.53885316\n[19] -1.05065289 -0.85818120 -0.30701225  0.61597701  0.54161295  0.89593584\n[25]  0.90031019 -0.77069407 -0.59134544 -1.13988977  0.30977204 -0.04017650\n[31]  0.65972058 -0.03142778\n\n\nNão é necessário, mas é instrutivo calcular a função de perda.\n\n(mse &lt;- mean((y - yhat)^2))\n\n[1] 3.240961\n\n\nAgora, calculamos o valor do gradiente neste ponto.\n\n(gb0 &lt;- sum(y - yhat) * (-2 / N))\n\n[1] 0.7022194\n\n(gb1 &lt;- sum((y - yhat) * x) * (-2 / N))\n\n[1] 3.339637\n\n\nPor fim, o valor dos parâmetros é atualizado segundo a fórmula matemática do algoritmo. Utiliza-se \\(g = 0.01\\) como valor para a learning-rate.\n\ng = 0.01\n\n(b0_new &lt;- b0 - g * gb0)\n\n[1] 0.3440875\n\n(b1_new &lt;- b1 - g * gb1)\n\n[1] 0.822628\n\n\nEste processo será repetido \\(T\\) vezes até que se atinja algum critério de convergência. Em geral, estabelece-se\n\nUm número máximo de iterações. (10.000 iterações, por exemplo).\nUm valor mínimo de mudança na estimativa dos parâmetros. Isto é, quando o valor das estimativas para de mudar significativamente, entende-se que ele convergiu para um valor satisfatório.\n\nPara deixar o loop abaixo mais simples, vou simplesmente estabelecer um número máximo de 5000 iterações. O código segue abaixo. Note que algumas partes do código acima foram repetidas por conveniência da leitura.\n\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\ng &lt;- 0.01\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nfor (i in seq_len(num_iterations)) {\n  if (i %% 100 == 0) {\n    cat(\"Iteração: \", i, \"\\n\")\n  }\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  if (i %% 100 == 0) {\n    cat(\"Valor da perda: \", as.numeric(mse), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2 / N)\n  gb1 &lt;- sum((y - yhat) * x) * (-2 / N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - g * gb0\n  b1_new &lt;- b1 - g * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n\n  if (i %% 100 == 0) {\n    cat(\"Betas: \", c(b0, b1), \"\\n\\n\")\n  }\n}\n\nIteração:  100 \nValor da perda:  0.3013997 \nBetas:  0.1179619 -0.6505911 \n\nIteração:  200 \nValor da perda:  0.2406465 \nBetas:  0.01564405 -0.8369768 \n\nIteração:  300 \nValor da perda:  0.2394667 \nBetas:  0.002074708 -0.8633224 \n\nIteração:  400 \nValor da perda:  0.2394437 \nBetas:  0.0002751468 -0.8670463 \n\nIteração:  500 \nValor da perda:  0.2394432 \nBetas:  3.648985e-05 -0.8675727 \n\nIteração:  600 \nValor da perda:  0.2394432 \nBetas:  4.839267e-06 -0.8676471 \n\nIteração:  700 \nValor da perda:  0.2394432 \nBetas:  6.417815e-07 -0.8676576 \n\nIteração:  800 \nValor da perda:  0.2394432 \nBetas:  8.511277e-08 -0.8676591 \n\nIteração:  900 \nValor da perda:  0.2394432 \nBetas:  1.128762e-08 -0.8676593 \n\nIteração:  1000 \nValor da perda:  0.2394432 \nBetas:  1.49696e-09 -0.8676594 \n\n\nPara recuperar o valor final dos betas.\n\nc(b0, b1)\n\n[1]  1.496960e-09 -8.676594e-01\n\n\nPara efeito didático, vamos comparar estas estimativas finais contra os valores estimados pela função lm. Note que os valores estão muito similares\n\nsummary(model_lm &lt;- lm(mpg ~ wt, data = mtcars_scaled))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars_scaled)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75381 -0.39236 -0.02077  0.23388  1.14033 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.040e-15  8.934e-02   0.000        1    \nwt          -8.677e-01  9.077e-02  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5054 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\n\n\nSempre que possível, é útil visualizar o funcionamento do algoritmo em gráficos. Como estamos trabalhando com um exemplo simples, pode-se plotar os resultados gradativamente num gráfico de dispersão. O código abaixo mostra como a linha de ajuste (linha de regressão) vai se alterando à medida que se aumenta o número de amostras.\n\n\nCode\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\nalpha &lt;- 0.01\n\ny &lt;- dat$mpg\nx &lt;- dat$wt\nN &lt;- nrow(dat)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nbetas &lt;- matrix(ncol = 2, nrow = num_iterations)\nfor (i in seq_len(num_iterations)) {\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2 / N)\n  gb1 &lt;- sum((y - yhat) * dat$wt) * (-2 / N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - alpha * gb0\n  b1_new &lt;- b1 - alpha * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n\n  betas[i, 1] &lt;- b0_new\n  betas[i, 2] &lt;- b1_new\n}\n\nsel &lt;- c(1:10, seq(20, 100, 10), seq(100, 1000, 50))\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tibble)\n\ntbl_betas &lt;- tibble(\n  iter = sel,\n  beta0 = betas[sel, 1],\n  beta1 = betas[sel, 2]\n)\n\nggplot() +\n  geom_point(\n    data = dat,\n    aes(x = wt, y = mpg),\n    shape = 21\n  ) +\n  geom_abline(\n    data = tbl_betas,\n    aes(intercept = beta0, slope = beta1),\n    color = \"#CB181D\",\n    lwd = 0.8\n  ) +\n  geom_text(\n    data = tbl_betas,\n    aes(x = 1.8, y = 1.8, label = paste(\"Iteration:\", iter)),\n    size = 5\n  ) +\n  transition_states(iter) +\n  enter_fade() +\n  exit_shrink() +\n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-múltipla",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-múltipla",
    "title": "Gradient Descent",
    "section": "",
    "text": "Agora vamos considerar o caso de regressão múltipla, onde temos várias variáveis independentes. A equação do modelo é generalizada para:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\n\\]\nIsto é, agora temos \\(k\\) variáveis independentes, \\(x_1, x_2, ..., x_k\\), e temos \\(k\\) coeficientes, \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k\\), a ser estimados. A função de custo torna-se:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_{1} x_{i1} + \\beta_2 x_{i2} + ... + \\beta_k X_{ik}))^2\n\\]\nAgora, as derivadas parciais da função de custo em relação a cada parâmetro \\(\\beta\\) são calculadas e utilizadas para atualizar os coeficientes durante o GD.\n\\[\n\\nabla_{\\beta} = (\\frac{\\partial\\text{MSE}}{\\partial\\beta_{0}}, \\frac{\\partial\\text{MSE}}{\\partial\\beta_{1}}, ..., \\frac{\\partial\\text{MSE}}{\\partial\\beta_{k}})\n\\]\n\n\nNa regressão múltipla, podemos representar os dados de entrada \\(X\\) e os parâmetros do modelo \\(\\beta\\) como matrizes. Esta forma de representação é mais prática quando temos muitas variáveis e permite dispensar o uso de somatórios.\nSuponha que tenhamos \\(n\\) observações e \\(k\\) variáveis independentes.\nAs observações de entrada podem ser organizadas em uma matriz \\(X\\) de dimensão \\(n \\times (k+1)\\), onde a primeira coluna é composta por \\(1\\)s para representar o intercepto do modelo. Assim, a matriz \\(X\\) é dada por:\n\\[\nX = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\]\nOs parâmetros do modelo \\(\\beta\\) podem ser representados como um vetor de coeficientes de dimensão \\((k+1) \\times 1\\). Assim, o vetor \\(\\beta\\) é dado por:\n\\[\n\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix}\n\\]\nA resposta \\(y\\) pode ser representada como um vetor de dimensão \\(n \\times 1\\).\n\\[\ny = \\begin{bmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\]\nNote que o problema de minimização acima é equivalente a minimizar a soma do erro ao quadrado do modelo. Isto acontece pois o erro, no caso mais simples, é simplesmente\n\\[\n\\varepsilon = y_i - \\hat{y_i} = y_i - \\beta_{0} - \\beta_{1}x_i\n\\]\ne o problema de minimizar:\n\\[\n\\text{min } \\varepsilon^2 = (y_i - \\hat{y_i})^2\n\\]\nEm termos matriciais temos:\n\\[\ne = y - \\hat{y} = y - X\\beta\n\\] e agora o problema de minimizar\n\\[\n\\text{min } e^te = (y - X\\beta)^t(y-XB)\n\\]\nPara encontrar o gradiente da função de custo \\(e^te\\), em relação aos parâmetros \\(\\beta\\), podemos usar cálculo matricial.\nO gradiente \\(\\nabla_{\\beta} (e'e)\\) é dado por:\n\\[\n\\nabla_{\\beta} (e'e) = -2X^T(y - X\\beta)\n\\]\nOnde \\(X^T\\) representa a transposta da matriz X. Este gradiente nos fornece a direção em que devemos ajustar os (múltiplos) parâmetros \\(\\beta\\) para minimizar a função de custo \\(e'e\\)."
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#implementando-o-algoritmo-1",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#implementando-o-algoritmo-1",
    "title": "Gradient Descent",
    "section": "",
    "text": "Desta vez, vamos implementar tanto o gradiente como a função de perda como functions no R.\n\ngrad &lt;- function(beta) {\n\n  (2/N) * t(X) %*% (X %*% beta - y)\n\n}\n\nloss &lt;- function(beta) {\n\n  e = y - X %*% beta\n\n  t(e) %*% e\n\n}\n\n\n\nNosso modelo de regressão agora terá a forma:\n\\[\n\\text{mpg} = \\beta_{0} + \\beta_{1}\\text{wt} + \\beta_{2}\\text{qsec}+ \\beta_{3}\\text{am}\n\\]\nonde mpg e wt tem as mesmas definições dadas acima; já qsec é uma medida de velocidade do veículo e am é uma variável binária que indica se o câmbio do veículo é manual ou automático.\n\n\n\nDesta vez, o preparo dos dados será feito usando o pacote dplyr.\n\nlibrary(dplyr)\n\ndat &lt;- mtcars |&gt;\n  select(c(\"mpg\", \"wt\", \"qsec\", \"am\")) |&gt;\n  mutate(across(everything(), ~as.numeric(scale(.x))))\n\ny &lt;- dat$mpg\nX &lt;- as.matrix(dat[, c(\"wt\", \"qsec\", \"am\")])\nX &lt;- cbind(1, X)\ncolnames(X)[1] &lt;- c(\"coef\")\nN &lt;- nrow(X)\n\n\n\n\nNovamente, para ganhar um pouco de intuição vamos rodar o\n\n#&gt; Valor inicial para os betas\nbeta &lt;- runif(ncol(X))\n\n# Opcional\n#&gt; Computa a \"predição\" do modelo\nyhat &lt;- X %*% beta\n#&gt; Calcula o valor da função de perda\nl &lt;- loss(beta)\n\n#&gt; Atualiza o valor dos beta\nbeta_new &lt;- beta - alpha * grad(beta)\n\n\n\n\nO código abaixo mostra o loop completo. Fora algumas pequenas modificações, ele é exatamente igual ao loop anterior. Neste segundo exemplo, eu reduzo o valor da learning-rate e aumento o número de iterações.\n\n\nCode\nbeta &lt;- runif(ncol(X))\nnum_iterations &lt;- 10000\nalpha &lt;- 0.001\n\nfor (i in seq_len(num_iterations)) {\n\n  if (i %% 1000 == 0) cat(\"Iteração: \", i, \"\\n\")\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- X %*% beta\n\n  #&gt; Calcula a \"função de perda\"\n  vl_loss &lt;- loss(beta)\n\n  if (i %% 1000 == 0) {\n    cat(\"Valor da perda: \", as.numeric(vl_loss), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  grad_current &lt;- grad(beta)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  beta_current &lt;- beta - alpha * grad_current\n\n  beta &lt;- beta_current\n\n  if (i %% 1000 == 0) {\n    cat(\"Betas: \", beta, \"\\n\\n\")\n  }\n\n}\n\n\nIteração:  1000 \nValor da perda:  7.843567 \nBetas:  0.1035636 -0.1690523 0.5729491 0.6570115 \n\nIteração:  2000 \nValor da perda:  5.904957 \nBetas:  0.01398776 -0.3430823 0.5113749 0.53897 \n\nIteração:  3000 \nValor da perda:  5.220956 \nBetas:  0.001889251 -0.4398704 0.4639382 0.4426585 \n\nIteração:  4000 \nValor da perda:  4.914118 \nBetas:  0.0002551707 -0.5040486 0.4311833 0.377357 \n\nIteração:  5000 \nValor da perda:  4.775269 \nBetas:  3.446451e-05 -0.5471717 0.4090413 0.3334177 \n\nIteração:  6000 \nValor da perda:  4.712415 \nBetas:  4.654933e-06 -0.5761808 0.3941314 0.3038562 \n\nIteração:  7000 \nValor da perda:  4.683963 \nBetas:  6.287163e-07 -0.5956981 0.3840982 0.2839671 \n\nIteração:  8000 \nValor da perda:  4.671083 \nBetas:  8.491727e-08 -0.6088296 0.3773476 0.2705854 \n\nIteração:  9000 \nValor da perda:  4.665252 \nBetas:  1.146931e-08 -0.6176647 0.3728056 0.2615821 \n\nIteração:  10000 \nValor da perda:  4.662613 \nBetas:  1.549098e-09 -0.6236091 0.3697497 0.2555244 \n\n\nPor fim, temos o valor final dos betas estimados.\n\nbeta_current\n\n              [,1]\ncoef  1.549098e-09\nwt   -6.236091e-01\nqsec  3.697497e-01\nam    2.555244e-01\n\n\nNovamente, podemos comparar estas estimativas com aquelas calculadas pela função lm. Note que, neste caso, mesmo após 10.000 iterações ainda há algumas pequenas divergências entre os valores.\n\nsummary(model_lm &lt;- lm(mpg ~ wt + qsec + am, data = dat))\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5776 -0.2581 -0.1204  0.2341  0.7734 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.485e-15  7.212e-02   0.000 1.000000    \nwt          -6.358e-01  1.155e-01  -5.507 6.95e-06 ***\nqsec         3.635e-01  8.559e-02   4.247 0.000216 ***\nam           2.431e-01  1.168e-01   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.408 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11"
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#conclusão",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#conclusão",
    "title": "Gradient Descent",
    "section": "",
    "text": "O Gradiente Descendente é uma ferramenta poderosa para otimização em aprendizado de máquina e outros campos. Neste post, explicamos a matemática por trás do GD, mostramos como derivar o gradiente tanto para regressão linear simples quanto múltipla, e como entender o funcionamento deste algoritmo fundamental."
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#footnotes",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#footnotes",
    "title": "Gradient Descent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApesar dos métodos serem formalmente diferentes, a sua essência é idêntica a ponto de ser comum confundi-los. O SGD é uma “aproximação” do GD, onde apenas uma parte (amostra) dos dados é utilizada para calcular o gradiente. Neste sentido, o SGD é muito mais eficiente do ponto de vista computacional.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-sp-grid-houses/index.html",
    "href": "posts/general-posts/2024-04-sp-grid-houses/index.html",
    "title": "Domicilios em Sao Paulo",
    "section": "",
    "text": "Code\n# Setup ---------------------------------------------------------\n\n# Libraries\nlibrary(ggplot2)\nlibrary(geobr)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(MetBrewer)\n# https://github.com/viniciusoike/tidypod\nlibrary(tidypod)\n\n# Fonts\nfont_add(\"HelveticaNeue\", \"HelveticaNeue.ttc\")\nshowtext_auto()\n\n# Data ----------------------------------------------------------\n\n# Import Sao Paulo shapefile\nsp_border &lt;- geobr::read_municipality(3550308, showProgress = FALSE)\n\n# Import locally Census data\ncenso22 &lt;- data.table::fread(\"/Volumes/T7 Touch/github/tidyibge/data-raw/35.csv\")\n\n# Convert column names to lowercase and filter only capital\ncenso22 &lt;- censo22 |&gt; \n  rename_with(tolower) |&gt; \n  filter(cod_mun == 3550308)\n\n# Variable dictionary\ndictionary_censo &lt;- tribble(\n  ~cod_especie, ~label_especie,\n  1, \"Domicílio particular\",\n  2, \"Domicílio coletivo\",\n  3, \"Estabelecimento agropecuário\",\n  4, \"Estabelecimento de ensino\",\n  5, \"Estabelecimento de saúde\",\n  6, \"Estabelecimento de outras finalidades\",\n  7, \"Edificação em construção\",\n  8, \"Estabelecimento religioso\"\n)\n\n# Join data with dictionary -------------------------------------\ncenso22 &lt;- left_join(censo22, dictionary_censo, by = \"cod_especie\")\n\n\n# Spatial data --------------------------------------------------\n\n# Import districts shapefile\ndstr &lt;- tidypod::districts\ndstr &lt;- filter(dstr, code_muni == 36)\n\ndomicilios &lt;- st_as_sf(\n  censo22[cod_especie == 1],\n  coords = c(\"longitude\", \"latitude\"),\n  crs = 4674\n  )\n\nsp_grid_rectangular &lt;- sp_border |&gt; \n  st_transform(crs = 32722) |&gt; \n  st_make_grid(cellsize = 100) |&gt; \n  st_as_sf() |&gt; \n  st_transform(crs = 4326) |&gt; \n  mutate(gid = row_number())\n\ndomi_grid &lt;- domicilios |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(sp_grid_rectangular) |&gt; \n  filter(!is.na(gid))\n\ndomi_grid_count &lt;- domi_grid |&gt; \n  st_drop_geometry() |&gt; \n  summarise(total = n(), .by = \"gid\")\n\ngrid_count &lt;- left_join(sp_grid_rectangular, domi_grid_count, by = \"gid\")\n\n# Plot ----------------------------------------------------------\n\nbreaks &lt;- BAMMtools::getJenksBreaks(na.omit(grid_count$total), 7)\nlabels &lt;- format(round(breaks, -1), big.mark = \".\")\n\nmap_sp &lt;- ggplot() +\n  geom_sf(data = sp_border, lwd = 0.05, fill = NA) +\n  geom_sf(\n    data = filter(grid_count, !is.na(total)),\n    aes(fill = total, color = total),\n    alpha = 0.8\n    ) +\n  scale_fill_fermenter(\n    name = \"\",\n    na.value = \"gray50\",\n    direction = 1,\n    palette = \"BuPu\",\n    breaks = breaks,\n    labels = labels\n    ) +\n  scale_color_fermenter(\n    name = \"\",\n    na.value = \"gray50\",\n    direction = 1,\n    palette = \"BuPu\",\n    breaks = breaks,\n    labels = labels\n    ) +\n  coord_sf() +\n  labs(\n    title = \"Domicílios em São Paulo\",\n    subtitle = \"Concentração de domicílios em grid 100x100m em São Paulo\") +\n  ggthemes::theme_map(base_family = \"HelveticaNeue\") +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(0.85, \"cm\"),\n    legend.key.width = unit(1.5, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 24, hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-sp-grid-houses/index.html#footnotes",
    "href": "posts/general-posts/2024-04-sp-grid-houses/index.html#footnotes",
    "title": "Domicilios em Sao Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDefinidos como domicílios particulares permanentes, segundo o Censo do IBGE (2022).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-05-generations-brazil/index.html",
    "href": "posts/general-posts/2024-05-generations-brazil/index.html",
    "title": "Generations in Brazil",
    "section": "",
    "text": "Like many other countries, Brazil is aging rapidly. The combination of increasing life expectancy and declining fertility rates means that, in the long run, the proportion of elderly people in the population will rise, while the share of young people will decline. Despite this long-term trend, the current demographic outlook reveals the largest contingent of teens and young adults in Brazilian history. Nearly 100 million people, close to half of the population, belong to Gen Z and Gen Y (Millennials), ranging in age from 12 to 43.\n\n\n\n\n\n\n\n\n\nThe cut-offs for each generation cohort, in the plot above, followed directly Beresford Research’s definition (which adapted the previous definition developed by the Pew Research Center. The data comes from the most recent Brazilian Census (2022).\n\n\n\n\n\n\n\nGenerations by birth year defined by the Pew Research Center\n\n\nThis should be the last great young population for Brazil which means the country should try to make the best of it. As this population begins to age, dependency ratios will increase rapidly pressuring the already fragile public pension system. This is particularly true of cities in the South and Southeast of Brazil.\n\n\n\n\n\n\n\n\nGeneration\nAge Range\nPopulation\nShare\nCumulative Share\n\n\n\n\nAlpha\n1-11\n29.515.539\n14,70%\n14,70%\n\n\nGen Z\n12-27\n47.564.885\n23,70%\n38,40%\n\n\nMillenial\n28-43\n50.847.471\n25,33%\n63,73%\n\n\nGen X\n44-59\n40.681.106\n20,27%\n84,00%\n\n\nBoomers\n60-79\n27.526.536\n13,71%\n97,71%\n\n\nElder\n80-100\n4.586.954\n2,29%\n100,00%\n\n\n\n\n\n\n\nA worrying trend is the rising number of unemployed and uneducated among the Brazilian youth. Recent unemployment numbers from PNAD still show a big gap in unemployment rates, specially for the Gen Z’s who are entering the labor market.\n\n\n\n\n\n\n\n\n\nA recent study from IBGE, published in the Summary of Social Indicators, shows that 22,3% of Brazilians aged 15 to 29 neither studied nor worked in 2022. This is equivalent to almost 11 million youngsters.\nA recent report from the OECD also notes that Brazil has a very large share of NEETs (neither employed nor in formal education or training) in the 18-24 age. The NEETs in Brazil are overwhelmingly black, female, and low-income, highlighting the countries unequal access to opportunities.\n\n\n\n\n\n\n\n\nAging Index In Brazil\nBrazilian Census"
  },
  {
    "objectID": "posts/general-posts/2024-05-generations-brazil/index.html#related-posts",
    "href": "posts/general-posts/2024-05-generations-brazil/index.html#related-posts",
    "title": "Generations in Brazil",
    "section": "",
    "text": "Aging Index In Brazil\nBrazilian Census"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html",
    "title": "Importando arquivos, visualizando linhas",
    "section": "",
    "text": "Uma das minhas maiores dificuldades quando comecei a mexer com R era conseguir importar a minha base de dados. Em geral, eu tinha um ou vários arquivos .csv ou planilhas .xlsx que precisavam ser importador para o R. Eu tinha três dificuldades\n\nSaber qual a função que eu precisava usar.\nEscrever o caminho até o arquivo específico (qualquer errinho e já não funcionava!).\nCalibrar os argumentos na hora de importar.\n\nDepois de muito esforço eu conseguia importar os dados, mas logo surgia ourto problema: o nome das variáveis vinha num formato muito ruim para trabalhar. No R, idealmente as variáveis são todas:\n\nMinúsculas\nNão tem acento nem caracteres especiais como $, %, etc.\nNão tem espaços\nNão começa com números\n\nIsso evita inúmeros problemas e facilita muito na hora de escrever o código.\nPor fim, eu tinha bastante dificuldade de “enxergar os dados” mesmo depois de ter importado eles. No caso de uma planilha de Excel eu poderia simplesmente abrir ela e explorar um pouco. Já se a base de dados fosse em formato dta ou sav isso já não era tão simples, pois eu não tinha Stata e nem SPSS no meu computador.\nTudo isso me desanimava quando comecei a mexer com R e vejo que isso é um daqueles obstáculos idiotas que acabam segurando muita gente de fazer a transição para o R. É o tipo de problema (que parece) simples, mas que na verdade é difícil de resolver e que faz com que você se sinta burro e fique frustrado.\nNeste post vou te dar algumas dicas de como lidar com todos estes passos. É o post que eu gostaria de poder enviar para mim mesmo no passado."
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#a-solução-correta",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#a-solução-correta",
    "title": "Importando arquivos, visualizando linhas",
    "section": "A solução correta",
    "text": "A solução correta\nA tabela abaixo resume as principais funções que você provavelmente vai ter que usar.\n\n\n\n\n\n\n\n\n\nFormato\nExtensao\nImportar\nExportar\n\n\n\n\nExcel\nxls, xlsx\nreadxl::read_excel()\nxlsx::write.xlsx()\n\n\nSeparados\ncsv, tsv, psv, csvy\ndata.table::fread() ou readr\ndata.table::fwrite() ou readr\n\n\nStata\ndta\nhaven::read_dta()\nhaven::write_dta()\n\n\nSPSS\nsav\nhaven::read_sav()\nhaven::write_sav()\n\n\nShapefiles\nshp, geosjon, gpkg, etc.\nsf::st_read()\nsf::st_write()\n\n\n\nOBS: Caso não esteja familiarizado com esta sintaxe, aqui readxl é o nome do pacote, read_excel() é a funcão e o :: indica que eu quero a função read_excel do pacote readxl. Em geral, é comum omitir a parte do nomepacote:: porque acaba sendo desnecessário (exceto no caso de conflitos de funções que têm o mesmo nome).\n\nSobre csvs\nVale uma nota: existem várias alternativas para importar arquivos csv, várias alternativas mesmo. Eu recomendo evitar as funções base e usar as funções equivalentes do readr. Então, por exemplo, ao invés de usar read.csv use readr::read_csv. Esta é uma boa decisão por três motivos:\n\nAs funções do readr são consideravelmente mais rápidas e versáteis do que as funções base equivalentes.\nAs funções read_* compartilham uma sintaxe padronizada e costumam ter argumentos muito similares. Assim as funções do readr são muito parecidas com as funções do pacote readxl e haven. O combo readr + readxl + haven resolve o problema em 95% dos casos.\nTodos estes pacotes e funções já estão bem integrados ao universo tidyverse1.\n\nSe velocidade começar a ser um problema, pode-se experimentar também com o pacote vroom que permite importar arquivos csv mais rapidamente. A função vroom::vroom também compartilha da sintaxe das funções read_*.\nUm típico problema com arquivos csv é que os delimitadores e separadores variam de país para país. Arquivos csv de fontes brasileiras costumam ser separadas por ; e usam a , como quebra de decimal, ao contrário dos csv de fontes dos EUA que usam a , como separador e . como quebra de decimal. Os arquivos no padrão EUA devem ser lidos com read_csv enquanto os arquivos no padrão brasileiro devem ser lidos com read_csv22.\nApesar de todas as vantagens listadas acima, ainda vale recomendar o data.table::fread()3 na hora de importar qualquer arquivo “separado” (csv, tsv, psv, etc.). Esta função é extremamente rápida, aloca os dados na memória de maneira eficiente e simplesmente funciona. Mesmo sem nenhum argumento adicional ela é muito boa na hora de adivinhar o tipo de separador utilizado e o tipo de dado em cada coluna.\nPor fim, apesar de ter recomendado o data.table::fwrite na tabela acima, vale notar que funções como readr::write_excel_csv2() podem ser muito úteis caso seu objetivo seja exportar um csv que vai ser consumido por um usuário brasileiro numa planilha de Excel.\n\n\nMais controle\nA prática faz a perfeição na hora de importar arquivos problemáticos. Como comentei acima, uma das vantagens de se ater ao combo readr + readxl + haven é que os argumentos adicionais destas funções seguem o mesmo padrão.\n\nskip = k: Pula as primeiras k linhas.\nna: Define quais valores devem ser interpretados como valores ausentes.\ncol_types: Permite que se declare explicitamente qual o tipo de dado (numérico, data, texto) que está armazenado em cada coluna.\ncol_names ou name_repair: O primeiro permite que se declare explicitamente o nome que cada coluna vai ter dentro do R enquanto o segundo permite que se use uma função que renomeia as colunas.\nlocale: Permite selecionar diferentes tipos de padrão de local. Em geral, usa-se locale = locale(\"pt\").\nrange: Este argumento só vale no caso de planilhas de Excel e permite que se importe uma seleção específica da planilha (e.g. “D4:H115”)\n\nO código abaixo mostra como importar um csv bastante sujo. Veremos detalhes sobre a função janitor::clean_names mais adiante.\n\n#&gt; Input de um csv sujo\ndados &lt;-\n'Data; Valor (R$/m2)\n\"01-maio-2020\";22,3\n\"01-junho-2020\";21,5\n\"06-julho-2021\";X\n\"07-novembro-2022\";22'\n\n#&gt; Lendo o arquivo\ndf &lt;- read_delim(\n  #&gt; Substitua esta linha pelo 'path' até o csv\n  I(dados),\n  delim = \";\",\n  #&gt; Usa , como separador decimal; lê meses em português (e.g. maio, junho, etc.)\n  locale = locale(decimal_mark = \",\", date_names = \"pt\", date_format = \"%d-%B-%Y\"),\n  #&gt; Interpreta X como valores ausentes (NA)\n  na = \"X\",\n  #&gt; Renomeia as colunas\n  name_repair = janitor::clean_names\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#importando-arquivos-o-atalho",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#importando-arquivos-o-atalho",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Importando arquivos: o atalho",
    "text": "Importando arquivos: o atalho\nUma função muito prática que inicialmente contorna todos estes problemas é a rio::import().\nA função rio::import() simplesmente importa seus dados e funciona com boa parte das extensões mais populares. Na prática, ela é uma função “facilitadora”. Por baixo dos panos, ela está chamando a função correta para o caso específico.\nEste site mostra exatamente qual função de qual pacote ele utiliza para importar os dados. A lista é bem completa e inclui bases do Minitab, Matlab, EViews, etc. Spoiler: data.table::fread() é utilizada para importar arquivos csv, psv e tsv.\nO par da função rio::import é a rio::export e serve justamente para exportar bases de dados do R para o formato desejado.\n\ndata &lt;- rio::import(\"data/meus_dados.dta\")\n\nrio::export(mtcars, \"data/mtcars.csv\")\n\nO único problema desta função é quando seus arquivos não estão num formato muito bacana e argumentos adicionais são necessários. É possível fornecer estes argumentos à função, mas é difícil saber quais são os argumentos, já que não se sabe qual função está sendo chamada. Assim, é preciso consultar a documentação (?rio::import) para verificar qual função está sendo utilizada e aí consultar a documentação desta função."
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#escrevendo-paths-relativos-com-here",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#escrevendo-paths-relativos-com-here",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Escrevendo paths relativos com here",
    "text": "Escrevendo paths relativos com here\nUm problema bem sério que eu enfrentava nos meus códigos era escrever o path até os arquivos externos. Primeiro, eu achava muito trabalhoso escrever ele inteiro. Depois, quando eu mandava meu código para outra pessoa, ou quando eu mesmo ia executar o meu código em outro computador, nada funcionava!\nO primeiro passo para lidar com isso é trabalhar com projetos do RStudio4. O melhor workflow é sempre começar seu trabalho num projeto novo e deixar todos os arquivos necessários neste mesmo diretório em pastas com nomes simples como data, report, graphics, etc.\nO segundo passo é utilizar paths relativos. Paths relativos, ao contrário de paths absolutos, começam no seu diretório da seguinte forma: \"data/subpasta/meus_dados.xlsx\". A pasta data está dentro da pasta do projeto: isto é indicado implicitamente.\nÉ bem diferente de um path absoluto: /Users/nome_do_usuario/Documentos/meus_projetos/Projeto 1/data/subpasta/meus_dados.xlsx.\nUsar paths absolutos no seu código é garantir que a única pessoa que conseguirá reproduzir ele com sucesso será você, e unicamente no computador em que você escreveu ele (isso se você não formatar ele!).\nAqui entra o pacote here, um pacotinho muito simples, centrado em uma única função homônima. A função here funciona de duas maneiras bastante simples\n\nlibrary(here)\nlibrary(haven)\n\nmeus_dados &lt;- read_dta(here(\"data/subpasta/minha_base.dta\"))\nmeus_dados &lt;- read_dta(here(\"data\", \"subpasta\", \"minha_base.dta\"))\n\nA segunda forma de sintaxe é muito útil na hora de criar paths. Isto será muito conveniente depois, quando formos importar vários arquivos de uma mesma pasta.\nA seguinte ilustração que serve de capa do projeto do here resume muito bem a sua utilidade.\n\n\n\n\n\n\n\n\n\nExiste um mal hábito dissemeniado de incluir uma linha com setwd(\"insira_seu_diretorio\") no início de todo código. Eu garanto que todo tipo de problema imaginável e inimaginável acontece com pessoas que fazem isso.\nO here simplesmente funciona e funciona com tudo. Ele é especialmente útil na hora de escrever scripts em RMarkdown e Quarto. O pacote here é talvez o único que esteja presente em todos os meus projetos e em todos os meus códigos.\nOutra dica boa para manter seus projetos organizados é de evitar colocar espaços ou caracteres especiais no nome das suas pastas. Em geral, o R consegue lidar bem com isso, mas volta e meia este mau hábito pode gerar problemas desnecessários e inesperados."
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#importando-todos-os-arquivos-de-uma-pasta",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#importando-todos-os-arquivos-de-uma-pasta",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Importando todos os arquivos de uma pasta",
    "text": "Importando todos os arquivos de uma pasta\nEste é um problema bastante recorrente e que é fácil de resolver usando here e funções base.\nImagine que você tem vários arquivos .csv numa pasta e os arquivos estão na seguinte estrutura: Dados/inflacao/2012/ e aí cada csv individual é um arquivo mensal (com nomes potencialmente fora de padrão) com os dados de inflação mensal por produto. Algo como 2012_jan.csv, 2012fevereiro.csv, etc.\nA estrutura do código para importar tudo isso no R é bastante simples.\n\nlibrary(here)\n# Define o diretório\ndir &lt;- here(\"Dados/inflacao/2012\")\n# Encontra o nome de todos os arquivos com extensão csv nesta pasta\nfilenames &lt;- list.files(dir, pattern = \"\\\\.csv$\")\n# Define o path até cada um dos arquivos\npathfiles &lt;- here(dir, filenames)\n# Importa todos os csv usando fread\ndata &lt;- lapply(pathfiles, data.table::fread)\n\n# Opcionalmente, empilha todos os resultados e cria uma coluna que identifica\n# de qual o arquivo a observação pertence\nnames(data) &lt;- basename(pathfiles)\nempilhado &lt;- data.table::rbindlist(data, idcol = \"nome_arquivo\")\n\nUma maneira ainda mais sucinta de escrever o código seria omitindo os objetos intermediários e simplesmente empilhando o resultado final. Note que, para que as bases de dados sejam empilhadas é necessário que o nome e o tipo das colunas seja compatível. Isto pode ser melhor controlado declarando o tipo das colunas na função fread.\n\n# Define o path até cada um dos arquivos\npathfiles &lt;- list.files(\n  here(\"Dados/inflacao/2012\"),\n  pattern = \"\\\\.csv$\",\n  full.names = TRUE\n  )\n# Importa todos os csv usando fread\nfiles &lt;- lapply(pathfiles, data.table::fread)\n# Empilha todos os resultados\ndat &lt;- data.table::rbindlist(files)"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#olhando-seus-dados-no-excel",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#olhando-seus-dados-no-excel",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Olhando seus dados no Excel",
    "text": "Olhando seus dados no Excel\nPor fim, vou deixar uma função customizada bem divertida, que permite você rapidamente dar uma espiada nos seus dados no bom e velho Excel. A função abaixo cria um arquivo temporário a partir da sua base de dados no R e abre isso no Excel.\nEvidentemente, é preciso ter o Excel instalado para que o código funcione.\n\nshow_in_excel &lt;- function(.data) {\n  if (interactive()) {\n    tmp &lt;- paste0(tempfile(), \".xlsx\")\n    writexl::write_xlsx(.data, tmp)\n    browseURL(tmp)\n  }\n  .data\n}\n\nshow_in_excel(mtcars)\n\nA figura abaixo mostra o resultado do código\n\n\n\n\n\n\n\n\n\nNovamente, como a função aceita um data.frame como argumento é bem fácil de colocá-la no final de um pipe. Esta função é bastante útil quando você precisa rapidamente compartilhar algum resultado ou tabela com alguém.\n\nmtcars |&gt; \n  filter(cyl &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(peso_medio = mean(wt)) |&gt; \n  show_in_excel()"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#footnotes",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#footnotes",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara saber mais sobre o tidyverse consulte o meu post sobre A Filosofia do Tidyverse.↩︎\nAlternativamente, pode-se usar a função read_delim, que é mais geral e permite especificar quais símbolos são usados como delimitador e separador de números.↩︎\nDisclaimer importante: depois de revisar este texto eu já não recomendo tão fortemente o data.table::fread por um movito bobo e simples. Atualmente, o data.table tem uma classe própria para datas chamada IDate. Esta classe é útil se você pretende fazer todas as suas análises usando as funções do data.table como shift, hour, etc. Contudo, se você pretende usar outros pacotes comuns de séries de tempo será necessário converter para Date todas as vezes. Além disso, como eu já estou bastante habituado a usar o pacote lubridate para manipular datas, não vejo muita vantagem em utilizar as funções do data.table.↩︎\nEvidentemente, projetos não são exclusivos ao RStudio; também é possível trabalhar com um workflow de projeto com VSCode, por exemplo. Para usuários que usam majoritariamente o R, contudo, o RStudio facilita muito a vida.↩︎\nEstas palavras são “nomes reservados” dentro do R e jamais devem ser utilizados na hora de definir um objeto ou o nome de uma coluna. Para consultar a lista de nomes reservados, veja help(\"Reserved\").↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-transport/index.html",
    "href": "posts/general-posts/2023-11-wz-transport/index.html",
    "title": "Weekly Viz: Transportation in São Paulo",
    "section": "",
    "text": "São Paulo Transportation\nThe Metropolitan Region of São Paulo (MRSP) accommodates over 20 million residents and facilitates over 40 million trips on a daily basis. The city of São Paulo recently announced that it aims to drastically reduce greenhouse gas emissions, achieving net-zero emissions by 2050. To attain this objective, the city must focus on improving transit options and reducing car-dependency.\nAccording to the most recent Metro Report, published in 2018, travel patterns in the Metropolitan Region of São Paulo are categorized as follows: one-third of journeys involve non-motorized modes (such as walking or cycling), while two-thirds are conducted through motorized means. Within the latter category, 55% of trips utilize collective modes of transportation (bus, subway, train), and 45% rely on individual modes (car, taxi, motorcycle).\nNotably, trips undertaken in private automobiles (excluding taxis) constitute around 27% of the total daily trips. In other words, roughly one in every four journeys in the metro region involves private car usage.\nThe diagram below delineates the entirety of daily trips within the MRSP. It’s important to acknowledge that the data pertains to the year 2017, during which many stations along the 4-Yellow and 5-Coral lines were still under construction. Similarly, the 13-Jade and 15-Silver lines (monorail) were not yet fully operational. For the sake of simplicity, the analysis encompasses all types of travel, although commuting between home and work constitutes the predominant share of this set."
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "Brazilian geographical hierarchy comprises several fundamental concepts. From an administrative standpoint, the smallest delineations are either zip code areas or neighborhoods. While zip codes offer greater precision, their accessibility in terms of shapefiles is limited. Therefore, our focus primarily lies on neighborhoods.\nMoving up the hierarchy, we encounter cities (also referred to as municipalities), metropolitan regions, states (including the Distrito Federal), regional divisions, and the entirety of the country. As of 2022, Brazil is composed of 5,568 cities, 77 metropolitan regions plus 1 RIDE, 26 states, and 5 regional divisions.\nFrom a statistical perspective, the Brazilian Institute of Geography and Statistics (IBGE) delineates three other crucial spatial units: census tracts (setores censitários), weighting areas (áreas de ponderação), and the statistical grid. Finally, there are also other sets of cities, that are more specially defined than typical metropolitan regions:\n\nPopulation arrangements (arranjos populacionais): a grouping of two or more municipalities where there is a strong population integration due to commuting for work or study, or due to contiguity between the main urbanized areas.\nUrban Concentration areas (concentracoes urbanas): isolated cities or population arrangements with over 100,000 inhabitants.\nMeso Regions and Micro Regions:\nIntermediate Regions and Imediate Regions:\n\nAdditionally, there are other noteworthy spatial delineations, such as the Unidades de Desenvolvimento Humano, introduced by the Institute for Applied Economic Research (IPEA) in a 2013 study, and the Origin-Destination Zones, utilized by Metro to segment the São Paulo metropolitan region.\nIn summary,\nAdministrative Divsions\n\nMacro Regions\nStates\nMetropolitan Regions\nCities\nNeighborhood\nDistricts and subdistricts\nZip-code area\n\nStatistical Division\n\nWeighting Areas\nCensus Tracts\nStatistical Grid\n\n\n\nTo illustrate the above shapes consider the city of Curitiba. Curitiba is the capital city of the state of Paraná and the 8th most populous capital in Brazil and also the 8th smallest capital which helps keep the maps smaller. The state of Paraná is also relatively small.\nImporting these shapefiles into R is made very easy thanks to the excellent geobr package.\n\n\nCode\nlibrary(geobr)\nlibrary(censobr)\nlibrary(sidrar)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(MetBrewer)\nlibrary(h3)\nlibrary(areal)\n\ncode_cur &lt;- 4106902\n\n# Region borders\nregion &lt;- read_region(showProgress = FALSE)\n# State borders\nstate &lt;- read_state(showProgress = FALSE)\n# Metropolitan regions borders\nmetro &lt;- read_metro_area(showProgress = FALSE)\n# City border\nborder &lt;- read_municipality(code_cur, simplified = FALSE, showProgress = FALSE)\n# Neighborhoods\nnb &lt;- read_neighborhood(showProgress = FALSE)\n\n# Weighting areas \nwa &lt;- read_weighting_area(code_cur, showProgress = FALSE, simplified = FALSE)\n# Census tracts\nct &lt;- read_census_tract(code_cur, showProgress = FALSE, simplified = FALSE)\n\n\n\n\nThe map below shows where Paraná is in Brazil. Paraná is in the South region. As mentioned, there are 5 regions in Brazil: North, Northeast, Midwest, Southeast, and South.\n\n\nCode\n#&gt; Define a factor variable to signal the South region\nregion &lt;- region |&gt; \n  mutate(south = factor(if_else(name_region == \"Sul\", 1L, 0L)))\n\n#&gt; Get the centroid of the city of Curitiba\ncuritiba &lt;- st_centroid(border)\n\ntranslate &lt;- c(\n  \"Norte\" = \"North\",\n  \"Nordeste\" = \"Northeast\",\n  \"Centro Oeste\" = \"Midwest\",\n  \"Sul\" = \"South\",\n  \"Sudeste\" = \"Southeast\"\n  )\n\nregion_label &lt;- region %&gt;%\n  st_centroid() %&gt;%\n  mutate(label = stringr::str_replace_all(name_region, translate))\n\nggplot() +\n  geom_sf(data = region, aes(fill = name_region), color = \"white\", lwd = 0.15) +\n  geom_sf_label(\n    data = region_label,\n    aes(label = label),\n    family = \"Lato\",\n    label.padding = unit(0.15, \"lines\")\n    ) +\n  geom_sf(data = curitiba) +\n  scale_fill_met_d(\"Hokusai1\") +\n  labs(title = \"Regions in Brazil\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 26 states (equivalent to provinces in some countries). Distrito Federal is an importatn exception. Although it is equivalent to a state, it doesn’t contain cities but administrative regions (regiões administrativas). The capital of Brazil, Brasília, is located within the Distrito Federal.\n\n\nCode\nstate &lt;- state |&gt; \n  mutate(parana = factor(if_else(code_state == 41, 1L, 0L)))\n\nparana_label &lt;- state |&gt; \n  filter(code_state == 41) |&gt; \n  st_centroid()\n\nggplot() +\n  geom_sf(\n    data = state,\n    aes(fill = parana),\n    color = \"white\"\n    ) +\n  geom_sf(data = curitiba) +\n  geom_sf_label(\n    data = parana_label,\n    aes(label = name_state),\n    family = \"Lato\",\n    nudge_x = -1,\n    size = 3\n    ) +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  guides(fill = \"none\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown above, there exist several groupings of cities in Brazil, each with a distinct goal. To keep this post simple, I will focus only on metropolitan regions. As of 2018, the metropolitan region of Curitiba contained 29 cities.\n\n\nCode\nggplot() +\n  geom_sf(\n    data = cities_metro,\n    aes(fill = curitiba),\n    alpha = 0.85,\n    color = \"white\") +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  # scale_fill_manual(values = pal[c(2, 4)]) +\n  labs(title = \"Curitiba Metro Region\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nThe second map below highlights the name of each city.\n\n\nCode\ncities_metro &lt;- cities_metro %&gt;%\n  mutate(\n    name_label = stringr::str_wrap(name_muni, width = 8)\n  )\n\nggplot() +\n  geom_sf(data = cities_metro, aes(fill = name_muni)) +\n  geom_sf_label(\n    data = cities_metro,\n    aes(label = name_label),\n    size = 4) +\n  scale_fill_manual(values = MetBrewer::met.brewer(\"Hokusai1\", nrow(cities_metro))) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThe map below shows only the city of Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = border) +\n  labs(title = \"The City of Curitiba\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, there is an official definition of neighborhoods provided by IBGE for only some cities. Most notably, the city of São Paulo does not have an official definition of its neighborhoods. There are also districts and subdistricts but we ignore both of these for the purpose of this exposition.\nThere are 78 neighborhoods in Curitiba. The map below highlights all of them.\n\n\nCode\nnb &lt;- nb |&gt; \n  filter(code_muni == code_cur)\n\nggplot() +\n  geom_sf(data = nb, lwd = 0.15, aes(fill = name_neighborhood), color = \"white\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", nrow(nb))) +\n  labs(title = \"Neighborhoods of Curitiba\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncur_sg &lt;- qs::qread(\n  here::here(\"static/data/statistical_grid_cur.qs\")\n)\n\n\n\n\nThe smallest statistical subdivision available is the statistical grid. This is a varying size squared grid that offers a 200 x 200 m resolution in urban areas. It contains minimal information and is most useful as an intermediate shape in dasymetric interpolations. More specifically, it contains a population and household counts. This grid splits Curitiba into 11259 equally sized quadrants.\nThe map below shows the population counts at the statistical grid level. Since the area of all quadrants is the same, this data can be interpreted as the population density at the 200 x 200 m level.\n\n\nCode\nggplot(cur_sg) +\n  geom_sf(aes(fill = sqrt(POP), color = sqrt(POP))) +\n  scale_fill_distiller(palette = \"BuPu\", direction = 1) +\n  scale_color_distiller(palette = \"BuPu\", direction = 1) +\n  labs(title = \"Population count\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nCensus tracts are the strata used by IBGE in their decennial Census. The shape of each census tract usually respects administrative borders, land barriers, public spaces (parks, beaches, etc.), and follows the shape of the city blocks. Census tracts also exhibit relatively homogeneous socioeconomic and demographic characteristics. This makes census tracts a very useful statistical tool in regression analysis and classification.\nThe map below shows the 2395 census tracts in Curitiba.\n\n\nCode\nggplot(ct) +\n  geom_sf(lwd = 0.15) +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nIt is important to note that these shapes are not temporally consistent, meaning that they changed from 1991 to 2010 and then again from 2010 to 2020. Academics have devised methods to make the census tracts compatible over time and IBGE has also announced that they will facilitate backwards compatibility.\nAll census tracts contain basic information on population and households (age, race, sex, family configuration, household type, income group) and some information of the infrastructure of the region (garbage, sewage, trees, etc.). Their shape tries to create areas that share similar socioeconomic and demographic characteristics.\n\n\n\nThe weighting areas are an aggregation of census tracts for which there are much more detailed information. For instance, one can estimate the number of 3-bedroom apartments that are rented in a specific weighting area. Information on income, education, and housing quality are also available.\nThere are 55 weighting areas in Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = wa) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nTo illustrate how one can aggregate census tracts into weighting areas consider the map below that overlaps both. For this map I only take a subset of weighting areas near the city’s CBD.\n\n\nCode\n#&gt; Create a bounding box around the city's Central Business District (CBD)\ncbd_bbox &lt;- st_bbox(\n  c(ymin = -25.442283, ymax = -25.421747, xmin = -49.285610, xmax = -49.256937),\n  crs = 4326)\n#&gt; Create an identifier to filter after joins\ncbd_bbox &lt;- st_as_sfc(cbd_bbox)\ncbd_bbox &lt;- st_as_sf(cbd_bbox)\ncbd_bbox$gid &lt;- 1L\n\ncenter &lt;- st_coordinates(st_centroid(cbd_bbox))\n\ncbd_wa &lt;- wa |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nct_inside &lt;- ct %&gt;%\n  st_centroid() %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_join(cbd_wa) %&gt;%\n  filter(!is.na(code_weighting)) |&gt; \n  pull(code_tract) |&gt; \n  unique()\n\ncbd_ct &lt;- filter(ct, code_tract %in% ct_inside)\n\nm1 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\nm2 &lt;- ggplot() +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\nm3 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Overlap\") +\n  theme_vini\n\nlibrary(patchwork)\n\nm1 | m2 | m3\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many possible ways to bin the world into equal geometric shapes. Hexagons, squares, and triangles are the most commonly used. Below I show Uber’s H3 hexagons and a native st_make_grid() together with the official statistical grid shown previously.\n\n\nCode\n#&gt; Join statistical grid with CBD\ncbd_sg &lt;- cur_sg |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nhex_id &lt;- polyfill(cbd_bbox, res = 9)\nh3_grid &lt;- h3_to_geo_boundary_sf(hex_id)\n\ngrid_500 &lt;- cbd_bbox |&gt; \n  st_transform(crs = 32722) |&gt; \n  st_make_grid(500, square = FALSE, flat_topped = FALSE) |&gt; \n  st_as_sf() |&gt; \n  st_transform(crs = 4326)\n\nm1 &lt;- ggplot(cbd_sg) +\n  geom_sf() +\n  ggtitle(\"Statistical Grid\") +\n  theme_vini\n\nm2 &lt;- ggplot(h3_grid) +\n  geom_sf() +\n  ggtitle(\"Uber H3 (res. 9)\") +\n  theme_vini\n\nm3 &lt;- ggplot(grid_500) +\n  geom_sf() +\n  ggtitle(\"st_make_grid()\") +\n  theme_vini\n\nm1 | m2 | m3\n\n\n\n\n\n\n\n\n\nThe interactive map below illustrates the size of each hexagon in relation to the city. For simplicity I show only hexagons near the city center.\n\n\nCode\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(data = h3_grid, weight = 1) %&gt;%\n  addProviderTiles(providers$CartoDB)"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#administrative-divisions",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#administrative-divisions",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "To illustrate the above shapes consider the city of Curitiba. Curitiba is the capital city of the state of Paraná and the 8th most populous capital in Brazil and also the 8th smallest capital which helps keep the maps smaller. The state of Paraná is also relatively small.\nImporting these shapefiles into R is made very easy thanks to the excellent geobr package.\n\n\nCode\nlibrary(geobr)\nlibrary(censobr)\nlibrary(sidrar)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(MetBrewer)\nlibrary(h3)\nlibrary(areal)\n\ncode_cur &lt;- 4106902\n\n# Region borders\nregion &lt;- read_region(showProgress = FALSE)\n# State borders\nstate &lt;- read_state(showProgress = FALSE)\n# Metropolitan regions borders\nmetro &lt;- read_metro_area(showProgress = FALSE)\n# City border\nborder &lt;- read_municipality(code_cur, simplified = FALSE, showProgress = FALSE)\n# Neighborhoods\nnb &lt;- read_neighborhood(showProgress = FALSE)\n\n# Weighting areas \nwa &lt;- read_weighting_area(code_cur, showProgress = FALSE, simplified = FALSE)\n# Census tracts\nct &lt;- read_census_tract(code_cur, showProgress = FALSE, simplified = FALSE)\n\n\n\n\nThe map below shows where Paraná is in Brazil. Paraná is in the South region. As mentioned, there are 5 regions in Brazil: North, Northeast, Midwest, Southeast, and South.\n\n\nCode\n#&gt; Define a factor variable to signal the South region\nregion &lt;- region |&gt; \n  mutate(south = factor(if_else(name_region == \"Sul\", 1L, 0L)))\n\n#&gt; Get the centroid of the city of Curitiba\ncuritiba &lt;- st_centroid(border)\n\ntranslate &lt;- c(\n  \"Norte\" = \"North\",\n  \"Nordeste\" = \"Northeast\",\n  \"Centro Oeste\" = \"Midwest\",\n  \"Sul\" = \"South\",\n  \"Sudeste\" = \"Southeast\"\n  )\n\nregion_label &lt;- region %&gt;%\n  st_centroid() %&gt;%\n  mutate(label = stringr::str_replace_all(name_region, translate))\n\nggplot() +\n  geom_sf(data = region, aes(fill = name_region), color = \"white\", lwd = 0.15) +\n  geom_sf_label(\n    data = region_label,\n    aes(label = label),\n    family = \"Lato\",\n    label.padding = unit(0.15, \"lines\")\n    ) +\n  geom_sf(data = curitiba) +\n  scale_fill_met_d(\"Hokusai1\") +\n  labs(title = \"Regions in Brazil\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 26 states (equivalent to provinces in some countries). Distrito Federal is an importatn exception. Although it is equivalent to a state, it doesn’t contain cities but administrative regions (regiões administrativas). The capital of Brazil, Brasília, is located within the Distrito Federal.\n\n\nCode\nstate &lt;- state |&gt; \n  mutate(parana = factor(if_else(code_state == 41, 1L, 0L)))\n\nparana_label &lt;- state |&gt; \n  filter(code_state == 41) |&gt; \n  st_centroid()\n\nggplot() +\n  geom_sf(\n    data = state,\n    aes(fill = parana),\n    color = \"white\"\n    ) +\n  geom_sf(data = curitiba) +\n  geom_sf_label(\n    data = parana_label,\n    aes(label = name_state),\n    family = \"Lato\",\n    nudge_x = -1,\n    size = 3\n    ) +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  guides(fill = \"none\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown above, there exist several groupings of cities in Brazil, each with a distinct goal. To keep this post simple, I will focus only on metropolitan regions. As of 2018, the metropolitan region of Curitiba contained 29 cities.\n\n\nCode\nggplot() +\n  geom_sf(\n    data = cities_metro,\n    aes(fill = curitiba),\n    alpha = 0.85,\n    color = \"white\") +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  # scale_fill_manual(values = pal[c(2, 4)]) +\n  labs(title = \"Curitiba Metro Region\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nThe second map below highlights the name of each city.\n\n\nCode\ncities_metro &lt;- cities_metro %&gt;%\n  mutate(\n    name_label = stringr::str_wrap(name_muni, width = 8)\n  )\n\nggplot() +\n  geom_sf(data = cities_metro, aes(fill = name_muni)) +\n  geom_sf_label(\n    data = cities_metro,\n    aes(label = name_label),\n    size = 4) +\n  scale_fill_manual(values = MetBrewer::met.brewer(\"Hokusai1\", nrow(cities_metro))) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThe map below shows only the city of Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = border) +\n  labs(title = \"The City of Curitiba\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, there is an official definition of neighborhoods provided by IBGE for only some cities. Most notably, the city of São Paulo does not have an official definition of its neighborhoods. There are also districts and subdistricts but we ignore both of these for the purpose of this exposition.\nThere are 78 neighborhoods in Curitiba. The map below highlights all of them.\n\n\nCode\nnb &lt;- nb |&gt; \n  filter(code_muni == code_cur)\n\nggplot() +\n  geom_sf(data = nb, lwd = 0.15, aes(fill = name_neighborhood), color = \"white\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", nrow(nb))) +\n  labs(title = \"Neighborhoods of Curitiba\") +\n  theme_vini"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#statistical-subdivisions",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#statistical-subdivisions",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "Code\ncur_sg &lt;- qs::qread(\n  here::here(\"static/data/statistical_grid_cur.qs\")\n)\n\n\n\n\nThe smallest statistical subdivision available is the statistical grid. This is a varying size squared grid that offers a 200 x 200 m resolution in urban areas. It contains minimal information and is most useful as an intermediate shape in dasymetric interpolations. More specifically, it contains a population and household counts. This grid splits Curitiba into 11259 equally sized quadrants.\nThe map below shows the population counts at the statistical grid level. Since the area of all quadrants is the same, this data can be interpreted as the population density at the 200 x 200 m level.\n\n\nCode\nggplot(cur_sg) +\n  geom_sf(aes(fill = sqrt(POP), color = sqrt(POP))) +\n  scale_fill_distiller(palette = \"BuPu\", direction = 1) +\n  scale_color_distiller(palette = \"BuPu\", direction = 1) +\n  labs(title = \"Population count\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nCensus tracts are the strata used by IBGE in their decennial Census. The shape of each census tract usually respects administrative borders, land barriers, public spaces (parks, beaches, etc.), and follows the shape of the city blocks. Census tracts also exhibit relatively homogeneous socioeconomic and demographic characteristics. This makes census tracts a very useful statistical tool in regression analysis and classification.\nThe map below shows the 2395 census tracts in Curitiba.\n\n\nCode\nggplot(ct) +\n  geom_sf(lwd = 0.15) +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nIt is important to note that these shapes are not temporally consistent, meaning that they changed from 1991 to 2010 and then again from 2010 to 2020. Academics have devised methods to make the census tracts compatible over time and IBGE has also announced that they will facilitate backwards compatibility.\nAll census tracts contain basic information on population and households (age, race, sex, family configuration, household type, income group) and some information of the infrastructure of the region (garbage, sewage, trees, etc.). Their shape tries to create areas that share similar socioeconomic and demographic characteristics.\n\n\n\nThe weighting areas are an aggregation of census tracts for which there are much more detailed information. For instance, one can estimate the number of 3-bedroom apartments that are rented in a specific weighting area. Information on income, education, and housing quality are also available.\nThere are 55 weighting areas in Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = wa) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nTo illustrate how one can aggregate census tracts into weighting areas consider the map below that overlaps both. For this map I only take a subset of weighting areas near the city’s CBD.\n\n\nCode\n#&gt; Create a bounding box around the city's Central Business District (CBD)\ncbd_bbox &lt;- st_bbox(\n  c(ymin = -25.442283, ymax = -25.421747, xmin = -49.285610, xmax = -49.256937),\n  crs = 4326)\n#&gt; Create an identifier to filter after joins\ncbd_bbox &lt;- st_as_sfc(cbd_bbox)\ncbd_bbox &lt;- st_as_sf(cbd_bbox)\ncbd_bbox$gid &lt;- 1L\n\ncenter &lt;- st_coordinates(st_centroid(cbd_bbox))\n\ncbd_wa &lt;- wa |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nct_inside &lt;- ct %&gt;%\n  st_centroid() %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_join(cbd_wa) %&gt;%\n  filter(!is.na(code_weighting)) |&gt; \n  pull(code_tract) |&gt; \n  unique()\n\ncbd_ct &lt;- filter(ct, code_tract %in% ct_inside)\n\nm1 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\nm2 &lt;- ggplot() +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\nm3 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Overlap\") +\n  theme_vini\n\nlibrary(patchwork)\n\nm1 | m2 | m3"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#other-statistical-subdivisions",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#other-statistical-subdivisions",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "There are many possible ways to bin the world into equal geometric shapes. Hexagons, squares, and triangles are the most commonly used. Below I show Uber’s H3 hexagons and a native st_make_grid() together with the official statistical grid shown previously.\n\n\nCode\n#&gt; Join statistical grid with CBD\ncbd_sg &lt;- cur_sg |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nhex_id &lt;- polyfill(cbd_bbox, res = 9)\nh3_grid &lt;- h3_to_geo_boundary_sf(hex_id)\n\ngrid_500 &lt;- cbd_bbox |&gt; \n  st_transform(crs = 32722) |&gt; \n  st_make_grid(500, square = FALSE, flat_topped = FALSE) |&gt; \n  st_as_sf() |&gt; \n  st_transform(crs = 4326)\n\nm1 &lt;- ggplot(cbd_sg) +\n  geom_sf() +\n  ggtitle(\"Statistical Grid\") +\n  theme_vini\n\nm2 &lt;- ggplot(h3_grid) +\n  geom_sf() +\n  ggtitle(\"Uber H3 (res. 9)\") +\n  theme_vini\n\nm3 &lt;- ggplot(grid_500) +\n  geom_sf() +\n  ggtitle(\"st_make_grid()\") +\n  theme_vini\n\nm1 | m2 | m3\n\n\n\n\n\n\n\n\n\nThe interactive map below illustrates the size of each hexagon in relation to the city. For simplicity I show only hexagons near the city center.\n\n\nCode\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(data = h3_grid, weight = 1) %&gt;%\n  addProviderTiles(providers$CartoDB)"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#neighborhoods-by-population-density",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#neighborhoods-by-population-density",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "Neighborhoods by population density",
    "text": "Neighborhoods by population density\nWe will make a simple map that shows the population and population density of each neighborhood in Curitiba. The first step is to get the population data from SIDRA using the sidrar package.\n\n\nCode\n# Import population table from SIDRA\npop_nb &lt;- sidrar::get_sidra(\n  x = 1378,\n  variable = 93,\n  classific = \"c2\",\n  geo = \"Neighborhood\",\n  geo.filter = list(\"City\" = code_cur)\n)\n\n# Clean table and make it wide\npop_nb &lt;- pop_nb |&gt; \n  as_tibble() |&gt; \n  janitor::clean_names() |&gt; \n  select(code_neighborhood = bairro_codigo, sex = sexo, count = valor) |&gt; \n  mutate(\n    sex = str_replace(sex, \"Homens\", \"Male\"),\n    sex = str_replace(sex, \"Mulheres\", \"Female\")\n    ) |&gt; \n  pivot_wider(\n    id_cols = \"code_neighborhood\",\n    names_from = \"sex\",\n    values_from = \"count\"\n    )\n\n# Make neighborhood codes compatible\npop_nb &lt;- pop_nb |&gt; \n  mutate(code_neighborhood = str_c(\n    str_sub(code_neighborhood, 1, 7), \"05\", str_sub(code_neighborhood, 8, 10))\n    )\n# Join census table with shapefile\ncur_nb &lt;- left_join(nb, pop_nb, by = \"code_neighborhood\")\n\n# Calculate population density\ncur_nb &lt;- cur_nb %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  mutate(\n    area = st_area(.),\n    area = as.numeric(area) / 1e5,\n    pop_dens = Total / area,\n    pop_ntile = ntile(pop_dens, 5)\n    )\n# Convert back to 4326 for leaflet\ncur_nb &lt;- st_transform(cur_nb, crs = 4326)\n\n\nAfter cleaning and merging the datasets we can make an interactive map using leaflet.\n\n\nCode\n# Color palette and bins\nbins &lt;- quantile(cur_nb$pop_dens, probs = seq(0.2, 0.8, 0.2))\nbins &lt;- c(0, bins, max(cur_nb$pop_dens))\npal &lt;- colorBin(\n  palette = as.character(met.brewer(\"Hokusai2\", 5)),\n  domain = cur_nb$pop_dens,\n  bins = bins)\n\n# Labels\nlabels &lt;- sprintf(\n  \"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt; %s people &lt;br/&gt; %g people / ha&lt;sup&gt;2&lt;/sup&gt;\",\n  cur_nb$name_neighborhood,\n  format(cur_nb$Total, big.mark = \".\"),\n  round(cur_nb$pop_dens, 1)\n  )\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\n# Center of the map for zoom\ncenter &lt;- st_coordinates(st_centroid(border))\n\nleaflet(cur_nb) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    color = \"white\",\n    fillColor = ~pal(pop_dens),\n    fillOpacity = 0.8,\n    highlightOptions = highlightOptions(\n      color = \"gray20\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~pop_dens,\n    title = \"Densidade Pop.\",\n    position = \"bottomright\"\n  ) %&gt;%\n  addProviderTiles(providers$CartoDB) %&gt;%\n  setView(lng = center[1], lat = center[2], zoom = 11)"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#interpolation",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#interpolation",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "Interpolation",
    "text": "Interpolation\nWhen using different shapefiles it’s necessary to interpolate data from the “source” shapefile to the desired “target” shapefile. For instance, we might have the total number of apartments by census tract (source) but desire to have the same number by H3 hexagons (target). In this example, the number of apartments would be our target variable that should be interpolated from one shape to another.\nFor simplicity sake, I show how to make a simple areal interpolation from Curitiba’s census tracts to a hexagonal H3 grid.\n\nImport data\nI import census tract level socioeconomic data using censobr. I retrieve the total number of houses, apartments, and some information on housing ownership (i.e. rented, owned, etc.).\n\n\nCode\nlibrary(censobr)\n\nhh &lt;- read_tracts(dataset = \"Domicilio\", showProgress = FALSE)\n\ncur_domicilios &lt;- hh |&gt; \n  filter(code_muni == code_cur) |&gt; \n  select(code_tract, domicilio01_V002:domicilio01_V011) |&gt; \n  collect()\n\ncol_names &lt;- c(\n  \"hh_total\", \"hh_house\", \"hh_cndm\", \"hh_apt\",\n  \"hh_owned1\", \"hh_owned2\", \"hh_rented\", \"hh_cedido1\", \"hh_cedido2\", \"hh_other\"\n)\n\nnames(cur_domicilios)[-1] &lt;- col_names\n\ncur_census &lt;- left_join(ct, cur_domicilios, by = \"code_tract\")\n\n\nThe map below shows the total number of apartments in Curitiba by census tract.\n\n\nCode\nggplot(cur_census) +\n  geom_sf(aes(fill = sqrt(hh_apt), color = sqrt(hh_apt))) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nInterpolation\nTo interpolate we use the areal package. It requires the shapefile to be inputed in a planar projection.\n\n\nCode\nindex &lt;- h3::polyfill(border, res = 9)\nh3_grid &lt;- h3::h3_to_geo_boundary_sf(index)\n\nh3_grid &lt;- st_transform(h3_grid, crs = 31984)\ncur_census &lt;- st_transform(cur_census, crs = 31984)\n\n#&gt; Compute areal interpolation\ninterp &lt;- aw_interpolate(\n  h3_grid,\n  tid = h3_index,\n  source = cur_census,\n  sid = \"code_tract\",\n  weight = \"sum\",\n  extensive = \"hh_apt\"\n)\n\ninterp &lt;- st_transform(interp, crs = 4326)\n\n\n\n\nResults\nFinally, the panel below shows the result of the interpolation.\n\n\nCode\nm1 &lt;- ggplot(cur_census) +\n  geom_sf(aes(fill = sqrt(hh_apt), color = sqrt(hh_apt))) +\n  ggtitle(\"Census Tract\") +\n  scale_fill_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  scale_color_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  ggthemes::theme_map(base_family = \"Lato\")\n\nm2 &lt;- ggplot(h3_grid) +\n  ggtitle(\"H3 (res. 9)\") +\n  geom_sf(lwd = 0.05) +\n  theme_vini\n\nm3 &lt;- ggplot(interp) +\n  geom_sf(aes(fill = sqrt(hh_apt), color = sqrt(hh_apt))) +\n  scale_fill_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  scale_color_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  ggtitle(\"Interpolated\") +\n  ggthemes::theme_map(base_family = \"Lato\")\n\npanel &lt;- m1 | m2 | m3\n\npanel + plot_layout(guides = \"collect\") &\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = 0.5,\n    legend.key.size = unit(0.75, \"cm\"),\n    legend.key.width = unit(1, \"cm\"),\n    legend.text = element_text(size = 12),\n    text = element_text(family = \"Lato\")\n    )"
  },
  {
    "objectID": "posts/general-posts/2025-03-map-brazil-census-race/index.html",
    "href": "posts/general-posts/2025-03-map-brazil-census-race/index.html",
    "title": "A Distribuição Racial do Brasil",
    "section": "",
    "text": "A Distribuição Racial do Brasil em Regiões\n\n\nCode\n# Setup -------------------------------------------------------------------\n\nlibrary(geobr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(sf)\nlibrary(ggnewscale)\nlibrary(showtext)\n\nimport::from(janitor, clean_names)\nimport::from(readxl, read_excel)\nimport::from(sidrar, get_sidra)\nimport::from(stringi, stri_trans_general)\nimport::from(tidyr, separate_wider_delim)\n\nfont_add_google(\"Lato\", \"Lato\")\nshowtext_auto()\n\n# Data --------------------------------------------------------------------\n\n# Download data from SIDRA (Brazilian Institute of Geography and Statistics database)\n# Table 9605: Population by race/color\nsidra9605 &lt;- get_sidra(9605, variable = 93, geo = \"City\")\n\nurl &lt;- \"https://geoftp.ibge.gov.br/organizacao_do_territorio/divisao_regional/divisao_regional_do_brasil/divisao_regional_do_brasil_em_regioes_geograficas_2017/tabelas/regioes_geograficas_composicao_por_municipios_2017_20180911.xlsx\"\n\nnmfile &lt;- tempfile(fileext = \"xlsx\")\ndownload.file(url, nmfile)\ndim_inter &lt;- readxl::read_excel(nmfile)\n\ndim_inter &lt;- dim_inter |&gt; \n  rename(\n    code_muni = CD_GEOCODI,\n    code_intermediate = cod_rgint,\n    code_immediate = cod_rgi,\n    name_muni = nome_mun,\n    name_intermediate = nome_rgi\n  ) |&gt; \n  mutate(across(starts_with(\"code\"), as.numeric))\n\n# Helper function to standardize text: remove accents and convert to lowercase\nstr_simplify &lt;- function(x) {\n  y &lt;- stringi::stri_trans_general(x, \"latin-ascii\")\n  y &lt;- stringr::str_to_lower(y)\n  return(y)\n}\n\npop &lt;- sidra9605 |&gt; \n  as_tibble() |&gt; \n  janitor::clean_names() |&gt; \n  mutate(\n    code_muni = municipio_codigo,\n    code_region = str_sub(code_muni, 1, 1),\n    code_state = str_sub(code_muni, 1, 2),\n    race = str_simplify(cor_ou_raca),\n    race_label = cor_ou_raca,\n    pop = valor\n  ) |&gt; \n  mutate(across(starts_with(\"code\"), as.numeric))\n\npop &lt;- left_join(\n  pop,\n  select(dim_inter, code_muni, code_intermediate, code_immediate),\n  by = \"code_muni\"\n)\n\npop &lt;- pop |&gt; \n  # Split municipality name and state abbreviation\n  separate_wider_delim(\n    cols = municipio,\n    delim = \" - \",\n    names = c(\"name_muni\", \"abbrev_state\")\n  ) |&gt; \n  # Select and reorder relevant columns\n  select(starts_with(\"code\"), name_muni, abbrev_state, race, race_label, pop) |&gt; \n  # Remove totals and calculate proportions by municipality\n  filter(race != \"total\") |&gt; \n  mutate(prop = pop / sum(pop, na.rm = TRUE), .by = \"code_muni\")\n\n# Define grouping variables for different geographic levels (id variables)\npop_summary &lt;- list(\n  region = c(\"code_region\"),\n  state = c(\"code_state\"),\n  intermediate = c(\"code_intermediate\"),\n  immediate = c(\"code_immediate\"),\n  brasil = c(\"race\")\n)\n\n# Function to create summary tables\nsummarise_tables &lt;- function(x) {\n  pop |&gt; \n    # Sums population by id variables + race\n    summarise(\n      total = sum(pop, na.rm = TRUE),\n      .by = c(all_of(x), \"race\", \"race_label\")\n    ) |&gt; \n    # Computes population shares by id variables\n    mutate(\n      prop = total / sum(total),\n      .by = all_of(x)\n    )\n}\n\n# Create all summary tables at once\ntbls &lt;- lapply(pop_summary, summarise_tables)\n\n# Find the highest share within each intermediate region\n\ntop_inter &lt;- tbls$intermediate |&gt; \n  group_by(code_intermediate) |&gt; \n  slice_max(prop, n = 1) |&gt; \n  ungroup()\n\ngeo_inter_full &lt;- read_intermediate_region(showProgress = FALSE, simplified = FALSE)\ngeo_inter &lt;- read_intermediate_region(showProgress = FALSE)\n\ngeo_inter_full &lt;- left_join(geo_inter_full, top_inter, by = \"code_intermediate\")\ninter &lt;- left_join(geo_inter, top_inter, by = \"code_intermediate\")\n\n# Map ---------------------------------------------------------------------\n\nlegend_breaks &lt;- seq(0.5, 0.8, 0.1)\nlegend_labels &lt;- legend_breaks * 100\nlegend_limits &lt;- c(0.4, 0.85)\n\nbase_map &lt;- ggplot() +\n  geom_sf(\n    data = filter(inter, race != \"branca\"),\n    aes(fill = prop),\n    lwd = 0.1,\n    color = \"white\"\n  ) +\n  scale_fill_distiller(\n    name = \"% Pardos\",\n    palette = \"Purples\",\n    direction = 1,\n    breaks = legend_breaks,\n    labels = legend_labels,\n    limits = legend_limits\n  ) +\n  # Creates a secondary scale using ggnewscale::new_scale_fill\n  new_scale_fill() +\n  geom_sf(\n    data = filter(inter, race == \"branca\"),\n    aes(fill = prop),\n    lwd = 0.1,\n    color = \"white\"\n  ) +\n  scale_fill_distiller(\n    name = \"% Brancos\",\n    palette = \"Greens\",\n    direction = 1,\n    breaks = legend_breaks,\n    labels = legend_labels,\n    limits = legend_limits\n  )\n\n# Find the region with the highest share for each race\nsf_top &lt;- inter |&gt; \n  group_by(race) |&gt; \n  slice_max(prop, n = 1, na_rm = TRUE) |&gt; \n  ungroup()\n\n# Get the position of the centroids (for the arrow)\ncoords_centroid_top &lt;- sf_top |&gt; \n  st_centroid() |&gt; \n  st_coordinates()\n\n# Auxiliar data.frame to position the text labels\ndf_annotation &lt;- tibble(\n  x = c(-40, -65, -43, -44),\n  y = c(-27, -18, -32, 2),\n  label = c(\n    \"Regiões em verde\\nindicam que\\nbrancos são maioria\",\n    \"Regiões em roxo\\nindicam que\\npardos são maioria\",\n    \"Santa Cruz é a RI\\ncom maior percentual\\nde brancos (82%)\",\n    \"Parintins é a RI\\ncom maior percentual\\nde pardos (80%)\"\n  )\n)\n\n# Add text labels to plot\nmap_annotations &lt;- base_map +\n  geom_label(\n    data = df_annotation,\n    aes(x = x, y = y, label = label),\n    family = \"Lato\",\n    size = c(4, 4, 3, 3),\n    fill = \"white\",      # background color\n    alpha = 0.8,         # transparency\n    label.padding = unit(0.2, \"lines\"),  # padding around text\n    label.size = 0.1     # border thickness\n  ) +\n  # Top %share -- Santa Cruz - Lajeado  | Parintins-- dark border \n  geom_sf(\n    data = filter(inter, code_intermediate %in% c(1304, 4308)),\n    fill = NA,\n    color = \"black\",\n    lwd = 0.15\n  ) +\n  # arrow/curve segment: Santa Cruz - Lajeado\n  geom_curve(\n    data = data.frame(x = coords_centroid_top[1, 1], y = coords_centroid_top[1, 2]),\n    aes(x = x, y = y + 0.25, xend = x + 5, yend = y - 2.25),\n    linewidth = 0.3,\n    angle = 45,\n    alpha = 0.8,\n    arrow = arrow(length = unit(2.5, \"pt\"))\n  ) +\n  # arrow/curve segment: Parintins\n  geom_curve(\n    data = data.frame(x = coords_centroid_top[2, 1], y = coords_centroid_top[2, 2]),\n    aes(x = x, y = y, xend = x + 10, yend = y + 5),\n    linewidth = 0.3,\n    angle = 45,\n    curvature = -0.1,\n    alpha = 0.8,\n    arrow = arrow(length = unit(2.5, \"pt\"))\n  ) +\n  # Remove excess white space from map\n  coord_sf(xlim = c(-72.2, -35.5))\n\n# Add title and thematic elements\nfinal_map &lt;- map_annotations +\n  labs(\n    title = \"A Divisão Racial do Brasil\",\n    subtitle = \"Percentual do grupo racial majoritário por Região Intermediária* (RI). Em todos as RIs, ou brancos ou pardos são a maioria.\\nOs dados são do Censo Demográfico de 2022 do IBGE e mostram um padrão norte-sul no país.\\nExceções ao padrão incluem a região do Rio de Janeiro, no sudeste, e de Caicó, no nordeste.\",\n    caption = \"Fonte: IBGE (Censo Demográfico 2022). @viniciusoike\\n(*) Regiões intermediárias agrupam municípios que compartilham relações econômicas e sociais em torno de um mesmo centro urbano principal.\") +\n  ggthemes::theme_map(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(\n      hjust = 0,\n      size = 22\n    ),\n    plot.subtitle = element_text(\n      hjust = 0,\n      size = 10\n    ),\n    plot.caption = element_text(hjust = 0),\n    legend.position =  \"inside\",\n    legend.position.inside = c(0.01, 0.01),\n    legend.box = \"horizontal\",\n    legend.key.size = unit(1, \"cm\"),\n    legend.text = element_text(size = 10),\n    legend.title = element_text(size = 12),\n    plot.background = element_rect(fill = \"#F6EEE3\", color = \"#F6EEE3\"),\n    legend.background = element_rect(fill = \"#F6EEE3\", color = \"#F6EEE3\")\n  )\n\nggsave(\n  here::here(\"static/images/censo_mapa_raca.svg\"),\n  final_map,\n  width = 8,\n  height = 9\n)\n\n\n\nO mapa abaixo mostra o grupo racial predominante em cada região do Brasil. A divisão escolhida tem o objetivo de facilitar a leitura dos dados. No agregado, 45,3% da população do Brasil é parda, 43,5% é branca, 10,2% é preta, 0,6% é indígena e 0,4% é amarela.\nTecnicamente, uma região intermediária é uma agregação de regiões imediatas, que são uma divisão territorial do Brasil que agrupa municípios com base em centros urbanos. Na prática, uma região intermediária é como uma região metropolitana, um grupo de cidades com laços econômicos significativos.\n\n\n\n\n\n\n\nDados: IBGE, Censo (2022)\nTipografia: Lato\nPaleta: Greens e Purple (MetBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-03-carry-over/index.html",
    "href": "posts/general-posts/2024-03-carry-over/index.html",
    "title": "Ano de 2024 começa mais difícil",
    "section": "",
    "text": "O carry-over estatístico de 2024 está no nível mais baixo desde a Crise Econômica de 2015-17. Isto aponta um desafio ainda maior para as metas de crescimento.\n\n\n\n\n\n\n\n\n\nOlhando os dados históricos, é o pior carry-over desde 1995 quando se exclui os anos de recessão.\n\n\n\n\n\n\n\n\nAno\nCarry-over\n\n\n\n\n1996 / 1997\n1.45\n\n\n1997 / 1998\n1.45\n\n\n1998 / 1999\n−0.40\n\n\n1999 / 2000\n1.18\n\n\n2000 / 2001\n1.87\n\n\n2001 / 2002\n−0.67\n\n\n2002 / 2003\n1.45\n\n\n2003 / 2004\n0.99\n\n\n2004 / 2005\n1.89\n\n\n2005 / 2006\n1.01\n\n\n2006 / 2007\n1.87\n\n\n2007 / 2008\n2.04\n\n\n2008 / 2009\n−1.60\n\n\n2009 / 2010\n3.49\n\n\n2010 / 2011\n1.78\n\n\n2011 / 2012\n0.84\n\n\n2012 / 2013\n1.34\n\n\n2013 / 2014\n0.71\n\n\n2014 / 2015\n−0.05\n\n\n2015 / 2016\n−2.09\n\n\n2016 / 2017\n−0.25\n\n\n2017 / 2018\n0.81\n\n\n2018 / 2019\n0.29\n\n\n2019 / 2020\n0.74\n\n\n2020 / 2021\n4.20\n\n\n2021 / 2022\n0.77\n\n\n2022 / 2023\n1.09\n\n\n2023 / 2024\n0.41\n\n\n\n\n\n\n\n\n\nO carry-over ou carry-over effect mensura qual seria a variação anual do PIB caso a economia estagnasse. No caso acima, toma-se o valor do PIB no último trimestre do ano e faz-se uma simulação de crescimento zero. O PIB no último trimstre de 2023 registrou o valor (indexado) de 183,88. Supondo que o país não crescesse em 2024, o valor do PIB no último trimestre de 2024 seria também de 183,88. Comparando este valor com o PIB médio de 2023 chega-se no valor de 0,2% apontado no gráfico acima.\nO carry-over é uma consequência direta da maneira como o PIB ou mais especificamente, o crescimento do PIB, costuma ser mensurado. Tipicamente, as agências de estatísticas reportam uma estimativa trimestral do PIB, que é dessazonalizada e comparada com o crescimento dos últimos quatro trimestres. Isto é, cria-se uma espeície de “ano móvel” para verificar a direção da economia. Isto significa que o crescimento anual do PIB depende da dinâmica dos quatro trimestres do ano anterior."
  },
  {
    "objectID": "posts/general-posts/2024-03-carry-over/index.html#o-que-é-o-carry-over",
    "href": "posts/general-posts/2024-03-carry-over/index.html#o-que-é-o-carry-over",
    "title": "Ano de 2024 começa mais difícil",
    "section": "",
    "text": "O carry-over ou carry-over effect mensura qual seria a variação anual do PIB caso a economia estagnasse. No caso acima, toma-se o valor do PIB no último trimestre do ano e faz-se uma simulação de crescimento zero. O PIB no último trimstre de 2023 registrou o valor (indexado) de 183,88. Supondo que o país não crescesse em 2024, o valor do PIB no último trimestre de 2024 seria também de 183,88. Comparando este valor com o PIB médio de 2023 chega-se no valor de 0,2% apontado no gráfico acima.\nO carry-over é uma consequência direta da maneira como o PIB ou mais especificamente, o crescimento do PIB, costuma ser mensurado. Tipicamente, as agências de estatísticas reportam uma estimativa trimestral do PIB, que é dessazonalizada e comparada com o crescimento dos últimos quatro trimestres. Isto é, cria-se uma espeície de “ano móvel” para verificar a direção da economia. Isto significa que o crescimento anual do PIB depende da dinâmica dos quatro trimestres do ano anterior."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-sp-carros/index.html",
    "href": "posts/general-posts/2023-11-wz-sp-carros/index.html",
    "title": "Carros em São Paulo",
    "section": "",
    "text": "Posse de carros em São Paulo\nA posse de automóveis em São Paulo, grosso modo, é positivamente correlacionada com a renda familiar. O mapa abaixo mostra as zonas da cidade com maiores e menores taxas de automóveis (número de carros por domicílio). Bairros centrais como Pacaembu, Alto de Pinheiros, Morumbi e Jardim Europa têm as taxas mais elevadas. Bairros periféricos como Morro do Índio, Cocaia e Bororé têm as taxas mais baixas. Vale notar que o Centro Antigo da cidade (República, Santa Ifigênia, etc.) também têm bairros com taxas bastante baixas. Na maior parte da cidade, as Zonas têm entre 0,4 e 0,8 carros por domicílio.\n\n\n\n\n\n\n\n\n\n\nDados: Metrô (Pesquisa Origem e Destino, 2017)\nTipografia: Helvetica Neue\nPaleta: PuBuGn (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-sp-renda/index.html",
    "href": "posts/general-posts/2024-02-wz-sp-renda/index.html",
    "title": "Distribuição de Renda em São Paulo",
    "section": "",
    "text": "Renda Familiar em São Paulo\nO mapa abaixo mostra o rendimento domiciliar em grupos de decis em São Paulo. Divide-se toda a população, do mais pobre ao mais rico, em dez grupos de igual tamanho; assim, os hexágonos em azul estão no top 10% da distribuição de renda (10% mais ricos); já os hexágonos em vermelho estão no bottom 10% da distribuição (10% mais pobres).\nO grid hexagonal segue o padrão H3, em resolução 9. Os dados de renda provém do Censo Demográfico do IBGE a nível setor censitário. Usa-se uma técnica de interpolação espacial para converter os dados para o padrão hexagonal.\nApesar dos dados brutos do Censo estarem defasados, é improvável que a sua distribuição espacial tenha se alterado significativamente na última década. Grosso modo, as maiores rendas se concentram no Centro Expandido e no Quadrante Sudoeste da cidade. As duas exceções são a região de Santana-Tucuruvi e o eixo Tatuapé - Jardim Anália Franco.\n\n\n\n\n\n\n\n\n\n\nDados: IPEA (Acesso a Oportunidades) acessado via aopdata\nTipografia: Lato\nPaleta: Spectral (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "Tende-se a pensar que a data de nascimento de um indivíduo é algo completamente aleatório. Afinal, ninguém escolhe precisamente quando vai nascer. Alguns atribuem significado profundo à data de nascimento: a depender do horário, dia e mês a pessoa terá tendências a ser mais de uma forma do que outra. Nascer no mês impróprio pode ser um mal negócio para a vida toda.\nJá na cultura popular é comum especular que os nascimentos seguem alguns ciclos da vida. As estações do ano regulam as safras de comida, a temperatura, a disposição para sair de casa e, muito acreditam, o desejo sexual. Os feriados, as festividades, o carnaval, as vitórias no campeonato de futebol, tudo isso - imagina-se - tem algum efeito sobre a natalidade no país, nove meses no futuro.\nNo campo da economia, pode-se especular que os ciclos de crescimento econômico e, sobretudo, os ciclos de desemprego devem ter algum efeito sobre os nascimentos.\n\n\nUsando dados do IBGE, mais especificamente das Estatísticas do Registro Civil, pode-se calcular o número total de nascimentos em cada mês desde 2003. Grosso modo, nos últimos vinte anos, março, abril e maio foram os três meses com maior número de nascimentos (27,23%). Já os meses do final do ano, outubro, novembro e dezembro foram os meses com menor número de nascimentos (22,95%).\n\n\n\n\n\n\n\n\n\nIsto significa que o maior número de concepções ocorreu no inverno, nos meses de julho a agosto. Já os meses quentes de janeiro a março tiveram os menores números de concepções. Este fato vai diretamente contra a popular percepção de que épocas quentes favorecem o número de nascimentos.\nOlhando para as tendências de médio e longo prazo, vê-se que o número de nascimentos no Brasil permaneceu relativamente estável entre 2000 e 2015, variando de 230 mil a 242 mil nascimentos por mês. A quebra na série coincide com a recessão de 2015-17, período de alta inflação e desemprego recorde: de fato, a série sai do seu pico, acima de 265 mil nascimentos para o seu ponto mais baixo, abaixo de 200 mil nascimentos, neste período.\nO fim da recessão não significou, contudo, um retorno à antiga tendência. Desde 2018-19, o número de nascidos vivos no Brasil segue em tendência de queda.\n\n\n\n\n\n\n\n\n\n\n\n\nA tendência geral, observada no Brasil, repete-se em quase todas as grandes regiões. Apenas a região Norte parece escapar da tendência de queda, ainda que os valores correntes estejam levemente abaixo dos valores observados em 2013-14.\n\n\n\n\n\n\n\n\n\nVisualmente, o padrão sazonal dos nascimentos nas regiões é muito similar ao padrão geral brasileiro. Novamente, a região norte é uma exceção, já que a proporção de nascimentos em cada mês é muito homogênea.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE, afinal, há sazonalidade nos nascimentos no Brasil? Visualmente, parece haver fortes indícios de sazonalidade. Uma maneira simples de simultaneamente testar e mensurar o efeito sazonal é via uma regressão linear. Supondo um modelo simples da forma1:\n\\[\nB_{t} = T_{t}S_{t}E_{t}\n\\]\nonde \\(B_{t}\\) é o número ajustado de nascimentos mensais2, \\(T_{t}\\) é o termo de tendência, \\(S_{t}\\) é o termo sazonal e \\(E_{t}\\) é o termo de resíduo. Aplicando logaritmo natural, temos um modelo aditivo. Para modelar a tendência vamos utilizar o mesmo filtro linear utilizado nos gráficos acima, isto é, uma média móvel 2x12 centrada3. Esta estimativa é descontada da série original para chegar num valor sem tendência. Por fim, vamos supor dummies sazonais da forma:\n\\[\nb_{t} = \\alpha_{0} + \\sum_{i = 2}^{12}\\beta_{i}\\delta_{i} + u_{t}\n\\]\nonde \\(b_{t}\\) agora é o logartimo natural da série de nascimentos livre de tendência. O parâmetro \\(\\alpha_{0}\\) é uma constante e \\(\\delta_{i}\\) é uma variável binária que indica com valor unitário se a observação pertence ao mês \\(i\\).\nA tabela abaixo mostra o resultado da regressão. Assim, como se viu nos gráficos, os três meses do final do ano tem um efeito negativo enquanto os meses de março a maio têm um efeito positivo sobre o número de nascimentos. Além destes, fevereiro e junho aparecem com sinal positivo, mas tem um efeito menor. Agosto também tem um efeito negativo, porém com magnitude inferior aos meses do final do ano.\n\n\nCode\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(gtExtras)\n\ntbl_brazil &lt;- tbl_brazil |&gt; \n  mutate(\n    days = lubridate::days_in_month(date),\n    births_adjusted = total * (365/12) / days\n    )\n\n\nnasc &lt;- ts(log(tbl_brazil$births_adjusted), start = c(2003, 1), frequency = 12)\n\nmm &lt;- stats::filter(nasc, filter = rep(1/12, 12), method = \"convolution\")\nmm &lt;- stats::filter(mm, filter = c(1/2, 1/2), method = \"convolution\")\n\nnasc_mm &lt;- nasc - mm\n\nfit_lm &lt;- tslm(nasc_mm ~ season)\n\ntab_reg &lt;- \n  tbl_regression(\n    fit_lm,\n    estimate_fun = ~style_sigfig(.x, digits = 3)\n    ) %&gt;%\n  bold_labels() %&gt;%\n  bold_p() %&gt;%\n  as_gt() %&gt;%\n  gt_theme_538()\n\ntab_reg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nseason\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n0.048\n\n\n&lt;0.001\n\n\n    3\n0.089\n\n\n&lt;0.001\n\n\n    4\n0.082\n\n\n&lt;0.001\n\n\n    5\n0.073\n\n\n&lt;0.001\n\n\n    6\n0.037\n\n\n&lt;0.001\n\n\n    7\n0.002\n\n\n0.7\n\n\n    8\n-0.031\n\n\n&lt;0.001\n\n\n    9\n-0.002\n\n\n0.7\n\n\n    10\n-0.069\n\n\n&lt;0.001\n\n\n    11\n-0.095\n\n\n&lt;0.001\n\n\n    12\n-0.101\n\n\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nVisualmente, o resultado da regressão acima pode ser visto no painel abaixo. Note que o padrão sazonal é constante: começa o ano subindo, atinge um pico e aí começa a cair.\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra a decomposição completa da série de nascimentos.\n\n\n\n\n\n\n\n\n\nUm exercício semelhante poderia ser feito analisando cada estado individualmente. Os dados de nascimentos por UF funcionam como um painel longitudinal e pode-se fazer um regressão com efeitos fixos. Por completude, faço este exercício, mas como se vê no gráfico final, o padrão sazonal é muito similar. Em verdade, os efeitos de UF parecem ter ligeiramente intensificado o efeito sazonal."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#o-mês-de-nascimento",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#o-mês-de-nascimento",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "Usando dados do IBGE, mais especificamente das Estatísticas do Registro Civil, pode-se calcular o número total de nascimentos em cada mês desde 2003. Grosso modo, nos últimos vinte anos, março, abril e maio foram os três meses com maior número de nascimentos (27,23%). Já os meses do final do ano, outubro, novembro e dezembro foram os meses com menor número de nascimentos (22,95%).\n\n\n\n\n\n\n\n\n\nIsto significa que o maior número de concepções ocorreu no inverno, nos meses de julho a agosto. Já os meses quentes de janeiro a março tiveram os menores números de concepções. Este fato vai diretamente contra a popular percepção de que épocas quentes favorecem o número de nascimentos.\nOlhando para as tendências de médio e longo prazo, vê-se que o número de nascimentos no Brasil permaneceu relativamente estável entre 2000 e 2015, variando de 230 mil a 242 mil nascimentos por mês. A quebra na série coincide com a recessão de 2015-17, período de alta inflação e desemprego recorde: de fato, a série sai do seu pico, acima de 265 mil nascimentos para o seu ponto mais baixo, abaixo de 200 mil nascimentos, neste período.\nO fim da recessão não significou, contudo, um retorno à antiga tendência. Desde 2018-19, o número de nascidos vivos no Brasil segue em tendência de queda."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#regiões",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#regiões",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "A tendência geral, observada no Brasil, repete-se em quase todas as grandes regiões. Apenas a região Norte parece escapar da tendência de queda, ainda que os valores correntes estejam levemente abaixo dos valores observados em 2013-14.\n\n\n\n\n\n\n\n\n\nVisualmente, o padrão sazonal dos nascimentos nas regiões é muito similar ao padrão geral brasileiro. Novamente, a região norte é uma exceção, já que a proporção de nascimentos em cada mês é muito homogênea."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#sazonalidade",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#sazonalidade",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "E, afinal, há sazonalidade nos nascimentos no Brasil? Visualmente, parece haver fortes indícios de sazonalidade. Uma maneira simples de simultaneamente testar e mensurar o efeito sazonal é via uma regressão linear. Supondo um modelo simples da forma1:\n\\[\nB_{t} = T_{t}S_{t}E_{t}\n\\]\nonde \\(B_{t}\\) é o número ajustado de nascimentos mensais2, \\(T_{t}\\) é o termo de tendência, \\(S_{t}\\) é o termo sazonal e \\(E_{t}\\) é o termo de resíduo. Aplicando logaritmo natural, temos um modelo aditivo. Para modelar a tendência vamos utilizar o mesmo filtro linear utilizado nos gráficos acima, isto é, uma média móvel 2x12 centrada3. Esta estimativa é descontada da série original para chegar num valor sem tendência. Por fim, vamos supor dummies sazonais da forma:\n\\[\nb_{t} = \\alpha_{0} + \\sum_{i = 2}^{12}\\beta_{i}\\delta_{i} + u_{t}\n\\]\nonde \\(b_{t}\\) agora é o logartimo natural da série de nascimentos livre de tendência. O parâmetro \\(\\alpha_{0}\\) é uma constante e \\(\\delta_{i}\\) é uma variável binária que indica com valor unitário se a observação pertence ao mês \\(i\\).\nA tabela abaixo mostra o resultado da regressão. Assim, como se viu nos gráficos, os três meses do final do ano tem um efeito negativo enquanto os meses de março a maio têm um efeito positivo sobre o número de nascimentos. Além destes, fevereiro e junho aparecem com sinal positivo, mas tem um efeito menor. Agosto também tem um efeito negativo, porém com magnitude inferior aos meses do final do ano.\n\n\nCode\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(gtExtras)\n\ntbl_brazil &lt;- tbl_brazil |&gt; \n  mutate(\n    days = lubridate::days_in_month(date),\n    births_adjusted = total * (365/12) / days\n    )\n\n\nnasc &lt;- ts(log(tbl_brazil$births_adjusted), start = c(2003, 1), frequency = 12)\n\nmm &lt;- stats::filter(nasc, filter = rep(1/12, 12), method = \"convolution\")\nmm &lt;- stats::filter(mm, filter = c(1/2, 1/2), method = \"convolution\")\n\nnasc_mm &lt;- nasc - mm\n\nfit_lm &lt;- tslm(nasc_mm ~ season)\n\ntab_reg &lt;- \n  tbl_regression(\n    fit_lm,\n    estimate_fun = ~style_sigfig(.x, digits = 3)\n    ) %&gt;%\n  bold_labels() %&gt;%\n  bold_p() %&gt;%\n  as_gt() %&gt;%\n  gt_theme_538()\n\ntab_reg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nseason\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n0.048\n\n\n&lt;0.001\n\n\n    3\n0.089\n\n\n&lt;0.001\n\n\n    4\n0.082\n\n\n&lt;0.001\n\n\n    5\n0.073\n\n\n&lt;0.001\n\n\n    6\n0.037\n\n\n&lt;0.001\n\n\n    7\n0.002\n\n\n0.7\n\n\n    8\n-0.031\n\n\n&lt;0.001\n\n\n    9\n-0.002\n\n\n0.7\n\n\n    10\n-0.069\n\n\n&lt;0.001\n\n\n    11\n-0.095\n\n\n&lt;0.001\n\n\n    12\n-0.101\n\n\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nVisualmente, o resultado da regressão acima pode ser visto no painel abaixo. Note que o padrão sazonal é constante: começa o ano subindo, atinge um pico e aí começa a cair.\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra a decomposição completa da série de nascimentos.\n\n\n\n\n\n\n\n\n\nUm exercício semelhante poderia ser feito analisando cada estado individualmente. Os dados de nascimentos por UF funcionam como um painel longitudinal e pode-se fazer um regressão com efeitos fixos. Por completude, faço este exercício, mas como se vê no gráfico final, o padrão sazonal é muito similar. Em verdade, os efeitos de UF parecem ter ligeiramente intensificado o efeito sazonal."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#footnotes",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#footnotes",
    "title": "Nascimentos no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEste é um modelo bastente convencional em séries de tempo, também conhecido, modelo “clássico” ou “decomposição clássica”. Veja, por exemplo Morettin, P & Toloi, C. Análise de Séries Temporais (2006).↩︎\nBecker (1989) sugere corrigir o número de nascimentos pelo número de dias no mês da seguinte maneira: \\(\\tilde{x}_{t} = x_{t}\\frac{365}{12z}\\) onde \\(z\\) é o número de dias do mês.↩︎\nIsto é equivalente a fazer primeiro uma média móvel de dozes meses e depois uma média móvel de dois meses. Na prática, todos os termos têm peso 1/12, exceto pelo primeiro e último que têm peso 1/24.↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "",
    "text": "Este é um repost antigo que fiz ainda na época do mestrado em economia. Apesar de intuitivo o código dos loops abaixo pode ser muito ineficiente. De maneira geral, for-loops são melhores do que loops feitos com repeat; melhor ainda é montar funções e usar parallel::mclapply ou furrr::future_map. Vale notar que é sempre bom pré-alocar (ou pré-definir) vetores antes de um for-loop\n\n# Bom\nx &lt;- vector(\"numeric\", 1000)\n# Ruim\nx &lt;- c()"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html#lei-dos-grande-números",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html#lei-dos-grande-números",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "Lei dos Grande Números",
    "text": "Lei dos Grande Números\nA Lei dos Grandes Números (LGN) é um resultado assintótico bastante utilizado em econometria. Numa definição informal, a LGN nos diz que uma média amostral converge para para a média verdadeira dos dados. Isto é, se temos uma sequência de variáveis aleatórias \\(x_{1}, x_{2}, \\dots , x_{n}\\) independentes e identicamente distribuídas:\n\\[\\begin{equation}\n  \\frac{1}{n}\\sum_{i = 1}^{n} x_{1} \\to \\mathbb{E}(x)\n\\end{equation}\\]\nVamos criar uma amostra de cinco observações a partir de uma distribuição uniforme e tirar a média destas observações. Lembre-se que este distribuição depende de dois parâmetros, digamos \\(a\\) e \\(b\\). A esperança de uma uniforme é simplesmente \\(\\frac{b-a}{2}\\). Podemos repetir este processo 1000 vezes e fazer um histograma dos resultados.\nNo código abaixo cria-se um vetor \\(x = (x_{1}, x_{2}, \\dots)\\) genérico para armazenar valoes. O loop vai inserindo neste vetor a média de uma amostra de cinco observações a partir de uma distribuição uniforme com \\(a = 0\\) e \\(b = 5\\). A cada iteração do loop uma nova amostra é gerada e sua média é salva no vetor \\(x\\) na posição \\(x_{i}\\). Depois de gerar estes valores faz-se um histograma deles.\n\nx &lt;- vector(\"numeric\", length = 1000) # cria um vetor para armazenar os valores\n\nfor(i in 1:1000){ #loop para gerar os valores\n  # computa a media de uma amostra com 5 observacoes\n  x[i] &lt;- mean(runif(n = 5, min = 0, max = 5)) \n\n}\n# Histograma\nhist(x, main = \"Histograma da media das amostras para n = 5\", xlab = \"\")\n# Linha vertical\nabline(v = 2.5, col = \"red\")\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para diferentes tamanhos de amostra. O código abaixo simplesmente faz um loop do código acima; o loop de fora varia n.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\n\nfor (n in c(10, 50, 100, 200)) { # loop para os diferentes tamanhos de amostra\n  x &lt;- vector(\"numeric\", 1000)\n  for (i in 1:1000) { # mesmo loop que o anterior\n    \n    x[i] &lt;- mean(runif(n, 0, 5))\n    \n  }\n    # Plotando o histograma\n    hist(x, main = paste(\"Histograma para n = \", n, sep = \"\"),\n       xlab = \"\")\n    abline(v = 2.5, col = \"red\")\n}\n\n\n\n\n\n\n\n\nNote que as escalas dos gráficos são diferentes. Como era de se esperar, à medida que cresce o tamanho da amostra os valores vão se acumulando em torno da média verdadeira.\nOutra maneira de visualizar a LGN é fazendo o seguinte experimento: sorteie um número a partir de uma distribuição particular e grave seu valor. Agora sorteie dois números a partir da mesma distribuição, tire a média dos valores e grave o resultado. Agora faça o mesmo com três números, quatro números e assim por diante. O código abaixo faz isto para uma distribuição normal padrão.\n\nx &lt;- vector(\"numeric\", 200)\nfor (n in 1:200){\n\n    x[n] &lt;- mean(rnorm(n))\n\n}\n\nplot(x, type = \"l\", xlab = \"\", ylab = \"\")\nabline(h = 0, col = \"red\", lty = 2)"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html#teorema-central-do-limite",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html#teorema-central-do-limite",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "Teorema Central do Limite",
    "text": "Teorema Central do Limite\nO segundo resultado importante que se usa em econometria é o Teorema Central do Limite (TCL). Existem algumas variantes do TCL que usam diferentes hipóteses, mas, novamente sendo informal, o TCL diz que se tivermos uma amostra qualquer \\(x_{1}, x_{2}, \\dots , x_{n}\\), então \\(\\sqrt{n}\\frac{\\overline{x} - \\mu}{\\sigma}\\) segue uma distribuição normal padrão, onde \\(\\overline{x}\\) é a média amostral, \\(\\mathbb{E}(x) = \\mu\\) e \\(\\text{Var}(x) = \\sigma^{2}\\). Para visualizar este resultado podemos novamente fazer o experimento usando a distribuição uniforme.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 200)){ # loop para os diferentes tamanhos de amostra\n  for (i in 1:1000){ # mesmo loop que o anterior\n\n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n\n  x_normalizado &lt;- sqrt(n)*(x - 5) / sqrt(100/12) # transforma a variavel\n  # plota o histograma usando a densidade da frequencia de cada observacao\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\", breaks = 20)\n  # superimpoe uma curva normal padrao \n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)  \n}\n\n\n\n\n\n\n\n\nNote que este resultado vale para qualquer sequência de variáveis i.i.d (independentes e identicamente distribuídas). Considere, por exemplo, uma sequência de variáveis aleatórias independentes que segue uma distribuição beta.\n\\[\\begin{equation}\nf(x) = \\frac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\end{equation}\\]\nonde \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\). A esperança da distribuição beta é dada por\n\\[\\begin{equation}\n    \\mathbb{E}(x) = \\frac{\\alpha}{\\alpha + \\beta}\n\\end{equation}\\]\nUma distribuição beta depende de dois parâmetros. Usando a função dbeta podemos simular algumas pdfs.\n\n\n\n\n\n\n\n\n\nNote que uma implicação do TCL é que, se \\(x_{i}\\) for i.i.d. com esperança igual a \\(\\mu\\), então\n\\[\\begin{equation}\n    \\sqrt{N} \\left (  \\frac{1}{n}\\sum_{i = 1}^{N}x_{i} - \\mu \\right ) \\to \\text{N}(0, \\sigma^{2})\n\\end{equation}\\]\nIsto é, não precisamos saber qual a forma da variância da distribuição para aplicar o TCL. Os loops abaixo são essencialmente idênticos aos anteriores: a diferença é que desta vez os histogramas vão representar variáveis normais de média zero com variância \\(\\sigma^{2}\\), que é aproximadamente igual à variância da distribuição beta.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # loop para os diferentes tamanhos de amostra\n \n  for (i in 1:1000){ # mesmo loop que o anterior\n\n    x[i] &lt;- mean(rbeta(n, 2, 5))\n    \n  }\n\n  x_normalizado &lt;- sqrt(n)*(x - 2/7) # transforma a variavel\n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\") # plota o histograma usando a densidade da frequencia de cada observacao\n}\n\n\n\n\n\n\n\n\n\nDois casos anômalos\nO TCL nos diz: \\[\\begin{equation}\n    \\sqrt{n} \\left ( \\frac{\\overline{x} - \\mu}{\\sigma} \\right ) \\to N(0,1)\n\\end{equation}\\] O termo \\(\\sqrt{n}\\) é essencial para garantir este resultado. Qualquer transformação maior do que \\(\\sqrt{n}\\) faz a variância crescer indefinidamente; qualquer transformação menor do que \\(\\sqrt{n}\\) faz a variância diminuir indefinidamente, isto é, faz a distribuição colapsar num único ponto. Os dois códigos abaixo apresentam exemplos destes casos. O primeiro usa \\(n^{\\frac{3}{4}}\\), o segundo \\(n^{\\frac{1}{4}}\\).\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # mesmo loop que o anterior\n  for (i in 1:1000){ \n    \n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n  \n  x_normalizado &lt;- n^(3/4)*(x - 5)/sqrt(100/12) # muda apenas o expoente de n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\")\n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # mesmo loop que o anterior\n  for (i in 1:1000){ \n    \n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n  \n  x_normalizado &lt;- n^(1/4)*(x - 5)/sqrt(100/12) # muda apenas o expoente de n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\")\n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)\n}"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html",
    "title": "Os Pecados da Visualização de Dados",
    "section": "",
    "text": "Como parte do curso de Inferência Estatística, oferecido pela Duke University, eu tive de ver um gráfico abominável. O gráfico abaixo faz parte de um press release da WIN-Gallup International de 2012. Ele mostra a relação entre religiosidade, mensurada por um índice que varia de 0 a 100, e o PIB per capita de vários países.\nA imagem abaixo foi retirada diretamente do press release e está em baixa resolução no original.\n\n\n\n\n\n\n\nSão vários os erros da visualização acima. Talvez o mais sério seja a dificuldade de leitura dos dados. O tamanho da fonte utilizado, junto com a baixa resolução da imagem dificultam muito a interpretação dos números.\nO uso excessivo de cores e emojis prejudicam a leitura do gráfico e, eventualmente, até podem levar a interpretações tendenciosas dos dados. Há também um excesso de informação nos eixos do gráfico, com as setas e cifrões. Em resumo,\n\nUso excessivo de cores, potencialmente tendenciosas.\nDados estão obscuros, difícil de interpretar.\nEixos com informação redundante.\nEmojis cafonas."
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#os-pecados-da-visualização-de-dados",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#os-pecados-da-visualização-de-dados",
    "title": "Os Pecados da Visualização de Dados",
    "section": "",
    "text": "São vários os erros da visualização acima. Talvez o mais sério seja a dificuldade de leitura dos dados. O tamanho da fonte utilizado, junto com a baixa resolução da imagem dificultam muito a interpretação dos números.\nO uso excessivo de cores e emojis prejudicam a leitura do gráfico e, eventualmente, até podem levar a interpretações tendenciosas dos dados. Há também um excesso de informação nos eixos do gráfico, com as setas e cifrões. Em resumo,\n\nUso excessivo de cores, potencialmente tendenciosas.\nDados estão obscuros, difícil de interpretar.\nEixos com informação redundante.\nEmojis cafonas."
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#pacotes",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#pacotes",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Pacotes",
    "text": "Pacotes\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(countries)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#dados",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#dados",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Dados",
    "text": "Dados\nPode-se conseguir os dados diretamente do PDF linkado. Os dados brutos estão na tabela 1, página 10. Vale dizer que o press release inteiro é uma aula de como não fazer gráficos e tabelas. Para importar os dados dentro do R vale ver o meu tutorial sobre como importar dados de PDF. Outra opção, ainda mais simples é copiar os números e importar as linhas diretamente de um arquivo de texto.\n\nrelig &lt;- as_tibble(relig)\n\ntab_relig &lt;- relig |&gt; \n  mutate(\n    index = as.numeric(str_extract(value, \"\\\\d+\")),\n    country = str_trim(str_remove(value, \"\\\\d+\")),\n    iso3c = country_name(country)\n    ) |&gt; \n  select(iso3c, index)\n\nPode-se importar os dados de PIB per capita diretamente do Banco Mundial via pacote WDI com o código abaixo. Importa-se o PIB per capita, em paridade de poder de compra (PPP) a dólares constantes de 2017. Como a pesquisa da WIN-Gallup é de 2012, vamos filtrar os dados deste ano.\n\ngdp &lt;- WDI::WDI(indicator = \"NY.GDP.PCAP.PP.KD\")\n\ngdp &lt;- gdp |&gt;\n  as_tibble() |&gt;\n  janitor::clean_names() |&gt;\n  filter(year == 2012) |&gt;\n  rename(gdppc = ny_gdp_pcap_pp_kd)\n\ndat &lt;- left_join(tab_relig, gdp, by = \"iso3c\")"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#gráfico",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#gráfico",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Gráfico",
    "text": "Gráfico\nNa primeira versão do gráfico, usa-se geom_label_repel para plotar o nome dos países. Esta é uma solução para evitar o overplotting. Além disso, quebro alguns nomes maiores em duas linhas\n\nggplot(dat, aes(gdppc, index)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggrepel::geom_label_repel(aes(label = str_wrap(country, 12)), size = 2) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nAjustando as escalas e os eixos é possível chegar num resultado bastante satisfatório.\n\nggplot(dat, aes(gdppc, index)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggrepel::geom_label_repel(aes(label = str_wrap(country, 12)), size = 2) +\n  scale_x_continuous(\n    breaks = seq(10000, 60000, 10000),\n    labels = scales::label_number(big.mark = \",\")\n    ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  labs(\n    title = \"Religiosity Index and National Income\",\n    x = \"GDP per capita (PPP, US$ constant 2017)\",\n    y = \"Index (100 = most religious)\",\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nUma solução alternativa para contornar os problemas de overplotting é de usar as siglas dos países, isto é, os códigos ISO3.\n\nplot_main &lt;- ggplot(dat, aes(gdppc, index)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggrepel::geom_label_repel(aes(label = iso3c), size = 2) +\n  scale_x_continuous(\n    breaks = seq(10000, 60000, 10000),\n    labels = scales::label_number(big.mark = \",\")\n    ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  labs(\n    title = \"Religiosity Index and National Income\",\n    x = \"GDP per capita (PPP, US$ constant 2017)\",\n    y = \"Index (100 = most religious)\",\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\nplot_main\n\n\n\n\n\n\n\n\nPodemos também acrescentar o texto informativo, ao lado do gráfico usando o pacote patchwork. Pode-se ver o resultado final abaixo. O gráfico está mais sucinto e informativo, ainda que muito menos colorido.\n\nlibrary(patchwork)\n\ntxt &lt;- \"The Religiosity Index represents the percentage of the\npopulation who self-describe themselves as a 'religious person' in the question worded as: Irrespective of whether you attend a palce of\nworship or not, would you say you are a religious person, not a\nreligious person or a convinced atheist?\"\n\ndf = tibble(x = 1, y = 1, label = txt)\n\nplot_txt = ggplot(df) +\n  geom_text(aes(x, y, label = str_wrap(label, width = 30)), hjust = 0, size = 2) +\n  theme_void()\n\npanel = plot_main +\n  theme(\n    plot.margin = margin(5, 100, 5, 5)\n  ) +\n  inset_element(plot_txt, left = 0.75, 1, 1.3, 0.4)\n\npanel"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#um-pouco-a-mais",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#um-pouco-a-mais",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Um pouco a mais",
    "text": "Um pouco a mais\nPara tornar o gráfico mais interessante podemos combinar o índice de religiosidade com as religiões predominantes de cada país. Os dados são da PewResearch Center e foram coletados no meu outro post. Pode-se também fazer o download deles diretamente do meu GitHub.\n\ntab_religion = religion |&gt; \n  filter(share == max(share), .by = \"iso3c\")\n\ndat = left_join(dat, tab_religion, by = \"iso3c\")\n\ndat = dat |&gt; \n  filter(!is.na(religion)) |&gt; \n  mutate(\n    religion = str_replace(religion, \"unaffil\", \"unaffiliated\"),\n    religion = str_to_title(religion),\n    religion = factor(religion)\n    )\n\nPara minha visualização, vou mapear cada religião numa cor diferente. Além disso vou usar uma escala log para reduzir a variância (e distorção visual) dos dados de renda per capita. Por fim, vou mapear o tamanho da população de cada país para o tamanho de cada círculo.\nO resultado final é um gráfico bastante interessante. Vê-se que a maior parte dos países é predominantemente de religão cristã. Há um grupo de países cristãos europeus, de alta renda, e baixa religiosidade; um grupo de países cristãos latino-americanos, de renda média e alta religiosidade; e, finalmente, um grupo de países cristãos africanos, de renda bastante baixa, mas com religiosidade similar à do último grupo.\nEm poucos países, a maior parte da população não segue uma religião. É o caso, por exemplo, de Hong Kong, Japão e China. Vale notar que os principais países de maioria budista não foram contemplados no estudo da WIN-Gallup (e.g. Tailândia, Mianmar, etc.).\nPor fim, é interessante notar como a variabilidade da correlação entre as variáveis aumenta conforme aumenta a renda do país. Na faixa de 20 a 30 mil, por exemplo, temos a Polônia, de maioria cristã, com índice próximo de 80 e a Turquia, de maioria muçulmana, com índice próximo de 20.\n\nggplot(dat, aes(log(gdppc), index, color = religion)) +\n  ggrepel::geom_label_repel(aes(label = iso3c), size = 2) +\n  geom_point(aes(size = sqrt(country_population))) +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(\n    breaks = log(c(5000, 10000, 20000, 30000, 40000, 60000)),\n    labels = format(c(5000, 10000, 20000, 30000, 40000, 60000), big.mark = \",\")\n  ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  scale_color_manual(\n    name = \"Main Religion\",\n    values = MetBrewer::met.brewer(\"Hokusai1\", 5)\n  ) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Religiosity Index and National Income\",\n    subtitle = \"Size of each circle is proportional to the country's population.\",\n    x = \"GDP per capita (PPP, US$ constant 2017)\",\n    y = \"Index (100 = most religious)\",\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-sp-idh-atlas/index.html",
    "href": "posts/general-posts/2024-02-wz-sp-idh-atlas/index.html",
    "title": "IDH por região em São Paulo",
    "section": "",
    "text": "IDH em São Paulo\nO mapa abaixo mostra o Índice de Desenvolvimento Humano (IDH) das regiões de São Paulo. As estimativas provêm do projeto Atlas Brasil, que computa o IDH a nível submunicipal nas chamadas Unidades de Desenvolvimento Humano (UDHs). Como se vê no desenho do mapa, as UDHs tem formatos bastante flexíveis. De forma geral, as UDHs seguem divisões administrativas pré-existentes mas também ressaltam áreas de vulnerabilidade social que estão imersas dentro de outras regiões.\nO gráfico de colunas mostra o percentual da população que vive em cada grupo de IDH. Este gráfico ajuda a reduzir a distorção visual causada pelo mapa. Visualmente, tende-se a associar o tamanho das áreas com a sua representatividade. Isto acontece tanto no extremo sul da cidade, que é esparsamente povoado, como também no Centro Expandido. De maneira geral, a maior parte da população vive na faixa entre 0,650 a 0,800 um IDH médio. Cerca de um terço da população vive em regiões com IDH acima de 0,800, um IDH alto ou altíssimo.\n\n\n\n\n\n\n\n\n\n\nDados: Atlas Brasil\nTipografia: Raleway\nPaleta: YlGnBu (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html",
    "href": "posts/general-posts/repost-emv-no-r/index.html",
    "title": "EMV no R",
    "section": "",
    "text": "A estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é consistente (converge para o valor verdadeiro), normalmente assintótico (distribuição assintórica segue uma normal padrão) e eficiente (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.\nA intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros \\(\\mu\\) e \\(\\sigma\\) de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido “gerada” por uma normal).\nHá vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no R. Neste post vou me ater apenas a dois pacotes: o optimx e o maxLik. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.\nVale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função optim do R. Os dois pacotes simplesmente trazem algumas comodidades.\n\nlibrary(maxLik)\nlibrary(optimx)\n# Para reproduzir os resultados\nset.seed(33)"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-optimx",
    "href": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-optimx",
    "title": "EMV no R",
    "section": "Usando o pacote optimx",
    "text": "Usando o pacote optimx\nA função optim já é bastante antiga e um novo pacote, chamado optimx, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são optimx e optimr. Mais informações sobre o pacote podem ser encontradas aqui.\nA sintaxe das funções é muito similar à sintaxe original do optim. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead\n\nsummary(fit &lt;- optimx(par = 1, fn = ll_pois, x = amostra))\n\n                p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nNelder-Mead 4.9375 2202.837     32     NA    NA        0 TRUE TRUE 0.001\nBFGS        4.9380 2202.837     34      9    NA        0 TRUE TRUE 0.002\n\n\nUma das principais vantagens do optimx é a possibilidade de usar vários métodos de otimização numérica numa mesma função.\n\nfit &lt;- optimx(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n  method = c(\"nlm\", \"BFGS\", \"Rcgmin\", \"nlminb\")\n  )\n\nfit\n\n             p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nnlm    4.937998 2202.837     NA     NA     8        0 TRUE TRUE 0.001\nBFGS   4.938000 2202.837     34      9    NA        0 TRUE TRUE 0.002\nRcgmin 4.937999 2202.837    708    112    NA        1 TRUE TRUE 0.039\nnlminb 4.938000 2202.837     10     12     9        0 TRUE TRUE 0.001\n\n\nComo este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos."
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-maxlik",
    "href": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-maxlik",
    "title": "EMV no R",
    "section": "Usando o pacote maxLik",
    "text": "Usando o pacote maxLik\nA função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, não é preciso montar a função com sinal de menos como fizemos acima; segundo, ela já calcula erros-padrão e estatísticas-t dos coeficientes estimados. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas aqui.\nPara usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o dpois que implementa a função densidade da Poisson. O argumento log = TRUE retorna as probabilidades \\(p\\) como \\(log(p)\\).\n\nll_pois &lt;- function(x, theta) {\n    ll &lt;- dpois(x, theta, log = TRUE)\n    return(sum(ll))\n}\n\nO comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.\n\nsummary(fit &lt;- maxLik(ll_pois, start = 1, x = amostra))\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 8 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07617   64.83  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função optimx. Derivando a função de log-verossimilhança:\n\\[\n\\begin{align}\n  \\frac{\\partial \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda} & = \\frac{1}{\\lambda}\\sum_{k = 1}^{n}x_{k} - n \\\\\n  \\frac{\\partial^2 \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda^2} & = -\\frac{1}{\\lambda^2}\\sum_{k = 1}^{n}x_{k}\n\\end{align}\n\\]\nO código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.\n\ngrad_pois &lt;- function(x, theta) {\n  (1 / theta) * sum(x) - length(x)\n  }\n\nhess_pois &lt;- function(x, theta) {\n    -(1 / theta^2) * sum(x)\n}\n\nfit2 &lt;- maxLik(\n  ll_pois,\n  grad = grad_pois,\n  hess = hess_pois,\n  start = 1,\n  x = amostra\n  )\n\nsummary(fit2)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 7 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07027   70.27  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#consistência",
    "href": "posts/general-posts/repost-emv-no-r/index.html#consistência",
    "title": "EMV no R",
    "section": "Consistência",
    "text": "Consistência\nVamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição \\(N(2, 3)\\); computamos as estimativas para \\(\\mu\\) e \\(\\sigma\\) e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.\n\nSimular uma amostra segundo uma distribuição.\nEstimata os parâmetros da distribuição.\nCalcula a variância assintótica dos estimadores.\nRepete 5000 vezes os passos 1-3.\n\nO código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.\n\nr &lt;- 5000\nn &lt;- 1000\n\nestimativas &lt;- matrix(ncol = 4, nrow = r)\n\nfor(i in 1:r) {\n    x &lt;- rnorm(n = n, mean = 2, sd = 3)\n    \n    fit &lt;- optimr(\n      par = c(1, 1),\n      fn = ll_norm,\n      method = \"BFGS\",\n      hessian = TRUE\n      )\n    # Guarda o valor estimado do parâmetro\n    estimativas[i, 1:2] &lt;- fit$par\n    estimativas[i, 3:4] &lt;- diag(n * solve(fit$hess))\n}\n\nA consistência dos estimadores \\(\\hat{\\theta}_{MV}\\) significa que eles aproximam os valores verdadeiros do parâmetros \\(\\theta_{0}\\) à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande \\(\\mathbb{N} \\to \\infty\\) então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros \\(\\hat{\\theta}_{\\text{MV}} \\to \\theta_{0}\\)\nO código abaixo calcula a média das estimativas para cada parâmetro - lembrando que \\(\\mu_{0} = 2\\) e que \\(\\sigma_{0} = 3\\). Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).\n\n# | fig-width: 10\npar(mfrow = c(1, 2))\n# Consistência dos estimadores de MV\nmu &lt;- estimativas[, 1]; sigma &lt;- estimativas[, 2]\nmean(mu)\n\n[1] 2.000883\n\nmean(sigma)\n\n[1] 2.997335\n\nhist(mu, main = bquote(\"Estimativas para \"~~mu), freq = FALSE, xlim = c(1.5, 2.5))\nabline(v = 2, col = \"indianred\")\nhist(sigma, main = bquote(\"Estimativas para \"~~sigma), freq = FALSE, xlim = c(2.7, 3.3))\nabline(v = 3, col = \"indianred\")"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#normalmente-assintótico",
    "href": "posts/general-posts/repost-emv-no-r/index.html#normalmente-assintótico",
    "title": "EMV no R",
    "section": "Normalmente assintótico",
    "text": "Normalmente assintótico\nDizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:\n\\[\nz_{\\theta} = \\sqrt{N}\\frac{\\hat{\\theta}_{MV} - \\theta}{\\sqrt{\\text{V}_{ast}}} \\to \\mathbb{N}(0, 1)\n\\]\nonde \\(\\text{V}_{ast}\\) é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\nNo loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\n# Normalidade assintótica\n\n# Define objetos para facilitar a compreensão\nmu_hat &lt;- estimativas[, 1]\nsigma_hat &lt;- estimativas[, 2]\nvar_mu_hat &lt;- estimativas[, 3]\nvar_sg_hat &lt;- estimativas[, 4]\n\n# Centra a estimativa\nmu_centrado &lt;- mu_hat - 2 \nsigma_centrado &lt;- sigma_hat - 3\n# Computa z_mu z_sigma\nmu_normalizado &lt;- sqrt(n) * mu_centrado / sqrt(var_mu_hat)\nsigma_normalizado &lt;- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)\n\n\n# Monta o gráfico para mu\n\n# Eixo x\ngrid_x &lt;- seq(-3, 3, 0.01)\n\nhist(\n  mu_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[mu]),\n  freq = FALSE,\n  xlim = c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[mu]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n\n\n\n\n\n\n\n\n\n# Monta o gráfico para sigma2\nhist(\n  sigma_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[sigma]),\n  freq = FALSE,\n  xlim =c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[sigma]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#escolha-de-valores-inicias",
    "href": "posts/general-posts/repost-emv-no-r/index.html#escolha-de-valores-inicias",
    "title": "EMV no R",
    "section": "Escolha de valores inicias",
    "text": "Escolha de valores inicias\nComo comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.\nO exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.\n\n# sensível a escolha de valores inicias\nx &lt;- rnorm(n = 1000, mean = 15, sd = 4)\nfit &lt;- optim(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n\n$par\n[1] 618.6792 962.0739\n\n$value\n[1] 7984.993\n\n$counts\nfunction gradient \n     107      100 \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]\n[1,]  0.001070703 -0.0013531007\n[2,] -0.001353101  0.0001884928\n\n\nNote que as estimativas estão muito distantes dos valores corretos \\(\\mu = 15\\) e \\(\\sigma = 4\\). Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.\nO código abaixo usa os momentos empíricos como valores inicias para \\(\\mu\\) e \\(\\sigma\\):\n\\[\n\\begin{align}\n  \\mu_{inicial} & = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\\\\n  \\sigma_{inicial} & = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} (x_{i} - \\mu_{inicial})^2}\n\\end{align}\n\\]\n\n(chute_inicial &lt;- c(mean(x), sqrt(var(x))))\n\n[1] 14.859702  3.930849\n\n(est &lt;- optimx(par = chute_inicial, fn = ll_norm))\n\n                  p1       p2    value fevals gevals niter convcode kkt1 kkt2\nNelder-Mead 14.85997 3.929097 2787.294     47     NA    NA        0 TRUE TRUE\nBFGS        14.85970 3.928884 2787.294     15      2    NA        0 TRUE TRUE\n            xtime\nNelder-Mead 0.001\nBFGS        0.001\n\n\nAgora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função optimx se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.\nNote como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método bobyqa retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos nlminb e Nelder-Mead ambos retornam os valores corretos.\n\n# Usando outros métodos numéricos\noptimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = c(\"BFGS\", \"Nelder-Mead\", \"CG\", \"nlminb\", \"bobyqa\")\n  )\n\n                   p1         p2    value fevals gevals niter convcode  kkt1\nBFGS        618.67917 962.073907 7984.993    107    100    NA        1  TRUE\nNelder-Mead  14.85571   3.929621 2787.294     83     NA    NA        0  TRUE\nCG           46.43586 628.570987 7358.601    204    101    NA        1  TRUE\nnlminb       14.85970   3.928883 2787.294     23     47    19        0  TRUE\nbobyqa       15.20011   8.993240 3211.556    109     NA    NA        0 FALSE\n             kkt2 xtime\nBFGS        FALSE 0.007\nNelder-Mead  TRUE 0.001\nCG          FALSE 0.008\nnlminb       TRUE 0.001\nbobyqa      FALSE 0.033\n\n\nVale notar também alguns detalhes técnicos da saída. Em particular, convcode == 0 significa que o otimizador conseguiu convergir com sucesso, enquanto convcode == 1 indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.\nJá o kkt1 e kkt2 verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte necessária do teorema enquanto a segunda condição verifica a parte suficiente. Note que o bobyqa falha em ambas as condições (pois ele não é feito para este tipo de problema).\nOs métodos que retornam os melhores valores, o Nelder-Mead e nlminb são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.\nMais detalhes sobre os métodos podem ser encontrados na página de ajuda da função ?optimx."
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html",
    "href": "posts/general-posts/2023-08-firjan-app/index.html",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório\n\n\n\n\nO app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16.\n\n\n\n\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html#introdução",
    "href": "posts/general-posts/2023-08-firjan-app/index.html#introdução",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório"
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html#sobre-o-app",
    "href": "posts/general-posts/2023-08-firjan-app/index.html#sobre-o-app",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "O app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16."
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "href": "posts/general-posts/2023-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Um caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Os mercados de aluguel e de venda de imóveis são interligados. Em tese, a decisão de comprar ou alugar um imóvel passa pelo preço. Se o aluguel estiver suficientemente “barato”, mais pessoas vão optar pelo aluguel, reduzindo a taxa de vacância e aumentando o preço do aluguel. O preço do aluguel sobe a tal ponto que se torna mais interessante comprar um imóvel do que alugar, incentivando pessoas que alugam a migrar para um apartamento próprio.\nVale notar que o mercado de aluguel é bastante lento. A “migração” do aluguel para um imóvel proprietário não é rápida nem simples. Além do custo monetário há diversos custos de transação envolvidos no processo. De fato, em muitos casos, não é possível fazer uma migração entre um e outro por motivos contratuais.\nPor fim, a literatura empírica do tema não é unânime sobre a relação entre os mercados de venda e de aluguel. Um estudo sobre o mercado de Cingapura encontrou evidência de cointegração apenas em regiões específicas e não conseguiu estabelecer a presença de cointegração a nível nacional.\n\n\n\nAs fontes de dados para verificar a dinâmica destes mercados é limitada no Brasil. O FipeZap oferece a série de aluguel e de venda mais extensa. Este índice coleta todos os anúncios online de imóveis publicados no sistema ZapImóveis e constroi um índice de mediana. Os dados são estratificados por número de dormitórios e territorialmente, usando regiões que se baseam nas áreas de ponderação do IBGE.\nO gráfico abaixo mostra as séries de aluguel e de venda (residenciais) a nível nacional, agregadas por tipologia. Como se vê, o comportamento das séries é muito similar no périodo 2008-2015. Durante a Crise Econômica, o mercado de aluguel parece sofrer mais do que o mercado de venda. Durante e após a pandemia do Covid-19 ambos os mercados se recuperam com força, mas o mercado de aluguel parece crescer mais.\n\n\n\n\n\n\n\n\n\nNa variação acumulada do período, vê-se que o mercado de venda acumulou uma alta significativamente maior do que do mercado de aluguel. A divergência parece ter começado em 2009. A diferença entre as séries fica estável durante a Crise Econômica e o mercado de aluguel parece começar a reduzir esta distância no período da pandemia.\n\n\n\n\n\n\n\n\n\n\n\nOlhando a variação contemporânea nos dois mercados, vê-se que parece existir uma correlação entre as variáveis. O gráfico abaixo mostra o ajuste da equação de um modelo aditivo generalizado e as cores dos pontos indicam ciclos econômicos. Grosso modo, define-se que o ciclo de commodities vai até a metade de 2012; a recessão Dilma começa na metade de 2014 e vai até o final de 2016; o período Covid começa em maio de 2020 e agrupa também o período pós-Covid até o presente.\n\n\n\n\n\n\n\n\n\nPode-se também buscar quebras de tendência entre os períodos. Agora, o gráfico ajusta linhas de regressão linear em cada um dos períodos. Visualmente, apenas o período Covid + Pós-Covid parece apresentar uma quebra na tendência, indicando variações maiores nos preços de aluguel para uma dada variação nos preços de venda. Os demais ciclos se distinguem por mudanças de média/nível e variância, mas preservam uma relação similar entre mudanças nos preços de aluguel e nos preços de venda.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsando uma abordagem mais rigorosa, podemos buscar algum tipo de cointegração entre as séries. Pelos testes de raiz unitária usais, é fácil concluir que ambas as séries são I(1)1. Contudo, tanto o teste de Johanssen como o teste Engle-Granger apontam que não há relação de cointegração entre as séries2.\nVale notar que estas séries passam por dois períodos de “exceção”: a Crise Econômica, do governo Dilma, e a Pandemia do Covid-19. Assim, há motivo para se crer que possa haver quebras estruturais na série. Estas quebras podem estar ocultando alguma relação de cointegração entre as séries. Por outro lado, como o horizonte temporal das séries não é muito longo, seria difícil sustentar uma tese de convergência de equilíbrio de longo prazo se houver mais do que uma quebra estrutural nas séries.\nUsando testes de detecção e verificação de quebras estruturais3, chega-se na conclusão de que há uma quebra na série do aluguel (em outubro de 2020) e 3 quebras na série de preços de venda (em julho de 2010, novembro de 2012 e dezembro de 2018). O gráfico abaixo destaca as quebras nas séries de preço de aluguel e de venda.\nIntuitivamente, pode-se associar a quebra da série de aluguel às mudanças no período da Pandemia e pós-Pandemia. As mudanças na série de vendas são mais opacas. A quebra na série de vendas na metade de 2010 pode estar refletindo o bom momento da economia brasileira na época, que ainda sentia os estímulos da política contracíclica de 2009, como também o recente lançamento do programa MCMV. A quebra no final de 2012 pode estar refletindo tanto o final do ciclo de commodities, como também o início do ciclo (forçado) de queda da taxa SELIC. A quebra no final de 2018 pode estar refletindo mudanças institucionais (reforma previdência, reforma dos distratos, etc.), a taxa de inflação estável e a baixa taxa SELIC.\n\n\n\n\n\n\n\n\n\nPara manter a simplicidade da análise, vamos considerar apenas a quebra na série de aluguel. O gráfico abaixo destaca apenas a quebra na série do aluguel. É interessante notar que o período “normal” apresenta uma correlação positiva entre a variação do preço do aluguel e do preço de venda, sugerindo que os mercados andam lado a lado. Já no período pós-Covid a correlação é inexistente. O que se verifica é que os preços de venda variam consideravelmente, de -0.2% a quase 2% em cada mês, enquanto os preços de aluguem variam muito menos. Neste sentido, os mercados parecem se comportam de maneira quase independente no período denominado “Pós-Covid”.\n\n\n\n\n\n\n\n\n\n\n\n\nPela análise dos dados, parece que o mercado de aluguel entrou num forte ciclo de alto, descolado do mercado de vendas. Isto pode ser resultado de diversos fatores, incluindo (1) o aumento da taxa de juros real da economia; (2) mudanças nos padrões de moradia no período pós-pandemia; ou, vale mencionar; (3) erros de mensuração. Vou tratar mais especificamente do último ponto.\nO Índice FipeZap captura a mediana dos preços de anúncios mês a mês e utiliza uma metodologia relativamente simples para converter este valor num número índice4. O preço do anúncio de um imóvel certamente é correlacionado com o preço final de venda/aluguel, mas esta relação pode ter se alterado - ainda que temporariamente - durante o período da pandemia. O Índice de Variação de Aluguel Residencial (IVAR), desenvolvido pela Fundação Getúlio Vargas (FGV), é uma alternativa ao FipeZap. O IVAR é um índice de alugueis repetidos, metodologia adaptada do famoso Índice Case-Shiller, que utiliza somente a informação de contratos de alugueis efetivamente firmados5.\nEm particular, vale notar que o FipeZap divergiu consideravelmente em relação ao IVAR durante o período da pandemia. Enquanto o IVAR registrou quedas nominais, indicando a tendência do mercado de ceder descontos durante o período de maior incerteza da Pandemia, o FipeZap registrou um longo período de estagnação entrecortado por quedas pontuais. A partir de 2022, ambos os índices registram aumento, mas a alta do IVAR é consideravelmente menor.\n\n\n\n\n\n\n\n\n\nNão é fácil afirmar que um ou outro índice seja mais correto. Contudo, vale afirmar que a metodologia do IVAR é considerada superior à metodologia do FipeZap segundos as melhores práticas internacionais. Ainda assim, é importante notar que a base territorial do FipeZap é consideravelmente superior: na sua versão mais recente, o índice abarca mais de 50 cidades; o IVAR, por outro lado, está disponível somente para quatro cidades (Belo Horizonte, Porto Alegre, Rio de Janeiro e São Paulo). De qualquer maneira, a série histórica do IVAR é muito recente; o índice remonta apenas até 2019, dificultando qualquer tipo de análise de cointegração.\nNum post futuro vou entrar em maiores detalhes sobre as séries do FipeZap e sobre os índices de preços imobiliários em geral. Também devo discutir em maiores detalhes os procedimentos de cointegração e de quebras estruturais em séries de tempo em posts futuros."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#os-mercados",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#os-mercados",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Os mercados de aluguel e de venda de imóveis são interligados. Em tese, a decisão de comprar ou alugar um imóvel passa pelo preço. Se o aluguel estiver suficientemente “barato”, mais pessoas vão optar pelo aluguel, reduzindo a taxa de vacância e aumentando o preço do aluguel. O preço do aluguel sobe a tal ponto que se torna mais interessante comprar um imóvel do que alugar, incentivando pessoas que alugam a migrar para um apartamento próprio.\nVale notar que o mercado de aluguel é bastante lento. A “migração” do aluguel para um imóvel proprietário não é rápida nem simples. Além do custo monetário há diversos custos de transação envolvidos no processo. De fato, em muitos casos, não é possível fazer uma migração entre um e outro por motivos contratuais.\nPor fim, a literatura empírica do tema não é unânime sobre a relação entre os mercados de venda e de aluguel. Um estudo sobre o mercado de Cingapura encontrou evidência de cointegração apenas em regiões específicas e não conseguiu estabelecer a presença de cointegração a nível nacional."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#os-dados",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#os-dados",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "As fontes de dados para verificar a dinâmica destes mercados é limitada no Brasil. O FipeZap oferece a série de aluguel e de venda mais extensa. Este índice coleta todos os anúncios online de imóveis publicados no sistema ZapImóveis e constroi um índice de mediana. Os dados são estratificados por número de dormitórios e territorialmente, usando regiões que se baseam nas áreas de ponderação do IBGE.\nO gráfico abaixo mostra as séries de aluguel e de venda (residenciais) a nível nacional, agregadas por tipologia. Como se vê, o comportamento das séries é muito similar no périodo 2008-2015. Durante a Crise Econômica, o mercado de aluguel parece sofrer mais do que o mercado de venda. Durante e após a pandemia do Covid-19 ambos os mercados se recuperam com força, mas o mercado de aluguel parece crescer mais.\n\n\n\n\n\n\n\n\n\nNa variação acumulada do período, vê-se que o mercado de venda acumulou uma alta significativamente maior do que do mercado de aluguel. A divergência parece ter começado em 2009. A diferença entre as séries fica estável durante a Crise Econômica e o mercado de aluguel parece começar a reduzir esta distância no período da pandemia.\n\n\n\n\n\n\n\n\n\n\n\nOlhando a variação contemporânea nos dois mercados, vê-se que parece existir uma correlação entre as variáveis. O gráfico abaixo mostra o ajuste da equação de um modelo aditivo generalizado e as cores dos pontos indicam ciclos econômicos. Grosso modo, define-se que o ciclo de commodities vai até a metade de 2012; a recessão Dilma começa na metade de 2014 e vai até o final de 2016; o período Covid começa em maio de 2020 e agrupa também o período pós-Covid até o presente.\n\n\n\n\n\n\n\n\n\nPode-se também buscar quebras de tendência entre os períodos. Agora, o gráfico ajusta linhas de regressão linear em cada um dos períodos. Visualmente, apenas o período Covid + Pós-Covid parece apresentar uma quebra na tendência, indicando variações maiores nos preços de aluguel para uma dada variação nos preços de venda. Os demais ciclos se distinguem por mudanças de média/nível e variância, mas preservam uma relação similar entre mudanças nos preços de aluguel e nos preços de venda."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#um-novo-ciclo-de-aluguel",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#um-novo-ciclo-de-aluguel",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Usando uma abordagem mais rigorosa, podemos buscar algum tipo de cointegração entre as séries. Pelos testes de raiz unitária usais, é fácil concluir que ambas as séries são I(1)1. Contudo, tanto o teste de Johanssen como o teste Engle-Granger apontam que não há relação de cointegração entre as séries2.\nVale notar que estas séries passam por dois períodos de “exceção”: a Crise Econômica, do governo Dilma, e a Pandemia do Covid-19. Assim, há motivo para se crer que possa haver quebras estruturais na série. Estas quebras podem estar ocultando alguma relação de cointegração entre as séries. Por outro lado, como o horizonte temporal das séries não é muito longo, seria difícil sustentar uma tese de convergência de equilíbrio de longo prazo se houver mais do que uma quebra estrutural nas séries.\nUsando testes de detecção e verificação de quebras estruturais3, chega-se na conclusão de que há uma quebra na série do aluguel (em outubro de 2020) e 3 quebras na série de preços de venda (em julho de 2010, novembro de 2012 e dezembro de 2018). O gráfico abaixo destaca as quebras nas séries de preço de aluguel e de venda.\nIntuitivamente, pode-se associar a quebra da série de aluguel às mudanças no período da Pandemia e pós-Pandemia. As mudanças na série de vendas são mais opacas. A quebra na série de vendas na metade de 2010 pode estar refletindo o bom momento da economia brasileira na época, que ainda sentia os estímulos da política contracíclica de 2009, como também o recente lançamento do programa MCMV. A quebra no final de 2012 pode estar refletindo tanto o final do ciclo de commodities, como também o início do ciclo (forçado) de queda da taxa SELIC. A quebra no final de 2018 pode estar refletindo mudanças institucionais (reforma previdência, reforma dos distratos, etc.), a taxa de inflação estável e a baixa taxa SELIC.\n\n\n\n\n\n\n\n\n\nPara manter a simplicidade da análise, vamos considerar apenas a quebra na série de aluguel. O gráfico abaixo destaca apenas a quebra na série do aluguel. É interessante notar que o período “normal” apresenta uma correlação positiva entre a variação do preço do aluguel e do preço de venda, sugerindo que os mercados andam lado a lado. Já no período pós-Covid a correlação é inexistente. O que se verifica é que os preços de venda variam consideravelmente, de -0.2% a quase 2% em cada mês, enquanto os preços de aluguem variam muito menos. Neste sentido, os mercados parecem se comportam de maneira quase independente no período denominado “Pós-Covid”."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#considerações-importantes",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#considerações-importantes",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Pela análise dos dados, parece que o mercado de aluguel entrou num forte ciclo de alto, descolado do mercado de vendas. Isto pode ser resultado de diversos fatores, incluindo (1) o aumento da taxa de juros real da economia; (2) mudanças nos padrões de moradia no período pós-pandemia; ou, vale mencionar; (3) erros de mensuração. Vou tratar mais especificamente do último ponto.\nO Índice FipeZap captura a mediana dos preços de anúncios mês a mês e utiliza uma metodologia relativamente simples para converter este valor num número índice4. O preço do anúncio de um imóvel certamente é correlacionado com o preço final de venda/aluguel, mas esta relação pode ter se alterado - ainda que temporariamente - durante o período da pandemia. O Índice de Variação de Aluguel Residencial (IVAR), desenvolvido pela Fundação Getúlio Vargas (FGV), é uma alternativa ao FipeZap. O IVAR é um índice de alugueis repetidos, metodologia adaptada do famoso Índice Case-Shiller, que utiliza somente a informação de contratos de alugueis efetivamente firmados5.\nEm particular, vale notar que o FipeZap divergiu consideravelmente em relação ao IVAR durante o período da pandemia. Enquanto o IVAR registrou quedas nominais, indicando a tendência do mercado de ceder descontos durante o período de maior incerteza da Pandemia, o FipeZap registrou um longo período de estagnação entrecortado por quedas pontuais. A partir de 2022, ambos os índices registram aumento, mas a alta do IVAR é consideravelmente menor.\n\n\n\n\n\n\n\n\n\nNão é fácil afirmar que um ou outro índice seja mais correto. Contudo, vale afirmar que a metodologia do IVAR é considerada superior à metodologia do FipeZap segundos as melhores práticas internacionais. Ainda assim, é importante notar que a base territorial do FipeZap é consideravelmente superior: na sua versão mais recente, o índice abarca mais de 50 cidades; o IVAR, por outro lado, está disponível somente para quatro cidades (Belo Horizonte, Porto Alegre, Rio de Janeiro e São Paulo). De qualquer maneira, a série histórica do IVAR é muito recente; o índice remonta apenas até 2019, dificultando qualquer tipo de análise de cointegração.\nNum post futuro vou entrar em maiores detalhes sobre as séries do FipeZap e sobre os índices de preços imobiliários em geral. Também devo discutir em maiores detalhes os procedimentos de cointegração e de quebras estruturais em séries de tempo em posts futuros."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#footnotes",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#footnotes",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsou-se o teste de Philips-Perron (ur.pp) e o teste Dickey Fuller Aumentado (ur.df) do pacote urca. Na série de venda incluiu-se uma constante e uma tendência linear. Na série de aluguel não se incluiu nem constante e nem tendência linear.↩︎\nEm ambos os casos não se rejeita a hipótese nula de \\(r = 0\\), isto é, de que há 0 relações de cointegração entre as variáveis. O teste foi realizado com e sem constante e com e sem tendência.↩︎\nEspecificamente, testou-se a presença de quebras na série usando um teste F (Fstats) e um teste generalizado (empirical fluctuation process), considerando um processo SARMA(1,0,0)(1,0,0)[12]. O teste utilizado foi o efp(type = \"OLS-CUSUM\"). O número ótimo de quebras na série foi detectado usando o método de Bai e Perron (2003) via breakspoints. Todas as funções listadas são do pacote strucchange.↩︎\nPara mais detalhes sobre a metodologia do Índice FipeZap veja Fipe (2011) e a atualização Fipe (2020)↩︎\nPara mais detalhes sobre a metodologia do IVAR veja FGV (2021).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "",
    "text": "Um índice de preço convencional mensura a evolução do preço de um bem ou de uma cesta de bens ao longo do tempo. No caso mais simples, acompanha-se a dinâmica de um único bem, cujo preço no período \\(t\\) é dado por \\(p_{t}\\). Assim, temos um índice \\(I_t\\) dado por:\n\\[\nI_{t} = \\frac{p_{t}}{p_{t-1}}\\times 100\n\\]\nTipicamente, nosso interesse é de acompanhar uma cesta de bens. No caso do IPCA, indicador oficial da inflação, por exemplo, acomapanha-se um conjunto de bens que representa o “consumidor típico”. Assim temos \\(i\\) bens diferentes cada um com um preço \\(p_{i, t}\\). Neste post, vamos ignorar o problema (bastante complexo) de como agregar estes diferentes bens num índice único. Para todos efeitos vamos imaginar que existam pesos \\(w_i\\) imutáveis que representam o quão importante cada bem é para o “consumidor típico”. Assim temos: \\[\nI_{t} = \\frac{\\sum_{i \\in I}w_{i}p_{i,t}}{\\sum_{i \\in I}w_i p_{i, t-1}}\n\\]\nO índice assim é chamado de Índice de Laspeyres e é um dos índices mais comuns. O supracitado IPCA é um índice de Laspeyres, por exemplo. Apesar de simples, ele serve como base para se pensar índices de preços imobiliários1. Um índice de preços imobiliário mensura a tendência dos preços no mercado de imóveis. Em geral, costuma-se distinguir os mercados residencial e comercial devido às idissincracias de cada mercado. Neste post vamos tratar somente do mercado residencial.\nHá alguns desafios gerais relacionados à criação de um índice de preços imobiliário residencial (RPPI2). Os pontos gerais são listados abaixo\n\nImóveis são bens heterogêneos\nImóveis não são transacionados com frequência\nImóveis operam em submercados específicos\n\nAbaixo entro em mais detalhes sobre estes pontos. Uma revisão geral sobre a literatura de índices de preços imobiliários no contexto brasileiro está disponível no texto do IPEA, Índice de Preços para Imóveis. A referência geral e completa é o manual da Eurostat (2013), Handbook on Residential Property Price Indices.\n\n\nEm economia, há bens homogêneos (ou commodities) e bens heterogêneos. Pode-se pensar em bens homogêneos como bens em que é fácil encontrar um substituto; já bens heterogêneos são aqueles em que é muito difícil encontrar um substituto. Imóveis são um caso limite de bens heterogêneos. Quando se considera a localização como parte de seus atributos, cada imóvel é literalmente único, não existe nenhum outro equivalente no mundo. Mesmo quando se constroi apartamentos com plantas idênticas num mesmo prédio, pode-se ainda encontrar diferenças entre as unidades (seja o andar, a posição solar, etc.).\nO preço de bens heterogêneos reflete diferenças de qualidade. No caso de imóveis, pode-se pensar nos seus atributos estruturais (metragem, número de dormitórios, etc.) e locacionais (rua, bairro, proximidade a pontos de interesse, etc.). Faz sentido que a maior ou menor disponibilidade destes atributos influencie o preço do imóvel.\nPara entender como isto dificulta a estimação de um índice considere o caso de um imóvel abandonado que passa por uma significativa reforma. Naturalmente, o preço de venda deste imóvel aumenta várias vezes. Um índice de preços que identifica isto como um movimento dos preços em geral será enviesado.\nUma maneira de contornar isto é usando a modelagem hedônica. A modelagem ou precificação hedônica decompõe o preço de um imóvel em função de duas características observáveis. Assim, atribui-se um “preço” para cada uma das características do imóvel. Isto permite que se atribua corretamente qual parte do movimento dos preços é, de fato, um movimento de mercado.\n\n\n\nDescobrir o preço do arroz é uma tarefa tão simples como de olhar o valor etiquetado sobre a sua embalagem no mercado. No caso de imóveis isto é muito mais complicado. Todo imóvel tem um preço, mas apenas uma minúscula fração do estoque total de imóveis está “no mercado”, disponível para ser transacionado. Isto dificulta a aferição de preços.\nAlém disso, torna-se difícil comparar “laranjas com laranjas”. Considere o caso de um índice mensal. Dificilmente, os mesmos imóveis serão vendidos todos mês; pelo contrário, o caso mais comum seria observar, a cada novo mês, uma lista completamente nova de imóveis vendidos. Isto significa que o Índice vai estar comparando a evolução de preços de imóveis diferentes a cada mês.\nUma maneira de contornar isto é usando modelagem hedônica para precificar os imóveis que não estão à venda. Assim, seria possível prever o preço que um imóvel teria, caso ele tivesse sido vendido num determinado mês, o que permite fazer comparações adequadas. A qualidade desta precificação, por sua vez, depende da qualidade de dados disponíveis.\n\n\n\nIsto é outra maneira de dizer que nem todos os imóveis competem entre si. O mercado de imóveis se comporta similarmente a um mercado de mathcing onde as demandas dos clientes limitam a oferta relevante. Uma família de quatro pessoas dificilmente vai estar interessada em comprar um studio ou apartamento de 1 dormitório, qualquer que seja o seu preço. Na prática, isto significa que há vários mercados imobiliários; é possível que o mercado de casas de 4 dormitórios esteja em alta enquanto o mercado de apartamentos de 1d esteja em baixa. Similarmente, o mercado imobiliário da capital pode seguir um processo distinto do mercado imobiliário do interior.\nNovamente, uma estratégia para resolver isto é usar a modelagem hedônica. Outra estratégia é estratificar o índice para criar subíndices. Esta é uma prática bastante comum. Tipicamente, cria-se subíndices segundo algum critério estrutural (número de dormitórios ou tipologia) ou regional (capital x região metropolitana, interior).\n\n\n\n\nExiste uma literatura considerável sobre a criação de índices de preços. Desde a Grande Crise Financeira de 2008, há um interesse renovado no monitoramento do preço dos imóveis3.\nO atual consenso da literatura é de que um RPPI deve controlar pela qualidade dos imóveis observados (quality-adjusted index). Idealmente, a melhor metodologia é construir um índice hedônico com alguma estratégia de estratificação e imputação dupla. O único motivo para não usar um índice hedônico é quando não há informação suficiente para estimá-lo.\nA estratégia de imputação dupla permite maior flexibilidade na escolha da forma funcional do modelo hedônico. Escolhas comuns incluem: (1) regressão linear; (2) modelos de econometria espacial (SAR, SEM, etc.); e (3) modelos aditivos generalizados. Mais recentemente, alguns pesquisadores têm experimentado com modelos de aprendizado de máquina como de redes neurais. Em 2023, a Zillow revisou a metodologia do Zillow Home Value Index (ZHVI); o índice agora utiliza um modelo de redes neurais.\nEnquanto um Índice de Fisher (ou Tornqvist) apresenta propriedades ideais, nem sempre existe informação disponível para calculá-los. Assim, é natural utilizar um índice de Laspeyres junto com informação censitária para ajustar os pesos. Infelizmente, o Brasil não dispõe de um Censo de Imóveis, assim é necessário adaptar os pesos a partir do Censo Demográfico."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#a-teoria",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#a-teoria",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "",
    "text": "Um índice de preço convencional mensura a evolução do preço de um bem ou de uma cesta de bens ao longo do tempo. No caso mais simples, acompanha-se a dinâmica de um único bem, cujo preço no período \\(t\\) é dado por \\(p_{t}\\). Assim, temos um índice \\(I_t\\) dado por:\n\\[\nI_{t} = \\frac{p_{t}}{p_{t-1}}\\times 100\n\\]\nTipicamente, nosso interesse é de acompanhar uma cesta de bens. No caso do IPCA, indicador oficial da inflação, por exemplo, acomapanha-se um conjunto de bens que representa o “consumidor típico”. Assim temos \\(i\\) bens diferentes cada um com um preço \\(p_{i, t}\\). Neste post, vamos ignorar o problema (bastante complexo) de como agregar estes diferentes bens num índice único. Para todos efeitos vamos imaginar que existam pesos \\(w_i\\) imutáveis que representam o quão importante cada bem é para o “consumidor típico”. Assim temos: \\[\nI_{t} = \\frac{\\sum_{i \\in I}w_{i}p_{i,t}}{\\sum_{i \\in I}w_i p_{i, t-1}}\n\\]\nO índice assim é chamado de Índice de Laspeyres e é um dos índices mais comuns. O supracitado IPCA é um índice de Laspeyres, por exemplo. Apesar de simples, ele serve como base para se pensar índices de preços imobiliários1. Um índice de preços imobiliário mensura a tendência dos preços no mercado de imóveis. Em geral, costuma-se distinguir os mercados residencial e comercial devido às idissincracias de cada mercado. Neste post vamos tratar somente do mercado residencial.\nHá alguns desafios gerais relacionados à criação de um índice de preços imobiliário residencial (RPPI2). Os pontos gerais são listados abaixo\n\nImóveis são bens heterogêneos\nImóveis não são transacionados com frequência\nImóveis operam em submercados específicos\n\nAbaixo entro em mais detalhes sobre estes pontos. Uma revisão geral sobre a literatura de índices de preços imobiliários no contexto brasileiro está disponível no texto do IPEA, Índice de Preços para Imóveis. A referência geral e completa é o manual da Eurostat (2013), Handbook on Residential Property Price Indices.\n\n\nEm economia, há bens homogêneos (ou commodities) e bens heterogêneos. Pode-se pensar em bens homogêneos como bens em que é fácil encontrar um substituto; já bens heterogêneos são aqueles em que é muito difícil encontrar um substituto. Imóveis são um caso limite de bens heterogêneos. Quando se considera a localização como parte de seus atributos, cada imóvel é literalmente único, não existe nenhum outro equivalente no mundo. Mesmo quando se constroi apartamentos com plantas idênticas num mesmo prédio, pode-se ainda encontrar diferenças entre as unidades (seja o andar, a posição solar, etc.).\nO preço de bens heterogêneos reflete diferenças de qualidade. No caso de imóveis, pode-se pensar nos seus atributos estruturais (metragem, número de dormitórios, etc.) e locacionais (rua, bairro, proximidade a pontos de interesse, etc.). Faz sentido que a maior ou menor disponibilidade destes atributos influencie o preço do imóvel.\nPara entender como isto dificulta a estimação de um índice considere o caso de um imóvel abandonado que passa por uma significativa reforma. Naturalmente, o preço de venda deste imóvel aumenta várias vezes. Um índice de preços que identifica isto como um movimento dos preços em geral será enviesado.\nUma maneira de contornar isto é usando a modelagem hedônica. A modelagem ou precificação hedônica decompõe o preço de um imóvel em função de duas características observáveis. Assim, atribui-se um “preço” para cada uma das características do imóvel. Isto permite que se atribua corretamente qual parte do movimento dos preços é, de fato, um movimento de mercado.\n\n\n\nDescobrir o preço do arroz é uma tarefa tão simples como de olhar o valor etiquetado sobre a sua embalagem no mercado. No caso de imóveis isto é muito mais complicado. Todo imóvel tem um preço, mas apenas uma minúscula fração do estoque total de imóveis está “no mercado”, disponível para ser transacionado. Isto dificulta a aferição de preços.\nAlém disso, torna-se difícil comparar “laranjas com laranjas”. Considere o caso de um índice mensal. Dificilmente, os mesmos imóveis serão vendidos todos mês; pelo contrário, o caso mais comum seria observar, a cada novo mês, uma lista completamente nova de imóveis vendidos. Isto significa que o Índice vai estar comparando a evolução de preços de imóveis diferentes a cada mês.\nUma maneira de contornar isto é usando modelagem hedônica para precificar os imóveis que não estão à venda. Assim, seria possível prever o preço que um imóvel teria, caso ele tivesse sido vendido num determinado mês, o que permite fazer comparações adequadas. A qualidade desta precificação, por sua vez, depende da qualidade de dados disponíveis.\n\n\n\nIsto é outra maneira de dizer que nem todos os imóveis competem entre si. O mercado de imóveis se comporta similarmente a um mercado de mathcing onde as demandas dos clientes limitam a oferta relevante. Uma família de quatro pessoas dificilmente vai estar interessada em comprar um studio ou apartamento de 1 dormitório, qualquer que seja o seu preço. Na prática, isto significa que há vários mercados imobiliários; é possível que o mercado de casas de 4 dormitórios esteja em alta enquanto o mercado de apartamentos de 1d esteja em baixa. Similarmente, o mercado imobiliário da capital pode seguir um processo distinto do mercado imobiliário do interior.\nNovamente, uma estratégia para resolver isto é usar a modelagem hedônica. Outra estratégia é estratificar o índice para criar subíndices. Esta é uma prática bastante comum. Tipicamente, cria-se subíndices segundo algum critério estrutural (número de dormitórios ou tipologia) ou regional (capital x região metropolitana, interior)."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#resumo",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#resumo",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "",
    "text": "Existe uma literatura considerável sobre a criação de índices de preços. Desde a Grande Crise Financeira de 2008, há um interesse renovado no monitoramento do preço dos imóveis3.\nO atual consenso da literatura é de que um RPPI deve controlar pela qualidade dos imóveis observados (quality-adjusted index). Idealmente, a melhor metodologia é construir um índice hedônico com alguma estratégia de estratificação e imputação dupla. O único motivo para não usar um índice hedônico é quando não há informação suficiente para estimá-lo.\nA estratégia de imputação dupla permite maior flexibilidade na escolha da forma funcional do modelo hedônico. Escolhas comuns incluem: (1) regressão linear; (2) modelos de econometria espacial (SAR, SEM, etc.); e (3) modelos aditivos generalizados. Mais recentemente, alguns pesquisadores têm experimentado com modelos de aprendizado de máquina como de redes neurais. Em 2023, a Zillow revisou a metodologia do Zillow Home Value Index (ZHVI); o índice agora utiliza um modelo de redes neurais.\nEnquanto um Índice de Fisher (ou Tornqvist) apresenta propriedades ideais, nem sempre existe informação disponível para calculá-los. Assim, é natural utilizar um índice de Laspeyres junto com informação censitária para ajustar os pesos. Infelizmente, o Brasil não dispõe de um Censo de Imóveis, assim é necessário adaptar os pesos a partir do Censo Demográfico."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#ivg-r",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#ivg-r",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "IVG-R",
    "text": "IVG-R\nO IVG-R foi desenvolvido pelo Banco Central do Brasil a partir de laudos que são feitos durante o processo de financiamento de imóveis para pessoas físicas. Neste sentido, a base do IVG-R é muito rica pois abarca a totalidade dos imóveis financiados pelos sistema financeiro nacional. Evidentemente, isto implica que os imóveis comprados à vista não estão incluidos neste índice. Territorialmente, o índice considera as mesmas regiões metropolitanas do IPCA, o que garante boa representatividade nacional.\nEm termos metodológicos o IVGR é um índice de mediana estratificado. Essencialmente, verifica-se o preço mediano do imóvel financiado mês a mês e encadeia-se este valor de maneira a gerar um índice. A interpretação do índice é bastante simples e usa-se a mediana, ao invés da média, pois o preço de imóveis costuma ser fortemente assimétrico à direita. Na prática, alguns cuidados adicionais são feitos para reduzir a volatilidade do indicador. As estimativas mensais são geradas em janelas móveis de três meses (right-aligned), considerando o preço do mês atual e dos dois meses anteriores. A série final é suavizada pelo filtro HP e as séries regionais são agregadas considerando pesos do Censo do IBGE (2010).\nComo resultado, o IVG-R é um índice bastante suave. O IVG-R é o mais próximo que existe de um índice “oficial” de preços de imóveis no Brasil. Ele é o índice utilizado pelo BIS na compilação de índices de preços imobiliários."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#fipezap",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#fipezap",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "FipeZap",
    "text": "FipeZap\nO Índice FipeZap é um índice de mediana estratificado construído a partir de anúncios online de imóveis. Atualmente, há bastante apoio teórico e empírico para a construção de indicadores baseados em anúncios:\n\nO preço do anúncio é fortemente correlacionado com o preço de venda;\nA difusão da internet e a digitalização do mercado imobiliário garante que bases de anúncios tenham uma abrangência boa e uma temporalidade quase instantânea;\nO custo de construção de um índice de anúncios é baixíssimo quando comparado com outros métodos de coleta de dados."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#igmi-r",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#igmi-r",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "IGMI-R",
    "text": "IGMI-R\nO Índice Geral do Mercado Imobiliário Residencial é o primeiro índice de preços imobiliário hedônico do Brasil. A base de dados do índice é similar a do IVG-R mas o seu tratamento é mais sofisticado. Mais de 40 variáveis são utilizadas no modelo hedônico que ajuda a decompor o preço dos imóveis.\nO IGMI-R foi desenvolvido pelo economista Paulo Pichetti, numa parceria entre a FGV e a Abecip. Alguns detalhes sobre a metodologia do índice podem ser vistos nesta apresentação institucional. Atualmente, o IGMI-R é o índice mais preciso sobre o mercado de vendas no Brasil.\nA principal fragilidade do IGMI-R é o seu histórico curto. Além disso, o timing de criação do índice foi bastante infortuito, pois a série inicia em 2014. Nos seus breves 10 anos de história, o IGMI-R passou pela maior recessão econômica da história recente do país, uma pandemia global, dois ciclos de alta de juros e a menor taxa de juros da história."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#comparando-os-índices",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#comparando-os-índices",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "Comparando os índices",
    "text": "Comparando os índices\n\nQuadro Geral\nA tabela abaixo mostra a variação anual acumulada dos três. Para facilitar a contextualização, coloca-se também a variação do IPCA e do INCC.\n\n\n\n\n\n\n\n\nAno\nIGMI-R\nIVG-R\nFipeZap\nIPCA\nINCC\n\n\n\n\n2003\n—\n14.17\n—\n9.30\n14.42\n\n\n2004\n—\n11.24\n—\n7.60\n11.04\n\n\n2005\n—\n8.64\n—\n5.69\n6.83\n\n\n2006\n—\n13.66\n—\n3.14\n5.04\n\n\n2007\n—\n19.43\n—\n4.46\n6.16\n\n\n2008\n—\n23.73\n14.67\n5.90\n11.86\n\n\n2009\n—\n26.00\n21.13\n4.31\n3.25\n\n\n2010\n—\n23.40\n26.86\n5.91\n7.77\n\n\n2011\n—\n16.28\n26.32\n6.50\n7.48\n\n\n2012\n—\n10.08\n13.03\n5.84\n7.12\n\n\n2013\n—\n8.71\n13.74\n5.91\n8.09\n\n\n2014\n1.09\n5.02\n6.70\n6.41\n6.94\n\n\n2015\n−0.20\n−1.57\n1.32\n10.67\n7.49\n\n\n2016\n−2.26\n−2.83\n0.57\n6.29\n6.10\n\n\n2017\n−0.60\n−1.11\n−0.53\n2.95\n4.25\n\n\n2018\n0.64\n1.48\n−0.21\n3.75\n3.83\n\n\n2019\n4.11\n4.97\n0.00\n4.31\n4.14\n\n\n2020\n10.28\n8.95\n3.67\n4.52\n8.81\n\n\n2021\n16.25\n6.16\n5.29\n10.06\n13.84\n\n\n2022\n15.06\n1.76\n6.16\n5.78\n9.27\n\n\n2023\n7.97\n4.76\n5.13\n4.62\n3.49\n\n\n\n\n\n\n\n\n\nGráficos\nPara fazer uma comparação visual entre as séries é preciso definir um período comum para servir de base. No gráfico abaixo, ajusto as três séries em torno de seus valores médios em 2019. Vê-se como o comportamento das séries é muito similar nas séries históricas. Há uma divergência - cada vez maior - a partir de 2021 no período da pandemia. O IGMI-R registra um aumento significativo dos preços enquanto o IVG-R fica estagnado. Já o FipeZap registra uma alta muito mais tímida do que o IGMI-R.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo encadeia todas as séries a partir de 2014. Este gráfico facilita a comparação dos índices no período mais recente.\n\n\n\n\n\n\n\n\n\nO próximo gráfico contrasta os índices de preços imobiliárias com a taxa de inflação geral da economia.\n\n\n\n\n\n\n\n\n\nPor fim, compara-se a evolução dos índices com relação aos custos de construção. Novamente, os preços finais dos imóveis não acompanham o aumento nos custos da construção civil, ao menos dentro desta janela de análise.\n\n\n\n\n\n\n\n\n\n\n\nCorrelação e Cointegração\nEm termos mais técnicos, pode-se ver que as séries naturalmente são correlacionadas, visto que elas estão tentando mensurar a mesma variável. Assim, não deve ser surpreendente que as séries tenham autocorrelação cruzada e que sejam cointegradas. O gráfico abaixo mostra a função de autocorrelação entre as séries após se tomar uma diferença simples e uma diferença sazonal.\n\n\n\n\n\n\n\n\n\nMais a título de curiosidade reporta-se as estimativas dos coeficientes dos termos de ajustamento do modelo de correção de erros, normalizadas segundo a série do IGMI.\n\n\n\n\n\n\n\n\n\nect1\nect2\n\n\n\n\nIGMI.R.l3\n1.0000\n0.0000\n\n\nIVG.R.l3\n0.0000\n1.0000\n\n\nFipeZap.l3\n−2.9974\n−1.1851\n\n\ntrend.l3\n0.1589\n0.1554\n\n\n\n\n\n\n\n\n\nLongo Prazo\nA maior janela de análise possível é comparar o IVG-R com os demais índices de preço da economia. Este gráfico é interessante, pois mostra como o preço dos imóveis no Brasil cresceu acima tanto da inflação como dos custos quando se olha o horizonte mais longo, desde 2001. Estes resultados constrastam com o verificado nos últimos 14 anos, desde 2010, como apontei em outro post. Uma discussão interessante sobre estes três indicadores está disponível em Lima (2022) num texto de discussão do Núcleo de Real Estate da Poli-USP.\nA dinâmica de preços de imóveis no Brasil, em partes reflete a própria economia brasileira. O ciclo de crescimento de preços é interrompido em 2014 e só volta a crescer em 2018, ganhando força em 2020 com a baixa taxa de juros."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#footnotes",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#footnotes",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUma discussão mais aprofundada da teoria de números-índice está disponível em Hill & Melser (2008), especificamente no contexto de imóveis. Os autores concluem que os índices de Fisher e de Törnqvist apresentam as melhores propriedades. A escolha entre um ou outro depende da forma funcional da equação de regressão utilizada no modelo hedônico. Num modelo semilog (onde a variável dependente, i.e., preço do imóvel, está em log) o melhor índice é o de Törnqvist.↩︎\nDo inglês Residential Property Price Index.↩︎\nUma lista de justificativas e usos de índices de preços imobiliários pode ser vista em …↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "O IBGE recentemente atualizou as contagens de nascimento e óbitos no Brasil, no estudo períodico Estatísticas do Registro Civil. Esta base de dados estima o número total de nascidos vivos e de óbitos em cada município a cada ano. Os dados são bastante detalhados e permitem diversos tipos de análise. Já usei estes dados em outro post, onde mostrei como as divergências do Censo 2022 eram surpreendentes, visto que houve “houve cerca de 400 mil mortes a mais do que o projetado e 800 mil nascimentos a menos”.\n\n\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(showtext)\nlibrary(biscale)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nrecords &lt;- qs::qread(\"...\")\nrec &lt;- as_tibble(st_drop_geometry(records))\n\nPode-se importar o shapefile dos estados via geobr.\n\nstate_border &lt;- geobr::read_state(showProgress = FALSE)\ndim_state &lt;- as_tibble(st_drop_geometry(state_border))\n\ndim_state &lt;- dim_state |&gt; \n  mutate(name_state = stringr::str_replace(\n    name_state,\n    \"Espirito Santo\",\n    \"Espírito Santo\")\n    )\n\n# Aggregate data by state\nstate_records &lt;- rec |&gt; \n  summarise(across(population:births, ~sum(.x, na.rm = TRUE)), .by = \"abbrev_state\") |&gt; \n  mutate(cbr = births / population * 1000, cdr = deaths / population * 1000)\n\nstate_pop &lt;- left_join(state_border, state_records, by = \"abbrev_state\")\n\n\n\nQuando se olha para a tendência dentro de cada estado, vê-se uma divisão regional. Os estados do Norte têm taxas de natalidade elevadas e taxas de mortalidade baixas. O Centro-Oeste tem TBN e TBM moderadas, com exceção do Distrito Federal, que tem ambas as taxas baixas. Já o litoral do país apresenta taxas de mortalidade moderadas ou elevadas e baixas taxas de natalidade. Isto é particularmente relevante, visto que a maior parte da população brasileira mora nestes estados.\nÉ importante notar também que o Nordeste começa a exibir padrões demográficos similares a do Sul e do Sudeste, apesar de ter um renda per capita significativamente menor. Isto tende a se traduzir em desafios ainda maiores para o desenvolvimento econômico da região.\n\n\nCode\nrates &lt;- bi_class(state_pop, cdr, cbr, style = \"jenks\", dim = 3)\n\npal &lt;- \"BlueOr\"\n\np_map &lt;- ggplot(rates) +\n  geom_sf(aes(fill = bi_class)) +\n  bi_scale_fill(pal = pal, dim = 3) +\n  guides(fill = \"none\") +\n  coord_sf(xlim = c(NA, -36.25)) +\n  theme_void()\n\np_legend &lt;- bi_legend(\n  pal = pal,\n  dim = 3,\n  xlab = \"Deaths\",\n  ylab = \"Births\",\n  size = 8)\n\nstate_map &lt;- p_map + inset_element(p_legend, 0, 0, 0.4, 0.4)\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nstate_map &lt;- state_map + plot_annotation(\n  title = \"Nascimentos e Mortes\",\n  subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n  theme = theme_map\n  )\n\nstate_map\n\n\n\n\n\n\n\n\n\nDe modo geral, a maior parte dos municípios continua apresentando números maiores de nascimentos do que óbitos. Cerca de 84% dos municípios tem taxas brutas de natalidade (TBN) superiores às suas respectivas taxas brutas de mortalidade (TBM). O gráfico abaixo mostra a dispersão entre a taxa bruta de natalidade, por mil habitantes, contra a taxa bruta de mortalidade, por mil habitantes. Outliers são removidos para facilitar a leitura dos dados.\nOs municípios abaixo da linha laranja apresentam TBM maior do que TBN, isto é, são municípios onde se registram mais óbitos do que nascimentos. O contrário é válido para os municípios acima da linha: nos municípios acima da linha laranja há mais nascimentos do que óbitos. Isto, de fato, reflete o acelerado envelhecimento da população, como mostrei em outro post.\n\n\nCode\nsub &lt;- rec |&gt; \n  filter(cdr &lt;= 20, cbr &lt;= 30)\n\nplot_scatter &lt;- ggplot(sub, aes(cdr, cbr)) +\n  geom_point(\n    aes(size = sqrt(population)),\n    alpha = 0.5,\n    shape = 21,\n    color = \"#023047\"\n    ) +\n  geom_abline(slope = 1, intercept = 0, lwd = 1, color = \"#fb8500\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_text(\n    data = tibble(x = 17.5, y = 3, label = \"Mais Óbitos\\ndo que\\nNascimentos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = tibble(x = 17.5, y = 25.25, label = \"Mais Nasc.\\ndo que\\nÓbitos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Nascimentos e Óbitos\",\n    subtitle = stringr::str_wrap(\"Taxa Bruta de Nascimentos e Óbitos por mil habitantes por município no Brasil (2022). Tamanho do círculo proporcional à população do município.\", 86),\n    x = \"Óbitos (p/ 1.000 habitantes)\",\n    y = \"Nascimentos (p/ 1.000 habitantes)\"\n  ) +\n  theme_minimal(base_family = \"Lato\")\n\nplot_scatter\n\n\n\n\n\n\n\n\n\nA tabela abaixo apresenta os principais municípios com baixas taxas de natalidade e altas taxas de mortalidade. Vê-se que municípios de médio porte, do interior do Rio Grande do Sul, predominam na lista. É o caso de cidade como São Borja, São Gabriel, Bagé e Vacaria.\n\n\n\n\n\n\n\n\nNome\nUF\nTaxa por mil\nAbsoluto\nPopulação\n\n\nNascimentos\nÓbitos\nNascimentos\nÓbitos\n\n\n\n\nCachoeira do Sul\nRS\n11,02\n12,71\n882\n1.018\n80.070\n\n\nPetrópolis\nRJ\n11,28\n11,68\n3.146\n3.258\n278.881\n\n\nSão Gabriel\nRS\n11,93\n11,61\n698\n679\n58.487\n\n\nSantos\nSP\n9,09\n11,56\n3.807\n4.838\n418.608\n\n\nRio Grande\nRS\n10,79\n11,43\n2.070\n2.194\n191.900\n\n\nSão Borja\nRS\n11,16\n11,43\n666\n682\n59.676\n\n\nTupã\nSP\n9,21\n11,42\n589\n730\n63.928\n\n\nPenápolis\nSP\n10,96\n11,30\n676\n697\n61.679\n\n\nValença\nRJ\n10,43\n11,26\n710\n767\n68.088\n\n\nVacaria\nRS\n13,89\n11,06\n892\n710\n64.197\n\n\nBagé\nRS\n11,22\n11,00\n1.323\n1.297\n117.938\n\n\nCamaquã\nRS\n12,09\n10,95\n752\n681\n62.200\n\n\nAlegrete\nRS\n10,26\n10,92\n743\n791\n72.409\n\n\nPeruíbe\nSP\n13,56\n10,91\n927\n746\n68.352\n\n\nCataguases\nMG\n9,34\n10,90\n619\n722\n66.261\n\n\n\n\n\n\n\nA taxa bruta de natalidade é uma boa proxy para a taxa de fecundidade e a sua redução, de fato, indica menor crescimento demográfico. A tabela abaixo mostra os principais municípios com TBN elevada. Todos os municípios listados estão na região Norte e os estados do Pará e, sobretudo, do Amazonas predominam na lista.\n\n\n\n\n\n\n\n\nNome\nUF\nTaxa por mil\nAbsoluto\nPopulação\n\n\nNascimentos\nÓbitos\nNascimentos\nÓbitos\n\n\n\n\nSão Gabriel da Cachoeira\nAM\n31,43\n4,13\n1.628\n214\n51.795\n\n\nPortel\nPA\n31,13\n2,14\n1.946\n134\n62.503\n\n\nBreves\nPA\n27,34\n2,52\n2.924\n270\n106.968\n\n\nTabatinga\nAM\n25,57\n4,46\n1.707\n298\n66.764\n\n\nMaués\nAM\n24,97\n4,02\n1.528\n246\n61.204\n\n\nCoari\nAM\n23,52\n4,84\n1.661\n342\n70.616\n\n\nTomé-Açu\nPA\n23,26\n3,79\n1.572\n256\n67.585\n\n\nSantana\nAP\n23,11\n4,00\n2.487\n431\n107.618\n\n\nJuruti\nPA\n22,64\n3,38\n1.152\n172\n50.881\n\n\nTefé\nAM\n21,79\n3,24\n1.605\n239\n73.669\n\n\nBuriticupu\nMA\n21,55\n3,77\n1.196\n209\n55.499\n\n\nOriximiná\nPA\n21,30\n4,61\n1.455\n315\n68.294\n\n\nTailândia\nPA\n21,12\n4,39\n1.531\n318\n72.493\n\n\nGrajaú\nMA\n21,12\n5,21\n1.560\n385\n73.872\n\n\nParintins\nAM\n21,06\n4,70\n2.030\n453\n96.372\n\n\n\n\n\n\n\n\n\n\n\nComo visto, a dinâmica populacional varia de região para região e até de estado para estado. O mapa abaixo normaliza a taxa de crescimento natural para o Rio Grande do Sul. Aqui, define-se que a TCN é a diferença entre a TBN e a TBM, isto é\n\\[\n\\text{TCN} = \\text{TBN} - \\text{TBM} = \\frac{N}{P}\\times1.000 - \\frac{O}{P}\\times1.000 = 1.000 (\\frac{N - O}{P})\n\\] onde \\(N\\) é o número total de nascidos vivos no ano, \\(O\\) é o número total de óbitos registrados no ano e \\(P\\) é a contagem total da população no ano.\nO gráfico abaixo mostra a distribuição da TCN entre os municípios do Rio Grande do Sul em contraste com os demais municípios do Brasil. Nota-se que o formato da distribuição é relativamente similar; contudo, a distribuição da TCN está deslocada à esquerda no gráfico, indicando que os municípios gaúchos tem TCN menores.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a distribuição espacial da TCN no Rio Grande do Sul. Os dados são normalizados e as cores destacam os municípios que estão “nas pontas” da distribuição. A distância é mensurada em termos de desvios-padrão.\nOs municípios em cinza apresentam TCN próximas à média do estado; já os municípios em vermelho apresentam TCN abaixo da média. Nota-se que o Centro-Sul do estado possui praticamente nenhum município com TCN acima da média. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul, indicando TCN positivas.\n\n\n\n\n\n\n\n\n\nPode-se olhar também para a distribuição simulatânea da TBN e da TBM no estado. A função abaixo implementa uma maneira fácil de montar esta visualização para qualquer estado do Brasil.\n\n\nCode\navailable_states &lt;- unique(dim_state$abbrev_state)\navailable_states &lt;- available_states[order(available_states)]\navailable_states &lt;- paste(available_states, collapse = \", \")\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nplot_biscale &lt;- function(\n    state,\n    t = 0.35,\n    r = 0.35,\n    b = 0,\n    l = 0,\n    pal = \"BlueOr\",\n    theme = theme_map\n    ) {\n  \n  if (is.numeric(state) && nchar(state) == 2) {\n    state &lt;- dplyr::filter(records, code_state == state)$abbrev_state\n  }\n  \n  if (length(state) == 0) {\n    stop(\"Argument `state` must be one of: \", available_states, \".\")\n  }\n  \n  sub &lt;- dplyr::filter(records, abbrev_state == state)\n  rates &lt;- bi_class(sub, cdr, cbr, style = \"jenks\", dim = 3)\n\n  p_map &lt;- ggplot(rates) +\n    geom_sf(aes(fill = bi_class)) +\n    bi_scale_fill(pal = pal, dim = 3) +\n    guides(fill = \"none\") +\n    theme_void()\n\n  p_legend &lt;- bi_legend(\n    pal = pal,\n    dim = 3,\n    xlab = \"Deaths\",\n    ylab = \"Births\",\n    size = 8)\n\n  p_map &lt;- p_map + inset_element(p_legend, left = l, bottom = b, right = r, top = t)\n  \n  p_map &lt;- p_map + plot_annotation(\n    title = stringr::str_glue(\"Nascimentos e Mortes ({state})\"),\n    subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n    theme = theme_map\n    )\n  \n  return(p_map)\n  \n}\n\n\n\n\nOlhando para o mapa, vê-se que a região Centro-Sul do estado apresenta taxas de mortalidade moderadas ou elevadas. Aqui, os municípios em laranja são os mais preocupantes, por apresentar simultaneamente mortalidade elevada e natalidade baixa. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul claro e azul escuro, indicando TBN moderada ou elevada e TBM baixa.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNascimentos no Brasil\nEnvelhecimento no Brasil"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html#análise-de-dados",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html#análise-de-dados",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "library(dplyr)\nlibrary(sf)\nlibrary(showtext)\nlibrary(biscale)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nrecords &lt;- qs::qread(\"...\")\nrec &lt;- as_tibble(st_drop_geometry(records))\n\nPode-se importar o shapefile dos estados via geobr.\n\nstate_border &lt;- geobr::read_state(showProgress = FALSE)\ndim_state &lt;- as_tibble(st_drop_geometry(state_border))\n\ndim_state &lt;- dim_state |&gt; \n  mutate(name_state = stringr::str_replace(\n    name_state,\n    \"Espirito Santo\",\n    \"Espírito Santo\")\n    )\n\n# Aggregate data by state\nstate_records &lt;- rec |&gt; \n  summarise(across(population:births, ~sum(.x, na.rm = TRUE)), .by = \"abbrev_state\") |&gt; \n  mutate(cbr = births / population * 1000, cdr = deaths / population * 1000)\n\nstate_pop &lt;- left_join(state_border, state_records, by = \"abbrev_state\")\n\n\n\nQuando se olha para a tendência dentro de cada estado, vê-se uma divisão regional. Os estados do Norte têm taxas de natalidade elevadas e taxas de mortalidade baixas. O Centro-Oeste tem TBN e TBM moderadas, com exceção do Distrito Federal, que tem ambas as taxas baixas. Já o litoral do país apresenta taxas de mortalidade moderadas ou elevadas e baixas taxas de natalidade. Isto é particularmente relevante, visto que a maior parte da população brasileira mora nestes estados.\nÉ importante notar também que o Nordeste começa a exibir padrões demográficos similares a do Sul e do Sudeste, apesar de ter um renda per capita significativamente menor. Isto tende a se traduzir em desafios ainda maiores para o desenvolvimento econômico da região.\n\n\nCode\nrates &lt;- bi_class(state_pop, cdr, cbr, style = \"jenks\", dim = 3)\n\npal &lt;- \"BlueOr\"\n\np_map &lt;- ggplot(rates) +\n  geom_sf(aes(fill = bi_class)) +\n  bi_scale_fill(pal = pal, dim = 3) +\n  guides(fill = \"none\") +\n  coord_sf(xlim = c(NA, -36.25)) +\n  theme_void()\n\np_legend &lt;- bi_legend(\n  pal = pal,\n  dim = 3,\n  xlab = \"Deaths\",\n  ylab = \"Births\",\n  size = 8)\n\nstate_map &lt;- p_map + inset_element(p_legend, 0, 0, 0.4, 0.4)\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nstate_map &lt;- state_map + plot_annotation(\n  title = \"Nascimentos e Mortes\",\n  subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n  theme = theme_map\n  )\n\nstate_map\n\n\n\n\n\n\n\n\n\nDe modo geral, a maior parte dos municípios continua apresentando números maiores de nascimentos do que óbitos. Cerca de 84% dos municípios tem taxas brutas de natalidade (TBN) superiores às suas respectivas taxas brutas de mortalidade (TBM). O gráfico abaixo mostra a dispersão entre a taxa bruta de natalidade, por mil habitantes, contra a taxa bruta de mortalidade, por mil habitantes. Outliers são removidos para facilitar a leitura dos dados.\nOs municípios abaixo da linha laranja apresentam TBM maior do que TBN, isto é, são municípios onde se registram mais óbitos do que nascimentos. O contrário é válido para os municípios acima da linha: nos municípios acima da linha laranja há mais nascimentos do que óbitos. Isto, de fato, reflete o acelerado envelhecimento da população, como mostrei em outro post.\n\n\nCode\nsub &lt;- rec |&gt; \n  filter(cdr &lt;= 20, cbr &lt;= 30)\n\nplot_scatter &lt;- ggplot(sub, aes(cdr, cbr)) +\n  geom_point(\n    aes(size = sqrt(population)),\n    alpha = 0.5,\n    shape = 21,\n    color = \"#023047\"\n    ) +\n  geom_abline(slope = 1, intercept = 0, lwd = 1, color = \"#fb8500\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_text(\n    data = tibble(x = 17.5, y = 3, label = \"Mais Óbitos\\ndo que\\nNascimentos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = tibble(x = 17.5, y = 25.25, label = \"Mais Nasc.\\ndo que\\nÓbitos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Nascimentos e Óbitos\",\n    subtitle = stringr::str_wrap(\"Taxa Bruta de Nascimentos e Óbitos por mil habitantes por município no Brasil (2022). Tamanho do círculo proporcional à população do município.\", 86),\n    x = \"Óbitos (p/ 1.000 habitantes)\",\n    y = \"Nascimentos (p/ 1.000 habitantes)\"\n  ) +\n  theme_minimal(base_family = \"Lato\")\n\nplot_scatter\n\n\n\n\n\n\n\n\n\nA tabela abaixo apresenta os principais municípios com baixas taxas de natalidade e altas taxas de mortalidade. Vê-se que municípios de médio porte, do interior do Rio Grande do Sul, predominam na lista. É o caso de cidade como São Borja, São Gabriel, Bagé e Vacaria.\n\n\n\n\n\n\n\n\nNome\nUF\nTaxa por mil\nAbsoluto\nPopulação\n\n\nNascimentos\nÓbitos\nNascimentos\nÓbitos\n\n\n\n\nCachoeira do Sul\nRS\n11,02\n12,71\n882\n1.018\n80.070\n\n\nPetrópolis\nRJ\n11,28\n11,68\n3.146\n3.258\n278.881\n\n\nSão Gabriel\nRS\n11,93\n11,61\n698\n679\n58.487\n\n\nSantos\nSP\n9,09\n11,56\n3.807\n4.838\n418.608\n\n\nRio Grande\nRS\n10,79\n11,43\n2.070\n2.194\n191.900\n\n\nSão Borja\nRS\n11,16\n11,43\n666\n682\n59.676\n\n\nTupã\nSP\n9,21\n11,42\n589\n730\n63.928\n\n\nPenápolis\nSP\n10,96\n11,30\n676\n697\n61.679\n\n\nValença\nRJ\n10,43\n11,26\n710\n767\n68.088\n\n\nVacaria\nRS\n13,89\n11,06\n892\n710\n64.197\n\n\nBagé\nRS\n11,22\n11,00\n1.323\n1.297\n117.938\n\n\nCamaquã\nRS\n12,09\n10,95\n752\n681\n62.200\n\n\nAlegrete\nRS\n10,26\n10,92\n743\n791\n72.409\n\n\nPeruíbe\nSP\n13,56\n10,91\n927\n746\n68.352\n\n\nCataguases\nMG\n9,34\n10,90\n619\n722\n66.261\n\n\n\n\n\n\n\nA taxa bruta de natalidade é uma boa proxy para a taxa de fecundidade e a sua redução, de fato, indica menor crescimento demográfico. A tabela abaixo mostra os principais municípios com TBN elevada. Todos os municípios listados estão na região Norte e os estados do Pará e, sobretudo, do Amazonas predominam na lista.\n\n\n\n\n\n\n\n\nNome\nUF\nTaxa por mil\nAbsoluto\nPopulação\n\n\nNascimentos\nÓbitos\nNascimentos\nÓbitos\n\n\n\n\nSão Gabriel da Cachoeira\nAM\n31,43\n4,13\n1.628\n214\n51.795\n\n\nPortel\nPA\n31,13\n2,14\n1.946\n134\n62.503\n\n\nBreves\nPA\n27,34\n2,52\n2.924\n270\n106.968\n\n\nTabatinga\nAM\n25,57\n4,46\n1.707\n298\n66.764\n\n\nMaués\nAM\n24,97\n4,02\n1.528\n246\n61.204\n\n\nCoari\nAM\n23,52\n4,84\n1.661\n342\n70.616\n\n\nTomé-Açu\nPA\n23,26\n3,79\n1.572\n256\n67.585\n\n\nSantana\nAP\n23,11\n4,00\n2.487\n431\n107.618\n\n\nJuruti\nPA\n22,64\n3,38\n1.152\n172\n50.881\n\n\nTefé\nAM\n21,79\n3,24\n1.605\n239\n73.669\n\n\nBuriticupu\nMA\n21,55\n3,77\n1.196\n209\n55.499\n\n\nOriximiná\nPA\n21,30\n4,61\n1.455\n315\n68.294\n\n\nTailândia\nPA\n21,12\n4,39\n1.531\n318\n72.493\n\n\nGrajaú\nMA\n21,12\n5,21\n1.560\n385\n73.872\n\n\nParintins\nAM\n21,06\n4,70\n2.030\n453\n96.372"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html#mapas",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html#mapas",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "Como visto, a dinâmica populacional varia de região para região e até de estado para estado. O mapa abaixo normaliza a taxa de crescimento natural para o Rio Grande do Sul. Aqui, define-se que a TCN é a diferença entre a TBN e a TBM, isto é\n\\[\n\\text{TCN} = \\text{TBN} - \\text{TBM} = \\frac{N}{P}\\times1.000 - \\frac{O}{P}\\times1.000 = 1.000 (\\frac{N - O}{P})\n\\] onde \\(N\\) é o número total de nascidos vivos no ano, \\(O\\) é o número total de óbitos registrados no ano e \\(P\\) é a contagem total da população no ano.\nO gráfico abaixo mostra a distribuição da TCN entre os municípios do Rio Grande do Sul em contraste com os demais municípios do Brasil. Nota-se que o formato da distribuição é relativamente similar; contudo, a distribuição da TCN está deslocada à esquerda no gráfico, indicando que os municípios gaúchos tem TCN menores.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a distribuição espacial da TCN no Rio Grande do Sul. Os dados são normalizados e as cores destacam os municípios que estão “nas pontas” da distribuição. A distância é mensurada em termos de desvios-padrão.\nOs municípios em cinza apresentam TCN próximas à média do estado; já os municípios em vermelho apresentam TCN abaixo da média. Nota-se que o Centro-Sul do estado possui praticamente nenhum município com TCN acima da média. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul, indicando TCN positivas.\n\n\n\n\n\n\n\n\n\nPode-se olhar também para a distribuição simulatânea da TBN e da TBM no estado. A função abaixo implementa uma maneira fácil de montar esta visualização para qualquer estado do Brasil.\n\n\nCode\navailable_states &lt;- unique(dim_state$abbrev_state)\navailable_states &lt;- available_states[order(available_states)]\navailable_states &lt;- paste(available_states, collapse = \", \")\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nplot_biscale &lt;- function(\n    state,\n    t = 0.35,\n    r = 0.35,\n    b = 0,\n    l = 0,\n    pal = \"BlueOr\",\n    theme = theme_map\n    ) {\n  \n  if (is.numeric(state) && nchar(state) == 2) {\n    state &lt;- dplyr::filter(records, code_state == state)$abbrev_state\n  }\n  \n  if (length(state) == 0) {\n    stop(\"Argument `state` must be one of: \", available_states, \".\")\n  }\n  \n  sub &lt;- dplyr::filter(records, abbrev_state == state)\n  rates &lt;- bi_class(sub, cdr, cbr, style = \"jenks\", dim = 3)\n\n  p_map &lt;- ggplot(rates) +\n    geom_sf(aes(fill = bi_class)) +\n    bi_scale_fill(pal = pal, dim = 3) +\n    guides(fill = \"none\") +\n    theme_void()\n\n  p_legend &lt;- bi_legend(\n    pal = pal,\n    dim = 3,\n    xlab = \"Deaths\",\n    ylab = \"Births\",\n    size = 8)\n\n  p_map &lt;- p_map + inset_element(p_legend, left = l, bottom = b, right = r, top = t)\n  \n  p_map &lt;- p_map + plot_annotation(\n    title = stringr::str_glue(\"Nascimentos e Mortes ({state})\"),\n    subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n    theme = theme_map\n    )\n  \n  return(p_map)\n  \n}\n\n\n\n\nOlhando para o mapa, vê-se que a região Centro-Sul do estado apresenta taxas de mortalidade moderadas ou elevadas. Aqui, os municípios em laranja são os mais preocupantes, por apresentar simultaneamente mortalidade elevada e natalidade baixa. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul claro e azul escuro, indicando TBN moderada ou elevada e TBM baixa."
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html#posts-relacionados",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html#posts-relacionados",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "Nascimentos no Brasil\nEnvelhecimento no Brasil"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "Web scraping consists of extracting information from a web page. The difficulty or ease of extracting this information depends on how well the page is constructed. In more complex cases, the information may be behind a captcha or within an interactive panel that depends on user input.\nIn this simple example, I will show how to find the locations of all Starbucks stores in Brazil. The full list of active Starbucks stores can be found on the Starbucks Brasil website. As usual, we will use the tidyverse along with the rvest and xml2 packages.\n\n\n\nlibrary(rvest)   # Used for web scraping and extracting HTML content\nlibrary(xml2)    # Helps in working with XML and HTML data\nlibrary(tidyverse)\n\n\n\n\nThe full list of active Starbucks stores can be found on the Starbucks Brasil website. To read the page, we use read_html.\n\nurl &lt;- \"https://starbucks.com.br/lojas\"\npage &lt;- xml2::read_html(url)   # Fetches the HTML content of the page\n\nThe “xpath” shows the path to a specific element on the page. For example, to find the Starbucks logo in the top-left corner of the page, we can use the following code:\n\n# Extracts the HTML element for the logo image\npage %&gt;%\n  html_element(xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\")\n\n{html_node}\n&lt;img alt=\"Starbucks Logo\" src=\"/public/img/icons/starbucks-nav-logo.svg\"&gt;\n\n\nTo learn more about xpaths, you can consult this cheatsheet.\nIn general, on well-constructed pages, the name of elements will be quite self-explanatory. In the case above, the alt attribute already indicates that the object is the Starbucks logo, and the src links to an image file in svg format called starbucks-nav-logo. Unfortunately, this won’t always be the case. On some pages, elements can be quite confusing.\nTo extract a specific attribute, we use the html_attr function.\n\npage %&gt;%\n  html_element(\n    xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\"\n    ) %&gt;%\n  # Extracts the \"src\" attribute (URL to the image)\n  html_attr(\"src\")  \n\n[1] \"/public/img/icons/starbucks-nav-logo.svg\"\n\n\nIf you combine this last link with “www.starbucks.com.br”, you should arrive at an image of the company’s logo1.\n\n\n\nStarbucks logo\n\n\nTo find the big list of stores in the left panel, we will take advantage of the fact that the div holding this list has a unique class called \"place-list\". It’s easy to verify this directly in your browser. If you use Chrome, for instance, just right-click on the panel and click on Inspect.\n\n\n\n\n\n\n\n\n\n\nAs I mentioned above, things aren’t always well organized. Note that since we want to extract multiple elements and multiple (all) attributes, we use the variants: html_elements and html_attrs.\n\nlist_attr &lt;- page %&gt;%\n  # Selects all divs under \"place-list\" that hold store info\n  html_elements(xpath = '//div[@class=\"place-list\"]/div')  %&gt;%\n  # Extracts all attributes of the selected elements\n  html_attrs()  \n\nThe extracted object is a list where each element is a text vector containing the following information. We have the store name, latitude/longitude, and the address.\n\n# Extracts the first store's information from the list\npluck(list_attr, 1)\n\n                  class           data-latitude          data-longitude \n\"place-item r-place-15\"           \"-23.5658059\"           \"-46.6508012\" \n              data-name             data-street              data-index \n  \"Shopping Top Center\" \"Avenida Paulista, 854\"                     \"0\" \n\n\nAt this point, the web scraping process is complete. Once again, the process was easy because the data is well structured on the Starbucks page. Now, we just need to clean the data.\n\n\n\n\nI won’t go in depth about the data cleaning process. Basically, we need to convert each element of the list into a data.frame, stack the results, and then adjust the data types of each column.\n\n# Convert the elements into data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Stack the results\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Rename the columns\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Select the columns of interest\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng to numeric\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nThe final dataset is presented below\n\n\n\n\n\n\n\n\n\nThe table above is already in a pretty satisfactory format. We can check the data by building a simple map.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nIt’s worth noting that data extracted via web scraping almost always contains some noise. In this case, the data seems relatively clean after a bit of processing. The addresses are not always very informative, like in the case of “Rodovia Hélio Smidt, S/N,” but this happens because many stores are located inside hospitals, shopping malls, or airports.\nWith this data, we can already perform interesting analyses. For example, we can find out that there are five Starbucks stores just on Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 4 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n3     2 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n4     5 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2 (-46.64809 -23.56784)\n3  (-46.65438 -23.5631)\n4 (-46.65895 -23.55785)\n\n\nWe can also count the number of stores in each airport. Apparently, there are eight stores at Guarulhos Airport, which seems like quite a high number to me.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 Brasília          3\n2 GRU               3\n3 Confins           2\n4 Galeão            2\n5 Viracopos         2\n6 Congonhas         1\n7 Curitiba          1\n8 Florianópolis     1\n9 Santos            1\n\n\nFinally, we can note that many Starbucks stores are located inside shopping malls. A simple calculation shows that around 75 stores are located inside malls, close to 50% of the total units2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 58"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#web-scraping",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#web-scraping",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "Web scraping consists of extracting information from a web page. The difficulty or ease of extracting this information depends on how well the page is constructed. In more complex cases, the information may be behind a captcha or within an interactive panel that depends on user input.\nIn this simple example, I will show how to find the locations of all Starbucks stores in Brazil. The full list of active Starbucks stores can be found on the Starbucks Brasil website. As usual, we will use the tidyverse along with the rvest and xml2 packages.\n\n\n\nlibrary(rvest)   # Used for web scraping and extracting HTML content\nlibrary(xml2)    # Helps in working with XML and HTML data\nlibrary(tidyverse)\n\n\n\n\nThe full list of active Starbucks stores can be found on the Starbucks Brasil website. To read the page, we use read_html.\n\nurl &lt;- \"https://starbucks.com.br/lojas\"\npage &lt;- xml2::read_html(url)   # Fetches the HTML content of the page\n\nThe “xpath” shows the path to a specific element on the page. For example, to find the Starbucks logo in the top-left corner of the page, we can use the following code:\n\n# Extracts the HTML element for the logo image\npage %&gt;%\n  html_element(xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\")\n\n{html_node}\n&lt;img alt=\"Starbucks Logo\" src=\"/public/img/icons/starbucks-nav-logo.svg\"&gt;\n\n\nTo learn more about xpaths, you can consult this cheatsheet.\nIn general, on well-constructed pages, the name of elements will be quite self-explanatory. In the case above, the alt attribute already indicates that the object is the Starbucks logo, and the src links to an image file in svg format called starbucks-nav-logo. Unfortunately, this won’t always be the case. On some pages, elements can be quite confusing.\nTo extract a specific attribute, we use the html_attr function.\n\npage %&gt;%\n  html_element(\n    xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\"\n    ) %&gt;%\n  # Extracts the \"src\" attribute (URL to the image)\n  html_attr(\"src\")  \n\n[1] \"/public/img/icons/starbucks-nav-logo.svg\"\n\n\nIf you combine this last link with “www.starbucks.com.br”, you should arrive at an image of the company’s logo1.\n\n\n\nStarbucks logo\n\n\nTo find the big list of stores in the left panel, we will take advantage of the fact that the div holding this list has a unique class called \"place-list\". It’s easy to verify this directly in your browser. If you use Chrome, for instance, just right-click on the panel and click on Inspect.\n\n\n\n\n\n\n\n\n\n\nAs I mentioned above, things aren’t always well organized. Note that since we want to extract multiple elements and multiple (all) attributes, we use the variants: html_elements and html_attrs.\n\nlist_attr &lt;- page %&gt;%\n  # Selects all divs under \"place-list\" that hold store info\n  html_elements(xpath = '//div[@class=\"place-list\"]/div')  %&gt;%\n  # Extracts all attributes of the selected elements\n  html_attrs()  \n\nThe extracted object is a list where each element is a text vector containing the following information. We have the store name, latitude/longitude, and the address.\n\n# Extracts the first store's information from the list\npluck(list_attr, 1)\n\n                  class           data-latitude          data-longitude \n\"place-item r-place-15\"           \"-23.5658059\"           \"-46.6508012\" \n              data-name             data-street              data-index \n  \"Shopping Top Center\" \"Avenida Paulista, 854\"                     \"0\" \n\n\nAt this point, the web scraping process is complete. Once again, the process was easy because the data is well structured on the Starbucks page. Now, we just need to clean the data."
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#data-cleaning",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#data-cleaning",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "I won’t go in depth about the data cleaning process. Basically, we need to convert each element of the list into a data.frame, stack the results, and then adjust the data types of each column.\n\n# Convert the elements into data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Stack the results\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Rename the columns\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Select the columns of interest\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng to numeric\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nThe final dataset is presented below"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#map",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#map",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "The table above is already in a pretty satisfactory format. We can check the data by building a simple map.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nIt’s worth noting that data extracted via web scraping almost always contains some noise. In this case, the data seems relatively clean after a bit of processing. The addresses are not always very informative, like in the case of “Rodovia Hélio Smidt, S/N,” but this happens because many stores are located inside hospitals, shopping malls, or airports.\nWith this data, we can already perform interesting analyses. For example, we can find out that there are five Starbucks stores just on Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 4 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n3     2 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n4     5 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2 (-46.64809 -23.56784)\n3  (-46.65438 -23.5631)\n4 (-46.65895 -23.55785)\n\n\nWe can also count the number of stores in each airport. Apparently, there are eight stores at Guarulhos Airport, which seems like quite a high number to me.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 Brasília          3\n2 GRU               3\n3 Confins           2\n4 Galeão            2\n5 Viracopos         2\n6 Congonhas         1\n7 Curitiba          1\n8 Florianópolis     1\n9 Santos            1\n\n\nFinally, we can note that many Starbucks stores are located inside shopping malls. A simple calculation shows that around 75 stores are located inside malls, close to 50% of the total units2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 58"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#finding-each-city",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#finding-each-city",
    "title": "Finding All Starbucks in Brazil",
    "section": "Finding each city",
    "text": "Finding each city\nFrom this data, we can add more information. Using the geobr package, we can identify in which cities the stores are located.\n\ndim_city &lt;- geobr::read_municipality(showProgress = FALSE)\ndim_city &lt;- st_transform(dim_city, crs = 4326)\nsf::sf_use_s2(FALSE)\n\nstarbucks &lt;- starbucks %&gt;%\n  st_join(dim_city) %&gt;%\n  relocate(c(name_muni, abbrev_state), .before = lat)\n\nNow we can see which cities have the most Starbucks locations. São Paulo alone has more the 40 units.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name_muni, abbrev_state, sort = TRUE) \n\n# A tibble: 38 × 3\n   name_muni      abbrev_state     n\n   &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;\n 1 São Paulo      SP              34\n 2 Rio De Janeiro RJ               9\n 3 Curitiba       PR               7\n 4 Brasília       DF               6\n 5 Campinas       SP               6\n 6 Guarulhos      SP               4\n 7 Jundiaí        SP               4\n 8 Porto Alegre   RS               4\n 9 Ribeirão Preto SP               3\n10 Confins        MG               2\n# ℹ 28 more rows"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#google-places",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#google-places",
    "title": "Finding All Starbucks in Brazil",
    "section": "Google Places",
    "text": "Google Places\n\nAdding information using Google Places API\nThe Google Places API allows access to data from Google Maps. The googleway package integrates this data into R already in tidy format.\n\nlibrary(googleway)\n\nI’ll create a simple search to return all Starbucks locations in Brazil. A full search across the entire country would take too long, so I will use the coordinates I found via web scraping as a starting point.\nThe function below searches for the term “starbucks” at all the points I provide. To simplify, the function returns only a few of the columns.\n\n# Function to grab starbucks info\nget_starbucks_info &lt;- function(lat, lng) {\n  \n  # Search for 'Starbucks' using the provided latitude and longitude.\n  places = google_places(\n    search_string = \"starbucks\",   # Search term \"starbucks\"\n    location = c(lat, lng)         # Coordinates (lat and lng) for the search\n  )\n  \n  # Define the columns of interest to keep from the results\n  sel_cols = c(\n    \"name\",                        # Store name\n    \"formatted_address\",           # Store address\n    \"lat\",                         # Latitude\n    \"lng\",                         # Longitude\n    \"rating\",                      # Store rating\n    \"user_ratings_total\",          # Number of user ratings\n    \"business_status\"              # Business status (e.g., operational or closed)\n  )\n  \n  # Process the results and select the relevant columns\n  places$results %&gt;%\n    tidyr::unnest(\"geometry\") %&gt;%   # Extract the nested 'geometry' field\n    tidyr::unnest(\"location\") %&gt;%   # Extract the nested 'location' field (lat and lng)\n    dplyr::select(dplyr::all_of(sel_cols))   # Select only the columns of interest\n}\n\nThe code below uses purrr to iterate the get_starbucks_info function over the lat/lng pairs.\n\n# Remove geometry and keep only coordinates\ncoords_starbucks &lt;- starbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  select(index, name, lat, lng)\n\nstarbucks_info &lt;- purrr::map2(\n  coords_starbucks$lat,\n  coords_starbucks$lng,\n  get_starbucks_info\n  )\n\ndat &lt;- starbucks_info %&gt;%\n  bind_rows(.id = \"search_id\") %&gt;%\n  distinct()\n\nTo clean the data, I will keep only the active stores that contain “Starbucks” in their name. Additionally, I will pair the data with my web scraping dataset using st_nearest_feature(x, y). This function finds the nearest point in y for each point in x.\n\ndat &lt;- dat |&gt; \n  # Keep only stores with \"Starbucks\" in the name and that are operational\n  filter(str_detect(name, \"Starbucks\"), business_status == \"OPERATIONAL\") |&gt; \n  # Arrange the results by address\n  arrange(formatted_address)\n\n# Convert to a spatial data frame using longitude and latitude\ngoogle_data &lt;- dat %&gt;%\n  # Set coordinate reference system to WGS 84 (EPSG:4326)\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326)  \n\n# Find the nearest Starbucks locations from the web scraping data (starbucks)\n# for each point in google_data\ninds &lt;- st_nearest_feature(google_data, starbucks)\n\n# Extract the metadata of the nearest points from the web scraping data and\n# convert to a tibble\nmetadata &lt;- starbucks %&gt;%\n  slice(inds) %&gt;%\n  st_drop_geometry() %&gt;%  # Remove spatial geometry\n  as_tibble()\n\n# Rename the columns in google_data and bind the metadata from the web scraping data\ngoogle_data &lt;- google_data |&gt; \n  rename(google_name = name, google_address = formatted_address) |&gt; \n  bind_cols(metadata)  # Combine google_data with the corresponding metadata\n\n\n\nFinal Map\nThe interactive map below shows all Starbucks locations in São Paulo. The color of each circle represents its rating, and the size of the circle represents the number of reviews.\nThe units along the Avenida Paulista corridor, for example, have high average ratings and a large number of reviews. One of the worst-rated units seems to be the one near Mackenzie University, which has a rating of 2.1 and 15 reviews. In the Eastern Zone, the store at Shopping Aricanduva also has a slightly lower rating, 3.9 with 158 reviews.\n\nsp &lt;- filter(google_data, name_muni == \"São Paulo\")\n\nsp &lt;- sp |&gt; \n  mutate(\n    rad = findInterval(user_ratings_total, c(25, 100, 1000, 2500, 5000)) * 2 + 5\n  )\n\npal &lt;- colorNumeric(\"RdBu\", domain = sp$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {sp$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {sp$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {sp$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(sp) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~rad,\n    color = ~pal(rating),\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt; \n  addLegend(pal = pal, values = ~rating) |&gt; \n  addProviderTiles(\"CartoDB\")"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#related-posts",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#related-posts",
    "title": "Finding All Starbucks in Brazil",
    "section": "Related posts",
    "text": "Related posts\n\nFinding coffee shop in Brazil\nFinding all The Coffee shops in Brazil"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#footnotes",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#footnotes",
    "title": "Finding All Starbucks in Brazil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://starbucks.com.br/public/img/icons/starbucks-nav-logo.svg↩︎\nHere, we are assuming that the “name” tag always includes the word “shopping” if the store is located inside a mall. This number might eventually be underestimated if there are stores inside malls that don’t have the word “shopping” in their name. Strictly speaking, we also haven’t verified whether the “shopping” tag is always associated with an active shopping mall.↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html",
    "title": "ARMA: um exemplo simples",
    "section": "",
    "text": "Neste post apresento como estimar e diagnosticar um modelo ARMA simples no R. O modelo ARMA decompõe uma série em função de suas observações passadas e de “passados” (às vezes chamados de inovações). O ARMA(1, 1) com constante tem a seguinte forma.\n\\[\ny_{t} = \\phi_{0} +\\phi_{1}y_{t-1} + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t - 1}\n\\]\nonde ambas as condições de estacionaridade como de invertibilidade devem ser atendidas. Assume-se que o termo \\(\\varepsilon_{t}\\) é um ruído branco. De maneira mais geral, um modelo ARMA(p, q) tem a forma:\n\\[\ny_{t} = \\phi_{0} +\\phi_{1}y_{t-1} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t - 1} + \\dots + \\theta_{q}\\varepsilon_{t- q}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-1",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-1",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 1",
    "text": "Modelo 1\nVou primeiro estimar o ARMA(1, 2) usando a função arima() do R.\n\n(m1 &lt;- arima(train, order = c(1, 0, 2)))\n#&gt; \n#&gt; Call:\n#&gt; arima(x = train, order = c(1, 0, 2))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1     ma1     ma2  intercept\n#&gt;       0.2324  0.0971  0.1623     0.0084\n#&gt; s.e.  0.2294  0.2247  0.0958     0.0012\n#&gt; \n#&gt; sigma^2 estimated as 0.0001038:  log likelihood = 566.91,  aic = -1123.82\n\nA estimativa tem a forma: \\[\n  y_{t} = \\underset{(0.0012)}{0.0084} + \\underset{(0.2324)}{0.2294}y_{t - 1} + \\underset{(0.2247)}{0.0971}\\epsilon_{t-1} + \\underset{(0.0958)}{0.1623}\\epsilon_{t-2}\n\\] Note que os erros-padrão de \\(\\hat{\\phi_{1}}\\) e \\(\\hat{\\theta_{1}}\\) são bastante elevados. De fato, um teste-t revela que estes coeficientes não são significantes.\n\nDiagnóstico de resíduos\nPode-se verificar os resíduos do modelo de muitas formas. Idealmente, quer-se que os resíduos não apresentem autocorrelação alguma. Uma forma gráfica de ver isto é usando a função lag1.plot que apresenta gráficos de dispersão entre o resíduo \\(u_{t}\\) contra suas defasagens \\(u_{t-1}, u_{t-2}, \\dots\\). Abaixo faço isto para as primieras quatro defasagens.\n\nresiduos &lt;- resid(m1)\nlag1.plot(residuos, max.lag = 4)\n\n\n\n\n\n\n\n\nNa prática, é mais conveniente analisar diretamente os gráficos da FAC e da FACP dos resíduos.\n\nacf2(residuos)\n\n\n\n\n\n\n\n#&gt;      [,1]  [,2] [,3]  [,4]  [,5]  [,6]  [,7]  [,8] [,9] [,10] [,11] [,12] [,13]\n#&gt; ACF  0.01 -0.01    0 -0.09 -0.12 -0.03 -0.04 -0.05 0.04  0.06  0.07 -0.12 -0.07\n#&gt; PACF 0.01 -0.01    0 -0.09 -0.12 -0.03 -0.05 -0.06 0.02  0.04  0.06 -0.15 -0.09\n#&gt;      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]\n#&gt; ACF  -0.03 -0.12  0.07  0.06  0.06  0.02  0.09  -0.1  0.00 -0.07 -0.04\n#&gt; PACF -0.02 -0.11  0.06  0.02  0.04 -0.02  0.05  -0.1  0.01 -0.05 -0.01\n\nUm teste basatante usual para verificar a presença de autocorrelação nos resíduos é o Ljung-Box. Para computá-lo uso a função Box.test() do pacote tseries. Note que é preciso suplementar o argumento fitdf com o número de parâmetros estimados do modelo. Isto serve para corrigir a estatística do teste. A escolha da ordem do teste é um tanto arbitrária e, na prática, seria recomendado fazer o teste para várias ordens diferentes. O código abaixo computa a estatística do teste para uma defasagem igual a 8.\nNote o uso do argumento fitdf que leva em conta o número de parâmetros estimados no modelo. Segundo o p-valor, não temos evidência para rejeitar a hipótese nula de que os resíduos não são conjuntamente autocorrelacionados. Isto é, temos evidência de que o modelo está bem ajustado pois os resíduos parecem se comportar como ruído branco.\n\nlibrary(tseries)\nBox.test(residuos, type = \"Ljung-Box\", lag = 8, fitdf = 4)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  residuos\n#&gt; X-squared = 4.9486, df = 4, p-value = 0.2926\n\nNa prática, é bom repetir o teste para várias defasagens diferentes. A tabela abaixo resume os valores do teste para várias ordens de defasagem. Vale lembrar que o teste Ljung-Box tende a não-rejeitar H0 para defasagens muito elevadas.\n\n\n\n\n\nDefasagem\nEstatística de teste\nP-valor\n\n\n\n\n8\n4.9486\n0.2926\n\n\n9\n5.2251\n0.3890\n\n\n10\n5.8669\n0.4383\n\n\n11\n6.7991\n0.4501\n\n\n12\n9.7788\n0.2809\n\n\n13\n10.8054\n0.2893\n\n\n14\n10.9313\n0.3629\n\n\n15\n13.6377\n0.2537\n\n\n16\n14.5903\n0.2646\n\n\n17\n15.2158\n0.2941\n\n\n18\n15.8735\n0.3212\n\n\n19\n15.9294\n0.3868\n\n\n20\n17.4548\n0.3568\n\n\n\n\n\n\n\nUsando o pacote astsa\nUma forma bastante conveniente de trabalhar com modelos ARMA é com a função sarima do pacote astsa. Esta função apresenta automaticamente algumas valiosas informações para o diagnóstico dos resíduos: o gráfico do ACF, o gráfico qq-plot (para verificar a normalidade dos resíduos) e os p-valores do teste Ljung-Box para várias ordens de defasagem.\nA saída abaixo reúne quatro gráficos. O primeiro deles apresenta o resíduo normalizado (ou escalado). Este gráfico não deve apresentar um padrão claro. O segundo gráfico é a FAC do resíduo: idealmente, nenhuma defasagem deve ser significativa neste gráfico. Ao lado da FAC temos o QQ-plot: se todos os pontos caem sobre a linha azul temos evidência de que os resíduos são normalmente distribuídos.\nPor fim, o último gráfico mostra o p-valor do teste Ljung-Box (já ajustado pelo número de parâmetros do modelo estimado) para diferentes defasagens. A linha tracejada em azul indica o valor 0.05. Idealmente, todos os pontos devem estar acima desta linha.\n\nsarima(train, p = 1, d = 0, q = 2)\n#&gt; initial  value -4.507231 \n#&gt; iter   2 value -4.512000\n#&gt; iter   3 value -4.582718\n#&gt; iter   4 value -4.583615\n#&gt; iter   5 value -4.583727\n#&gt; iter   6 value -4.583754\n#&gt; iter   7 value -4.583896\n#&gt; iter   8 value -4.583941\n#&gt; iter   9 value -4.583957\n#&gt; iter  10 value -4.583958\n#&gt; iter  10 value -4.583958\n#&gt; final  value -4.583958 \n#&gt; converged\n#&gt; initial  value -4.586016 \n#&gt; iter   2 value -4.586020\n#&gt; iter   3 value -4.586022\n#&gt; iter   4 value -4.586023\n#&gt; iter   5 value -4.586025\n#&gt; iter   6 value -4.586026\n#&gt; iter   7 value -4.586027\n#&gt; iter   8 value -4.586027\n#&gt; iter   9 value -4.586028\n#&gt; iter   9 value -4.586028\n#&gt; iter   9 value -4.586028\n#&gt; final  value -4.586028 \n#&gt; converged\n\n\n\n\n\n\n\n#&gt; $fit\n#&gt; \n#&gt; Call:\n#&gt; arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n#&gt;     xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n#&gt;     optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1     ma1     ma2   xmean\n#&gt;       0.2324  0.0971  0.1623  0.0084\n#&gt; s.e.  0.2294  0.2247  0.0958  0.0012\n#&gt; \n#&gt; sigma^2 estimated as 0.0001038:  log likelihood = 566.91,  aic = -1123.82\n#&gt; \n#&gt; $degrees_of_freedom\n#&gt; [1] 175\n#&gt; \n#&gt; $ttable\n#&gt;       Estimate     SE t.value p.value\n#&gt; ar1     0.2324 0.2294  1.0130  0.3125\n#&gt; ma1     0.0971 0.2247  0.4319  0.6664\n#&gt; ma2     0.1623 0.0958  1.6946  0.0919\n#&gt; xmean   0.0084 0.0012  6.7491  0.0000\n#&gt; \n#&gt; $AIC\n#&gt; [1] -6.278312\n#&gt; \n#&gt; $AICc\n#&gt; [1] -6.277028\n#&gt; \n#&gt; $BIC\n#&gt; [1] -6.189279"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-2",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-2",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 2",
    "text": "Modelo 2\nEstimo também o modelo ARMA(1, 1). A análise de resíduos é análoga à apresentada acima.\n\nm2 &lt;- arima(train, order = c(1, 0, 1))"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-3",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-3",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 3",
    "text": "Modelo 3\nPara o terceiro modelo uso o método automático do auto.arima(). A função escolhe o AR(1) com constante como melhor modelo para representar os dados. Note que a na inspeação visual também tínhamos verificado que o AR(1) seria um possível candidato.\n\n(m3 &lt;- auto.arima(train))\n#&gt; Series: train \n#&gt; ARIMA(1,0,0) with non-zero mean \n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1    mean\n#&gt;       0.3568  0.0084\n#&gt; s.e.  0.0695  0.0012\n#&gt; \n#&gt; sigma^2 = 0.0001066:  log likelihood = 565.52\n#&gt; AIC=-1125.04   AICc=-1124.91   BIC=-1115.48\n\n\nSeleção\nPara escolher o melhor modelo pode-se usar algum critério de informação. Abaixo comparo os modelos segundo os critérios AIC, AICc (AIC corrigido) e BIC (Bayesian Information Criterion). Na prática, não é comum que os três critérios escolham o mesmo modelo; em particular, o BIC penaliza o número de parâmetros mais fortemente que o AIC. Neste caso específico, os três critérios escolhem o AR(1).\n\n\n\n\n\n\nAIC\nAICc\nBIC\n\n\n\n\nARMA(1, 2)\n-1123.818\n-1124.908\n-1115.483\n\n\nARMA(1, 1)\n-1123.563\n-1124.908\n-1115.483\n\n\nAR(1)\n-1125.045\n-1124.908\n-1115.483\n\n\n\n\n\n\n\nPrevisão\nPara gerar previsões usamos a função forecast() (outra opção é usar a função base predict()). Abaixo computo previsões para os três modelos acima mais um modelo ingênuo que será usado como bench-mark. O modelo ingênuo é simplesmente um random-walk que prevê sempre o valor anterior, isto é, \\(\\hat{y_{T+1}} = y_{T}\\).\n\nprev1 &lt;- forecast(m1, h = length(teste))\nprev2 &lt;- forecast(m2, h = length(teste))\nprev3 &lt;- forecast(m3, h = length(teste))\nprev4 &lt;- forecast(naive(train, h = length(teste)), h = length(teste))\n\nerros &lt;- t(matrix(c(teste - prev1$mean,\n                    teste - prev2$mean,\n                    teste - prev3$mean,\n                    teste - prev4$mean),\n                    ncol = 4))\nerros &lt;- as.data.frame(erros)\ncolnames(erros) &lt;- paste(\"t =\", as.numeric(time(teste)))\nrow.names(erros) &lt;- c(\"ARMA(1, 2)\",\n                      \"ARMA(1, 1)\",\n                      \"AR(1)\",\n                      \"Y[t+1] = Y[t]\")\n\nOs erros de previsão são apresentados na tabela abaixo.\n\n\n\n\n\n\n\nt = 1992\nt = 1992.25\nt = 1992.5\nt = 1992.75\nt = 1993\nt = 1993.25\nt = 1993.5\nt = 1993.75\nt = 1994\nt = 1994.25\nt = 1994.5\nt = 1994.75\nt = 1995\nt = 1995.25\nt = 1995.5\nt = 1995.75\nt = 1996\nt = 1996.25\nt = 1996.5\nt = 1996.75\nt = 1997\nt = 1997.25\nt = 1997.5\nt = 1997.75\nt = 1998\nt = 1998.25\nt = 1998.5\nt = 1998.75\nt = 1999\nt = 1999.25\nt = 1999.5\nt = 1999.75\nt = 2000\nt = 2000.25\nt = 2000.5\nt = 2000.75\nt = 2001\nt = 2001.25\nt = 2001.5\nt = 2001.75\nt = 2002\nt = 2002.25\nt = 2002.5\n\n\n\n\nARMA(1, 2)\n0.0017385\n0.0009996\n-0.0011832\n0.0047685\n-0.0075880\n-0.0034300\n-0.0032720\n0.0041505\n0.0009645\n0.0048053\n-0.0031499\n0.0038158\n-0.0038534\n-0.0059706\n-0.0026298\n0.0011636\n-0.0008728\n0.0066953\n-0.0041940\n0.0035309\n0.0012488\n0.0064862\n0.0012950\n-0.0021395\n0.0069883\n-0.0032794\n-0.0002956\n0.0083934\n0.0023188\n-0.0031434\n0.0033963\n0.0099058\n-0.0030742\n0.0039295\n-0.0078546\n-0.0045499\n-0.0119065\n-0.0103021\n-0.0110772\n0.0006393\n0.0005900\n-0.0074360\n0.0023173\n\n\nARMA(1, 1)\n0.0015678\n0.0011472\n-0.0010609\n0.0048359\n-0.0075556\n-0.0034158\n-0.0032667\n0.0041516\n0.0009637\n0.0048036\n-0.0031520\n0.0038136\n-0.0038557\n-0.0059730\n-0.0026323\n0.0011611\n-0.0008752\n0.0066928\n-0.0041964\n0.0035284\n0.0012464\n0.0064838\n0.0012925\n-0.0021420\n0.0069858\n-0.0032818\n-0.0002980\n0.0083910\n0.0023164\n-0.0031458\n0.0033939\n0.0099034\n-0.0030766\n0.0039271\n-0.0078570\n-0.0045523\n-0.0119090\n-0.0103046\n-0.0110796\n0.0006369\n0.0005876\n-0.0074384\n0.0023149\n\n\nAR(1)\n0.0013079\n0.0009620\n-0.0011721\n0.0047728\n-0.0075918\n-0.0034379\n-0.0032817\n0.0041400\n0.0009537\n0.0047944\n-0.0031608\n0.0038049\n-0.0038643\n-0.0059815\n-0.0026408\n0.0011526\n-0.0008837\n0.0066843\n-0.0042049\n0.0035199\n0.0012379\n0.0064753\n0.0012840\n-0.0021505\n0.0069773\n-0.0032903\n-0.0003065\n0.0083825\n0.0023079\n-0.0031543\n0.0033854\n0.0098949\n-0.0030851\n0.0039186\n-0.0078655\n-0.0045608\n-0.0119175\n-0.0103131\n-0.0110881\n0.0006284\n0.0005791\n-0.0074469\n0.0023064\n\n\nY[t+1] = Y[t]\n0.0024552\n0.0025186\n0.0005306\n0.0065276\n-0.0058184\n-0.0016579\n-0.0014994\n0.0059232\n0.0027372\n0.0065781\n-0.0013772\n0.0055886\n-0.0020806\n-0.0041978\n-0.0008571\n0.0029363\n0.0009000\n0.0084680\n-0.0024213\n0.0053036\n0.0030215\n0.0082589\n0.0030677\n-0.0003668\n0.0087610\n-0.0015066\n0.0014772\n0.0101662\n0.0040916\n-0.0013707\n0.0051690\n0.0116786\n-0.0013015\n0.0057023\n-0.0060818\n-0.0027771\n-0.0101338\n-0.0085294\n-0.0093044\n0.0024120\n0.0023628\n-0.0056632\n0.0040900\n\n\n\n\n\n\nPode-se melhor comparar a performance das previsões usando alguma medida agregada de erro. Duas medidas bastante comuns são o Erro Absoluto Médio (EAM) e o Erro Quadrático Médio. A primeira toma o módulo da diferença entre o previsto (\\(\\hat{y}\\)) e o observado (\\(y\\)) e tira uma média, enquanto a última toma a diferença quadrática. Formalmente, para um horizonte de previsão \\(h\\) começando na última observação \\(T\\):\n\\[\\begin{align}\n\\text{EAM} & = \\frac{1}{h}\\sum_{i = T + 1}^{T + h} |y_{i} - \\hat{y}_{i}| \\\\\n\\text{EQM} & = \\frac{1}{h}\\sum_{i = T + 1}^{T + h} (y_{i} - \\hat{y}_{i})^2\n\\end{align}\\]\nA tabela abaixo compara os modelos segundo estas medidas de erro.\n\n\n\n\n\n\n\nEAM\nEQM\n\n\n\n\nARMA(1, 2)\n0.0042173\n0.0042062\n\n\nARMA(1, 1)\n0.0000268\n0.0000268\n\n\nAR(1)\n0.0042143\n0.0043644\n\n\nY[t+1] = Y[t]\n0.0000268\n0.0000280\n\n\n\n\n\n\nNem sempre é fácil comparar estas medidas, i.e., verificar se elas são estatisticamente significantes. Pode-se usar o teste Diebold-Mariano para comparar estas medidas de erro, mas é importante frisar que ele contém uma série de hipóteses sobre a distribuição dos erros. A função dm.test do pacote forecast faz este teste e mais informações sobre ele podem ser encontradas usando ?dm.test.\nPode-se visualizar as previsões usando as funções autoplot() e autolayer() do pacote forecast. Abaixo mostro os resultados para o modelo AR(1) e também para o modelo ingênuo. note como o erros-padrão deste último cresce muito rapidamente (pois a variância de um processo random-walk cresce sem limite).\n\nlibrary(ggplot2)\n\nautoplot(prev2, include = 50) +\n  autolayer(teste) +\n  theme_bw()\n\n\n\n\n\n\n\n\nautoplot(prev3, include = 50) +\n  autolayer(teste) +\n  labs(x = \"\", y = \"(%)\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nautoplot(prev4, include = 50) +\n  autolayer(teste) +\n  labs(x = \"\", y = \"(%)\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "",
    "text": "A taxa de juros é talvez a variável macroeconômica mais importante para se observar quando se pensa em financiamento imobiliário. Quanto maior for a taxa de juros, mais “caro” fica o financiamento habitacional. Ou seja, mais difícil fica de comprar um imóvel.\nO financiamento imobiliário nada mais é do que um empréstimo que uma família contrai com o sistema financeiro (um banco); este empréstimo é uma dívida que a família deve repagar em parcelas mensais e sobre cada pagamento incide um valor de juros.\n\n\nA taxa de juros tem um efeito geral sobre a economia, mas o seu efeito é mais notável para o consumidor na hora de fazer compras grandes, de longo prazo: caso de um automóvel ou de um imóvel. Uma taxa menor significa que fica mais “barato” tomar crédito, enquanto uma taxa maior significa o contrário.\nQuem define a taxa de juros “geral” da economia, a taxa SELIC, é o Cômite de Política Monetária (COPOM). A SELIC é, na verdade, uma meta de taxa de juros, que o Banco Central do Brasil (BCB) deve perseguir. O COPOM se reúne periodicamente para definir a taxa SELIC; na ocasião mais recente, no início de agosto, decidiu-se reduzir a taxa de 13,75% a.a. para 13,25% a.a. - a primeira queda depois de um ciclo de alta de 3 anos.\nTipicamente, a taxa de juros é a ferramenta de política monetária que se usa para controlar a inflação: quando a taxa de inflação aumenta muito, o COPOM decide aumentar a taxa de juros. Foi isto o que aconteceu em 2014-16 e também em 2021-23.\n\n\n\n\n\n\n\n\n\nO gráfico acima mostra as oscilações recentes da SELIC. No final de 2016 encerrou-se um longo ciclo de alta que começou em reação à escalada da inflação em 2014. A taxa, então, chegou a cair até 2% a.a. no final de 2020, acompanhando a tendência internacional de taxas de juros baixíssimas, muito próximas de 0% a.a. A volta da inflação com a pandemia levou o COPOM a agressivamente aumentar a taxa SELIC nos meses seguintes. A taxa manteve-se estável em 13,75% a.a. desde agosto do ano passado.\nA taxa SELIC influencia as demais taxas de juros da economia indiretamente. Como será mostrado mais adiante, há várias taxas de financiamento imobiliário disponíveis."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#um-pouco-sobre-juros",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#um-pouco-sobre-juros",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "",
    "text": "A taxa de juros tem um efeito geral sobre a economia, mas o seu efeito é mais notável para o consumidor na hora de fazer compras grandes, de longo prazo: caso de um automóvel ou de um imóvel. Uma taxa menor significa que fica mais “barato” tomar crédito, enquanto uma taxa maior significa o contrário.\nQuem define a taxa de juros “geral” da economia, a taxa SELIC, é o Cômite de Política Monetária (COPOM). A SELIC é, na verdade, uma meta de taxa de juros, que o Banco Central do Brasil (BCB) deve perseguir. O COPOM se reúne periodicamente para definir a taxa SELIC; na ocasião mais recente, no início de agosto, decidiu-se reduzir a taxa de 13,75% a.a. para 13,25% a.a. - a primeira queda depois de um ciclo de alta de 3 anos.\nTipicamente, a taxa de juros é a ferramenta de política monetária que se usa para controlar a inflação: quando a taxa de inflação aumenta muito, o COPOM decide aumentar a taxa de juros. Foi isto o que aconteceu em 2014-16 e também em 2021-23.\n\n\n\n\n\n\n\n\n\nO gráfico acima mostra as oscilações recentes da SELIC. No final de 2016 encerrou-se um longo ciclo de alta que começou em reação à escalada da inflação em 2014. A taxa, então, chegou a cair até 2% a.a. no final de 2020, acompanhando a tendência internacional de taxas de juros baixíssimas, muito próximas de 0% a.a. A volta da inflação com a pandemia levou o COPOM a agressivamente aumentar a taxa SELIC nos meses seguintes. A taxa manteve-se estável em 13,75% a.a. desde agosto do ano passado.\nA taxa SELIC influencia as demais taxas de juros da economia indiretamente. Como será mostrado mais adiante, há várias taxas de financiamento imobiliário disponíveis."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#exemplo-guiado",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#exemplo-guiado",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "Exemplo Guiado",
    "text": "Exemplo Guiado\n\nO empréstimo\nVamos simular o financiamento de um imóvel de R$450.000. Supondo um LTV de 80%, o valor da entrada é de R$90.000 e o valor a ser financiado, portanto, é de R$360.000. Num contrato de 30 anos, o valor da amortização é de:\n\\[\nA = \\frac{R\\$360.000}{R\\$360} = R\\$1.000\n\\]\nVamos começar assumindo que a taxa de juros seja de 10% a.a. A tabela abaixo mostra o fluxo de pagamentos do primeiro ano do empréstimo. Note como o valor da amortização é sempre o mesmo. À medida que a dívida vai sendo paga, o valor cobrado de juros também diminui e, por conseguinte, diminui também o valor da parcela mensal.\n\n\n\n\n\n\nFluxo de pagamento do financiamento de um imóvel de 450.000 (SAC).\n\n\nPeríodo (mês)\nAmortização\nJuros\nParcela\nDívida\n\n\n\n\n1\nR$1.000\nR$2.870,69\nR$3.870,69\nR$360.000\n\n\n2\nR$1.000\nR$2.862,72\nR$3.862,72\nR$359.000\n\n\n3\nR$1.000\nR$2.854,74\nR$3.854,74\nR$358.000\n\n\n4\nR$1.000\nR$2.846,77\nR$3.846,77\nR$357.000\n\n\n5\nR$1.000\nR$2.838,79\nR$3.838,79\nR$356.000\n\n\n6\nR$1.000\nR$2.830,82\nR$3.830,82\nR$355.000\n\n\n7\nR$1.000\nR$2.822,85\nR$3.822,85\nR$354.000\n\n\n8\nR$1.000\nR$2.814,87\nR$3.814,87\nR$353.000\n\n\n9\nR$1.000\nR$2.806,90\nR$3.806,90\nR$352.000\n\n\n10\nR$1.000\nR$2.798,92\nR$3.798,92\nR$351.000\n\n\n11\nR$1.000\nR$2.790,95\nR$3.790,95\nR$350.000\n\n\n12\nR$1.000\nR$2.782,98\nR$3.782,98\nR$349.000\n\n\n\n\n\n\n\nAo longo dos 360 meses do financiamento, o valor dos juros e da parcela vão diminuindo até que a dívida tenha sido totalmente paga. Note como no início do financiamento, a parcela mensal está na faixa de R$3800, mas já no final está próxima de R$1000.\n\n\n\n\n\n\n\n\n\n\n\nRenda necessária\nAgora podemos responder uma dúvida importante: qual a renda necessária para financiar este imóvel? Cada banco ou instituição financeira usa regras próprias para decidir se libera ou não o valor do financiamento imobiliário. Uma regra de bolso comum é de que o valor da parcela inicial não pode ser maior do que 30% da renda do requerente.\nNo exemplo acima, o valor da primeira parcela é de R$3870. A renda mínima necessária (RMN) para estar elegível a este financiamento é:\n\\[\nRMN = \\frac{R\\$ 3.870,69}{0,3} = R\\$ 12.902,3\n\\]\nComo que este resultado final depende da taxa de juros? Podemos simular o mesmo financiamento para diferentes taxas juros e calcular novamente a renda mínima necessária. A tabela abaixo mostra como a renda varia para valores de taxa de juros de 7% a 12%. É notável como a taxa de juros tem impacto direto no poder de compra e capacidade de pagamento das famílias. A uma taxa favorável de 7%, é necessário ter uma renda de R$10 mil para ser aprovado no financimento; já a uma taxa de 12% é necessário ter quase R$15 mil.\n\n\n\n\n\n\nRenda mínima necessária para financiar um imóvel de R$450.000 a diferentes taxas de juros.\n\n\nJuros (% a.a)\nRenda Mínima\n\n\n\n\n7,00%\nR$10.118,31\n\n\n7,50%\nR$10.587,24\n\n\n8,00%\nR$11.054,17\n\n\n8,50%\nR$11.519,13\n\n\n9,00%\nR$11.982,12\n\n\n9,50%\nR$12.443,17\n\n\n10,00%\nR$12.902,30\n\n\n10,50%\nR$13.359,52\n\n\n11,00%\nR$13.814,85\n\n\n11,50%\nR$14.268,30\n\n\n12,00%\nR$14.719,88"
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#o-impacto-do-aumento-dos-juros",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#o-impacto-do-aumento-dos-juros",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "O impacto do aumento dos juros",
    "text": "O impacto do aumento dos juros\nNos últimos anos vimos uma alta significativa das taxas de juros. A taxa média de financiamento habitacional era próxima de 7% em 2020 e subiu para 11,6%. Considerando o imóvel do exemplo acima, seria necessário um aumento de mais de R$4000 na renda para conseguir comprar o mesmo imóvel - sem levar em conta o aumento de preço do imóvel.\nVamos retomar o exemplo do imóvel de R$450.000 acima. Mantendo este preço constante, podemos calcular qual a renda necessária para financiar este mesmo imóvel à medida que a taxa de juros foi aumentando. Por simplicidade, uso a taxa de juros média mensal em cada período3.\nO gráfico abaixo apresenta, a cada mês, a renda necessária para financiar um imóvel de R$450.0004. No ponto mais baixo da taxa, seria necessário R$8.550 (a uma taxa de 6,63%) para ser aprovado num financiamento; já no ponto mais recente, seria necessário R$12.600 (a uma taxa de 11,6%).\n\n\n\n\n\n\n\n\n\nA análise acima olha somente o impacto do aumento dos juros e não leva em consideração o aumento médio do preços dos imóveis durante este período. Segundo o IGMI-R (Abrainc/FGV), de janeiro de 2018 a abril 2023, houve um aumento médio de 58% no preços dos imóveis. A tabela abaixo mostra a variação acumulada do IGMI-R em cada ano. Nota-se como os preços aumentam significativamente a partir de 2020.\n\n\n\n\n\n\nVariação acumulada do IGMI-R por ano.\n\n\nAno\nVar. Acum. (%)\n\n\n\n\n2018\n0.64%\n\n\n2019\n4.11%\n\n\n2020\n10.28%\n\n\n2021\n16.25%\n\n\n2022\n15.06%\n\n\n2023\n2.52%\n\n\n\n\n\n\n\nO gráfico abaixo refaz o experimento acima, mas leva em conta também o aumento médio do preços dos imóveis. Em cada mês vê-se a renda necessária para financiar um imóvel médio, que em janeiro de 2018 valia R\\$450.000.\nFica evidente como a combinação simultânea de aumento de juros e de preços tornou os imóveis menos acessíveis. Em abril de 2023, seria necessário uma renda em torno de R\\$20.000 para financiar o mesmo imóvel5.\n\n\n\n\n\n\n\n\n\nA análise omite ainda um fator: o crescimento médio da renda ao longo do tempo. No Brasil, o salário mínimo é indexado à variação da inflação e, de maneira geral, quando a economia vai bem a renda média costuma crescer. Deixo esta última etapa da análise de acessibilidade financeira para outro post.\nAlém disso, o programa habitacional do Brasil, o Minha Casa Minha Vida (MCMV) oferece empréstimos com taxas mais atrativas do que as taxas médias de mercado, conforme a renda da família e o preço do imóvel6. Como o programa foi revisto recentemente, vou dedicar um post somente ao MCMV e como ele deve impactar a acessibilidade à moradia no Brasil."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#o-caminho-futuro",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#o-caminho-futuro",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "O caminho futuro",
    "text": "O caminho futuro\nNa última semana o COPOM decidiu reduzir a taxa SELIC em 0.5 p.p., diminuindo a taxa de 13,75% para 13,25%. Esta foi a primeira queda desde que se iniciou o ciclo de altas no início de 2021. Espera-se que o Banco Central agora entre num ciclo de queda de taxa de juros que devem se estabilizar em torno de 8,5% no longo prazo.\nComo se viu na análise acima, as oscilações de SELIC eventualmente traduzem-se em mudanças nas taxas do financiamento imobiliário. Além da queda na taxa de juros, os índices de preços imobiliários, como o IGMI-R e o IVGR, começam a apontar para uma relativa estabilidade nos preços dos imóveis. Esta combinação deve aumentar o poder de compras das famílias e melhorar a acessibilidade financeira à moradia.\nHá um último componente, da equação da acessibilidade à moradia, que ficou inexplorado neste post: a renda das famílias. Evidentemente, um aumento da renda média das famílias permite que elas tenham acesso a imóveis melhores e mais caros. Além disso, para imóveis com ticket menores, o MCMV oferece condições mais favoráveis de financiamento. Deixo esta discussão, contudo, para um outro momento."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#footnotes",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#footnotes",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEm abril de 2023, cerca de 95% do estoque de contratos de financiamentos imobiliários para pessoas físicas era indexado pela TR. O IPCA era utilizado em cerca de 2% e outros indexadores eram utilizados em 2,2% dos contratos. Apenas 0,8% dos contratos eram pré-fixados.↩︎\nAlguns exemplos incluem: cadastro positivo, regulamentação das fintechs de crédito, regulamentação das Letras Imobiliárias Garantidas (LIGs), portabilidade de crédito. Para mais informações veja a Agenda BC#.↩︎\nA taxa de juros média do financiamento habitacional para pessoas físicas toma as cinco taxas apresentadas e pondera elas pelo volume de crédito. Assim, as principais linhas (FGTS e SFH) têm maior peso.↩︎\nPor simplicidade, suponho um financiamento estilo SAC com LTV de 70% e prazo de 360 meses. Para ser aprovado no financiamento, suponho que o valor da primeira parcela não possa ser maior do que 30% da renda familiar bruta.↩︎\nVale notar que o IGMI-R é um índice de preços hedônico então ele provê um índice de preços “ajustado pela qualidade”, isto é, um quality adjusted price index. Assim, este aumento de preços não reflete meramente uma mudança no mix de imóveis disponíveis no mercado.↩︎\nO preço de R\\$450.000 não foi escolhido ao acaso já que ele supera o teto atual do Minha Casa Minha Vida e não estaria elegível ao programa.↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html",
    "title": "Visualizando uma única variável",
    "section": "",
    "text": "Há algum tempo atrás tive o seguinte problema: como visualizar várias observações de uma única variável numérica num gráfico? Tentei algumas soluções óbvias, mas nenhuma pareceu funcionar muito bem. Neste post junto algumas das minhas tentativas.\nOs dados provêm do Mapa da Desigualdade 2019. Neste site pode-se baixar todos os dados além de baixar o relatório completo que apresenta informações socioeconômicas atualizadas para todos os distritos de São Paulo, compilando e sistematizando dados de diversas fontes públicas."
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#histograma",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#histograma",
    "title": "Visualizando uma única variável",
    "section": "Histograma",
    "text": "Histograma\nUma solução bastante clássica seria de fazer um histograma. Neste tipo de gráfico a dispersão fica clara, mas pode ser difícil de dar destaque para distritos específicos. O histograma conta a frequência de observações dentro de janelas de tamanho fixo. A princípio, a única dificuldade em usar o histograma está em definir a amplitude de cada um destes intervalos, mas, na prática, este problema não costuma ser muito complexo.\nAinda que o gráfico seja comumente usado, ele não é tão popular, sendo raramente visto em publicações de jornal, por exemplo.\n\n\nCode\nggplot(df, aes(x = expec_vida)) +\n  geom_histogram(bins = 11, colour = \"white\", fill = \"#08519c\") +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    data = df_aux_label,\n    aes(x = expec_vida,\n        y = c(15, 17, 19, 16, 12),\n        label = stringr::str_wrap(label_distrito, 5)),\n    colour = \"gray25\",\n    family = \"Roboto Light\",\n    size = 4\n  ) +\n  geom_segment(\n    data = df_aux_label,\n    aes(x = expec_vida,\n        xend = expec_vida,\n        y = 0,\n        yend = c(15, 17, 19, 15, 12) - 0.5),\n    colour = \"gray70\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 20, 4)) +\n  scale_x_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"Anos de idade\",\n    y = \"\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022.\"\n  ) +\n  theme_vini +\n  theme(panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-colunas",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-colunas",
    "title": "Visualizando uma única variável",
    "section": "Gráfico de colunas",
    "text": "Gráfico de colunas\nUm simples gráfico de colunas também poderia ser uma alternativa. Neste caso, como há muitas observações (distritos) diferentes, o gráfico acaba sobrecarregado e confuso.\n\n\nCode\nordered_df &lt;- df %&gt;%\n  mutate(\n    label_distrito = factor(label_distrito),\n    label_distrito = forcats::fct_reorder(label_distrito, expec_vida)\n  )\n\nggplot(ordered_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_col(colour = \"white\", fill = \"#08519c\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"Anos de idade\",\n    y = \"\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022.\"\n  ) +\n  theme_vini +\n  theme(\n    text = element_text(size = 6),\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    panel.grid.major.y = element_blank()\n    )\n\n\n\n\n\n\n\n\n\nOutra saída seria dividir os dados em grupos menores (e.g. alta, média-alta, média-baixa, baixa) e usar a função facet_wrap. O lado negativo disto, além de tornar o código mais complexo, é de acrescentar divisões nos dados que eventualmente são muito artificiais.\nAqui eu faço um divisão por quintil e crio uma denominação um tanto arbitrária para permitir a leitura dos dados.\n\n\nCode\nxl &lt;- c(\"Baixo\", \"Médio-Baixo\", \"Médio\", \"Médio-Alto\", \"Alto\")\n\nquintile_df &lt;- ordered_df %&gt;%\n  mutate(\n    life_group = ntile(expec_vida, 5),\n    life_group = factor(life_group, labels = xl)\n    )\n\nggplot(quintile_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_col(colour = \"white\", fill = \"#08519c\") +\n  coord_flip() +\n  facet_wrap(vars(life_group), scales = \"free_y\") +\n  labs(x = NULL, y = NULL, title = \"Expectativa de Vida\") +\n  theme_vini +\n  theme(\n    text = element_text(size = 8),\n    strip.text = element_text(size = 10),\n    panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#lolipop",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#lolipop",
    "title": "Visualizando uma única variável",
    "section": "Lolipop",
    "text": "Lolipop\nEste tipo de gráfico vem ganhando espaço mesmo em veículos de mídia populares por ser bastante simples. Ele é mais comumente usado para mostrar a evolução de uma variável em dois momentos do tempo, mas também pode-se usá-lo analogamente a um gráfico de colunas.\nInfelizmente, neste exemplo, ele vai sofrer do mesmo problema que o gráfico de colunas. A título de exemplo faço um gráfico deste estilo apenas para os distritos com as maiores e menores expectativas de vida.\n\n\nCode\n# Ordena a base de dados pela expectativa de vida\nordered_df &lt;- arrange(ordered_df, expec_vida)\n# Cria uma tabela com as primeiras 5 e últimas 5 linhas\ntop_df &lt;- rbind(head(ordered_df, 5), tail(ordered_df, 5))\n\nggplot(top_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_segment(aes(xend = label_distrito, yend = 55)) +\n  geom_point(colour = \"black\", fill = \"#08519c\", size = 4, shape = 21) +\n  coord_flip() +\n    labs(\n      y = \"Anos de vida\",\n      x = NULL,\n      title = \"Expectativa de vida\",\n      subtitle = \"Idade média ao morrer em 2022. Dados apenas dos distritos\\ncom maiores e menores expectativas de vida.\",\n      caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\"\n    ) +\n  theme_vini +\n  theme(panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-dispersão",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-dispersão",
    "title": "Visualizando uma única variável",
    "section": "Gráfico de dispersão",
    "text": "Gráfico de dispersão\nGráficos de dispersão apresentam a relação entre duas variáveis. Neste caso, podemos fazer alguns truques para inventar uma variável falsa que serve somente para que o R faça o gráfico.\nUma alternativa seria impor um expec_vida constante e arbitrário para uma das variáveis. Neste caso escolho x = 1 e escondo o eixo. No gráfico abaixo, cada distrito é um ponto sobre uma mesma linha. Para amenizar a sobreposição de observações uso alpha = 0.5 que acrescenta um pouco de transparência nas observações.\n\n\nCode\ndf_aux &lt;- tibble(\n  x = 1.025,\n  y = df_media$expec_vida,\n  label = paste0(\"Média = \", round(y, 1))\n)\n\nggplot(df, aes(x = 1, y = expec_vida)) +\n  geom_vline(xintercept = 1, colour = \"gray60\", alpha = 0.5) +\n  geom_point(\n    aes(colour = highlight, alpha = highlight, size = populacao_total),\n    shape = 21,\n    fill = \"#08519c\",\n  ) +\n  geom_text(\n    data = df_aux_label,\n    aes(x = c(0.95, 0.95, 1.05, 1.05, 1.05), y = expec_vida, label = label_distrito),\n    family = \"Roboto Light\",\n    size = 4\n  ) +\n  geom_segment(\n    data = df_aux_label,\n    colour = \"gray25\",\n    aes(\n      x = 1, xend = c(0.955, 0.955, 1.045, 1.045, 1.045),\n      y = expec_vida, yend = expec_vida\n    )\n  ) +\n  geom_segment(\n    data = tibble(x = 1, xend = 1.02, y = df_media$expec_vida, yend = y),\n    aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"gray25\",\n  ) +\n  geom_text(\n    data = df_aux,\n    aes(x = x, y = y, label = label),\n    family = \"Roboto Light\",\n    size = 3\n  ) +\n  coord_flip() +\n  scale_x_continuous(limits = c(0.94, 1.06)) +\n  scale_y_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  scale_alpha_manual(values = c(0.45, 0.8)) +\n  scale_size_continuous(range = c(1, 7.5)) + \n  scale_colour_manual(values = c(\"#08519c\", \"black\")) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"\",\n    y = \"Anos de idade\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022. Cada ponto representa um distrito de São Paulo.\"\n  ) +\n  theme_vini +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nUma solucão alternativa seria criar uma variável aleatória qualquer para servir como a variável “falsa”. Estranhamente, cria-se uma sensação melhor de dispersão e não há mais o problema de sobreposição. Contudo, o gráfico pode ser bastante confuso, pois dá a entender que estamos vendo a relação entre duas variáveis distintas, quando uma delas, na verdade, não representa coisa alguma.\nAinda assim, ideias similares foram implementadas pelo portal Nexo nesta postagem.\n\n\nCode\n# Cria uma variável aleatória qualquer seguindo uma Gaussiana\ndf &lt;- df %&gt;% mutate(x = rnorm(nrow(.)))\n\n# Para destacar o expec_vida médio\ndf_aux &lt;- tibble(\n  x = 2.25,\n  y = df_media$expec_vida + 0.75,\n  label = paste(\"Média =\", round(y - 0.75, 1))\n)\n\ndf_aux_label &lt;- df %&gt;%\n  mutate(label_distrito = if_else(highlight == 1L, label_distrito, \"\"))\n\nggplot(df, aes(x = x, y = expec_vida)) +\n  geom_jitter(aes(alpha = highlight, size = populacao_total),\n    shape = 21,\n    fill = \"#08519c\"\n  ) +\n  ggrepel::geom_text_repel(\n    data = df_aux_label,\n    aes(label = label_distrito),\n    force = 5,\n    family = \"Roboto Light\",\n    size = 3\n  ) +\n  geom_text(\n    data = df_aux,\n    aes(x = x, y = y, label = label),\n    hjust = -0.15\n  ) +\n  geom_hline(aes(yintercept = mean(expec_vida)), colour = \"gray70\", size = 1) +\n  coord_flip() +\n  scale_alpha_manual(values = c(0.65, 1)) +\n  scale_y_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  scale_x_continuous(limits = c(-3, 3)) +\n  labs(\n    title = \"Expectativa de Vida (idade média ao morrer)\",\n    x = \"\",\n    y = \"Anos de idade\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\"\n  ) +\n  guides(colour = FALSE, alpha = FALSE) +\n  theme_vini +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#mapa",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#mapa",
    "title": "Visualizando uma única variável",
    "section": "Mapa",
    "text": "Mapa\nDeixei a visualização mais óbvia para o final. Como a variável está distribuída espacialmente, pode-se fazer um simples mapa de São Paulo separado por distritos.\nAqui, eu uso o shapefile de distritos da Pesquisa Origem e Destino. Apesar de não haver um identificador comum entre as bases, foi relativamente simples fazer o join compatibilizando o nome dos distritos.\n\n\nCode\n# Importa o shapefile\ndistritos &lt;- st_read(\n  here::here(\"posts\", \"general-posts\", \"repost-mapa-desigualdade\", \"districts.gpkg\"),\n  quiet = TRUE\n  )\n\ndistritos &lt;- distritos %&gt;%\n  # Filtra apenas distritos de SP (capital)\n  filter(code_district &lt;= 96) %&gt;%\n  # Renomeia a coluna para facilitar o join\n  rename(distrito = name_district)\n\n# Verifica se ha distritos com nome diferente (Mooca)\nanti &lt;- anti_join(select(df, distrito), distritos, by = \"distrito\")\n# Altera a grafia para garantir o join\ndf &lt;- df %&gt;%\n  mutate(\n    distrito = if_else(distrito == \"Moóca\", \"Mooca\", distrito)\n  )\n# Junta os dados no sf\ndistritos &lt;- left_join(distritos, df, by = \"distrito\")\n\n\nO gráfico abaixo mostra a expectativa de vida em cada distrito na cidade.\n\n\nCode\nggplot(distritos) +\n  geom_sf(aes(fill = expec_vida), linewidth = 0.1) +\n  scale_fill_viridis_c(name = \"Expectativa\\nde Vida\") +\n  ggtitle(\"Expectativa de Vida por Distrito\") +\n  theme_void() +\n  theme(\n    legend.title = element_text(hjust = 0.5, size = 10),\n    legend.position = c(0.8, 0.3)\n  )\n\n\n\n\n\n\n\n\n\nOutra maneira de apresentar este dado é agrupando-o de alguma forma. Eu sou particularmente parcial ao algoritmo de Jenks.\n\n\nCode\n# Encontra os intervalos de cada grupo\nbreaks &lt;- classInt::classIntervals(distritos$expec_vida, n = 7, style = \"jenks\")\n# Classifica os valores em grupos\ndistritos &lt;- distritos %&gt;%\n  mutate(\n    jenks_group = cut(expec_vida, breaks = breaks$brks, include.lowest = TRUE)\n  )\n\nggplot(distritos, aes(fill = jenks_group)) +\n  geom_sf(linewidth = 0.1, color = \"gray80\") +\n  scale_fill_brewer(palette = \"BrBG\", name = \"Expectativa\\nde Vida\") +\n  ggtitle(\"Expectativa de Vida por Distrito\") +\n  theme_void() +\n  theme(\n    legend.title = element_text(hjust = 0.5, size = 10),\n    legend.position = c(0.8, 0.3)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-rdt-brasil/index.html",
    "href": "posts/general-posts/2024-04-wz-rdt-brasil/index.html",
    "title": "Razão de Dependência no Brasil",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(leaflet)\n\nbrasil = geobr::read_country(showProgress = FALSE)\ncenter = st_coordinates(st_centroid(brasil))\nstate_border = geobr::read_state(showProgress = FALSE)\ndim_state = as_tibble(st_drop_geometry(state_border))\n\ncodes = c(93070, 93084:93098, 49108, 49109, 60040, 60041, 6653)\n\ntab_population = sidrar::get_sidra(\n  9514,\n  variable = 93,\n  geo = \"State\",\n  classific = \"c287\",\n  category = list(codes)\n)\n\ntab_pop &lt;- tab_population |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble() |&gt; \n  filter(sexo == \"Total\", forma_de_declaracao_da_idade == \"Total\") |&gt; \n  select(\n    code_state = unidade_da_federacao_codigo,\n    age_group = idade,\n    count = valor\n  )\n\ntab_pop &lt;- tab_pop |&gt; \n  mutate(\n    code_state = as.numeric(code_state),\n    age_min = as.numeric(stringr::str_extract(age_group, \"\\\\d+\")),\n    age_group = factor(age_group),\n    age_group = forcats::fct_reorder(age_group, age_min),\n    age_ibge = case_when(\n      age_min &lt; 15 ~ \"young\",\n      age_min &gt;= 15 & age_min &lt; 65 ~ \"adult\",\n      age_min &gt;= 65 ~ \"elder\"\n    ),\n    factor(age_ibge, levels = c(\"young\", \"adult\", \"elder\"))\n  )\n\npop_state &lt;- tab_pop %&gt;%\n  summarise(\n    total = sum(count), .by = c(\"age_ibge\", \"code_state\")\n  ) %&gt;%\n  pivot_wider(\n    id_cols = \"code_state\",\n    names_from = \"age_ibge\",\n    values_from = \"total\"\n  ) %&gt;%\n  mutate(\n    dre = elder / adult * 100,\n    dry = young / adult * 100,\n    tdr = dre + dry\n  )\n\ntab_pop_state &lt;- left_join(dim_state, pop_state, by = \"code_state\")\npop &lt;- left_join(state_border, pop_state, by = \"code_state\")\n\npal_tdr &lt;- colorBin(\n  palette = as.character(MetBrewer::met.brewer(\"Hokusai2\", 5)),\n  domain = pop$tdr,\n  bins = BAMMtools::getJenksBreaks(pop$tdr, k = 6)\n)\n\npal_rdi &lt;- colorBin(\n  palette = as.character(MetBrewer::met.brewer(\"Hokusai2\", 5)),\n  domain = pop$dre,\n  bins = BAMMtools::getJenksBreaks(pop$dre, k = 6)\n)\n\npal_rdj &lt;- colorBin(\n  palette = as.character(MetBrewer::met.brewer(\"Hokusai2\", 5)),\n  domain = pop$dry,\n  bins = BAMMtools::getJenksBreaks(pop$dry, k = 6)\n)\n\nlabels &lt;- sprintf(\n  \"&lt;b&gt;RDT&lt;b/&gt;: %s &lt;br&gt;\n   &lt;b&gt;RDJ&lt;b/&gt;: %s &lt;br&gt;\n   &lt;b&gt;RDI&lt;b/&gt;: %s &lt;br&gt;\",\n  format(round(pop$tdr, 1), decimal.mark = \",\"),\n  format(round(pop$dry, 1), decimal.mark = \",\"),\n  format(round(pop$dre, 1), decimal.mark = \",\")\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nmap &lt;- leaflet(pop) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    group = \"RDT (Total)\",\n    fillColor = ~ pal_tdr(tdr),\n    weight = 2,\n    color = \"white\",\n    fillOpacity = 0.9,\n    highlightOptions = highlightOptions(\n      color = \"#e09351\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE\n    ),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addPolygons(\n    group = \"RDJ (Jovem)\",\n    fillColor = ~ pal_rdj(dry),\n    weight = 2,\n    color = \"white\",\n    fillOpacity = 0.9,\n    highlightOptions = highlightOptions(\n      color = \"#e09351\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE\n    ),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addPolygons(\n    group = \"RDI (Idoso)\",\n    fillColor = ~ pal_rdi(dre),\n    weight = 2,\n    color = \"white\",\n    fillOpacity = 0.9,\n    highlightOptions = highlightOptions(\n      color = \"#e09351\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE\n    ),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal_tdr,\n    values = ~tdr,\n    labFormat = labelFormat(digits = 1),\n    title = \"RDT (2022)\",\n    position = \"bottomright\",\n    group = \"RDT (Total)\"\n  ) %&gt;%\n  addLegend(\n    pal = pal_rdj,\n    values = ~dry,\n    labFormat = labelFormat(digits = 1),\n    title = \"RDJ (2022)\",\n    position = \"bottomright\",\n    group = \"RDJ (Jovem)\"\n  ) %&gt;%\n  addLegend(\n    pal = pal_rdi,\n    values = ~dre,\n    labFormat = labelFormat(digits = 1),\n    title = \"RDI (2022)\",\n    position = \"bottomright\",\n    group = \"RDI (Idoso)\"\n  ) %&gt;%\n  addLayersControl(\n    overlayGroups = c(\"RDT (Total)\", \"RDJ (Jovem)\", \"RDI (Idoso)\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %&gt;%\n  addProviderTiles(providers$CartoDB) |&gt;\n  setView(lng = -53.1873, lat = -15.58913, zoom = 4) %&gt;%\n  groupOptions(group = \"RDT (Total)\", zoomLevels = 4) %&gt;%\n  groupOptions(group = \"RDJ (Jovem)\", zoomLevels = c(1, 18)) %&gt;%\n  groupOptions(group = \"RDI (Idoso)\", zoomLevels = c(1, 18))"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-rdt-brasil/index.html#sobre-a-razão-de-dependência",
    "href": "posts/general-posts/2024-04-wz-rdt-brasil/index.html#sobre-a-razão-de-dependência",
    "title": "Razão de Dependência no Brasil",
    "section": "Sobre a Razão de Dependência",
    "text": "Sobre a Razão de Dependência\nA razão de dependência mede a proporção da população que depende, num sentido amplo, do trabalho da população economicamente ativa. No Brasil, o IBGE define a Razão de Dependência Total (RDT), como a razão entre o número de jovens e idosos e o número de adultos (em idade de trabalhar). Como critério de corte, define-se jovem como uma pessoa com até 14 anos e idoso como uma pessoa com 65 anos ou mais.Formalmente, define-se a RDT como:\n\\[\\text{RDT} = \\frac{\\text{Idade}\\leq14 \\, \\lor \\, \\text{Idade} \\geq 65}{\\text{Idade} &gt;14 \\, \\land \\, \\text{Idade} &lt; 65}\n\\]\nO denominador da fórmula representa a População Economicamente Ativa (PEA) e é uma proxy para a população trabalhadora.\nAdicionalmente, também pode-se definir a Razão de Dependência Jovem (RDJ) e a Razão de Dependência Idosa (RDI)\n\\[\n\\text{RDJ} = \\frac{\\text{Idade}\\leq14}{\\text{Idade} &gt;14 \\, \\land \\, \\text{Idade} &lt; 65} = \\frac{\\text{Idade}\\leq14}{\\text{PEA}}\n\\]\n\\[\n\\text{RDI} = \\frac{\\text{Idade} \\geq 65}{\\text{Idade} &gt;14 \\, \\land \\, \\text{Idade} &lt; 65} = \\frac{\\text{Idade} \\geq 65}{\\text{PEA}}\n\\]\nVale notar que:\n\\[\n\\text{RDT} = \\text{RDJ} + \\text{RDI}\n\\]"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html",
    "title": "Uruguay in numbers",
    "section": "",
    "text": "Uruguay has been making the headlines in the past decade in a good way: many consider the country to be an model democracy: topping the Economist’s Democracy Index and being the least corrupt country in Latin America are some factors that bring attention to this small country. It also appears to be one the least unequal countries in the region with a GINI index close to 40 (Brazil’s is somewhere around 52-53 for comparison).\nAs a native Brazilian, who lived most of his life in Porto Alegre - a mere 800km travel away from Uruguay’s capital Montevideo -, and a relatively fluent reader of the Spanish language, I must confess that I know very little about Uruguay. In fact, the little I know about Uruguay’s history is the brief time period when Uruguay was a contested territory between the Kingdoms of Spain and Portugal1.\nSo this saturday afternoon I decided to learn all that I could about Uruguay and (sort of) document my steps."
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#where-to-find-the-data",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#where-to-find-the-data",
    "title": "Uruguay in numbers",
    "section": "Where to find the data?",
    "text": "Where to find the data?\nWhen looking for socioeconomic and demographic data on countries I usually start at usual suspects:\n\nWorld Bank\nIMF\nOur World in Data\nUN\n\nUnfortunately, Uruguay is not a member of the OECD so we can’t use that. For more precise/specific comparison, specially on GDP I also recommend both the Maddison Project and the Penn World Table. All of the aforementioned sources also have convenience packages built for R!\n\n\n\nName\nSite\nR-package\n\n\n\n\nIMF\nlink\nIMFData, imfr\n\n\nWorld Bank\nlink\nWDI\n\n\nUN\nlink\nwpp2022\n\n\nMaddison Project\nlink\nmaddison\n\n\nPenn World Table\nlink\npwt10\n\n\nOECD\nlink\noecd\n\n\nOur World in Data\nlink\nowidR\n\n\n\nThe list above is not intended to be comprehensive but it’s a good starting point (plus, they’re all free). For Latin American countries it might be useful to include Global Data Lab and CEPAL. Also, I always like to check in for the National Statistical Bureau and the Central Bank of the country. In Uruguay’s case that would be the Instituto Nacional de Estadística (INE) and the Banco Central de Uruguay. By the way, some useful glossay/translations for foreigners include:\n\n\n\nSpanish\nEnglish\n\n\n\n\nencuesta\nsurvey/interview\n\n\ningreso/salario\nincome/wage\n\n\ndesarollo\ndevelopment\n\n\ninversión\ninvestment\n\n\nempleo\nemployment\n\n\ndesempleo\nunemployment\n\n\npromedio\naverage\n\n\nalquiler/renta\nrent\n\n\nventa\nsale\n\n\n\nFinally, if you don’t mind working with slightly outdated data (not my case) you can use packages such as gapminder to get a broad view on the country."
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#economy-and-population",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#economy-and-population",
    "title": "Uruguay in numbers",
    "section": "Economy and population",
    "text": "Economy and population\nA good starting point is the Penn World Table included in the {pwt10} package. We can work this table to find some basic facts about Uruguay such as:\n\nReal GDP (rgdpe)\nTotal population (pop)\nAvg. hours worked per year (avh)\n\n\nlibrary(pwt10)\n\npwt = pwt10.0\n\ntab_decade &lt;- pwt %&gt;%\n  # Get only the last year of each decade\n  filter(isocode == \"URY\", year %in% seq(1959, 2019, 10)) %&gt;%\n  mutate(\n    # Real GDP per capita\n    rgdppc = rgdpe / pop,\n    # Convert population\n    pop = pop * 1e6,\n    # Convert year to character for easier formatting of the table\n    year = as.character(year)) %&gt;%\n  select(year, rgdpe, rgdppc, pop, avh, hc, csh_x)\n\n# Swap names for table\nold_names &lt;- names(tab_decade)\nnew_names &lt;- c(\n  \"Year\", \"Real GDP\", \"Real GDP per capita\", \"Population\", \"Avg. Hours Worked\",\n  \"Human Capital Index\", \"Share of Exports (%GDP)\")\nnames(new_names) &lt;- old_names\n\n# The table\ngt(tab_decade) %&gt;%\n  cols_label(.list = new_names) %&gt;%\n  fmt_number(columns = c(rgdpe:pop, avh), decimals = 0) %&gt;%\n  fmt_number(columns = hc, decimals = 2) %&gt;%\n  fmt_percent(columns = csh_x) %&gt;%\n  sub_missing(missing_text = \"-\") %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\nYear\nReal GDP\nReal GDP per capita\nPopulation\nAvg. Hours Worked\nHuman Capital Index\nShare of Exports (%GDP)\n\n\n\n\n1959\n18,377\n7,402\n2,482,770\n-\n1.80\n4.69%\n\n\n1969\n22,091\n7,923\n2,788,100\n-\n1.92\n5.06%\n\n\n1979\n27,998\n9,669\n2,895,688\n-\n2.12\n7.82%\n\n\n1989\n30,551\n9,892\n3,088,595\n-\n2.33\n10.97%\n\n\n1999\n42,008\n12,699\n3,308,012\n1,722\n2.51\n11.26%\n\n\n2009\n53,079\n15,846\n3,349,676\n1,604\n2.57\n16.44%\n\n\n2019\n73,411\n21,206\n3,461,734\n1,533\n2.78\n17.47%\n\n\n\n\n\n\n\nNow, the table above could be improved since variables such as the unemployment rate or average hours worked could be averaged by decade for a more fair presentation. But the goal here is to keep things simple.\nThe first striking feature of the table for me was Uruguay’s population. In the 60 year period from 1959 to 2019, total population increased around 39% reaching 3,46 million. For comparison, the similar sized Brazilian state of Rio Grande do Sul has a population close to 10 million, despite suffering population losses due to migration in recent years."
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#latam",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#latam",
    "title": "Uruguay in numbers",
    "section": "LATAM",
    "text": "LATAM\nLets compare how Uruguay fared against other LATAM countries. For this comparison I will take: Argentina, Brazil, Paraguay and Chile. I also add the most recent HDI estimate from the UN. Here I simply scrape the wikipedia page for “List of countries by Human Development Index”.\n\ncountry_selection &lt;- c(\"Argentina\", \"Brazil\", \"Chile\", \"Paraguay\", \"Uruguay\")\n\ntab_latam &lt;- pwt %&gt;%\n  # Get only the last year of each decade\n  filter(country %in% country_selection, year == 2019) %&gt;%\n  mutate(\n    # Real GDP per capita\n    rgdppc = rgdpe / pop,\n    # Convert population\n    pop = pop * 1e6) %&gt;%\n  select(country, rgdpe, rgdppc, pop, avh, hc, csh_x)\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index\"\n\nhdi &lt;- url |&gt; \n  xml2::read_html() |&gt; \n  html_table() |&gt; \n  pluck(2)\n\nnm &lt;- paste(names(hdi), unlist(hdi[1, ]), sep = \"_\")\ndat &lt;- data.frame(as.matrix(hdi[-1, ]))\nnames(dat) &lt;- janitor::make_clean_names(nm)\ndat &lt;- as_tibble(dat)\n\nhdi_clean &lt;- dat |&gt; \n  select(\n    country = nation,\n    hdi = hdi_na,\n    hdi_growth = percent_growth\n  ) |&gt; \n  mutate(\n    hdi = as.numeric(hdi),\n    hdi_growth = as.numeric(str_remove(hdi_growth, \"%\")) / 100\n  )\n\ntab_latam &lt;- left_join(tab_latam, hdi_clean, by = \"country\")\n\n# Swap names for table\nold_names &lt;- names(tab_latam)\nnew_names &lt;- c(\n  \"Country\", \"Real GDP\", \"Real GDP per capita\", \"Population\", \"Avg. Hours Worked\",\n  \"Human Capital Index\", \"Share of Exports (%GDP)\", \"HDI\", \"HDI Growth\")\nnames(new_names) &lt;- old_names\n\n# The table\ngt(tab_latam) %&gt;%\n  cols_label(.list = new_names) %&gt;%\n  fmt_number(columns = c(rgdpe:pop, avh), decimals = 0) %&gt;%\n  fmt_number(columns = hc, decimals = 2) %&gt;%\n  fmt_number(columns = hdi, decimals = 3) %&gt;%\n  fmt_percent(columns = c(csh_x, hdi_growth)) %&gt;%\n  sub_missing(missing_text = \"-\") %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\nCountry\nReal GDP\nReal GDP per capita\nPopulation\nAvg. Hours Worked\nHuman Capital Index\nShare of Exports (%GDP)\nHDI\nHDI Growth\n\n\n\n\nArgentina\n990,312\n22,115\n44,780,677\n1,609\n3.10\n10.43%\n0.842\n0.09%\n\n\nBrazil\n3,087,570\n14,630\n211,049,527\n1,708\n3.09\n13.47%\n0.754\n0.38%\n\n\nChile\n446,942\n23,583\n18,952,038\n1,914\n3.15\n24.32%\n0.855\n0.46%\n\n\nParaguay\n85,462\n12,132\n7,044,636\n-\n2.66\n15.28%\n0.717\n0.42%\n\n\nUruguay\n73,411\n21,206\n3,461,734\n1,533\n2.78\n17.47%\n0.809\n0.25%\n\n\n\n\n\n\n\nUruguay’s HDI is barely in the “very high” zone (&gt;=0.8). It’s above both Brazil and Paraguay, but lags behind Argentina and Chile. In terms of real GDP per capita, Uruguay is very close to Argentina and Chile.\n\nPopulation\nIn the next chart, I explore the population dynamics of these countries.\n\ncountry_selection &lt;- c(\"Argentina\", \"Brazil\", \"Chile\", \"Paraguay\", \"Uruguay\")\n\npopindex &lt;- pwt %&gt;%\n  filter(country %in% country_selection, !is.na(pop)) %&gt;%\n  group_by(country) %&gt;%\n  mutate(index_pop = pop / first(pop) * 100)\n\nggplot(popindex, aes(x = year, y = index_pop, color = country)) +\n  geom_hline(yintercept = 100) +\n  geom_line() +\n  scale_color_manual(name = \"\", values = colors_report) +\n  labs(\n    title = \"Falling behind\",\n    subtitle = \"Population growth in selected LATAM countries.\\nUruguay exhibits much lower populational growth than nearby neighbors.\",\n    caption = \"Source: PWT 10.0\",\n    x = NULL,\n    y = \"Index (100 = 1951)\") +\n  theme_report\n\n\n\n\n\n\n\n\nAs suspected, Uruguay has much lower population growth than its neighbors in its recent history. In the same time frame that Uruguay’s population grew around 58,5%, Brazil’s population doubled twice.\nFor some of these comparisons it can be useful to look at regional averages. The issue is grouping countries appropriately. The best way is to rely on standard conventions on country groups and regions. An easy way to get a table with such a grouping is the WDI::WDI_data$country. This is a data.frame with country names (and iso3c codes) with regions defined by the World Bank plus some useful information.\nIn the case of Uruguay, the WB classifies as part of the Latin America & Caribbean reigon and as a high-income country.\n\ncountry_groups &lt;- WDI::WDI_data$country\n\ncountry_groups %&gt;%\n  filter(iso3c == \"URY\")\n\n  iso3c iso2c country                    region    capital longitude latitude\n1   URY    UY Uruguay Latin America & Caribbean Montevideo  -56.0675 -34.8941\n       income lending\n1 High income    IBRD\n\n\nWe can join this table with the pwt table and get averages across LATAM. The code below is probably overkill, since it sums, averages, and indexes all variables across the LATAM countries.\n\n# Drop country name from table to avoid duplicate column name after join\ncountry_groups &lt;- country_groups %&gt;%\n  select(-country)\n\nlatam &lt;- pwt %&gt;%\n  mutate(rgdppc = rgdpe / pop) %&gt;%\n  left_join(country_groups, by = c(\"isocode\" = \"iso3c\")) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    across(\n      where(is.numeric),\n      list(\"total\" = sum, \"avg\" = mean),\n      na.rm = TRUE,\n      .names = \"latam_{.fn}_{.col}\")\n    )\n\nlatam_index &lt;- latam %&gt;%\n  pivot_longer(-year) %&gt;%\n  filter(!is.na(value)) %&gt;%\n  group_by(name) %&gt;%\n  mutate(index = value / first(value) * 100) %&gt;%\n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"name\",\n    values_from = \"index\"\n    )\n\ntab_comparison &lt;- pwt %&gt;%\n  mutate(rgdppc = rgdpe / pop) %&gt;%\n  filter(country == \"Uruguay\") %&gt;%\n  select(year, pop, rgdppc) %&gt;%\n  mutate(\n    uruguay_pop = pop / first(pop) * 100,\n    uruguay_rgdppc = rgdppc / first(rgdppc) * 100) %&gt;%\n  left_join(select(latam_index, year, latam_total_pop, latam_avg_rgdppc))\n\ntab_comparison &lt;- tab_comparison %&gt;%\n  select(-pop, -rgdppc) %&gt;%\n  pivot_longer(-year, names_to = \"series\", values_to = \"index\") %&gt;%\n  separate(series, into = c(\"region\", \"variable\"), extra = \"merge\") %&gt;%\n  mutate(\n    variable = str_remove(variable, \"(total_)|(avg_)\"),\n    variable = factor(variable, labels = c(\"Real GDP per capita\", \"Population\"))\n    )\n\nggplot(tab_comparison, aes(x = year, y = index, color = region)) +\n  geom_hline(yintercept = 100) +\n  geom_line() +\n  scale_color_manual(\n    name = \"\",\n    values = colors_report,\n    labels = c(\"Latin America & Carribean Avg.\", \"Uruguay\")) +\n  facet_wrap(vars(variable)) +\n  labs(\n    caption = \"Source: PWT\",  \n    x = NULL,\n    y = \"Index (100 = 1951)\"\n  ) +\n  theme_report\n\n\n\n\n\n\n\n\nSo, Uruguay is way behind LATAM in growth. Note that in both plots the LATAM series has spikes which most likely indicate that new countries have been introduced in the series. This may not be ideal and in a more thorough analysis we should account for that in some manner. A simple solution would be to simply work with more recent data, since it is less likely to contain missing observations.\n\n\nEconomy\nLets go back to our basic fact table. To better visualize how these variables change we can repeat the analysis above. To avoid copy-pasting and to keep things simple we can build a simple function that compares variables from the pwt table. The code below defines the plot_comparison function that compares the historic values of a variable across the five countries we selected. I also defined two additional arguments that can prove to be helpful. First a start_year to filter the time horizon of the anaylsis. Second a logical index indicating if the variable should be indexed to the first available value (TRUE).\n\npwt &lt;- pwt %&gt;%\n  mutate(\n    # Real GDP per capita\n    rgdppc = rgdpe / pop\n  )\n\nplot_comparison &lt;- function(variable, start_year = 1950, index = TRUE, ...) {\n  \n  dat &lt;- pwt %&gt;%\n    filter(\n      country %in% country_selection,\n      year &gt;= start_year,\n      !is.na({{variable}})\n      )\n  \n  if (isTRUE(index)) {\n    dat &lt;- dat %&gt;%\n       group_by(country) %&gt;%\n      mutate(index_var = {{variable}} / first({{variable}}) * 100)\n    \n    p &lt;- ggplot(dat, aes(x = year, y = index_var, color = country)) +\n      geom_hline(yintercept = 100)\n  } else {\n    \n    p &lt;- ggplot(dat, aes(x = year, y = {{variable}}, color = country))\n    \n    }\n  \n  p &lt;- p +\n    geom_line(lwd = 0.8) +\n    scale_color_manual(name = \"\", values = colors_report) +\n    labs(x = NULL, ...) +\n    theme_report\n  \n  return(p)\n  \n}\n\nTo exemplify how this function works lets see how was the growth in Real GDP per capita across these countries.\n\nplot_comparison(rgdppc, title = \"Real GDP per capita\", y = \"Index (1950 = 100)\")\n\n\n\n\n\n\n\n\nBy default, the values are indexed to first available (i.e. non-NA) value. During this time window the best performing country was Brazil, followed by Argentina. Note that the plot above shows the growth of Real GDP per capita. To find the actual values of the variable we set index = FALSE.\n\nplot_comparison(rgdppc, index = FALSE) +\n  ggtitle(\"Real GDP per capita\") +\n  ylab(\"US$ constant\")\n\n\n\n\n\n\n\n\nWhile Uruguay didn’t fare as well in the full time-period, we could analyze its growth pattern following the most recent Commodity Boom. For those unfamiliar with LATAM economies it is important to note that commodity prices have a strong correlation with boom and bust cycles. The most recent and relevant commodity boom was the 2002-2012 period, fueled by Chinese exceptional growth.\nAs the plot below reveals all countries perform well during this time window. Uruguay has the strongest growth among them but note that all of them slowdown past 2015. Brazil, in fact, faces a strong internal recession and, a few years later, Argentina also goes to bust.\nFinally, since the data is available only until 2019 we don’t see how these economies fared during the COVID years.\n\nplot_comparison(rgdppc, 2002) +\n  ggtitle(\"Real GDP per capita\") +\n  ylab(\"Index (2002 = 100)\")\n\n\n\n\n\n\n\n\nWhen looking at average hours worked we see a similar pattern in all countries. Uruguay has the second highest fall (unfortunately there is no information on Paraguay) meaning the average worker is working ~15% less hours in 2019 than in 1990. This is common trend among most economies.\n\nplot_comparison(avh, 1990) +\n  ggtitle(\"Average hours of work\") +\n  ylab(\"Index (1990 = 100)\")\n\n\n\n\n\n\n\n\nAs mentioned above, most LATAM countries are commodity-exporters and commodity prices have strong repercussions on the domestic economy. This is true, despite most LATAM countries having fairly low exports/imports shares. Exports account for less than 20% of GDP even during the 2002-2012 commodity boom.\nGiven that smaller countries tend to have higher international trade volumes I was surprised by how low both Paraguay and Uruguay appear on this plot. Chile is a LATAM outlier in this sense\n\nplot_comparison(csh_x, index = FALSE) +\n  ggtitle(\"Exports as share of GDP\") +\n  ylab(\"% GDP\")\n\n\n\n\n\n\n\n\nLong-term growth is only possible with sustained productivity growth. A decent proxy for this is the “total factor productivity” of the economy. Historically, LATAM countries have struggled to sustain growth for long periods of time: business cycles tend to be extremely volatile in the region.\nAs seen in the plot, total productivity is growing since 2002 but at very modest rates.\n\nplot_comparison(ctfp, 2002) +\n  ggtitle(\"Productivity\") +\n  ylab(\"TFP, Index (2002 = 100)\")\n\n\n\n\n\n\n\n\nCompare Uruguay to another emerging country such as Poland and we see a stark contrast.\n\npwt |&gt; \n  filter(country %in% c(\"Poland\", \"Uruguay\"), year &gt;= 1980) %&gt;%\n  ggplot(aes(x = year, y = ctfp, color = country)) +\n  geom_line() +\n  scale_color_manual(name = \"\", values = colors_report) +\n  theme_report\n\n\n\n\n\n\n\n\nGrowth in Latin American countries is volatile and Uruguay should be no exception. The plot below shows the year on year growth of real GDP per capita. The smooth line is the 6-year moving average.\n\npwt &lt;- pwt %&gt;%\n  group_by(country) %&gt;%\n  mutate(drgdppc = rgdppc / lag(rgdppc) - 1)\n\npwt %&gt;%\n  filter(country == \"Uruguay\") %&gt;%\n  mutate(trend = RcppRoll::roll_mean(drgdppc, n = 6, fill = NA)) %&gt;%\n  ggplot(aes(x = year)) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(y = drgdppc), color = colors_report[1], linewidth = 1) +\n  geom_line(aes(y = trend), color = colors_report[5]) +\n  theme_report"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#demographics",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#demographics",
    "title": "Uruguay in numbers",
    "section": "Demographics",
    "text": "Demographics\nAs we’ve seen above, Uruguay’s population is not growing much. I suspect this likely due to falling total fertility rates. Also, I’m curious to find out the age structure of the population as I suspect that Uruguay had a relatively fast demographic transition.\nI find that the World Population Projections by the UN is a good fit to answer these sort of questions.\nFirst lets look at the population projections for this century. This data, split by age, is in the popprojAge1dt table. To make for a cleaner visualization I group the ages into \"Less than 14\", \"15-24\", \"25-64\", \"65-84\", \"85+\".\nUruguay’s population is expected to grow only until 2027! From that point onwards it will start shrinking. By 2100 the country’s population is expected to be at around 2,4 million (smaller than what it was in 1959). Interestingly, the actual “structure” of the population seems to stay somewhat constant.\n\nlibrary(wpp2022)\n\n# Projections by age 2022-2100\ndata(\"popprojAge1dt\")\n# Estimates by age 1950-2022\ndata(\"popAge1dt\")\n\nproj_age &lt;- popprojAge1dt %&gt;%\n  filter(name == \"Uruguay\") %&gt;%\n  mutate(\n    year = as.numeric(year),\n    age_group = factor(\n      case_when(\n        age &lt;= 14 ~ \"Less than 14\",\n        age &gt; 14 & age &lt;= 24 ~ \"15-24\",\n        age &gt; 24 & age &lt;= 64 ~ \"25-64\",\n        age &gt; 64 & age &lt;= 84 ~ \"65-84\",\n        age &gt; 84 ~ \"85 or over\"),\n      levels = c(\"Less than 14\", \"15-24\", \"25-64\", \"65-84\", \"85 or over\")\n      )\n  ) %&gt;%\n  group_by(year, age_group) %&gt;%\n  summarise(total_pop = sum(pop)) %&gt;%\n  ungroup()\n\nggplot(proj_age, aes(x = year, y = total_pop, fill = age_group)) +\n  geom_area() +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = seq(2020, 2100, 10)) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n  scale_fill_manual(name = \"\", values = colors_report) +\n  labs(\n    title = \"Uruguay's population is expected to shrink\",\n    subtitle = \"UN population projections by age group from 2022 to 2100.\",\n    x = NULL,\n    y = \"Population (thousands)\",\n    caption = \"Source: WPP (2022)\"\n  ) +\n  theme_report +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\nThe share of the elder population (above 65 years) is expected to triple in the years to come.\n\nproj_age &lt;- popprojAge1dt %&gt;%\n  filter(name == \"Uruguay\") %&gt;%\n  mutate(\n    year = as.numeric(year),\n    age_group = case_when(\n        age &lt;= 14 ~ \"Young\",\n        age &gt; 14 & age &lt;= 64 ~ \"Adult\",\n        age &gt; 64 ~ \"Elder\")\n  ) %&gt;%\n  group_by(year, age_group) %&gt;%\n  summarise(total_pop = sum(pop)) %&gt;%\n  group_by(year) %&gt;%\n  mutate(share = total_pop / sum(total_pop)) %&gt;%\n  ungroup()\n\nproj_text &lt;- proj_age %&gt;%\n  filter(year == max(year)) %&gt;%\n  mutate(x = 2095, label = paste0(round(share * 100, 1), \"%\"))\n\nggplot(proj_age, aes(x = year, y = share, fill = age_group)) +\n  geom_area() +\n  geom_text(\n    data = proj_text,\n    aes(x = x, y = share, label = label),\n    position = position_stack(vjust = 0.5)) +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = seq(2020, 2100, 10)) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_fill_manual(name = \"\", values = colors_report) +\n  labs(\n    title = \"Uruguay's elder population expected to triple\",\n    subtitle = \"Projected population share of each group age 2022-2100\",\n    caption = \"Source: WPP (2022)\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_report +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n    )\n\n\n\n\n\n\n\n\nThere currently is a 1:1 ratio between the\n\nage_index &lt;- proj_age %&gt;%\n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"age_group\",\n    values_from = \"total_pop\"\n    ) %&gt;%\n  mutate(index = Elder / Young * 100)\n\nggplot(age_index, aes(x = year, y = index)) +\n  geom_hline(yintercept = 100) +\n  geom_line(color = colors_report[1], linewidth = 1) +\n  labs(\n    title = \"Aging Index\",\n    subtitle = \"Proportion of elder population in relation to young population\",\n    caption = \"Source: WPP (2022).\",\n    x = NULL,\n    y = \"Index (elder/young)\"\n  ) +\n  theme_report\n\n\n\n\n\n\n\n\nComparison of the demographic pyramid of Uruguay and Brazil reveals two very different patterns. Notice that Brazil’s pyramid initially takes on a triangular shape, with a broad base indicating a large share of young individuals. As the decades pass, there is a gradual reduction in the base of the pyramid, accompanied by an expansion in both the middle and upper segments, reflecting shifts in the country’s age structure. In contrast, Uruguay’s demographic shifts are more nuanced. While there is a noticeable rise in the elderly population and a decline in the younger demographic, these changes are comparatively less pronounced and lack the dramatic shifts observed in Brazil.\n\nx1 &lt;- seq(0, 80, 5)\nx2 &lt;- seq(4, 84, 5)\n\nxlabels &lt;- c(paste(x1, x2, sep = \"-\"), \"85+\")\n\npop_pyramid &lt;- popAge1dt %&gt;%\n  rename(country = name) %&gt;%\n  filter(\n    country %in% c(\"Brazil\", \"Uruguay\"),\n    year %in% c(1950, 1970, 1990, 2010, 2020)) %&gt;%\n  mutate(\n    age_group = findInterval(age, x2, left.open = TRUE),\n    age_group = factor(age_group, labels = xlabels)) %&gt;%\n  group_by(year, country, age_group) %&gt;%\n  summarise(female = sum(popF), male = sum(popM)) %&gt;%\n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"pop\") %&gt;%\n  group_by(year, country, sex) %&gt;%\n  mutate(share = pop / sum(pop) * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(share = if_else(sex == \"male\", -share, share))\n\nggplot(pop_pyramid, aes(x = age_group, y = share)) +\n  geom_col(aes(fill = sex)) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = seq(-15, 15, 5),\n    labels = c(15, 10, 5, 0, 5, 10, 15)) +\n  scale_fill_manual(values = colors_report[c(1, 3)]) +\n  guides(fill = \"none\") +\n  facet_grid(rows = vars(country), cols = vars(year)) +\n  labs(\n    title = \"Uruguay had a smaller demographic bonus\",\n    subtitle = \"Age pyramids for Brazil and Uruguay\",\n    caption = \"Source: WPP (2022)\",\n    y = \"%\",\n    x = NULL) +\n  theme_report +\n  theme(\n    text = element_text(size = 6),\n    panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#montevideo",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#montevideo",
    "title": "Uruguay in numbers",
    "section": "Montevideo",
    "text": "Montevideo\nObtaining reliable information about cities tends to be more challenging compared to data available for countries. In this analysis, I use the latest Mercer Quality of Living City Ranking to assess and rank the major cities in Latin America. The findings highlight Montevideo, the capital of Uruguay, as the highest-ranking city among its Latin American counterparts. However, it’s noteworthy that Montevideo only secures a place just within the top 100 on the overall list.\n\nlibrary(rvest)\nlibrary(emoji)\n\nmercer_url &lt;- \"https://mobilityexchange.mercer.com/Insights/quality-of-living-rankings\"\n\ncountry_latam &lt;- c(\n  \"Argentina\", \"Bolivia\", \"Brazil\", \"Chile\", \"Colombia\", \"Ecuador\", \n  \"Guyana\", \"Paraguay\", \"Peru\", \"Suriname\", \"Uruguay\", \"Venezuela\",\n  \"Mexico\", \"Panama\", \"Cuba\", \"Costa Rica\"\n  )\n\ntables &lt;- mercer_url %&gt;%\n  xml2::read_html() %&gt;%\n  html_table()\n\n# mercer &lt;- bind_rows(tables[[1]], tables[[2]])\nmercer &lt;- tables[[1]]\n\nmercer &lt;- mercer %&gt;%\n  janitor::clean_names() %&gt;%\n  rename(country = location, rank = ranking_2023) %&gt;%\n  mutate(\n    city = str_to_title(city),\n    country = str_to_title(country)\n    )\n  \n\n# mercer &lt;- mercer %&gt;%\n#   janitor::clean_names() %&gt;%\n#   unite(\"country\", c(country_region, country_region_2), na.rm = TRUE)\n\ntab_rank &lt;- mercer %&gt;%\n  filter(country %in% country_latam) %&gt;%\n  mutate(flag = map_chr(country, emoji::flag)) %&gt;%\n  select(rank, city, flag)\n\ntab_rank %&gt;%\n  gt() %&gt;%\n  cols_label(rank = \"Rank\", city = \"City\", flag = \"\") %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\nRank\nCity\n\n\n\n\n\n89\nMontevideo\n🇺🇾\n\n\n100\nBuenos Aires\n🇦🇷\n\n\n103\nPanama City\n🇵🇦\n\n\n104\nSantiago\n🇨🇱\n\n\n108\nSao Paulo\n🇧🇷\n\n\n115*\nSan Jose\n🇨🇷\n\n\n115*\nRio De Janeiro\n🇧🇷\n\n\n119\nBrasilia\n🇧🇷\n\n\n120\nBelo Horizonte\n🇧🇷\n\n\n121\nQuito\n🇪🇨\n\n\n122\nMonterrey\n🇲🇽\n\n\n123\nAsuncion\n🇵🇾\n\n\n129\nBogota\n🇨🇴\n\n\n130\nMexico City\n🇲🇽\n\n\n133\nLima\n🇵🇪\n\n\n150\nManaus\n🇧🇷\n\n\n167*\nLa Paz\n🇧🇴\n\n\n200\nHavana\n🇨🇺\n\n\n208*\nCaracas\n🇻🇪\n\n\n\n\n\n\n\n\nwiki_url &lt;- \"https://en.wikipedia.org/wiki/List_of_cities_in_Uruguay\"\n\ntables &lt;- html_table(xml2::read_html(wiki_url))\n\npop_muni &lt;- tables[[1]]\n\nas_numeric_string &lt;- Vectorize(function(x) {\n  \n  digits &lt;- str_extract_all(x, \"[[:digit:]]\")\n  digits &lt;- paste(unlist(digits), collapse = \"\")\n  as.numeric(digits)\n  \n  })\n\npop_muni &lt;- pop_muni %&gt;%\n  janitor::clean_names() %&gt;%\n  select(city, department, starts_with(\"pop\")) %&gt;%\n  mutate(across(starts_with(\"pop\"), as_numeric_string))\n\npop_depto &lt;- pop_muni %&gt;%\n  group_by(department) %&gt;%\n  summarise(total_pop = sum(population2011_census, na.rm = TRUE))\n\nFinally, I map the population of each district (departamento) of Uruguay.\n\nlibrary(geouy)\n\ndeptos &lt;- load_geouy(\"Departamentos\")\n\nReading layer `departamentos_v2' from data source \n  `https://mapas.mides.gub.uy/geoserver/IDE/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=IDE:departamentos_v2' \n  using driver `GML'\nSimple feature collection with 19 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 366582.2 ymin: 6127919 xmax: 858252.1 ymax: 6671738\nProjected CRS: WGS 84 / UTM zone 21S\n\ndeptos &lt;- st_make_valid(deptos)\n\npop_depto &lt;- pop_depto %&gt;%\n  mutate(\n    depto_name = str_remove(department, \"Department\"),\n    depto_name = str_to_upper(depto_name),\n    depto_name = stringi::stri_trans_general(depto_name, \"latin-ascii\"),\n    depto_name = str_trim(depto_name)\n  )\n\ndeptos_pop &lt;- left_join(deptos, pop_depto, by = c(\"nombre\" = \"depto_name\"))\n\n\nlibrary(tmap)\nlibrary(tmaptools)\ntmap_mode(mode = \"view\")\n\nm &lt;- tm_shape(deptos_pop) +\n  tm_fill(\n    col = \"total_pop\",\n    style = \"jenks\",\n    n = 5,\n    title = \"Population\",\n    id = \"department\") +\n  tm_borders() +\n  tm_basemap(server = \"CartoDB.Positron\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#footnotes",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#footnotes",
    "title": "Uruguay in numbers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a brief period, Uruguay was a part of Brazil (technically, the United Kingdom of Portugal, Brazil, and the Algarves). To this day, both Uruguay and Rio Grande do Sul, the southernmost Brazilian state, that borders Uruguay, have some degree of shared culture, mostly amongst the gaucho figure.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html",
    "href": "posts/general-posts/2024-04-radar-plots/index.html",
    "title": "Radar Plots",
    "section": "",
    "text": "Radar plots show a static visualization of the numeric values of several different categories of a same entity. These charts are also known as spider or web plots, since they visually resemble a spider web. The values of each variable are connected by a straight line and the resulting image is a polygon. These charts are commonly used to quickly access a collection of important variables and to make simple comparisons, and are frequently used in dashboards.\n\n\n\n\n\nExample of a radar plot\n\n\n\n\nThe plot above shows 11 characteristics of a car (Datsun 710, from the mtcars dataset). The vertical distance of each dot from the center represents the magnitude of each variable. Since the overall axis is “fixed” this type of plot is ideal to represent scaled or indexed values. In more general cases, this means some data transformation is needed to achieve a better final result."
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#the-data",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#the-data",
    "title": "Radar Plots",
    "section": "The data",
    "text": "The data\nThe Abrainc Radar is a collection of 12 variables, structured into 4 subgroups, that aim to capture the overall condition of the real estate market. These groups are: (1) macroeconomic conditions; (2) real estate credit conditions; (3) real estate demand; (4) real estate conditions.\nVariables include real estate prices, input costs, and interest rates. All variables are scaled in the 0-10 range, where 10 represents more favorable conditions.\nThe code below grabs the most recent data and makes minor improvements to the labels of the variables.\n\n# Download the data\nradar &lt;- get_abrainc_indicators(category = \"radar\")\n\n# Better labels for the plot\nradar_labels &lt;- c(\n  \"Confidence\", \"Activity\", \"Interest Rate\", \"Financing Conditions\",\n  \"Real Concessions\", \"Atractivity\", \"Employment\", \"Wages\",\n  \"Real Estate Investing\", \"Input Costs\", \"New Launches\",\n  \"Real Estate Prices\")\n\nradar &lt;- radar |&gt; \n  mutate(\n    month = lubridate::month(date),\n    variable = factor(variable, levels = unique(variable)),\n    variable_label = factor(\n      variable,\n      levels = unique(variable),\n      labels = radar_labels)\n  )\n\n# Get data only from the most recent period\ncurrent_period &lt;- radar |&gt; \n  filter(date == max(date)) |&gt; \n  pivot_wider(\n    id_cols = \"date\",\n    names_from = \"variable\",\n    values_from = \"value\"\n    )\n\nOur data is two columns and all variables are scaled in the 0-10 range, where higher values indicate more favorable conditions.\n\n\n\n\n\n\n\n\nNAME\nVALUE\n\n\n\n\nMacroeconomic\n\n\nConfidence\n5.97\n\n\nActivity\n9.05\n\n\nInterest Rate\n2.67\n\n\nReal Estate Credit\n\n\nFinancing Conditions\n0.63\n\n\nReal Concessions\n2.53\n\n\nAtractivity\n4.74\n\n\nMarket Demand\n\n\nEmployment\n8.51\n\n\nWages\n9.00\n\n\nReal Estate Investing\n3.08\n\n\nReal Estate\n\n\nInput Costs\n6.38\n\n\nNew Launches\n7.68\n\n\nReal Estate Prices\n4.82\n\n\n\n\n\n\n\nThe Abrainc Radar is a monthly longitudinal panel of data, i.e., a collection of time series. To visualize this data we will select first the most recent period available and compare it to (1) the equivalent period in the previous year; (2) the best equivalent period.\n\nlatest_date &lt;- max(radar$date)\nlast_year &lt;- latest_date - lubridate::years(1)\n\nbest_year &lt;- radar |&gt; \n  filter(month == lubridate::month(latest_date)) |&gt; \n  group_by(date) |&gt; \n  summarise(total = sum(value)) |&gt; \n  slice_max(total, n = 1) |&gt; \n  pull(date)\n\ncompare_best &lt;- radar |&gt; \n  filter(date %in% c(latest_date, best_year)) |&gt; \n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"variable\",\n    values_from = \"value\"\n    )\n\ncompare &lt;- radar |&gt; \n  filter(year &gt;= lubridate::year(latest_date) - 1,\n         month == lubridate::month(latest_date)) |&gt; \n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"variable\",\n    values_from = \"value\"\n    )"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#ggradar",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#ggradar",
    "title": "Radar Plots",
    "section": "ggradar",
    "text": "ggradar\nUsing default settings, a minimal radar plot requires specifying the scale of the grid and its units. By default, ggradar assumes that all the numeric data is in a 0-100 range. The data.frame input should be a \\(1\\times k\\) table where \\(k\\) is the number of variables. The function ignores non-numerical variables by default.\n\nggradar(\n  current_period,\n  grid.min = 0,\n  grid.mid = 5,\n  grid.max = 10,\n  values.radar = c(\"0\", \"5\", \"10\"),\n  axis.label.size = 3\n  )\n\n\n\n\n\n\n\n\nThere are several arguments to improve the visualization. Since we will make several radar plots it’s convenient to create a function to do this.\n\n\nCode\n# Line break the labels\nradar_labels &lt;- str_wrap(radar_labels, width = 10)\n\nplot_ggradar &lt;- function(.dat, colors, legend = FALSE) {\n  \n  p &lt;- ggradar(\n    .dat,\n    # Define grid limits\n    grid.min = 0,\n    grid.mid = 5,\n    grid.max = 10,\n    # Define grid labels\n    values.radar = c(\"0\", \"5\", \"10\"),\n    grid.label.size = 5,\n    # Shade polygon\n    fill = TRUE,\n    fill.alpha = 0.3,\n    # Axis labels (outside the circle)\n    axis.labels = radar_labels,\n    axis.label.size = 3,\n    font.radar = \"Lato\",\n    # \"Size\" of the plot\n    plot.extent.x.sf = 1.5,\n    plot.extent.y.sf = 1.4,\n    # Color of the lines and circles\n    group.colours = colors,\n    # Size of the circles\n    group.point.size = 3,\n    # Width of the line connecting the points\n    group.line.width = 1\n    )\n  \n  if (legend | length(colors) &gt; 1) {\n    p &lt;- p + theme(legend.position = \"top\")\n  }\n  \n  return(p)\n  \n}\n\n\nThe improved plot is shown below. From the radar plot, we see that the real economy seems to be in a favorable spot: the activity indicator is high and so are wages and employment. Credit conditions are not favorable as can be seen from the low values of interest rate, financing conditions, atractivity, and real concessions.\n\nplot_ggradar(current_period, \"#264653\")\n\n\n\n\n\n\n\n\nWe can use radar plots to make simple comparisons. This radar plot compares the current period with the comparable period in the previous year. We can see that wages and employment improved considerably while atractivity and real concessions worsened.\n\nplot_ggradar(compare, c(\"#e76f51\", \"#264653\"))\n\n\n\n\n\n\n\n\nFinally, we can compare the current period with the “best” period. For this exercise, I defined “best” as the period we the highest cumulative score using equal weights for all indicators. Compared to the best period, the current period is worse in all indicators with exception of wages and new launches.\n\nplot_ggradar(compare_best, c(\"#e76f51\", \"#264653\"))\n\n\n\n\n\n\n\n\n\nSome Problems\nI’ll discuss some general problems of radar plots below but is should be noted that ggradar produces “circular” plots instead of “web” plots. Indeed, a distinctive feature of the radar charts produced by ggradar is that the radar is circular which means the dots are positioned on a radial frame (polar coordinate frame) instead of a conventional cartesian (rectangular) frame. This causes a slight visual distortion: connection dots with a straight line in a circle doesn’t make much sense geometrically.\nThis distortion is more visible when there are fewer categories; conversly, the distortion is less noticeable when there are many categories. Looking at the documentation of ggradar this might be a conscious decision since the inspiration for the ggradar function is a code developed by Paul Williamson that used radar plots to evaluate clusters defined by 44 different variables."
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#fmsb",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#fmsb",
    "title": "Radar Plots",
    "section": "fmsb",
    "text": "fmsb\nThe fmsb package is a companion to the book “Practices of Medical and Health Data Analysis using R”. It offers a multitude of functions and datasets. The main function we will use is the radarchart function. This function expects a wide dataset with atleast three rows. The first row should indicate the maximum value of each column (variable), the second row should indicate the minimum value of each column, and the third row should be the actual/current value of each column.\n\nwide_dat &lt;- pivot_wider(\n  filter(radar, date == max(date)),\n  id_cols = \"date\",\n  names_from = \"variable_label\",\n  values_from = \"value\"\n)\n\nwide_dat &lt;- rbind(rep(10, 12), rep(0, 12), wide_dat[, 2:13])\n\nwide_dat\n\n# A tibble: 3 × 12\n  Confidence Activity `Interest Rate` `Financing Conditions` `Real Concessions`\n       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n1      10       10              10                    10                  10   \n2       0        0               0                     0                   0   \n3       5.97     9.05            2.67                  0.630               2.53\n  Atractivity Employment Wages `Real Estate Investing` `Input Costs`\n        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;         &lt;dbl&gt;\n1       10         10    10                      10            10   \n2        0          0     0                       0             0   \n3        4.74       8.51  9.00                    3.08          6.38\n  `New Launches` `Real Estate Prices`\n           &lt;dbl&gt;                &lt;dbl&gt;\n1          10                   10   \n2           0                    0   \n3           7.68                 4.82\n\n\nIt’s fairly simple to produce a radar chart using this function.\n\nradarchart(wide_dat)\n\n\n\n\n\n\n\n\nWe can customize the result to get a better visualization.\n\nradarchart(\n  wide_dat,\n  axistype = 1,\n  seg = 5,\n  \n  # Polygon\n  pcol = \"#023047\",\n  pfcol = alpha(\"#023047\", 0.3),\n  plwd = 4, \n\n  # Grid\n  cglcol = \"grey90\",\n  cglty = 1,\n  axislabcol = \"grey90\",\n  caxislabels = seq(0, 10, 2),\n  cglwd = 0.8,\n  # Labels\n  vlcex = 0.8)\n\n\n\n\n\n\n\n\nNote that radarchart uses straight grid lines (instead of circular) so it doesn’t cause the same visual distortion seen in the ggradar examples above. In this sense, radarchart produces radar plots that are more visually accurate.\nAgain, we can create a custom function to help make our radar plots.\n\n\nCode\nplot_radarchart &lt;- function(.dat, colors = \"#023047\", legend = FALSE, labels = radar_labels) {\n  \n  radarchart(\n    .dat,\n    axistype = 1,\n    seg = 5,\n    # Polygon\n    pcol = colors,\n    pfcol = alpha(colors, 0.3),\n    plwd = 4,\n    # Grid\n    cglcol = \"grey90\",\n    cglty = 1,\n    axislabcol = \"grey90\",\n    caxislabels = seq(0, 10, 2),\n    cglwd = 0.8,\n    # Labels\n    vlabels = labels,\n    vlcex = 0.8\n    )\n  \n}\n\n\nNow we can easily create new radar plots.\n\nwide_compare &lt;- rbind(rep(10, 12), rep(0, 12), compare[, 2:13])\n\nplot_radarchart(wide_compare, colors = c(\"#e76f51\", \"#264653\"))\n\n\n\n\n\n\n\n\n\nwide_best &lt;- rbind(rep(10, 12), rep(0, 12), compare_best[, 2:13])\n\nplot_radarchart(wide_best, colors = c(\"#e76f51\", \"#264653\"))"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#problems-with-radar-plots",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#problems-with-radar-plots",
    "title": "Radar Plots",
    "section": "Problems with radar plots",
    "text": "Problems with radar plots\nDespite their growing prevalence in data visualization, radar charts are subject to several criticisms. To summarize the main issues, specialists argue that:\n\nThe circular layout makes it harder to interpret the data when compared to simple horizontal/vertical axis. This is essentially the same critique made to pie charts.\nRanking and ordering. The overall shape and interpretation of radar plots can vary substantially depending on the layout of the variables along the edges.\nArea distortion. Since a radar plot shows a polygon, it’s area scales quadratically instead of linearly. This can lead to biased interpretation of the data, leading one to over-estimate changes.\nOverplotting. Radar plots are not very useful when comparing 4 groups or more.\n\n\nRanking distortion\nAll four plots below show the exact same data but the order of the variables is changed. This results in different shaped polygons and pottentialy different or even conflicting interpretations from the same underlying data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverplotting\nThe plot below compares 5 different periods. Radar charts suffer from overplotting and are generally not useful for comparing more than 3 groups at a time.\n\n\n\n\n\n\n\n\n\nFor a more in depth analysis on the shortcomings of radar charts see The Radar Chart and Its Caveats."
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#the-alternatives",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#the-alternatives",
    "title": "Radar Plots",
    "section": "The alternatives",
    "text": "The alternatives\nThere are three alternatives to radar plots.\n\nLollipop charts\nParallel trends charts\nBarplots or Column charts\n\nIn essence, all of these alternatives simplify radar plots by making the underlying space horizontal/vertical instead of “circular”.\n\nLollipop charts\nLollipop charts are good at showing the temporal change between the same group of variables. I featured this chart in my ggplot2 tutorials (in Portuguese). The code below shows how to make a simple lollipop chart using this data.\n\n\nCode\nfont_add_google(\"IBM Plex Sans\", \"IBM Plex Sans\")\n\nlolli &lt;- radar |&gt; \n  mutate(month = lubridate::month(date)) |&gt; \n  filter(year %in% c(2019, 2023), month == 6) |&gt; \n  mutate(year = factor(year))\n\nggplot(lolli, aes(variable_label, ma6, color = year)) +\n  geom_line(aes(group = variable_label), color = \"gray30\") +\n  geom_point(size = 3) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10)) +\n  scale_color_manual(name = \"\", values = c(\"#023047\", \"#fb8500\")) +\n  labs(\n    title = \"Pre and Post-pandemic Market Conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\",\n    x = NULL,\n    y = \"Index (10 = most favorable)\"\n  ) +\n  theme_light(base_family = \"IBM Plex Sans\") +\n  theme(\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\nThe chart shows the average values of all 12 indicators in the first semester of 2019 and 2023. When the dots are close, very little has changed; conversely, when the dots are far apart there has been considerable evolution. The colors help guide the visualization.\nWe can easily see that real estate prices and new launches are on the rise in 2023. Similarly, the job market and overall economy seems stronger as wages, employment, and the level of activity have all increased. Credit conditions, on the other hand, have deteriorated, and the overall actractivity of real estate investment has worsened. The fall in financing conditions and interest rate reflects both the increase in the SELIC rate as well as the increase in all real estate credit lines.\n\n\nParallel Trends\nA parallel trends chart is quite literally a line plot. In a way, it’s a simplification of radar plots since it connects the numerical values of different categories along a simple rectangular frame. These charts can be adapted to encompass many comparison groups simultaneously so they can be used to overcome one of the shortcomings of radar plots. I’m not the greatest fan of parallel trends chart as they can become very messy.\nThe code below shows how to build a simple parallel trends chart.\n\n\nCode\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\n\ncompare &lt;- radar |&gt; \n  mutate(\n    month = lubridate::month(date)\n  ) |&gt; \n  filter(month == 6, year %in% c(2012, 2017, 2019, 2023)) |&gt; \n  mutate(year = factor(year))\n\nggplot(compare, aes(variable_label, ma6, color = year, group = year)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = c(0, 10)) +\n  scale_x_discrete(labels = \\(x) str_wrap(x, 10)) +\n  scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10)) +\n  scale_color_manual(name = \"\", values = c(\"#023047\", \"#219ebc\", \"#8ecae6\", \"#fb8500\")) +\n  labs(\n    x = NULL, y = \"Index (10 = most favorable)\",\n    title = \"Real Estate Market Conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\"\n  ) +\n  theme_ipsum(base_family = \"Roboto Condensed\") +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 8)\n    )\n\n\n\n\n\n\n\n\n\nIn the visualization above we compare four different years: 2012, 2017, 2019, and 2023. The first of these years is a “boom” year, the economy was still riding the end of the commodity cycle and interest rates were at historically low values; the labor market was also doing well, with record low unemployment rates. The indices reflect these facts: the darkblue line is usually at the top of the chart. The “midblue” line, represents 2017, the last year of the recession started in 2015: almost all indicators are low but notably input costs are very favorable, most likely due to the decreased construction demand. Finally, 2019 is a pre-pandemic benchmark and 2023 is the most recent available data.\nThis visualization would look very cluttered on a radar chart but works well as a parallel trend chart. We can easily see how the current period (2023) is different form the 2012 boom: while the real economy and labor markets are similar, credit conditions are much worse; input costs are comparable, while new launches are currently higher and prices lower.\n\n\nBarplots\nA final alternative is to use a simple bar chart combined will small multiples. The code below shows how to make a simple version of this visulization.\n\n\nCode\ndat = radar |&gt; \n  mutate(\n    month = lubridate::month(date),\n    variable_label = forcats::fct_rev(variable_label)\n    ) |&gt; \n  filter(year &gt;= 2021, month == 6)\n\nggplot(dat, aes(variable_label, ma6)) +\n  geom_col(fill = \"#023047\") +\n  geom_hline(yintercept = 0) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 10, 2)) +\n  facet_wrap(vars(year)) +\n  labs(\n    x = NULL, y = \"Index (10 = most favorable)\",\n    title = \"Real Estate Market Conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\"\n  )  +\n  theme_bw(base_family = \"IBM Plex Sans\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\nThis graphic allows for both a vertical analysis, comparing the different index values within the same year, as well as a horizontal analysis, comparing the same index across different time periods. It can also be enhanced using specific colors to highlight key indicators or text labels to make the values more precise. We could, for instance, highlight indicators that show how overall credit conditions have worsened in the past years.\n\n\nCode\ncols_credit &lt;- c(\"interest\", \"finance_condition\", \"real_concession\")\n\ndat &lt;- dat |&gt; \n  mutate(\n    is_credit = factor(if_else(variable %in% cols_credit, 1L, 0L))\n  )\n\nggplot(dat, aes(variable_label, ma6, fill = is_credit)) +\n  geom_col() +\n  geom_text(\n    data = filter(dat, is_credit == 1),\n    aes(y = ma6 + 0.8, label = round(ma6, 1)),\n    family = \"IBM Plex Sans\"\n  ) +\n  geom_hline(yintercept = 0) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 10, 2)) +\n  facet_wrap(vars(year)) +\n  scale_fill_manual(name = \"\", values = c(\"#8ecae6\", \"#023047\")) +\n  guides(fill = \"none\") +\n  labs(\n    x = NULL, y = \"Index (10 = most favorable)\",\n    title = \"Deterioration of real estate credit conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\"\n  )  +\n  theme_bw(base_family = \"IBM Plex Sans\") +\n  theme(panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html",
    "href": "posts/general-posts/2024-03-google-places/index.html",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "O Google Places API permite acessar os dados do Google Maps. O pacote googleway integra estes dados dentro de R já em formato tidy. Neste post vou mostrar como importar dados desta API com foco no Places, que encontra informações sobre estabelecimentos ou pontos de interesse, em geral. Há diversos outros usos desta API, como de encontrar rotas, estimar a elevação, recuperar fotos do Street View, entre outros. Para uma lista completa veja a documentação do pacote.\n\nlibrary(sf)\nlibrary(googleway)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(leaflet)\n\n\n\nPara usar o pacote é preciso registrar uma API. Se você não tem uma key é preciso criar uma. Um tutorial do Google está disponível aqui.\n\ngoogleway::set_key(\"sua_chave\")\n\n\n\n\nPara exemplificar o uso da API vamos começar com um exemplo simples. Via de regra, é preciso definir apenas dois parâmetros para começar a busca:\n\nUm termo de busca\nUm ponto\n\nNo exemplo abaixo buscamos o termo “starbucks” a partir da coordenada do Museu de Arte de São Paulo (MASP).\n\nponto = c(-23.561462, -46.655937)\n\nsearch_sbux = google_places(\n  search_string = \"starbucks\",\n  location = ponto\n)\n\nO objeto guarda vários resultados da query.\n\n# Resultados da query em formato data.frame\nplaces$results\n# 'token' para continuar a busca\nplaces$next_page_token\n# Status da query\nplaces$status\n\nA query de busca encontra apenas 20 resultados (no máximo). Para encontrar mais resultados é preciso fazer uma nova busca, usando $next_page_token para encontrar mais resultados.\n\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_2 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux$next_page_token\n)\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_3 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux_2$next_page_token\n)\n# Agrega todos os resultados encontrados\nres &lt;- bind_rows(\n  search_sbux$results,\n  search_sbux_2$results,\n  search_sbux_3$results\n)\n\nAo todo, a query encontrou 50 resultados que estão agregados no objeto res acima. A query retorna várias das informações que normalmente se encontram numa busca via Google Maps: a nota média do estabelecimento, algumas fotos, o endereço, a posição geográfica (lat/lng), o tipo do estabelecimento, etc.\nFormalmentem, as informações estão estruturadas num “nested” data.frame, que é um data.frame comum que possui outros data.frame ou list como colunas. Isto pode causar algum estranhamento, mas não é nada demais. Para selecionar uma coluna acaba sendo necessário fazer df$col1$col2 ou usar os comandos tidyr::unnest.\n\nsubres &lt;- res %&gt;%\n  unnest(cols = \"geometry\") %&gt;%\n  unnest(cols = \"location\") %&gt;%\n  select(\n    business_status, name, formatted_address, rating, user_ratings_total, types,\n    lat, lng\n  )\n\nsubres &lt;- st_as_sf(subres, coords = c(\"lng\", \"lat\"), crs = 4326)\n\n# Mapa iterativo simples\nleaflet(subres) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(label = ~name) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\nA coluna types é uma lista que contém cinco strings que categorizam o estabelecimento.\n\nsubres$types[[1]]\n\nOs dados do Google Maps não costumam ser 100% limpos. Não é incomum encontrar estabelecimentos fantasmas, ou mal-construídos. Um jeito fácil de encontrar estes casos é filtrar pelo número de reviews ou pelo nome. Na tabela abaixo há dois estabelecimentos suspeitos: o STARBUCKS CO e o Starbucks Coffee: ambos têm um baixo número de reviews e têm uma nota muito baixa (2 e 2.1, respectivamente).\n\nsubres |&gt; \n  arrange(user_ratings_total) |&gt; \n  head(10)\n\n\n\n\nVou montar uma busca simples para retornar todos os Starbucks do Brasil. Seria muito demorado fazer uma busca completa, no país inteiro, então vou usar como ponto de partida os pares de coordenadas que encontrei via webscrape.\nA função abaixo procura pelo termo “starbucks” em todos os pontos que forneço. Para simplificar, a função devolve apenas algumas das colunas.\n\n# Function to grap starbucks info\nget_starbucks_info &lt;- function(lat, lng) {\n  \n  places = google_places(\n    search_string = \"starbucks\",\n    location = c(lat, lng)\n  )\n  \n  sel_cols = c(\n    \"name\", \"formatted_address\", \"lat\", \"lng\", \"rating\", \"user_ratings_total\",\n    \"business_status\")\n  \n  places$results %&gt;%\n    tidyr::unnest(\"geometry\") %&gt;%\n    tidyr::unnest(\"location\") %&gt;%\n    dplyr::select(dplyr::all_of(sel_cols))\n  \n}\n\nO código abaixo roda a função acima em todos os 142 estabelecimentos, encontrados na página oficial do Starbucks Brasil.\n\n# Remove geometry and keep only coordinates\ncoords_starbucks &lt;- starbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  select(index, name, lat, lng)\n\nstarbucks_info = purrr::map2(\n  coords_starbucks$lat,\n  coords_starbucks$lng,\n  get_starbucks_info\n  )\n\ndat &lt;- starbucks_info %&gt;%\n  bind_rows(.id = \"search_id\") %&gt;%\n  distinct()\n\nNovamente, é preciso limpar os resultados. Olhando para a coluna de nome vemos que há vários estabelecimentos errados.\n\nunique(dat$name)\n\n [1] \"Starbucks - Top Center\"                                         \n [2] \"Starbucks\"                                                      \n [3] \"Starbucks Coffee\"                                               \n [4] \"Starbucks Shopping Higienópolis\"                                \n [5] \"STARBUCKS CO\"                                                   \n [6] \"Starbucks Shopping Plaza Sul\"                                   \n [7] \"Starbucks Einstein - Maternidade\"                               \n [8] \"Starbucks Interlagos\"                                           \n [9] \"Starbucks Grand Plaza Shopping\"                                 \n[10] \"Starbucks Shopping Tamboré\"                                     \n[11] \"STARBUCKS COFFEE\"                                               \n[12] \"Starbucks T3 Desembarque\"                                       \n[13] \"Starbucks - Mogi Shopping\"                                      \n[14] \"Lago Azul Restaurant - North\"                                   \n[15] \"Starbucks Litoral Plaza Shopping\"                               \n[16] \"Starbucks - Parque Dom Pedro\"                                   \n[17] \"Starbucks Livraria Leitura - Parque Dom Pedro\"                  \n[18] \"Starbuks\"                                                       \n[19] \"Starbucks Ribeirão Shopping\"                                    \n[20] \"Starbucks Coffee - Shopping Mueller\"                            \n[21] \"Starbucks - Palladium\"                                          \n[22] \"Starbucks (Sala de Embarque) Aeroporto Curitiba Afonso Pena\"    \n[23] \"Starbucks Shopping Jockey\"                                      \n[24] \"Starbucks Shopping Curitiba\"                                    \n[25] \"Casa Bauducco\"                                                  \n[26] \"Havanna Café Palladium Curitiba\"                                \n[27] \"Season Coffee & Co.\"                                            \n[28] \"Starbucks - Santos Dumont\"                                      \n[29] \"DarkCoffee Centro Rio\"                                          \n[30] \"Starbucks Garten Shopping\"                                      \n[31] \"Starbucks Norte Shopping\"                                       \n[32] \"Starbucks Itaú Power Shopping\"                                  \n[33] \"Starbucks | Beiramar Shopping\"                                  \n[34] \"Starbucks Boulevard Shopping\"                                   \n[35] \"Starbucks Airport Gate 115\"                                     \n[36] \"Starbucks - Barra Shopping Sul\"                                 \n[37] \"Starbucks Portão 8 Aeroporto Internacional de Brasília Lago Sul\"\n[38] \"Caffetteria\"                                                    \n[39] \"Black Coffee\"                                                   \n[40] \"Duckbill Cookies & Coffee Taguatinga Shopping - Brasília/DF\"    \n[41] \"Caffetteria WPS Santa Lúcia\"                                    \n\n\nPara fazer a limpeza, vou manter apenas as unidades ativas com contêm “Starbucks” no seu nome. Além disso, vou parear os dados com a minha base de webscape usando st_nearest_feature(x, y). Esta função encontra o ponto mais próximo em y para cada ponto de x.\n\ndat &lt;- dat |&gt; \n  filter(str_detect(name, \"Starbucks\"), business_status == \"OPERATIONAL\") |&gt; \n  arrange(formatted_address)\n\ngoogle_data &lt;- dat %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326)\n\ninds &lt;- st_nearest_feature(google_data, starbucks)\n\nmetadata &lt;- starbucks %&gt;%\n  slice(inds) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble()\n\ngoogle_data &lt;- google_data |&gt; \n  rename(google_name = name, google_address = formatted_address) |&gt; \n  bind_cols(metadata)\n\n\n\nO mapa interativo abaixo mostra todos os Starbucks de São Paulo. A cor de cada círculo representa a sua nota e o tamanho do círculo, o número de avaliações. As unidades no corredor da Av. Paulista, por exemplo, têm notas médias elevadas e grande número de avaliações. Uma das piores unidades parece ser a da Uv. Mackenzie, que tem nota 2,1 e 15 avaliações. Na Zona Leste, a unidade no Shopping Aricanduva também tem nota um pouco inferior, 3,9 com 158 avaliações.\n\nsp &lt;- filter(google_data, name_muni == \"São Paulo\")\n\nsp &lt;- sp |&gt; \n  mutate(\n    rad = findInterval(user_ratings_total, c(25, 100, 1000, 2500, 5000))*2 + 5\n  )\n\npal &lt;- colorNumeric(\"RdBu\", domain = sp$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {sp$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {sp$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {sp$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(sp) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~rad,\n    color = ~pal(rating),\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt; \n  addLegend(pal = pal, values = ~rating) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\n\n\n\nTecnicamente, o Starbucks mais bem avaliado de São Paulo é o Starbucks da Ala Consultório do Hospital Albert Einstein, com nota 4,8. Contudo, considerando as unidades com maior número de avaliações, podemos montar um mapa dos “melhores” Starbucks de São Paulo: as unidades com nota igual ou superior a 4,4 e com mais de 500 avaliações.\nDe maneira geral, o mapa mostra as unidades dentro do Centro Expandido, no eixo da Paulista e do Itaim Bibi. Nas bordas do Centro Expandido temos quase exclusivamente, unidades dentro de shoppings.\n\nmelhores_starbucks &lt;- sp |&gt; \n  filter(rating &gt;= 4.4 & user_ratings_total &gt; 500)\n\npal &lt;- colorNumeric(\"Blues\", domain = melhores_starbucks$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {melhores_starbucks$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {melhores_starbucks$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {melhores_starbucks$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(melhores_starbucks) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~sqrt(user_ratings_total / 10),\n    color = \"#045a8d\",\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt;\n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\n\n\n\n\n\nEncontrando todos os Starbucks do Brasil\nTutorial Leaflet: mapas interativos com leaflet"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html#starbucks",
    "href": "posts/general-posts/2024-03-google-places/index.html#starbucks",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "Para exemplificar o uso da API vamos começar com um exemplo simples. Via de regra, é preciso definir apenas dois parâmetros para começar a busca:\n\nUm termo de busca\nUm ponto\n\nNo exemplo abaixo buscamos o termo “starbucks” a partir da coordenada do Museu de Arte de São Paulo (MASP).\n\nponto = c(-23.561462, -46.655937)\n\nsearch_sbux = google_places(\n  search_string = \"starbucks\",\n  location = ponto\n)\n\nO objeto guarda vários resultados da query.\n\n# Resultados da query em formato data.frame\nplaces$results\n# 'token' para continuar a busca\nplaces$next_page_token\n# Status da query\nplaces$status\n\nA query de busca encontra apenas 20 resultados (no máximo). Para encontrar mais resultados é preciso fazer uma nova busca, usando $next_page_token para encontrar mais resultados.\n\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_2 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux$next_page_token\n)\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_3 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux_2$next_page_token\n)\n# Agrega todos os resultados encontrados\nres &lt;- bind_rows(\n  search_sbux$results,\n  search_sbux_2$results,\n  search_sbux_3$results\n)\n\nAo todo, a query encontrou 50 resultados que estão agregados no objeto res acima. A query retorna várias das informações que normalmente se encontram numa busca via Google Maps: a nota média do estabelecimento, algumas fotos, o endereço, a posição geográfica (lat/lng), o tipo do estabelecimento, etc.\nFormalmentem, as informações estão estruturadas num “nested” data.frame, que é um data.frame comum que possui outros data.frame ou list como colunas. Isto pode causar algum estranhamento, mas não é nada demais. Para selecionar uma coluna acaba sendo necessário fazer df$col1$col2 ou usar os comandos tidyr::unnest.\n\nsubres &lt;- res %&gt;%\n  unnest(cols = \"geometry\") %&gt;%\n  unnest(cols = \"location\") %&gt;%\n  select(\n    business_status, name, formatted_address, rating, user_ratings_total, types,\n    lat, lng\n  )\n\nsubres &lt;- st_as_sf(subres, coords = c(\"lng\", \"lat\"), crs = 4326)\n\n# Mapa iterativo simples\nleaflet(subres) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(label = ~name) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\nA coluna types é uma lista que contém cinco strings que categorizam o estabelecimento.\n\nsubres$types[[1]]\n\nOs dados do Google Maps não costumam ser 100% limpos. Não é incomum encontrar estabelecimentos fantasmas, ou mal-construídos. Um jeito fácil de encontrar estes casos é filtrar pelo número de reviews ou pelo nome. Na tabela abaixo há dois estabelecimentos suspeitos: o STARBUCKS CO e o Starbucks Coffee: ambos têm um baixo número de reviews e têm uma nota muito baixa (2 e 2.1, respectivamente).\n\nsubres |&gt; \n  arrange(user_ratings_total) |&gt; \n  head(10)"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html#busca-estruturada",
    "href": "posts/general-posts/2024-03-google-places/index.html#busca-estruturada",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "Vou montar uma busca simples para retornar todos os Starbucks do Brasil. Seria muito demorado fazer uma busca completa, no país inteiro, então vou usar como ponto de partida os pares de coordenadas que encontrei via webscrape.\nA função abaixo procura pelo termo “starbucks” em todos os pontos que forneço. Para simplificar, a função devolve apenas algumas das colunas.\n\n# Function to grap starbucks info\nget_starbucks_info &lt;- function(lat, lng) {\n  \n  places = google_places(\n    search_string = \"starbucks\",\n    location = c(lat, lng)\n  )\n  \n  sel_cols = c(\n    \"name\", \"formatted_address\", \"lat\", \"lng\", \"rating\", \"user_ratings_total\",\n    \"business_status\")\n  \n  places$results %&gt;%\n    tidyr::unnest(\"geometry\") %&gt;%\n    tidyr::unnest(\"location\") %&gt;%\n    dplyr::select(dplyr::all_of(sel_cols))\n  \n}\n\nO código abaixo roda a função acima em todos os 142 estabelecimentos, encontrados na página oficial do Starbucks Brasil.\n\n# Remove geometry and keep only coordinates\ncoords_starbucks &lt;- starbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  select(index, name, lat, lng)\n\nstarbucks_info = purrr::map2(\n  coords_starbucks$lat,\n  coords_starbucks$lng,\n  get_starbucks_info\n  )\n\ndat &lt;- starbucks_info %&gt;%\n  bind_rows(.id = \"search_id\") %&gt;%\n  distinct()\n\nNovamente, é preciso limpar os resultados. Olhando para a coluna de nome vemos que há vários estabelecimentos errados.\n\nunique(dat$name)\n\n [1] \"Starbucks - Top Center\"                                         \n [2] \"Starbucks\"                                                      \n [3] \"Starbucks Coffee\"                                               \n [4] \"Starbucks Shopping Higienópolis\"                                \n [5] \"STARBUCKS CO\"                                                   \n [6] \"Starbucks Shopping Plaza Sul\"                                   \n [7] \"Starbucks Einstein - Maternidade\"                               \n [8] \"Starbucks Interlagos\"                                           \n [9] \"Starbucks Grand Plaza Shopping\"                                 \n[10] \"Starbucks Shopping Tamboré\"                                     \n[11] \"STARBUCKS COFFEE\"                                               \n[12] \"Starbucks T3 Desembarque\"                                       \n[13] \"Starbucks - Mogi Shopping\"                                      \n[14] \"Lago Azul Restaurant - North\"                                   \n[15] \"Starbucks Litoral Plaza Shopping\"                               \n[16] \"Starbucks - Parque Dom Pedro\"                                   \n[17] \"Starbucks Livraria Leitura - Parque Dom Pedro\"                  \n[18] \"Starbuks\"                                                       \n[19] \"Starbucks Ribeirão Shopping\"                                    \n[20] \"Starbucks Coffee - Shopping Mueller\"                            \n[21] \"Starbucks - Palladium\"                                          \n[22] \"Starbucks (Sala de Embarque) Aeroporto Curitiba Afonso Pena\"    \n[23] \"Starbucks Shopping Jockey\"                                      \n[24] \"Starbucks Shopping Curitiba\"                                    \n[25] \"Casa Bauducco\"                                                  \n[26] \"Havanna Café Palladium Curitiba\"                                \n[27] \"Season Coffee & Co.\"                                            \n[28] \"Starbucks - Santos Dumont\"                                      \n[29] \"DarkCoffee Centro Rio\"                                          \n[30] \"Starbucks Garten Shopping\"                                      \n[31] \"Starbucks Norte Shopping\"                                       \n[32] \"Starbucks Itaú Power Shopping\"                                  \n[33] \"Starbucks | Beiramar Shopping\"                                  \n[34] \"Starbucks Boulevard Shopping\"                                   \n[35] \"Starbucks Airport Gate 115\"                                     \n[36] \"Starbucks - Barra Shopping Sul\"                                 \n[37] \"Starbucks Portão 8 Aeroporto Internacional de Brasília Lago Sul\"\n[38] \"Caffetteria\"                                                    \n[39] \"Black Coffee\"                                                   \n[40] \"Duckbill Cookies & Coffee Taguatinga Shopping - Brasília/DF\"    \n[41] \"Caffetteria WPS Santa Lúcia\"                                    \n\n\nPara fazer a limpeza, vou manter apenas as unidades ativas com contêm “Starbucks” no seu nome. Além disso, vou parear os dados com a minha base de webscape usando st_nearest_feature(x, y). Esta função encontra o ponto mais próximo em y para cada ponto de x.\n\ndat &lt;- dat |&gt; \n  filter(str_detect(name, \"Starbucks\"), business_status == \"OPERATIONAL\") |&gt; \n  arrange(formatted_address)\n\ngoogle_data &lt;- dat %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326)\n\ninds &lt;- st_nearest_feature(google_data, starbucks)\n\nmetadata &lt;- starbucks %&gt;%\n  slice(inds) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble()\n\ngoogle_data &lt;- google_data |&gt; \n  rename(google_name = name, google_address = formatted_address) |&gt; \n  bind_cols(metadata)\n\n\n\nO mapa interativo abaixo mostra todos os Starbucks de São Paulo. A cor de cada círculo representa a sua nota e o tamanho do círculo, o número de avaliações. As unidades no corredor da Av. Paulista, por exemplo, têm notas médias elevadas e grande número de avaliações. Uma das piores unidades parece ser a da Uv. Mackenzie, que tem nota 2,1 e 15 avaliações. Na Zona Leste, a unidade no Shopping Aricanduva também tem nota um pouco inferior, 3,9 com 158 avaliações.\n\nsp &lt;- filter(google_data, name_muni == \"São Paulo\")\n\nsp &lt;- sp |&gt; \n  mutate(\n    rad = findInterval(user_ratings_total, c(25, 100, 1000, 2500, 5000))*2 + 5\n  )\n\npal &lt;- colorNumeric(\"RdBu\", domain = sp$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {sp$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {sp$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {sp$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(sp) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~rad,\n    color = ~pal(rating),\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt; \n  addLegend(pal = pal, values = ~rating) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\n\n\n\nTecnicamente, o Starbucks mais bem avaliado de São Paulo é o Starbucks da Ala Consultório do Hospital Albert Einstein, com nota 4,8. Contudo, considerando as unidades com maior número de avaliações, podemos montar um mapa dos “melhores” Starbucks de São Paulo: as unidades com nota igual ou superior a 4,4 e com mais de 500 avaliações.\nDe maneira geral, o mapa mostra as unidades dentro do Centro Expandido, no eixo da Paulista e do Itaim Bibi. Nas bordas do Centro Expandido temos quase exclusivamente, unidades dentro de shoppings.\n\nmelhores_starbucks &lt;- sp |&gt; \n  filter(rating &gt;= 4.4 & user_ratings_total &gt; 500)\n\npal &lt;- colorNumeric(\"Blues\", domain = melhores_starbucks$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {melhores_starbucks$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {melhores_starbucks$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {melhores_starbucks$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(melhores_starbucks) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~sqrt(user_ratings_total / 10),\n    color = \"#045a8d\",\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt;\n  addProviderTiles(\"CartoDB\")"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html#posts-relacionados",
    "href": "posts/general-posts/2024-03-google-places/index.html#posts-relacionados",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "Encontrando todos os Starbucks do Brasil\nTutorial Leaflet: mapas interativos com leaflet"
  },
  {
    "objectID": "posts/general-posts/2024-12-demographic-pyramid/index.html",
    "href": "posts/general-posts/2024-12-demographic-pyramid/index.html",
    "title": "Demographic Pyramids in R",
    "section": "",
    "text": "A demographic pyramid is a graphical representation of the age and gender distribution of a population. It typically shows the population in horizontal bars, with males represented on the left and females on the right. Each bar represents a specific age group, and the length of the bar indicates the number of people in that group.\nDemographic pyramids are widely used in demographic analysis to visualize the structure of a population, providing valuable insights into trends like population growth, aging, and gender distribution.\nThese pyramids are crucial for understanding the socio-economic dynamics of a country, as they highlight the proportions of young versus older populations and reveal patterns related to fertility rates, life expectancy, and migration.\nBy analyzing demographic pyramids, policymakers, businesses, and researchers can make informed decisions related to healthcare, education, and workforce planning.\nIn this tutorial, we will demonstrate how to create demographic pyramids using R, leveraging the United Nations World Population Prospects (WPP) 2024 dataset to analyze population structures in countries like Pakistan, Brazil, Japan, and Ireland. The data is available through their package on GitHub.\n\n# Load the necessary packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(wpp2024)\nlibrary(showtext)\nlibrary(patchwork)\n\n# Optional for better plotting\nfont_add_google(\"Open Sans\", \"Open Sans\")\nshowtext_auto()\n\n# to install wpp2024\n# remotes::install_github(\"PPgp/wpp2024\")\n\n\n\nThe simplest way that demographic data is presented is on a year-on-year basis. These estimates are available in the popAge1dt dataset.\n\ndata(popAge1dt)\n\n# Brazil, Ireland, Pakistan, Japan\ncountries &lt;- tibble(\n  country_code = c(392, 586, 372, 76),\n  name = c(\"Japan\", \"Pakistan\", \"Ireland\", \"Brazil\")\n)\n\n\n\nA population pyramid follows certain conventions. The data is presented in age-sex groups: males on the left, females on the right. Bars typically represent each group’s share of total population, though sometimes they show shares within each sex separately.\nThe code below calculates population shares by age and sex, making male values negative to position them on the left side.\n\npopage &lt;- popAge1dt |&gt; \n  filter(\n    country_code %in% countries$country_code,\n    year %in% seq(1950, 2020, 10)\n    )\n\npopage &lt;- popage |&gt; \n  select(country_code, name, year, age, popM, popF) |&gt; \n    pivot_longer(\n    cols = popM:popF,\n    names_to = \"sex\",\n    values_to = \"population\"\n  )\n\npopage &lt;- popage |&gt; \n  mutate(\n    sex = if_else(sex == \"popM\", \"Male\", \"Female\"),\n    sex = factor(sex, levels = c(\"Male\", \"Female\")),\n  ) |&gt; \n  group_by(country_code, name, year) |&gt; \n  mutate(share = population / sum(population, na.rm = TRUE) * 100) |&gt; \n  ungroup() |&gt; \n  mutate(share = if_else(sex == \"Male\", -share, share))\n\n\n\n\nThe code below creates a basic population pyramid using Japan’s 1950 demographic data. Each bar shows what percentage of the total population belongs to a specific age group and sex combination. For example, a bar might show that 1% of Japan’s total population was males aged 10 years.\nThe visualization is called a “pyramid” because of its distinctive shape: wide at the bottom (many young people), gradually narrowing through the middle ages (fewer adults), and smallest at the top (very few elderly). This triangular shape was typical for most countries in the past and is still common in developing nations today. The shape reveals a population structure with high birth rates and shorter life expectancy.\n\njpn50 &lt;- popage |&gt; \n  filter(name == \"Japan\", year == 1950)\n\nggplot(jpn50, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\")\n\n\n\n\n\n\n\n\n\n\n\nThe code below demonstrates how to enhance a population pyramid with colors and explanatory labels. These additions are essential as they make the visualization more informative and self-contained, allowing readers to understand the data without needing external explanations.\nRegarding color choices for gender representation, modern data visualization practices avoid traditional stereotypes (like blue for males and pink for females). DataWrapper has an insightful discussion on this matter and recommends using green for men and purple for women.\nNote that while these specific colors (green and purple) work well, there’s no strict rule - the key is to choose colors that are:\n\nClearly distinguishable\nAccessible to colorblind viewers\nConsistent with your overall design scheme\n(Preferably) Free from gender stereotypes\n\n\ncolors &lt;- c(\"#1B9E77\", \"#7570B3\")\nfont &lt;- \"Open Sans\"\n\np &lt;- ggplot(jpn50, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\")\n\np + \n  geom_hline(yintercept = 0) +\n  annotate(\n    \"label\",\n    x = 85,\n    y = -1.25,\n    label = \"Male\",\n    color = colors[1],\n    family = font,\n    size = 4\n  ) +\n    annotate(\n    \"label\",\n    x = 85,\n    y = 1.25,\n    label = \"Female\",\n    color = colors[2],\n    family = font,\n    size = 4\n  ) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nThe code below shows how to make interesting comparisions using small facet plots.\nThe first example shows how to see the demographic evolution of a single country over the years.\nThe second example shows how to compare a cross-country selection across the same time period.\n\njpn &lt;- popage |&gt; \n  filter(name == \"Japan\", year %in% seq(1950, 2000, 10))\n\nbase_plot &lt;- ggplot(jpn, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\nbase_plot +\n  facet_wrap(~year) +\n  ggtitle(\"The demographic aging of Japan\")\n\n\n\n\n\n\n\n\n\ncountry_comp50 &lt;- popage |&gt; \n  filter(year == 1980)\n\nbase_plot &lt;- ggplot(country_comp50, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\nbase_plot +\n  facet_wrap(~name) +\n  ggtitle(\"Country comparison: 1980\")\n\n\n\n\n\n\n\n\n\n\n\n\nDemographers typically work with grouped age data (usually in 5 or 10-year intervals) rather than single-year ages for two main reasons:\n\nStatistical Reliability: Grouped data provides more precise estimates by reducing random variations that can occur in single-year measurements.\nPractical Analysis: Broader age groups make it easier to identify and understand demographic patterns and trends. For instance, knowing the proportion of people aged 65-69 is often more useful for policy planning than specific single-year counts.\n\nThis data is available in the popAge5dt dataset.\nSome data manipulation is necessary since the age groups are interpreted as text/strings, and there is no obvious way to order this data. The code below converts this text data into a factor (a ordered string, 0-4, 5-9, …, 80-84, 85+).\n\ndata(\"popAge5dt\")\n\npopage &lt;- popAge5dt |&gt; \n  filter(country_code %in% countries$country_code) |&gt; \n  mutate(\n    age_trunc = if_else(\n      age %in% c(\"85-89\", \"90-94\", \"95-99\", \"100+\"), \"85-\", age\n      ),\n    age_min = as.integer(str_extract(age_trunc, \"[0-9]{1,2}(?=-)\")),\n    age_trunc = str_replace(age_trunc, \"85-\", \"85+\"),\n    age_trunc = factor(age_trunc),\n    age_trunc = forcats::fct_reorder(age_trunc, age_min)\n  ) |&gt; \n  group_by(country_code, name, year, age_trunc) |&gt; \n  summarise(\n    pop_male = sum(popM),\n    pop_female = sum(popF)\n  ) |&gt; \n  ungroup()\n\npopage &lt;- popage |&gt; \n    select(country_code, name, year, age_trunc, pop_male, pop_female) |&gt; \n    pivot_longer(\n    cols = starts_with(\"pop\"),\n    names_to = \"sex\",\n    values_to = \"population\"\n  )\n\npopage &lt;- popage |&gt; \n  mutate(\n    sex = if_else(sex == \"pop_male\", \"Male\", \"Female\"),\n    sex = factor(sex, levels = c(\"Male\", \"Female\")),\n  ) |&gt; \n  group_by(country_code, name, year) |&gt; \n  mutate(share = population / sum(population, na.rm = TRUE) * 100) |&gt; \n  ungroup() |&gt; \n  mutate(share = if_else(sex == \"Male\", -share, share))\n\n\n\nThe code below shows how to make a simple plot following the conventions above. The data shows the demographic distribution of Brazil in 1990.\n\ndat &lt;- filter(popage, name == \"Brazil\", year == 1990)\n\nbreaks_share &lt;- seq(-6, 6, 1)\nlabels_share &lt;- str_remove(as.character(breaks_share), \"-\")\n\nggplot(dat, aes(x = age_trunc, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nAside from side-by-side plots, we can make different kinds of visualizations to highlight demographic changes through time.\nThis first example uses a simple line to compare Brazil’s demographic pyramid in 1970 and 2000.\n\ndat &lt;- filter(popage, name == \"Brazil\")\n\nbreaks_share &lt;- seq(-10, 10, 2)\nlabels_share &lt;- str_remove(as.character(breaks_share), \"-\")\n\nggplot() +\n  geom_col(\n    data = filter(dat, year == 2000),\n    aes(age_trunc, share, fill = sex, color = sex)\n  ) +\n  geom_step(\n    data = filter(dat, year == 1970),\n    aes(age_trunc, share, group = sex),\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share\n  ) +\n  labs(\n    title = \"Brazil: 1970 x 2000\",\n    x = \"Age\", y = \"Share of Population (%)\",\n    subtitle = \"Solid line shows the demographic pyramid in 1970. Colored bars show the same data in 2000.\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\nThis second example is essentially the same but uses a connected line to contrast Brazil’s demographic in 2000 and 2010.\n\nggplot() +\n  geom_col(\n    data = filter(dat, year == 2010),\n    aes(age_trunc, share, fill = sex, color = sex)\n  ) +\n  geom_line(\n    data = filter(dat, year == 2000),\n    aes(age_trunc, share, group = sex),\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_point(\n    data = filter(dat, year == 2000),\n    aes(age_trunc, share, group = sex),\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\nThis final visualization demonstrates an innovative approach to population pyramids by comparing two time periods side-by-side within each age group. This design choice offers several advantages:\n\nDirect Comparison: Placing bars next to each other makes it easier to see how population structure changes over time for each age group.\nEnhanced Readability: The text labels are centered between the bars, creating a clean and balanced layout that guides the reader’s eye naturally through the data.\nEfficient Legend: The color legend serves multiple purposes - it identifies time periods, explains the color coding, and provides context for the entire visualization. This eliminates the need for additional explanatory text.\n\nThis design represents a thoughtful evolution of the traditional population pyramid, maintaining its familiar elements while improving its comparative capabilities.\nMaking this plot, however, is a bit more complex and requires the patchwork package.\n\np1 &lt;- ggplot(\n  data = filter(dat, sex == \"Male\", year %in% c(1970, 2000)),\n  aes(age_trunc, share, alpha = as.factor(year))) +\n  geom_col(position = position_dodge(),\n           fill = colors[1]) +\n  scale_alpha_manual(\n    name = \"\",\n    values = c(0.5, 1),\n    labels = c(\"Male (1970)\", \"Male (2000)\")) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share,\n    limits = c(-8, 0)\n  ) +\n  labs(x = NULL, y = NULL) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.y = element_blank()\n  )\n\np2 &lt;- ggplot(\n  data = filter(dat, sex == \"Female\", year %in% c(1970, 2000)),\n  aes(age_trunc, share, alpha = as.factor(year))) +\n  geom_col(position = position_dodge(),\n           fill = colors[2]) +\n  scale_alpha_manual(\n    name = \"\",\n    values = c(0.5, 1),\n    labels = c(\"Female (1970)\", \"Female (2000)\")) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share,\n    limits = c(0, 8)\n  ) +\n  labs(x = NULL, y = NULL) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.y = element_text(hjust = 0.5)\n  )\n\npanel &lt;- (p1 | p2) & theme(legend.position = \"bottom\")\npanel &lt;- panel + plot_layout(guides = \"collect\")\n\npanel\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation pyramids are powerful tools for visualizing demographic structure and change. Through the examples shown above, we’ve explored how to create increasingly sophisticated versions of these charts - from basic representations to comparative visualizations that reveal temporal changes.\nKey takeaways from this tutorial:\n\nStart with a basic structure showing population shares by age and sex Add clear labels and thoughtful color choices to make visualizations self-explanatory.\nUse age groups rather than single years for more meaningful analysis.\nConsider innovative layouts, like side-by-side comparisons, to enhance understanding.\nWhile the traditional pyramid shape remains useful, modern visualization techniques and tools allow us to adapt this classic demographic tool to better serve our analytical needs. The final example, with its side-by-side comparison and centered labels, demonstrates how thoughtful design choices can significantly improve the communication of demographic data.\n\nRemember that the goal is always to make complex demographic information more accessible and understandable to your audience."
  },
  {
    "objectID": "posts/general-posts/2024-12-demographic-pyramid/index.html#simple-age-data",
    "href": "posts/general-posts/2024-12-demographic-pyramid/index.html#simple-age-data",
    "title": "Demographic Pyramids in R",
    "section": "",
    "text": "The simplest way that demographic data is presented is on a year-on-year basis. These estimates are available in the popAge1dt dataset.\n\ndata(popAge1dt)\n\n# Brazil, Ireland, Pakistan, Japan\ncountries &lt;- tibble(\n  country_code = c(392, 586, 372, 76),\n  name = c(\"Japan\", \"Pakistan\", \"Ireland\", \"Brazil\")\n)\n\n\n\nA population pyramid follows certain conventions. The data is presented in age-sex groups: males on the left, females on the right. Bars typically represent each group’s share of total population, though sometimes they show shares within each sex separately.\nThe code below calculates population shares by age and sex, making male values negative to position them on the left side.\n\npopage &lt;- popAge1dt |&gt; \n  filter(\n    country_code %in% countries$country_code,\n    year %in% seq(1950, 2020, 10)\n    )\n\npopage &lt;- popage |&gt; \n  select(country_code, name, year, age, popM, popF) |&gt; \n    pivot_longer(\n    cols = popM:popF,\n    names_to = \"sex\",\n    values_to = \"population\"\n  )\n\npopage &lt;- popage |&gt; \n  mutate(\n    sex = if_else(sex == \"popM\", \"Male\", \"Female\"),\n    sex = factor(sex, levels = c(\"Male\", \"Female\")),\n  ) |&gt; \n  group_by(country_code, name, year) |&gt; \n  mutate(share = population / sum(population, na.rm = TRUE) * 100) |&gt; \n  ungroup() |&gt; \n  mutate(share = if_else(sex == \"Male\", -share, share))\n\n\n\n\nThe code below creates a basic population pyramid using Japan’s 1950 demographic data. Each bar shows what percentage of the total population belongs to a specific age group and sex combination. For example, a bar might show that 1% of Japan’s total population was males aged 10 years.\nThe visualization is called a “pyramid” because of its distinctive shape: wide at the bottom (many young people), gradually narrowing through the middle ages (fewer adults), and smallest at the top (very few elderly). This triangular shape was typical for most countries in the past and is still common in developing nations today. The shape reveals a population structure with high birth rates and shorter life expectancy.\n\njpn50 &lt;- popage |&gt; \n  filter(name == \"Japan\", year == 1950)\n\nggplot(jpn50, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\")\n\n\n\n\n\n\n\n\n\n\n\nThe code below demonstrates how to enhance a population pyramid with colors and explanatory labels. These additions are essential as they make the visualization more informative and self-contained, allowing readers to understand the data without needing external explanations.\nRegarding color choices for gender representation, modern data visualization practices avoid traditional stereotypes (like blue for males and pink for females). DataWrapper has an insightful discussion on this matter and recommends using green for men and purple for women.\nNote that while these specific colors (green and purple) work well, there’s no strict rule - the key is to choose colors that are:\n\nClearly distinguishable\nAccessible to colorblind viewers\nConsistent with your overall design scheme\n(Preferably) Free from gender stereotypes\n\n\ncolors &lt;- c(\"#1B9E77\", \"#7570B3\")\nfont &lt;- \"Open Sans\"\n\np &lt;- ggplot(jpn50, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\")\n\np + \n  geom_hline(yintercept = 0) +\n  annotate(\n    \"label\",\n    x = 85,\n    y = -1.25,\n    label = \"Male\",\n    color = colors[1],\n    family = font,\n    size = 4\n  ) +\n    annotate(\n    \"label\",\n    x = 85,\n    y = 1.25,\n    label = \"Female\",\n    color = colors[2],\n    family = font,\n    size = 4\n  ) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nThe code below shows how to make interesting comparisions using small facet plots.\nThe first example shows how to see the demographic evolution of a single country over the years.\nThe second example shows how to compare a cross-country selection across the same time period.\n\njpn &lt;- popage |&gt; \n  filter(name == \"Japan\", year %in% seq(1950, 2000, 10))\n\nbase_plot &lt;- ggplot(jpn, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\nbase_plot +\n  facet_wrap(~year) +\n  ggtitle(\"The demographic aging of Japan\")\n\n\n\n\n\n\n\n\n\ncountry_comp50 &lt;- popage |&gt; \n  filter(year == 1980)\n\nbase_plot &lt;- ggplot(country_comp50, aes(x = age, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(0, 100, 10),\n    expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(-1.5, 1.5, 0.5),\n    labels = c(\"1.5\", \"1\", \"0.5\", \"0\", \"0.5\", \"1\", \"1.5\")\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\nbase_plot +\n  facet_wrap(~name) +\n  ggtitle(\"Country comparison: 1980\")"
  },
  {
    "objectID": "posts/general-posts/2024-12-demographic-pyramid/index.html#grouped-age-data",
    "href": "posts/general-posts/2024-12-demographic-pyramid/index.html#grouped-age-data",
    "title": "Demographic Pyramids in R",
    "section": "",
    "text": "Demographers typically work with grouped age data (usually in 5 or 10-year intervals) rather than single-year ages for two main reasons:\n\nStatistical Reliability: Grouped data provides more precise estimates by reducing random variations that can occur in single-year measurements.\nPractical Analysis: Broader age groups make it easier to identify and understand demographic patterns and trends. For instance, knowing the proportion of people aged 65-69 is often more useful for policy planning than specific single-year counts.\n\nThis data is available in the popAge5dt dataset.\nSome data manipulation is necessary since the age groups are interpreted as text/strings, and there is no obvious way to order this data. The code below converts this text data into a factor (a ordered string, 0-4, 5-9, …, 80-84, 85+).\n\ndata(\"popAge5dt\")\n\npopage &lt;- popAge5dt |&gt; \n  filter(country_code %in% countries$country_code) |&gt; \n  mutate(\n    age_trunc = if_else(\n      age %in% c(\"85-89\", \"90-94\", \"95-99\", \"100+\"), \"85-\", age\n      ),\n    age_min = as.integer(str_extract(age_trunc, \"[0-9]{1,2}(?=-)\")),\n    age_trunc = str_replace(age_trunc, \"85-\", \"85+\"),\n    age_trunc = factor(age_trunc),\n    age_trunc = forcats::fct_reorder(age_trunc, age_min)\n  ) |&gt; \n  group_by(country_code, name, year, age_trunc) |&gt; \n  summarise(\n    pop_male = sum(popM),\n    pop_female = sum(popF)\n  ) |&gt; \n  ungroup()\n\npopage &lt;- popage |&gt; \n    select(country_code, name, year, age_trunc, pop_male, pop_female) |&gt; \n    pivot_longer(\n    cols = starts_with(\"pop\"),\n    names_to = \"sex\",\n    values_to = \"population\"\n  )\n\npopage &lt;- popage |&gt; \n  mutate(\n    sex = if_else(sex == \"pop_male\", \"Male\", \"Female\"),\n    sex = factor(sex, levels = c(\"Male\", \"Female\")),\n  ) |&gt; \n  group_by(country_code, name, year) |&gt; \n  mutate(share = population / sum(population, na.rm = TRUE) * 100) |&gt; \n  ungroup() |&gt; \n  mutate(share = if_else(sex == \"Male\", -share, share))\n\n\n\nThe code below shows how to make a simple plot following the conventions above. The data shows the demographic distribution of Brazil in 1990.\n\ndat &lt;- filter(popage, name == \"Brazil\", year == 1990)\n\nbreaks_share &lt;- seq(-6, 6, 1)\nlabels_share &lt;- str_remove(as.character(breaks_share), \"-\")\n\nggplot(dat, aes(x = age_trunc, y = share)) +\n  geom_col(aes(fill = sex, color = sex)) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nAside from side-by-side plots, we can make different kinds of visualizations to highlight demographic changes through time.\nThis first example uses a simple line to compare Brazil’s demographic pyramid in 1970 and 2000.\n\ndat &lt;- filter(popage, name == \"Brazil\")\n\nbreaks_share &lt;- seq(-10, 10, 2)\nlabels_share &lt;- str_remove(as.character(breaks_share), \"-\")\n\nggplot() +\n  geom_col(\n    data = filter(dat, year == 2000),\n    aes(age_trunc, share, fill = sex, color = sex)\n  ) +\n  geom_step(\n    data = filter(dat, year == 1970),\n    aes(age_trunc, share, group = sex),\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share\n  ) +\n  labs(\n    title = \"Brazil: 1970 x 2000\",\n    x = \"Age\", y = \"Share of Population (%)\",\n    subtitle = \"Solid line shows the demographic pyramid in 1970. Colored bars show the same data in 2000.\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\nThis second example is essentially the same but uses a connected line to contrast Brazil’s demographic in 2000 and 2010.\n\nggplot() +\n  geom_col(\n    data = filter(dat, year == 2010),\n    aes(age_trunc, share, fill = sex, color = sex)\n  ) +\n  geom_line(\n    data = filter(dat, year == 2000),\n    aes(age_trunc, share, group = sex),\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  geom_point(\n    data = filter(dat, year == 2000),\n    aes(age_trunc, share, group = sex),\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  coord_flip() +\n  guides(fill = \"none\", color = \"none\") +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share\n  ) +\n  labs(x = \"Age\", y = \"Share of Population (%)\") +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2)\n  )\n\n\n\n\n\n\n\n\nThis final visualization demonstrates an innovative approach to population pyramids by comparing two time periods side-by-side within each age group. This design choice offers several advantages:\n\nDirect Comparison: Placing bars next to each other makes it easier to see how population structure changes over time for each age group.\nEnhanced Readability: The text labels are centered between the bars, creating a clean and balanced layout that guides the reader’s eye naturally through the data.\nEfficient Legend: The color legend serves multiple purposes - it identifies time periods, explains the color coding, and provides context for the entire visualization. This eliminates the need for additional explanatory text.\n\nThis design represents a thoughtful evolution of the traditional population pyramid, maintaining its familiar elements while improving its comparative capabilities.\nMaking this plot, however, is a bit more complex and requires the patchwork package.\n\np1 &lt;- ggplot(\n  data = filter(dat, sex == \"Male\", year %in% c(1970, 2000)),\n  aes(age_trunc, share, alpha = as.factor(year))) +\n  geom_col(position = position_dodge(),\n           fill = colors[1]) +\n  scale_alpha_manual(\n    name = \"\",\n    values = c(0.5, 1),\n    labels = c(\"Male (1970)\", \"Male (2000)\")) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share,\n    limits = c(-8, 0)\n  ) +\n  labs(x = NULL, y = NULL) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.y = element_blank()\n  )\n\np2 &lt;- ggplot(\n  data = filter(dat, sex == \"Female\", year %in% c(1970, 2000)),\n  aes(age_trunc, share, alpha = as.factor(year))) +\n  geom_col(position = position_dodge(),\n           fill = colors[2]) +\n  scale_alpha_manual(\n    name = \"\",\n    values = c(0.5, 1),\n    labels = c(\"Female (1970)\", \"Female (2000)\")) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = breaks_share,\n    labels = labels_share,\n    limits = c(0, 8)\n  ) +\n  labs(x = NULL, y = NULL) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = colors) +\n  theme_minimal(base_size = 12, base_family = font) +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.y = element_text(hjust = 0.5)\n  )\n\npanel &lt;- (p1 | p2) & theme(legend.position = \"bottom\")\npanel &lt;- panel + plot_layout(guides = \"collect\")\n\npanel"
  },
  {
    "objectID": "posts/general-posts/2024-12-demographic-pyramid/index.html#conclusion",
    "href": "posts/general-posts/2024-12-demographic-pyramid/index.html#conclusion",
    "title": "Demographic Pyramids in R",
    "section": "",
    "text": "Population pyramids are powerful tools for visualizing demographic structure and change. Through the examples shown above, we’ve explored how to create increasingly sophisticated versions of these charts - from basic representations to comparative visualizations that reveal temporal changes.\nKey takeaways from this tutorial:\n\nStart with a basic structure showing population shares by age and sex Add clear labels and thoughtful color choices to make visualizations self-explanatory.\nUse age groups rather than single years for more meaningful analysis.\nConsider innovative layouts, like side-by-side comparisons, to enhance understanding.\nWhile the traditional pyramid shape remains useful, modern visualization techniques and tools allow us to adapt this classic demographic tool to better serve our analytical needs. The final example, with its side-by-side comparison and centered labels, demonstrates how thoughtful design choices can significantly improve the communication of demographic data.\n\nRemember that the goal is always to make complex demographic information more accessible and understandable to your audience."
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html",
    "href": "posts/general-posts/repost-gapminder/index.html",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Muitos já devem estar familiarizados com a apresentação do historiador Hans Rosling sobre a evolução da expectativa de vida e do PIB per capita dos países em torno do mundo. Este post vai mostrar como usar o ggplot2 e o tidyverse para explorar estes dados. Pode-se acessar uma versão simplificada da base de dados pelo pacote gapminder.\n\n# Tutorial Gapminder\n\n# Pacotes\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(kableExtra)\nlibrary(ggrepel)\nlibrary(showtext)\n\n# Carregar fontes\nsysfonts::font_add_google(\"Jost\", \"Jost\")\nshowtext::showtext_auto()\n# Carregar os dados\ndata(\"gapminder\")\ndata(\"continent_colors\")\n\n\n\n\nDe início é sempre importante verificar se há problemas com os dados. Como estamos usando uma base que já foi tratada é de se esperar que tudo esteja em ordem. Tipicamente, queremos verificar quantas observações ausentes (NAs) existem; se as variáveis estão no formato correto (ex: variáveis de texto como factor, números como numeric, etc.). Neste caso, além destas checagens também vamos criar uma nova variável que é o log do PIB per capita.\n\n# Checagens iniciais #\n# Verifica se há valores ausentes\ngapminder %&gt;%\n  summarise(across(everything(), ~sum(is.na(.x))))\n\n# A tibble: 1 × 6\n  country continent  year lifeExp   pop gdpPercap\n    &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1       0         0     0       0     0         0\n\n# Informações gerais sobre os dados\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n# Nome das variáveis minúsculas\nnames(gapminder) &lt;- tolower(names(gapminder))\n# Tranformações #\n# Computa o log do pib per capita\ngapminder &lt;- mutate(gapminder, lgdppc = log10(gdppercap))\n\n\n\n\nPara manter o padrão das visualizações pode-se criar um tema personalizado. A maneira mais simples de fazer isto é a partir de um tema padrão do ggplot2 mas é possível começar do zero. Aqui, por simplicidade, começo com o tema bw e apenas mudo a posição da legenda e o tamanho e a fonte do texto que será plotado nos eixos e no título do gráfico. Além disso, como o nome dos eixos vai ser repetido muitas vezes defino uma lista com o nome mais comum deles.\n\n# Tema customizado #\ntheme_vini &lt;- theme_bw() +\n  theme(\n    text = element_text(family = \"Jost\", size = 12, colour = \"gray20\"),\n    plot.title = element_text(size = 16),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n    )\n\n# Nomes que serão usados várias vezes para os eixos\nnomes &lt;- list(\n  title = \"Expectativa de vida e PIB per capita\",\n    x = \"Log do PIB per capita (US$ 2010)\",\n    y = \"Expectativa de vida ao nascer\",\n    fonte = \"Fonte: Gapmineder (www.gapminder.org) e World Bank Open Data.\"\n  )\n\n\n\n\n\n\n\nNote que não temos dados para todos os anos do período. Os dados estão disponíveis de cinco em cinco anos começando em 1952 e terminando em 2007. Podemos começar com um gráfico de dispersão para ver a relação entre a “economia” (PIB per capita) e a “qualidade da saúde/vida” (expectativa de vida ao nascer) de um país. Apenas como exemplo incluo também uma linha de regressão quadrática no gráfico.\n\nunique(gapminder$year)\n\n [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\n\n\n\ngap2007 &lt;- filter(gapminder, year == 2007)\n\nggplot(gap2007, aes(lgdppc, lifeexp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    subtitle = \"Relação entre expectativa de vida ao nascer e o logaritmo do PIB per capita (fit linear)\",\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nPodemos nos valer de outras informações disponíveis na base para alterar atributos estéticos do gráfico. O tamanho de cada círculo pode refletir o tamanho da população daquele país; a cor do círculo, por sua vez, pode representar o continente daquele país. Por conveniência uso as cores pré-definidas do pacote gapminder. Além disso também podemos destacar o nome de alguns países usando o pacote ggrepel.\n\ndestaque &lt;- c(\n  \"Australia\", \"Argentina\", \"Brazil\", \"Chile\", \"India\", \"Nigeria\", \n  \"Sudan\", \"Taiwan\", \"Mozambique\", \"Angola\", \"Vietnam\"\n  )\n\ngap_highlight &lt;- gap2007 %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  mutate(sel = ifelse(country %in% destaque, country, \"\"))\n\nggplot(gap_highlight, aes(lgdppc, lifeexp)) +\n  geom_point(aes(size = pop, colour = continent), alpha = .75) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_text_repel(\n    aes(label = sel),\n    family = \"Jost\",\n    force = 20,\n    max.overlaps = 30,\n    size = 5\n    ) + \n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  scale_color_manual(values = continent_colors, name = \"\") +\n  scale_size_continuous(range = c(1, 20)) +\n  guides(size = FALSE) + \n  theme_vini\n\n\n\n\n\n\n\n\nO gráfico acima é um retrato do momento, mas pode ser interessante entender como estas variáveis se comportaram ao longo do tempo.\n\n\n\n\n# Calcula a expectativa de vida média\ngap_life &lt;- gapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(media = mean(lifeexp))\n\nggplot(gap_life, aes(year, media)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = paste(nomes$title, \"(média mundial)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos desagregar a análise acima por país. É claro que fica difícil discernir um país específico, mas pode-se ver uma tendência geral de crescimento, ainda que haja alguns outliers. A maior parte das quedas significativas pode ser relacionada com alguma guerra. O país cuja expectativa de vida cai bruscamente no começo dos anos 90, por exemplo, é a Ruanda, que vivia uma guerra civil nesta época.\n\nggplot(gapminder, aes(year, lifeexp, group = country, colour = continent)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 30, to = 80, by = 10)) +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nSe nos atermos somente ao nível de continente a visualização fica mais simples.\n\ngap_life_continent &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(expec_media = mean(lifeexp))\n\nggplot(gap_life_continent, aes(year, expec_media, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida média (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para o log do PIB per capita. Aqui o gráfico é feito usando o log do PIB per capita, mas no eixo indico o valor equivalente em dólares para facilitar a interpretação.\n\n# Calcula o PIB per capita médio por continente a cada ano\ngap_gdp &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(avg_gdp = mean(gdppercap))\n\nggplot(gap_gdp, aes(year, avg_gdp, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  # Escala log\n  scale_y_log10() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"PIB per capita médio (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$x\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nAntes de começar a análise tabular dos dados vale definir uma função simples para apresentar as tabelas.\n\nprint_table &lt;- function(df, ...) {\n  \n  str_title &lt;- function(string) {\n    \n    x &lt;- stringr::str_replace_all(string, \"_\", \" \")\n    x &lt;- stringr::str_to_title(x)\n    return(x)\n    \n  }\n  \n  knitr::kable(df, col.names = str_title(colnames(df)), ...) %&gt;%\n    kableExtra::kable_styling(\n      full_width = FALSE,\n      bootstrap_options = c(\"hover\", \"condensed\")\n    )\n  \n}\n\nPodemos encontrar fatos interessantes simplesmente agregando e reorganizando os dados. No gráfico anterior vimos que o continente com maior PIB per capita médio é a Oceania. Curiosamente, a base inclui somente dois países na Oceania: Austrália e Nove Zelândia.\n\n# Note que na Oceania os dados só incluem Autrália e Nova Zelândia\ngapminder %&gt;%\n  filter(continent == \"Oceania\") %&gt;%\n  count(country) %&gt;%\n  print_table()\n\n\n\n\nCountry\nN\n\n\n\n\nAustralia\n12\n\n\nNew Zealand\n12\n\n\n\n\n\nPodemos encontrar os países que mais cresceram (em termos absolutos e relativos) durante o período observado. Note como há vários países asiáticos listados qual seja a métrica escolhida.\n\n# Países que mais cresceram no período da amostra em termos absolutos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = last(gdppercap) - first(gdppercap)) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(caption = \"Países que mais cresceram de 1952 a 2007\")\n\n\nPaíses que mais cresceram de 1952 a 2007\n\n\nCountry\nGrowth\n\n\n\n\nSingapore\n44828.04\n\n\nNorway\n39261.77\n\n\nHong Kong, China\n36670.56\n\n\nIreland\n35465.72\n\n\nAustria\n29989.42\n\n\nUnited States\n28961.17\n\n\nIceland\n28913.10\n\n\nJapan\n28439.11\n\n\nNetherlands\n27856.36\n\n\nTaiwan\n27511.33\n\n\n\n\n\n\n# Países que mais cresceram no período da amostra em termos relativos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = (last(gdppercap) / first(gdppercap) - 1) * 100) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(\n    caption = \"Países que mais cresceram de 1952 a 2007 (%)\",\n    digits = 2\n    )\n\n\nPaíses que mais cresceram de 1952 a 2007 (%)\n\n\nCountry\nGrowth\n\n\n\n\nEquatorial Guinea\n3135.54\n\n\nTaiwan\n2279.41\n\n\nKorea, Rep.\n2165.51\n\n\nSingapore\n1936.30\n\n\nBotswana\n1376.65\n\n\nHong Kong, China\n1200.57\n\n\nChina\n1138.39\n\n\nOman\n1120.64\n\n\nThailand\n884.22\n\n\nJapan\n884.04\n\n\n\n\n\nA mesma análise também pode ser feita para a expectativa de vida. Adicionalmente também podemos encontrar qual foi a maior variação (negativa) entre um ponto observado e outro dentro da amostra.\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = last(lifeexp) - first(lifeexp)) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 0)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n38\n\n\nVietnam\n34\n\n\nIndonesia\n33\n\n\nSaudi Arabia\n33\n\n\nLibya\n31\n\n\nKorea, Rep.\n31\n\n\nNicaragua\n31\n\n\nWest Bank and Gaza\n30\n\n\nYemen, Rep.\n30\n\n\nGambia\n29\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = (last(lifeexp) / first(lifeexp) - 1) * 100) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 2)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n101.29\n\n\nGambia\n98.16\n\n\nYemen, Rep.\n92.63\n\n\nIndonesia\n88.56\n\n\nVietnam\n83.73\n\n\nSaudi Arabia\n82.51\n\n\nNepal\n76.41\n\n\nIndia\n73.11\n\n\nLibya\n73.10\n\n\nNicaragua\n72.28\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  mutate(\n    life_diff = lifeexp - lag(lifeexp),\n    life_abs  = abs(lifeexp - lag(lifeexp))) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(life_abs)) %&gt;%\n  select(country, year, life_diff) %&gt;%\n  slice(1:10) %&gt;%\n  print_table(digits = 1)\n\n\n\n\nCountry\nYear\nLife Diff\n\n\n\n\nRwanda\n1992\n-20.4\n\n\nCambodia\n1982\n19.7\n\n\nChina\n1967\n13.9\n\n\nZimbabwe\n1997\n-13.6\n\n\nRwanda\n1997\n12.5\n\n\nLesotho\n2002\n-11.0\n\n\nSwaziland\n2002\n-10.4\n\n\nBotswana\n1997\n-10.2\n\n\nCambodia\n1977\n-9.1\n\n\nNamibia\n2002\n-7.4"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#preparativos",
    "href": "posts/general-posts/repost-gapminder/index.html#preparativos",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Muitos já devem estar familiarizados com a apresentação do historiador Hans Rosling sobre a evolução da expectativa de vida e do PIB per capita dos países em torno do mundo. Este post vai mostrar como usar o ggplot2 e o tidyverse para explorar estes dados. Pode-se acessar uma versão simplificada da base de dados pelo pacote gapminder.\n\n# Tutorial Gapminder\n\n# Pacotes\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(kableExtra)\nlibrary(ggrepel)\nlibrary(showtext)\n\n# Carregar fontes\nsysfonts::font_add_google(\"Jost\", \"Jost\")\nshowtext::showtext_auto()\n# Carregar os dados\ndata(\"gapminder\")\ndata(\"continent_colors\")\n\n\n\n\nDe início é sempre importante verificar se há problemas com os dados. Como estamos usando uma base que já foi tratada é de se esperar que tudo esteja em ordem. Tipicamente, queremos verificar quantas observações ausentes (NAs) existem; se as variáveis estão no formato correto (ex: variáveis de texto como factor, números como numeric, etc.). Neste caso, além destas checagens também vamos criar uma nova variável que é o log do PIB per capita.\n\n# Checagens iniciais #\n# Verifica se há valores ausentes\ngapminder %&gt;%\n  summarise(across(everything(), ~sum(is.na(.x))))\n\n# A tibble: 1 × 6\n  country continent  year lifeExp   pop gdpPercap\n    &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1       0         0     0       0     0         0\n\n# Informações gerais sobre os dados\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n# Nome das variáveis minúsculas\nnames(gapminder) &lt;- tolower(names(gapminder))\n# Tranformações #\n# Computa o log do pib per capita\ngapminder &lt;- mutate(gapminder, lgdppc = log10(gdppercap))\n\n\n\n\nPara manter o padrão das visualizações pode-se criar um tema personalizado. A maneira mais simples de fazer isto é a partir de um tema padrão do ggplot2 mas é possível começar do zero. Aqui, por simplicidade, começo com o tema bw e apenas mudo a posição da legenda e o tamanho e a fonte do texto que será plotado nos eixos e no título do gráfico. Além disso, como o nome dos eixos vai ser repetido muitas vezes defino uma lista com o nome mais comum deles.\n\n# Tema customizado #\ntheme_vini &lt;- theme_bw() +\n  theme(\n    text = element_text(family = \"Jost\", size = 12, colour = \"gray20\"),\n    plot.title = element_text(size = 16),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n    )\n\n# Nomes que serão usados várias vezes para os eixos\nnomes &lt;- list(\n  title = \"Expectativa de vida e PIB per capita\",\n    x = \"Log do PIB per capita (US$ 2010)\",\n    y = \"Expectativa de vida ao nascer\",\n    fonte = \"Fonte: Gapmineder (www.gapminder.org) e World Bank Open Data.\"\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#visualizando",
    "href": "posts/general-posts/repost-gapminder/index.html#visualizando",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Note que não temos dados para todos os anos do período. Os dados estão disponíveis de cinco em cinco anos começando em 1952 e terminando em 2007. Podemos começar com um gráfico de dispersão para ver a relação entre a “economia” (PIB per capita) e a “qualidade da saúde/vida” (expectativa de vida ao nascer) de um país. Apenas como exemplo incluo também uma linha de regressão quadrática no gráfico.\n\nunique(gapminder$year)\n\n [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\n\n\n\ngap2007 &lt;- filter(gapminder, year == 2007)\n\nggplot(gap2007, aes(lgdppc, lifeexp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    subtitle = \"Relação entre expectativa de vida ao nascer e o logaritmo do PIB per capita (fit linear)\",\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nPodemos nos valer de outras informações disponíveis na base para alterar atributos estéticos do gráfico. O tamanho de cada círculo pode refletir o tamanho da população daquele país; a cor do círculo, por sua vez, pode representar o continente daquele país. Por conveniência uso as cores pré-definidas do pacote gapminder. Além disso também podemos destacar o nome de alguns países usando o pacote ggrepel.\n\ndestaque &lt;- c(\n  \"Australia\", \"Argentina\", \"Brazil\", \"Chile\", \"India\", \"Nigeria\", \n  \"Sudan\", \"Taiwan\", \"Mozambique\", \"Angola\", \"Vietnam\"\n  )\n\ngap_highlight &lt;- gap2007 %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  mutate(sel = ifelse(country %in% destaque, country, \"\"))\n\nggplot(gap_highlight, aes(lgdppc, lifeexp)) +\n  geom_point(aes(size = pop, colour = continent), alpha = .75) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_text_repel(\n    aes(label = sel),\n    family = \"Jost\",\n    force = 20,\n    max.overlaps = 30,\n    size = 5\n    ) + \n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  scale_color_manual(values = continent_colors, name = \"\") +\n  scale_size_continuous(range = c(1, 20)) +\n  guides(size = FALSE) + \n  theme_vini\n\n\n\n\n\n\n\n\nO gráfico acima é um retrato do momento, mas pode ser interessante entender como estas variáveis se comportaram ao longo do tempo.\n\n\n\n\n# Calcula a expectativa de vida média\ngap_life &lt;- gapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(media = mean(lifeexp))\n\nggplot(gap_life, aes(year, media)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = paste(nomes$title, \"(média mundial)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos desagregar a análise acima por país. É claro que fica difícil discernir um país específico, mas pode-se ver uma tendência geral de crescimento, ainda que haja alguns outliers. A maior parte das quedas significativas pode ser relacionada com alguma guerra. O país cuja expectativa de vida cai bruscamente no começo dos anos 90, por exemplo, é a Ruanda, que vivia uma guerra civil nesta época.\n\nggplot(gapminder, aes(year, lifeexp, group = country, colour = continent)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 30, to = 80, by = 10)) +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nSe nos atermos somente ao nível de continente a visualização fica mais simples.\n\ngap_life_continent &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(expec_media = mean(lifeexp))\n\nggplot(gap_life_continent, aes(year, expec_media, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida média (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para o log do PIB per capita. Aqui o gráfico é feito usando o log do PIB per capita, mas no eixo indico o valor equivalente em dólares para facilitar a interpretação.\n\n# Calcula o PIB per capita médio por continente a cada ano\ngap_gdp &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(avg_gdp = mean(gdppercap))\n\nggplot(gap_gdp, aes(year, avg_gdp, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  # Escala log\n  scale_y_log10() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"PIB per capita médio (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$x\n    ) +\n  theme_vini"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#analisando-os-dados",
    "href": "posts/general-posts/repost-gapminder/index.html#analisando-os-dados",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Antes de começar a análise tabular dos dados vale definir uma função simples para apresentar as tabelas.\n\nprint_table &lt;- function(df, ...) {\n  \n  str_title &lt;- function(string) {\n    \n    x &lt;- stringr::str_replace_all(string, \"_\", \" \")\n    x &lt;- stringr::str_to_title(x)\n    return(x)\n    \n  }\n  \n  knitr::kable(df, col.names = str_title(colnames(df)), ...) %&gt;%\n    kableExtra::kable_styling(\n      full_width = FALSE,\n      bootstrap_options = c(\"hover\", \"condensed\")\n    )\n  \n}\n\nPodemos encontrar fatos interessantes simplesmente agregando e reorganizando os dados. No gráfico anterior vimos que o continente com maior PIB per capita médio é a Oceania. Curiosamente, a base inclui somente dois países na Oceania: Austrália e Nove Zelândia.\n\n# Note que na Oceania os dados só incluem Autrália e Nova Zelândia\ngapminder %&gt;%\n  filter(continent == \"Oceania\") %&gt;%\n  count(country) %&gt;%\n  print_table()\n\n\n\n\nCountry\nN\n\n\n\n\nAustralia\n12\n\n\nNew Zealand\n12\n\n\n\n\n\nPodemos encontrar os países que mais cresceram (em termos absolutos e relativos) durante o período observado. Note como há vários países asiáticos listados qual seja a métrica escolhida.\n\n# Países que mais cresceram no período da amostra em termos absolutos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = last(gdppercap) - first(gdppercap)) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(caption = \"Países que mais cresceram de 1952 a 2007\")\n\n\nPaíses que mais cresceram de 1952 a 2007\n\n\nCountry\nGrowth\n\n\n\n\nSingapore\n44828.04\n\n\nNorway\n39261.77\n\n\nHong Kong, China\n36670.56\n\n\nIreland\n35465.72\n\n\nAustria\n29989.42\n\n\nUnited States\n28961.17\n\n\nIceland\n28913.10\n\n\nJapan\n28439.11\n\n\nNetherlands\n27856.36\n\n\nTaiwan\n27511.33\n\n\n\n\n\n\n# Países que mais cresceram no período da amostra em termos relativos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = (last(gdppercap) / first(gdppercap) - 1) * 100) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(\n    caption = \"Países que mais cresceram de 1952 a 2007 (%)\",\n    digits = 2\n    )\n\n\nPaíses que mais cresceram de 1952 a 2007 (%)\n\n\nCountry\nGrowth\n\n\n\n\nEquatorial Guinea\n3135.54\n\n\nTaiwan\n2279.41\n\n\nKorea, Rep.\n2165.51\n\n\nSingapore\n1936.30\n\n\nBotswana\n1376.65\n\n\nHong Kong, China\n1200.57\n\n\nChina\n1138.39\n\n\nOman\n1120.64\n\n\nThailand\n884.22\n\n\nJapan\n884.04\n\n\n\n\n\nA mesma análise também pode ser feita para a expectativa de vida. Adicionalmente também podemos encontrar qual foi a maior variação (negativa) entre um ponto observado e outro dentro da amostra.\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = last(lifeexp) - first(lifeexp)) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 0)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n38\n\n\nVietnam\n34\n\n\nIndonesia\n33\n\n\nSaudi Arabia\n33\n\n\nLibya\n31\n\n\nKorea, Rep.\n31\n\n\nNicaragua\n31\n\n\nWest Bank and Gaza\n30\n\n\nYemen, Rep.\n30\n\n\nGambia\n29\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = (last(lifeexp) / first(lifeexp) - 1) * 100) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 2)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n101.29\n\n\nGambia\n98.16\n\n\nYemen, Rep.\n92.63\n\n\nIndonesia\n88.56\n\n\nVietnam\n83.73\n\n\nSaudi Arabia\n82.51\n\n\nNepal\n76.41\n\n\nIndia\n73.11\n\n\nLibya\n73.10\n\n\nNicaragua\n72.28\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  mutate(\n    life_diff = lifeexp - lag(lifeexp),\n    life_abs  = abs(lifeexp - lag(lifeexp))) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(life_abs)) %&gt;%\n  select(country, year, life_diff) %&gt;%\n  slice(1:10) %&gt;%\n  print_table(digits = 1)\n\n\n\n\nCountry\nYear\nLife Diff\n\n\n\n\nRwanda\n1992\n-20.4\n\n\nCambodia\n1982\n19.7\n\n\nChina\n1967\n13.9\n\n\nZimbabwe\n1997\n-13.6\n\n\nRwanda\n1997\n12.5\n\n\nLesotho\n2002\n-11.0\n\n\nSwaziland\n2002\n-10.4\n\n\nBotswana\n1997\n-10.2\n\n\nCambodia\n1977\n-9.1\n\n\nNamibia\n2002\n-7.4"
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "Agregando todos os acidentes de trânsito não-fatais vê-se que há dois principais clusters de acidentes: o maior deles começa na região do Centro Histórico e se estende até a região da Paulista e mais ao oeste chegando até o Itaim Bibi; o segundo cluster, menos intenso, aparece entre a região de Santo Amaro e Campo Limpo. No geral, os focos de acidente, seguem as principais de vias de tráfego.\nOs dados se referem a todos os acidentes de trânsito não-fatais registrados dentro do município de São Paulo em 2023.\n\n\n\n\n\n\n\n\n\n\n\n\nO painel de mapas abaixo subdivide os dados entre acidentes que ocorreram no final de semana x dias de trabalho e pelo período do dia. Os dados são normalizados dentro de cada célula para facilitar a leitura do padrão: em termos absolutos, a maior parte dos acidentes ocorre durante a manhã e a tarde nos dias úteis.\nÉ interessante notar como o padrão de acidentes se torna muito mais disperso nos finais de semana em relação aos dias úteis. Durante a semana, os acidentes à tarde estão quase que totalmente concentrados no Centro Histórico da cidade; já no final de semana, durante o mesmo período, os acidentes estão espacialmente dispersos por todas as zonas da cidade.\n\n\n\n\n\n\n\n\n\n\n\n\nAs horas do dia com maior número de acidente são, grosso modo, os horários de pico, quando há maior volume de veículos trafegando.\n\n\n\n\n\n\n\n\n\n* Dados: Painel de Dados SigaSP"
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#panorama",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#panorama",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "Agregando todos os acidentes de trânsito não-fatais vê-se que há dois principais clusters de acidentes: o maior deles começa na região do Centro Histórico e se estende até a região da Paulista e mais ao oeste chegando até o Itaim Bibi; o segundo cluster, menos intenso, aparece entre a região de Santo Amaro e Campo Limpo. No geral, os focos de acidente, seguem as principais de vias de tráfego.\nOs dados se referem a todos os acidentes de trânsito não-fatais registrados dentro do município de São Paulo em 2023."
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#finais-de-semana",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#finais-de-semana",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "O painel de mapas abaixo subdivide os dados entre acidentes que ocorreram no final de semana x dias de trabalho e pelo período do dia. Os dados são normalizados dentro de cada célula para facilitar a leitura do padrão: em termos absolutos, a maior parte dos acidentes ocorre durante a manhã e a tarde nos dias úteis.\nÉ interessante notar como o padrão de acidentes se torna muito mais disperso nos finais de semana em relação aos dias úteis. Durante a semana, os acidentes à tarde estão quase que totalmente concentrados no Centro Histórico da cidade; já no final de semana, durante o mesmo período, os acidentes estão espacialmente dispersos por todas as zonas da cidade."
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#hora-do-dia",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#hora-do-dia",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "As horas do dia com maior número de acidente são, grosso modo, os horários de pico, quando há maior volume de veículos trafegando.\n\n\n\n\n\n\n\n\n\n* Dados: Painel de Dados SigaSP"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html",
    "title": "Pipes",
    "section": "",
    "text": "A partir da versão 4.1.0, o R passou a oferecer o operador |&gt; chamado de pipe (literalmente, cano)1. Este operador foi fortemente inspirado no operador homônimo %&gt;% do popular pacote magrittr. Neste post, explico como utilizar o pipe nativo e como ele difere do pipe do magrittr.\nO operador pipe (em ambos os casos), essencialmente, ordena uma função composta.\nLembrando um pouco sobre funções compostas: a expressão abaixo mostra a aplicação de três funções onde primeiro aplica-se a função f sobre x, depois a função g e, por fim, a função h. Lê-se a função de dentro para fora.\n\\[\nh(g(f(x))) = \\dots\n\\]\nPara tornar o exemplo mais concreto considere o exemplo abaixo onde calcula-se a média geométrica de uma sequência de números aleatórios.\nA média geométrica é dada pela expressão:\n\\[\n\\overline{x} = (\\prod_{i = 1}^{n}x_{i})^{\\frac{1}{n}} = \\text{exp}(\\frac{1}{n}\\sum_{i = 1}^{n}\\text{log}(x_{i}))\n\\]\n\nx &lt;- rnorm(n = 100, mean = 10)\n#&gt; Calcula a média geométrica\nexp(mean(log(x)))\n\n[1] 10.221\n\n\nUsando a mesma notação acima, aplica-se primeiro a função log (f), depois a função mean (g) e, por fim, a função exp (h). Usando o operador pipe, pode-se reescrever a expressão da seguinte forma.\n\nx |&gt; log() |&gt; mean() |&gt; exp()\n\n[1] 10.221\n\n\nNote que o resultado da função vai sendo “carregado” da esquerda para a direita sucessivamente. Para muitos usuários, a segunda sintaxe é mais intuitiva e/ou fácil de ler. No segundo código a ordem em que o nome das funções aparecem coincide com a ordem da sua aplicação.\nPor fim, note que o uso de várias funções numa mesma linha de código também nos poupa de ter de criar objetos intermediários como no exemplo abaixo.\n\nlog_x &lt;- log(x)\nlog_media &lt;- mean(log_x)\nmedia_geometrica_x &lt;- exp(log_media)\n\nmedia_geometrica_x\n\n[1] 10.221\n\n\nOs exemplos acima funcionaram sem problemas porque usou-se o operador pipe para “abrir” uma função composta. O argumento de cada função subsequente é o resultado da função antecedente: funciona como uma linha de montagem, em que cada nova etapa soma-se ao resultado da etapa anterior.\nQuando o resultado da função anterior não vai diretamente no primeiro argumento da função subsequente, precisa-se usar o operador _ (underline/underscore)2. Este operador serve como um placeholder: indica onde que o resultado da etapa anterior deve entrar. No exemplo abaixo, uso o placeholder para colocar a base de dados filtrada no argumento data dentro da função lm.\n\ncarros_4 &lt;- subset(mtcars, cyl == 4)\nfit &lt;- lm(mpg ~ wt, data = carros_4)\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _)\n\nPor fim, temos o caso das funções anônimas3. Uma função anônima é simplesmente uma função sem nome que é chamada uma única vez. Infelizmente, a sintaxe de um pipe com uma função anônima é bastante carregada.\n\nobjeto |&gt; (\\(x, y, z, ...) {define função})()\n\n# Nova sintaxe de funções anônimas (similar a lambda no Python)\nobjeto |&gt; (\\(x, y) {x^2 + y^2})()\n# Antiga sintaxe de funções anônimas\nobjeto |&gt; (function(x, y) {x^2 + y^2})()\n\nO exemplo repete o código acima, mas agora usa uma função anônima para pegar o R2 ajustado da regressão.\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _) |&gt; \n  (\\(x) {summary(x)$adj.r.squared})()\n\n\n\n\nImagine agora que se quer calcular o erro absoluto médio de uma regressão. Lembre-se que o EAM é dado por\n\\[\n\\text{EAM} = \\frac{1}{N}\\sum_{i = 1}^{N}|e_{i}|\n\\]\nonde \\(e_{i}\\) é o resíduo da regressão. O código abaixo mostra como fazer isto usando pipes.\n\n#&gt; Estima uma regressão qualquer\nfit &lt;- lm(mpg ~ wt, data = mtcars)\n\n#&gt; Calcula o erro absoluto médio\nfit |&gt; residuals() |&gt; abs() |&gt; mean()\n\n[1] 2.340642\n\n\nNote, contudo, que a situação fica um pouco mais complicada no caso em que se quer calcular a raiz do erro quadrado médio.\n\\[\n\\text{REQM} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^{N}(e_{i})^2}\n\\]\nNa sintaxe convencional temos\n\nsqrt(mean(residuals(fit)^2))\n\n[1] 2.949163\n\n\nO problema é que a exponenciação acontece via um operador e não uma função. Nenhum dos exemplos abaixo funciona.\n\nfit |&gt; residuals() |&gt; ^2 |&gt; mean() |&gt; sqrt()\n\nError: &lt;text&gt;:1:23: unexpected '^'\n1: fit |&gt; residuals() |&gt; ^\n                          ^\n\n\n\nfit |&gt; residuals()^2 |&gt; mean() |&gt; sqrt()\n\nError: function '^' not supported in RHS call of a pipe\n\n\nPara chegar no mesmo resultado, novamente precisa-se usar uma sintaxe bastante esotérica que envolve passar o resultado de residuals para uma função anônima.\n\nfit |&gt; residuals() |&gt; (\\(y) {sqrt(mean(y^2))})()\n\n[1] 2.949163\n\n\n\n\n\nAssim, apesar de muito útil, o operador pipe tem suas limitações. O operador sempre espera encontrar uma função à sua direita; a única maneira de seguir |&gt; com um operador é criando uma função anônima, cuja sintaxe é um pouco carregada. Pode-se resumir os principais fatos sobre o operador pipe:\n\nSimplifica funções compostas. Na expressão x |&gt; f |&gt; g o operador |&gt; aplica a função f sobre o objeto x usando x como argumento de f. Depois, aplica a função g sobre o resultado de f(x). Isto é equivalente a g(f(x)).\nEvita a definição de objetos intermediários. O uso de pipes evita que você precise “salvar” cada passo intermediário da aplicação de funções. Isto deixa seu espaço de trabalho mais limpo e também consome menos memória.\nPlaceholder. Quando o objeto anterior não serve como o primeiro argumento da função subsequente, usa-se o placeholder para indicar onde ele deve ser inserido. x |&gt; f(y = 2, data = _).\nFunção anônima. Em casos mais complexos, é necessário montar uma função anônima usando x |&gt; (\\(y) {funcao})().\n\n\n\n\n\n\nO uso mais comum de pipes é junto com funções do tidyverse, que foram desenvolvidas com este intuito.\n\nlibrary(tidyverse)\n\nAs funções do tidyverse (quase) sempre recebem um data.frame como primeiro argumento; isto facilita a construção de código usando pipe, pois basta encadear as funções em sequência.\n\nfiltered_df &lt;- filter(mtcars, wt == 2)\ngrouped_df &lt;- group_by(filtered_df, cyl)\ntbl &lt;- summarise(grouped_df, avg = mean(mpg), count = n())\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg))\n\nA leitura do código fica mais “gramatical”: pegue o objeto mtcars filtre as linhas onde wt &gt; 2 depois agrupe pela variável cyl e, por fim, tire uma média de mpg.\nPode-se terminar um pipe com uma chamada para um plot em ggplot2 para uma rápida visualização dos resultados\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg)) |&gt; \n  ggplot(aes(x = as.factor(cyl), y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNão se recomenda fazer longas sequências de pipes, pois o código pode acabar muito confuso para quem está lendo. O exemplo abaixo mostra justamente isto.\n\nlibrary(realestatebr)\n\nabecip &lt;- get_abecip_indicators(cached = TRUE)\n\nabecip$units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) |&gt; \n  filter(category == \"units\", type != \"total\") |&gt; \n  ggplot(aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nÉ recomendável quebrar o código acima em passos distintos. Além de ficar mais organizado, pode-se salvar objetos úteis como a tabela agrupada por trimestre, antes de se aplicar o filtro de unidades. A tabela final também fica salva num objeto, permitindo que se faça outros gráficos e análises com estes dados.\n\nunits &lt;- abecip$units\n\n#&gt; Converte em long e agrega os dados por trimestre\ntab_quarter &lt;- units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) \n\n#&gt; Filtra apenas dados de unidades e retira o 'total'\ntab_units &lt;- tab_quarter |&gt; \n  filter(category == \"units\", type != \"total\")\n\n#&gt; Faz o gráfico\nggplot(tab_units, aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\nO pacote sf também funciona bem com pipes pois há vários casos em que se quer aplicar múltiplas funções num mesmo objeto.\n\n# Transforma um data.frame num objeto espacial (pontos)\n# depois faz a interseção dos pontos num polígono e\n# por fim limpa as geometrias\n\ndat |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  st_join(poly) |&gt; \n  filter(!is.na(gid)) |&gt; \n  st_make_valid()\n\nO exemplo abaixo é emprestado do pacote censobr e mostra como combinar a manipulação de dados do dplyr com objetos espaciais manipulados via sf.\n\nlibrary(censobr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(mapview)\n\n# Importa alguns dados do Censo IBGE 2010\npop &lt;- read_population(\n  year = 2010,\n  columns = c(\"code_weighting\", \"abbrev_state\", \"V0010\")\n  )\n# Calcula a população total das áreas de ponderação no Rio de Janeiro\ndf &lt;- pop |&gt;\n      filter(abbrev_state == \"RJ\") |&gt;\n      group_by(code_weighting) |&gt;\n      summarise(total_pop = sum(V0010)) |&gt;\n      collect()\n\n# Import o shape das áreas de ponderação do Censo\nareas &lt;- read_weighting_area(3304557, showProgress = FALSE)\n\nareas |&gt; \n  # Converte o CRS da geometria\n  st_transform(crs = 4326) |&gt; \n  # \"Limpa\" as geometrias\n  st_make_valid() |&gt; \n  # Junta com os dados do Censo\n  left_join(df, by = \"code_weighting\") |&gt; \n  # Visualiza os dados num mapa interativo\n  mapview(zcol = \"total_pop\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#introdução",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#introdução",
    "title": "Pipes",
    "section": "",
    "text": "A partir da versão 4.1.0, o R passou a oferecer o operador |&gt; chamado de pipe (literalmente, cano)1. Este operador foi fortemente inspirado no operador homônimo %&gt;% do popular pacote magrittr. Neste post, explico como utilizar o pipe nativo e como ele difere do pipe do magrittr.\nO operador pipe (em ambos os casos), essencialmente, ordena uma função composta.\nLembrando um pouco sobre funções compostas: a expressão abaixo mostra a aplicação de três funções onde primeiro aplica-se a função f sobre x, depois a função g e, por fim, a função h. Lê-se a função de dentro para fora.\n\\[\nh(g(f(x))) = \\dots\n\\]\nPara tornar o exemplo mais concreto considere o exemplo abaixo onde calcula-se a média geométrica de uma sequência de números aleatórios.\nA média geométrica é dada pela expressão:\n\\[\n\\overline{x} = (\\prod_{i = 1}^{n}x_{i})^{\\frac{1}{n}} = \\text{exp}(\\frac{1}{n}\\sum_{i = 1}^{n}\\text{log}(x_{i}))\n\\]\n\nx &lt;- rnorm(n = 100, mean = 10)\n#&gt; Calcula a média geométrica\nexp(mean(log(x)))\n\n[1] 10.221\n\n\nUsando a mesma notação acima, aplica-se primeiro a função log (f), depois a função mean (g) e, por fim, a função exp (h). Usando o operador pipe, pode-se reescrever a expressão da seguinte forma.\n\nx |&gt; log() |&gt; mean() |&gt; exp()\n\n[1] 10.221\n\n\nNote que o resultado da função vai sendo “carregado” da esquerda para a direita sucessivamente. Para muitos usuários, a segunda sintaxe é mais intuitiva e/ou fácil de ler. No segundo código a ordem em que o nome das funções aparecem coincide com a ordem da sua aplicação.\nPor fim, note que o uso de várias funções numa mesma linha de código também nos poupa de ter de criar objetos intermediários como no exemplo abaixo.\n\nlog_x &lt;- log(x)\nlog_media &lt;- mean(log_x)\nmedia_geometrica_x &lt;- exp(log_media)\n\nmedia_geometrica_x\n\n[1] 10.221\n\n\nOs exemplos acima funcionaram sem problemas porque usou-se o operador pipe para “abrir” uma função composta. O argumento de cada função subsequente é o resultado da função antecedente: funciona como uma linha de montagem, em que cada nova etapa soma-se ao resultado da etapa anterior.\nQuando o resultado da função anterior não vai diretamente no primeiro argumento da função subsequente, precisa-se usar o operador _ (underline/underscore)2. Este operador serve como um placeholder: indica onde que o resultado da etapa anterior deve entrar. No exemplo abaixo, uso o placeholder para colocar a base de dados filtrada no argumento data dentro da função lm.\n\ncarros_4 &lt;- subset(mtcars, cyl == 4)\nfit &lt;- lm(mpg ~ wt, data = carros_4)\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _)\n\nPor fim, temos o caso das funções anônimas3. Uma função anônima é simplesmente uma função sem nome que é chamada uma única vez. Infelizmente, a sintaxe de um pipe com uma função anônima é bastante carregada.\n\nobjeto |&gt; (\\(x, y, z, ...) {define função})()\n\n# Nova sintaxe de funções anônimas (similar a lambda no Python)\nobjeto |&gt; (\\(x, y) {x^2 + y^2})()\n# Antiga sintaxe de funções anônimas\nobjeto |&gt; (function(x, y) {x^2 + y^2})()\n\nO exemplo repete o código acima, mas agora usa uma função anônima para pegar o R2 ajustado da regressão.\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _) |&gt; \n  (\\(x) {summary(x)$adj.r.squared})()"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#limitações",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#limitações",
    "title": "Pipes",
    "section": "",
    "text": "Imagine agora que se quer calcular o erro absoluto médio de uma regressão. Lembre-se que o EAM é dado por\n\\[\n\\text{EAM} = \\frac{1}{N}\\sum_{i = 1}^{N}|e_{i}|\n\\]\nonde \\(e_{i}\\) é o resíduo da regressão. O código abaixo mostra como fazer isto usando pipes.\n\n#&gt; Estima uma regressão qualquer\nfit &lt;- lm(mpg ~ wt, data = mtcars)\n\n#&gt; Calcula o erro absoluto médio\nfit |&gt; residuals() |&gt; abs() |&gt; mean()\n\n[1] 2.340642\n\n\nNote, contudo, que a situação fica um pouco mais complicada no caso em que se quer calcular a raiz do erro quadrado médio.\n\\[\n\\text{REQM} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^{N}(e_{i})^2}\n\\]\nNa sintaxe convencional temos\n\nsqrt(mean(residuals(fit)^2))\n\n[1] 2.949163\n\n\nO problema é que a exponenciação acontece via um operador e não uma função. Nenhum dos exemplos abaixo funciona.\n\nfit |&gt; residuals() |&gt; ^2 |&gt; mean() |&gt; sqrt()\n\nError: &lt;text&gt;:1:23: unexpected '^'\n1: fit |&gt; residuals() |&gt; ^\n                          ^\n\n\n\nfit |&gt; residuals()^2 |&gt; mean() |&gt; sqrt()\n\nError: function '^' not supported in RHS call of a pipe\n\n\nPara chegar no mesmo resultado, novamente precisa-se usar uma sintaxe bastante esotérica que envolve passar o resultado de residuals para uma função anônima.\n\nfit |&gt; residuals() |&gt; (\\(y) {sqrt(mean(y^2))})()\n\n[1] 2.949163"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#resumo",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#resumo",
    "title": "Pipes",
    "section": "",
    "text": "Assim, apesar de muito útil, o operador pipe tem suas limitações. O operador sempre espera encontrar uma função à sua direita; a única maneira de seguir |&gt; com um operador é criando uma função anônima, cuja sintaxe é um pouco carregada. Pode-se resumir os principais fatos sobre o operador pipe:\n\nSimplifica funções compostas. Na expressão x |&gt; f |&gt; g o operador |&gt; aplica a função f sobre o objeto x usando x como argumento de f. Depois, aplica a função g sobre o resultado de f(x). Isto é equivalente a g(f(x)).\nEvita a definição de objetos intermediários. O uso de pipes evita que você precise “salvar” cada passo intermediário da aplicação de funções. Isto deixa seu espaço de trabalho mais limpo e também consome menos memória.\nPlaceholder. Quando o objeto anterior não serve como o primeiro argumento da função subsequente, usa-se o placeholder para indicar onde ele deve ser inserido. x |&gt; f(y = 2, data = _).\nFunção anônima. Em casos mais complexos, é necessário montar uma função anônima usando x |&gt; (\\(y) {funcao})()."
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#aplicações-comuns",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#aplicações-comuns",
    "title": "Pipes",
    "section": "",
    "text": "O uso mais comum de pipes é junto com funções do tidyverse, que foram desenvolvidas com este intuito.\n\nlibrary(tidyverse)\n\nAs funções do tidyverse (quase) sempre recebem um data.frame como primeiro argumento; isto facilita a construção de código usando pipe, pois basta encadear as funções em sequência.\n\nfiltered_df &lt;- filter(mtcars, wt == 2)\ngrouped_df &lt;- group_by(filtered_df, cyl)\ntbl &lt;- summarise(grouped_df, avg = mean(mpg), count = n())\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg))\n\nA leitura do código fica mais “gramatical”: pegue o objeto mtcars filtre as linhas onde wt &gt; 2 depois agrupe pela variável cyl e, por fim, tire uma média de mpg.\nPode-se terminar um pipe com uma chamada para um plot em ggplot2 para uma rápida visualização dos resultados\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg)) |&gt; \n  ggplot(aes(x = as.factor(cyl), y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNão se recomenda fazer longas sequências de pipes, pois o código pode acabar muito confuso para quem está lendo. O exemplo abaixo mostra justamente isto.\n\nlibrary(realestatebr)\n\nabecip &lt;- get_abecip_indicators(cached = TRUE)\n\nabecip$units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) |&gt; \n  filter(category == \"units\", type != \"total\") |&gt; \n  ggplot(aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nÉ recomendável quebrar o código acima em passos distintos. Além de ficar mais organizado, pode-se salvar objetos úteis como a tabela agrupada por trimestre, antes de se aplicar o filtro de unidades. A tabela final também fica salva num objeto, permitindo que se faça outros gráficos e análises com estes dados.\n\nunits &lt;- abecip$units\n\n#&gt; Converte em long e agrega os dados por trimestre\ntab_quarter &lt;- units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) \n\n#&gt; Filtra apenas dados de unidades e retira o 'total'\ntab_units &lt;- tab_quarter |&gt; \n  filter(category == \"units\", type != \"total\")\n\n#&gt; Faz o gráfico\nggplot(tab_units, aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\nO pacote sf também funciona bem com pipes pois há vários casos em que se quer aplicar múltiplas funções num mesmo objeto.\n\n# Transforma um data.frame num objeto espacial (pontos)\n# depois faz a interseção dos pontos num polígono e\n# por fim limpa as geometrias\n\ndat |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  st_join(poly) |&gt; \n  filter(!is.na(gid)) |&gt; \n  st_make_valid()\n\nO exemplo abaixo é emprestado do pacote censobr e mostra como combinar a manipulação de dados do dplyr com objetos espaciais manipulados via sf.\n\nlibrary(censobr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(mapview)\n\n# Importa alguns dados do Censo IBGE 2010\npop &lt;- read_population(\n  year = 2010,\n  columns = c(\"code_weighting\", \"abbrev_state\", \"V0010\")\n  )\n# Calcula a população total das áreas de ponderação no Rio de Janeiro\ndf &lt;- pop |&gt;\n      filter(abbrev_state == \"RJ\") |&gt;\n      group_by(code_weighting) |&gt;\n      summarise(total_pop = sum(V0010)) |&gt;\n      collect()\n\n# Import o shape das áreas de ponderação do Censo\nareas &lt;- read_weighting_area(3304557, showProgress = FALSE)\n\nareas |&gt; \n  # Converte o CRS da geometria\n  st_transform(crs = 4326) |&gt; \n  # \"Limpa\" as geometrias\n  st_make_valid() |&gt; \n  # Junta com os dados do Censo\n  left_join(df, by = \"code_weighting\") |&gt; \n  # Visualiza os dados num mapa interativo\n  mapview(zcol = \"total_pop\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#referências",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#referências",
    "title": "Pipes",
    "section": "Referências",
    "text": "Referências\n\nGuia de estilo do tidyverse\nDifferences between the base R and magrittr pipes\nChanges in R 4.0-4.1\nChanges in R 4.2.0\nChanges in R 4.3.0"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#footnotes",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#footnotes",
    "title": "Pipes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara a lista completa de mudanças veja News and Notes.↩︎\nTecnicamente, o placeholder foi apenas introduzido na versão 4.2.0 como uma melhoria em relação ao pipe nativo implementado anteriormente. “In a forward pipe |&gt; expression it is now possible to use a named argument with the placeholder _ in the rhs call to specify where the lhs is to be inserted. The placeholder can only appear once on the rhs.”. Link original.↩︎\nA notação abaixo de função anônima, usando \\(x), também foi introduzida na versão 4.1.0 do R. Antigamente, para se definir uma função era necessário usar function(x).↩︎\nVale notar que este comportamento foi introduzido também no placeholder do pipe nativo a partir da versão 4.3.0 do R. Contudo, este comportamento ainda está em fase experimental. “As an experimental feature the placeholder _ can now also be used in the rhs of a forward pipe |&gt; expression as the first argument in an extraction call, such as _$coef.”. Link Original.↩︎\nUma quantidade enorme de pacotes utiliza o magrittr como dependência. Veja a página do CRAN.↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html",
    "href": "posts/general-posts/repost-ols-timeseries/index.html",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "",
    "text": "[Este post foi originalmente escrito no início de 2019. Muitos dos pacotes apresentados aqui evoluíram bastante, mas acredito que o post original ainda tenha bastante valor didático para quem está iniciando seus estudos em econometria e R.]\nOs cursos de econometria de séries de tempo, usualmente, começam pelo ensino de modelos lineares univariados para séries estacionárias. Estes modelos são da família ARMA e tentam representar uma série de tempo \\(y_{t}\\) em função de suas defasagens \\(y_{t-1}, y_{t-2}, \\dots, y_{t-n}\\) e de choques aleatórios (inovações) \\(\\epsilon_{t}, \\epsilon_{t-1}, \\epsilon_{t-2}, \\dots, y_{t-n}\\). Contudo, pode ser mais interessante relacionar duas séries de tempo \\(y_{t}\\) e \\(x_{t}\\) diferentes via um modelo linear. Em alguns casos isto pode ser equivalente a um VAR ou VARMA, mas o modelo linear tem a vantagem de ser mais simples de implementar e de interpretar. O tipo de modelo linear que estamos interessados é da forma\n\\[\n  y_{t} = \\beta_{0} + \\beta_{1}x_{t} + w_{t}\n\\]\nonde \\(y_{t}\\) é a série que queremos “explicar” em função da série \\(x_{t}\\). É evidente que podemos estender este modelo para incluir defasagens das variáveis \\(x_{t}\\) e \\(y_{t}\\), além de incluir outras séries, dummies, efeitos sazonais e tendências temporais.\nQuando se usa dados em forma de séries de tempo numa regressão linear, é bastante comum que se enfrente algum nível de autocorrelação nos resíduos. Uma das hipóteses do modelo “clássico” de regressão linear é de que as observações são i.i.d., isto é, que os dados são independentes e identicamente distribuídos. Isto obviamente não se aplica no contexto de séries de tempo (os dados não são independentes), então é preciso algum cuidado no uso de modelos de regressão linear. Neste sentido, o diagnósito dos resíduos é o mais importante passo para verificar a qualidade do modelo. Idealmente, os resíduos do modelo devem se comportar como ruído branco (i.e., não devem apresentar autocorrelação).\nOutro problema típico deste tipo de análise, chamado de “regressão espúria”, acontece quando se faz a regressão de séries não-estacionárias. Quaisquer duas séries com tendência serão fortemente linearmente relacionadas. Isto leva a uma regressão com \\(R^2\\) altíssimo e estatísticas-t muito significativas e a vários modelos sem sentido. Exemplos disto podem ser vistos neste site (em inglês).\nAinda assim, há casos em que podemos utilizar estas regressões para encontrar relações úteis. Em particular, se as séries forem cointegradas podemos usar uma metodologia Engle-Granger. Esta abordagem não será discutida neste post."
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-simples",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-simples",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo simples",
    "text": "Exemplo simples\nUm modelo para explicar \\(y\\) em função de seu valor defasado em um período, do valor contemporâneo de \\(x\\) e do valor defasado de \\(x\\) em um período.\n\\[\n  y_{t} = \\beta_{0} + \\beta_{1}y_{t - 1} + \\beta_{2}x_{t} + \\beta_{3}x_{t - 1} + w_{t}\n\\]\nNote que o modelo acima não seria muito útil para gerar previsões de \\(y_{t + 1}\\) pois ele exigiria conhecimento de \\(x_{t + 1}\\). Então, seria necessário primeiro prever o valor de \\(x_{t + 1}\\) para computar uma estimativa para \\(y_{t + 1}\\).\n\\[\n\\mathbb{E}(y_{t + 1} | \\mathbb{I}_{t}) = \\beta_{0} + \\beta_{1}y_{t} + \\beta_{2}x_{t + 1} + \\beta_{3}x_{t}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-índice-de-produção-industrial",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-índice-de-produção-industrial",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: Índice de Produção Industrial",
    "text": "Exemplo: Índice de Produção Industrial\nPara o exemplo abaixo uso o pacote GetBCBData para carregar a série do Índice de Produção Industrial (IPI).\n\n\nCode\n# Baixa os dados\nipi &lt;- gbcbd_get_series(21859, first.date = as.Date(\"2002-01-01\"))\n# Converte a série para ts\nprod &lt;- ts(ipi$value, start = c(2002, 01), frequency = 12)\n# Gráfico da série\nautoplot(prod) +\n  labs(title = \"Índice de Produção Industiral - geral (2012 = 100)\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nPode-se visualizar a relação linear entre valores correntes e defasados do IPI usando a função lag.plot. Na imagem abaixo, cada quadrado mostra um gráfico de dispersão dos valores do IPI em \\(t\\) contra os valores do IPI em \\(t-k\\). Alguns lags parecem não exibir muita relação como o lag 6. Já o primeiro e último lag parecem apresentar uma relação linear mais acentuada.\n\n\nCode\ngglagplot(prod, 12, do.lines = FALSE, colour = FALSE) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nComo exemplo, podemos propor o modelo abaixo para o IPI. A escolha dos lags aqui foi um tanto arbitrária e há métodos mais apropriados para escolhê-los.\n\\[\n  IPI_{t} = \\beta_{0} + \\beta_{1}IPI_{t - 1} + \\beta_{2}IPI_{t - 2} + \\beta_{3}IPI_{t - 4} + \\beta_{2}IPI_{t - 12}\n\\]\nPara estimar este modelo no R há um pequeno inconveniente da função lag que, na verdade, funciona como um operador foreward. Além disso, agora a função base lm (e por conseguinte, também a função tslm) se prova um tanto inconveniente, pois ela não funciona bem com variáveis defasadas. Para usar a função lm seria necessário primeiro “emparelhar” as diferentes defasagens da série, isto é, seria necessário criar um data.frame (ou ts) em que cada coluna mostra os valores das defasagens escolhidas. Por motivo de completude, deixo um código de exemplo que faz isto. Na prática, vale mais a pena escolher alguma outra função como dynlm::dynlm ou dyn::dyn$lm. O código abaixo usa o forecast::tslm, mas nos exemplos seguintes uso o dyn::dyn$lm.\n\n\nCode\n# Exemplo usando forecast::tslm (tb funcionaria com stats::lm)\ndf &lt;- ts.intersect(\n  prod, lag(prod, -1), lag(prod, -2), lag(prod, -4), lag(prod, -12),\n  dframe = TRUE\n  )\n\nfit &lt;- tslm(prod ~ ., data = df)\n\n\nPode-se contornar o problema da função lag definindo uma nova função, L, que funciona da maneira desejada. O código abaixo estima a regressão usando dyn::dyn$lm. A sintaxe dentro da função é praticamente idêntica à que vimos acima com as funções lm e tslm.\n\n\nCode\n# Define uma função L\nL &lt;- function(x, k) {lag(x, -k)}\nfit &lt;- dyn$lm(prod ~ L(prod, 1) + L(prod, 2) + L(prod, 4) + L(prod, 12))\n\n\nOs resultados da regressão acima estão resumidos na tabela abaixo.\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nipi\n\n\n\n\n\n\n\n\nipi [1]\n\n\n0.442***\n\n\n\n\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\nipi [4]\n\n\n0.139**\n\n\n\n\n\n\n(0.062)\n\n\n\n\n\n\n\n\n\n\nipi [8]\n\n\n-0.114***\n\n\n\n\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\nipi [12]\n\n\n0.446***\n\n\n\n\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\nconstante\n\n\n9.478**\n\n\n\n\n\n\n(3.960)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n245\n\n\n\n\nR2\n\n\n0.783\n\n\n\n\nAdjusted R2\n\n\n0.779\n\n\n\n\nResidual Std. Error\n\n\n5.304 (df = 240)\n\n\n\n\nF Statistic\n\n\n216.286*** (df = 4; 240)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nPodemos combinar a informação de outros indicadores industriais para adicionar informação potencialmente relevante para nossa regressão. Neste exemplo, uso outros indicadores industriais para encontrar aqueles que “ajudam a explicar” o indicador geral.\nO código abaixo importa uma série de indicadores industriais e junta todos eles num único objeto ts.\nOs códigos utilizados são diretamente copiados do sistema de séries temporais do BCB.\n\n\nCode\n# Codigos das series do BACEN\ncodigos_series = c(21859, 21861:21868)\n# Vetor com nomes para facilitar o uso dos dados\nnomes = c(\n  \"geral\", \"extrativa_mineral\", \"transformacao\", \"capital\", \"intermediarios\",\n  \"consumo\", \"consumo_duraveis\", \"semiduraveis_e_nao_duraveis\",\n  \"insumos_da_construcao_civil\"\n  )\n# Junta estes dados num data.frame que serve de dicionário (metadata)\ndicionario &lt;- data.frame(id.num = codigos_series, nome_serie = nomes)\n\n# Baixa todas as series\nseries &lt;- gbcbd_get_series(codigos_series, first.date = as.Date(\"2002-01-01\"))\n# Junta as séries com o dicionário\nseries &lt;- merge(series, dicionario, by = \"id.num\")\n# Converte para wide usando os nomes do dicionario como nome das colunas\nseries_wide &lt;- reshape2::dcast(series, ref.date ~ nome_serie, value.var = \"value\")\n# Convert para ts\nseries &lt;- ts(as.matrix(series_wide[, -1]), start = c(2002, 1), frequency = 12)\n\n# Visualizar todas as series\nautoplot(series) +\n  facet_wrap(vars(series)) +\n  scale_color_viridis_d() +\n  theme_light() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFeito o trabalho de importação dos dados podemos propor um modelo simples que toma o valor defasado das variáveis. Novamente a escolha das defasagens e dos regressores foi completamente arbitrária. O modelo estimado usa defasagens de outras séries para modelar o comportamento da série do índice de produção da indústria de tranformação\n\\[\n  Transf_{t} = \\beta_{0} + \\beta_{1}Durav_{t - 1} + \\beta_{2}Durav_{t - 12} + \\beta_{3}Capital_{t - 1} + \\beta_{4}Capital_{t - 6} + \\beta_{5}Transf_{t - 1} + \\alpha_{1}t + \\sum_{k = 2}^{12}\\alpha_{k}d_{k}\n\\]\n\n\nCode\nfit &lt;- dyn$lm(\n  transformacao ~ L(consumo_duraveis, 1) + L(consumo_duraveis, 12) +\n                  L(capital, 1) + L(capital, 6) +\n                  L(intermediarios, 12) +\n                  L(transformacao, 1) +\n                  time(transformacao) + as.factor(cycle(transformacao)),\n  data = series\n  )\n\nautoplot(series[, 1]) +\n  autolayer(fitted(fit)) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nOlhando apenas para as observações mais recentes vemos que, com exceção do período da pandemia, o ajuste aos dados é relativamente satisfatório.\n\n\nCode\n# Filtra apenas as observações mais recentes, após jan/2015\nprod_recente &lt;- window(prod, start = c(2015, 1))\n# Reestima o modelo\nsummary(fit &lt;- tslm(prod_recente ~ trend + season))\n\n\n\nCall:\ntslm(formula = prod_recente ~ trend + season)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.8428  -1.8730   0.5559   2.3254   8.8825 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 95.60468    1.73833  54.998  &lt; 2e-16 ***\ntrend       -0.04273    0.01593  -2.682 0.008745 ** \nseason2     -2.56839    2.19658  -1.169 0.245455    \nseason3      5.54101    2.19676   2.522 0.013456 *  \nseason4      0.17262    2.19705   0.079 0.937553    \nseason5      7.93757    2.19745   3.612 0.000505 ***\nseason6      8.27116    2.26418   3.653 0.000440 ***\nseason7     14.48889    2.26413   6.399 7.35e-09 ***\nseason8     17.80661    2.26418   7.864 8.76e-12 ***\nseason9     14.04934    2.26435   6.205 1.75e-08 ***\nseason10    16.35457    2.26463   7.222 1.75e-10 ***\nseason11     9.44729    2.26502   4.171 7.09e-05 ***\nseason12    -1.25998    2.26553  -0.556 0.579517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 88 degrees of freedom\nMultiple R-squared:  0.722, Adjusted R-squared:  0.6841 \nF-statistic: 19.05 on 12 and 88 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nautoplot(prod_recente) +\n  autolayer(fitted(fit)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-tendência-e-sazonalidade",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-tendência-e-sazonalidade",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: tendência e sazonalidade",
    "text": "Exemplo: tendência e sazonalidade\nÉ relativamente simples prever os valores futuros de modelos de tendência e sazonalidade determinísticas. Como o adjetivo “determinístico” sugere sabe-se de antemão todos os valores que esta série vai exibir. O exemplo abaixo estima um modelo simples para a demanda por passagens aéreas (voos internacionais).\nVale notar que não se costuma fazer previsões de longo prazo com este tipo de modelo, pois a hipótese de que a sazonalidade/tendência continua exatamente igual ao longo do tempo vai se tornando cada vez mais frágil. A curto prazo, contudo, pode ser razoável supor que este modelo linear simples ofereça uma boa aproximação da realidade.\n\n\nCode\nfit &lt;- tslm(AirPassengers ~ trend + season)\n\nautoplot(forecast(fit, h = 24), include = 24) +\n  theme_light()"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-previsão-de-cenário",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-previsão-de-cenário",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: previsão de cenário",
    "text": "Exemplo: previsão de cenário\nNo caso da regressão acima do IPCA, pode-se estimar o impacto de uma nova greve dos caminhoneiros. [Mal sabiamos que em 2020 teriamos um evento extraordinário…].\n\n\nCode\ndummies = cbind(greve_caminhao, greve_2013, precos_adm)\nfit &lt;- Arima(ipca, order = c(1, 0, 0), xreg = coredata(dummies))\ngreve_caminhao_2020 = c(rep(0, 9), 1, 0, 0)\nnovo_xreg = cbind(greve_caminhao = greve_caminhao_2020, greve_2013 = rep(0, 12), precos_adm = rep(0, 12))\n\nautoplot(forecast(fit, xreg = novo_xreg), include = 20) +\n  theme_light()"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-variáveis-defasadas",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-variáveis-defasadas",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: variáveis defasadas",
    "text": "Exemplo: variáveis defasadas\nPode ser um tanto difícil fazer previsões com modelos que usam informação de outras séries. Num modelo simples como \\(y_{t} = \\beta_{0} + \\beta_{1}x_{t - 1}\\) para prever valores futuros de \\(y_{t}\\) é preciso fazer previsõs para a série \\(x_{t}\\), pois, \\(y_{t + 2}\\) é função linear de \\(x_{t + 1}\\). Há muitas maneiras de abordar este problema e eu provavelmente vou discutir mais sobre as alternativas num post futuro. O exemplo abaixo mostra como usar informação disponível de outras séries\n\n\nCode\ndf_ajustada &lt;-\n  ts.intersect(transf = series[, \"transformacao\"],\n               lag(series[, \"consumo_duraveis\"], -1),\n               lag(series[, \"consumo_duraveis\"], -12),\n               lag(series[, \"capital\"], -1),\n               lag(series[, \"capital\"], -6),\n               lag(series[, \"intermediarios\"], -12),\n               lag(series[, \"transformacao\"], -1),\n               dframe = TRUE\n               )\nfit &lt;- tslm(transf ~ ., data = df_ajustada)\nsub &lt;- df_ajustada[(length(df_ajustada[, \"transf\"]) - 12):length(df_ajustada[, \"transf\"]), ]\n\n\n\n\nCode\nautoplot(forecast(fit, sub), include = 36) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nPode-se, como de costume, separar os dados em train e test para avaliar a qualidade das previsões dentro da amostra."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-energy/index.html",
    "href": "posts/general-posts/2024-02-wz-energy/index.html",
    "title": "Energia Elétrica e Crescimento Econômico no Brasil",
    "section": "",
    "text": "Energia e PIB\nTipicamente, entende-se que a demanda ou produção de energia elétrica é uma variável proxy razoável para o PIB. O aumento da renda tende a ser acompanhado de maior demanda por energia elétrica pelo lado dos consumidores. Por outro lado, a indústria, comércio e serviços em geral utilizam a energia elétrica como importante insumo; assim, um aumento da produção será acompanhado por maior demanda de energia elétrica.\nQuando se olha para o comportamento destas séries de tempo no Brasil surge um curioso gráfico. De 2003 a 2014, o crescimento das séries do PIB, do IBC-Br e de demanda total de energia elétrica no país são muito similares. A partir deste ponto, parece haver uma divergência na taxa de crescimento das séries. O consumo por energia elétrica rapidamente se recupera e volta a exibir uma tendência de crescimento. Já o PIB e especialmente o IBC-Br passam um maior tempo estagnados e demoram a voltar a crescer.\nNo gráfico abaixo, tanto a série do PIB como a do IBC-Br estão dessazonalizadas. Mostra-se a tendência do consumo de energia elétrica, usando uma decomposição STL.\n\n\n\n\n\n\n\n\n\nO segundo gráfico divide o consumo em três grandes grupos: comercial, industrial e residencial. Neste gráfico, vê-se como a demanda por energia elétrica da indústria começa a divergir das demais após a Crise de 2008 e depois cai novamente na Crise de 2014. De maneira geral, o consumo de energia elétrica residencial parece ser o menos afetado por crises econômicas."
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "A inflação voltou a ser uma pauta, não só no Brasil, mas no mundo todo, nos últimos meses. No Brasil, a combinação de câmbio desvalorizado, desajustes logísticos, crise hídrica e choques de preços externos, culminaram no maior nível de inflação desde 2002.\nMesmo em países avançados, os níveis de inflação estão em altas históricas. Nos Estados Unidos, por exemplo, o nível do CPI está no valor mais alto desde o final dos anos 1970.\nVisualizar a magnitude da inflação no Brasil pode ser um pouco desafiador. A série do IPCA é calculada desde 1979. O número de cidades avaliadas pelo índice cresceu no tempo: nos primeiros anos o índice contemplava Rio de Janeiro, Porto Alegre, Belo Horizonte, Recife, São Paulo, Brasília, Belém, Fortaleza, Salvador e Curitiba. Em 1991, Goiânia entra no índice e, mais recentemente, em 2014, Vitória e Campo Grande também entraram no cômputo do índice.\nMais importante do que a variação no número das cidades, é o período hiperinflacionário da década de 1980. Os números da inflação são incomparavelmente mais altos do que os atuais. Como regra, os cortes temporais mais relevantes para enxergar a inflação são Jul/94 (Plano Real), Jul/99 (Regime de Metas de Inflação), Mai/00 (Lei de Responsabilidade Fiscal) e Mai/03 (pós choque de 2002).\nNeste post vou mostrar o comportamento da inflação desde 1999.\n\n\n\n# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(ragg)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)\n\n\n\n\nImporto os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(\n  id = 433,\n  first.date = as.Date(\"1998-01-01\")\n)\n\nipca &lt;- subset(ipca, ref.date &lt;= as.Date(\"2022-12-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(\n    as.Date(\"1998-01-01\"),\n    as.Date(\"2022-12-01\"),\n    \"1 month\"\n  )\n)\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda\n)\n\n\nipca &lt;- ipca %&gt;%\n  inner_join(grid) %&gt;%\n  fill() %&gt;%\n  mutate(\n    month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n    year = year(ref.date),\n    value = value / 100,\n    acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n    acum12m = acum12m * 100\n  ) %&gt;%\n  left_join(ipca_meta) %&gt;%\n  mutate(deviation = acum12m - meta)\n\n\n\n\nO cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\", x = NULL, y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nUma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca, linetype = 2) +\n  geom_text(\n    data = dftext,\n    aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n    family = \"Helvetica\",\n    size = 3\n  ) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(\n    title = \"Inflação Acumulada em 12 Meses\",\n    x = NULL,\n    y = \"%\",\n    caption = \"Fonte: BCB\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;%\n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;%\n  pivot_longer(cols = -ref.date) %&gt;%\n  na.omit()\n\nggplot() +\n  geom_line(\n    data = filter(historico_meta, name == \"acum12m\"),\n    aes(x = ref.date, y = value),\n    color = \"#e63946\"\n  ) +\n  geom_line(\n    data = filter(historico_meta, name != \"acum12m\"),\n    aes(x = ref.date, y = value, color = name)\n  ) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n    labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")\n  ) +\n  labs(\n    title = \"Histórico de Metas de Inflação\",\n    y = \"% (acum. 12 meses)\",\n    x = NULL,\n    caption = \"Fonte: BCB\"\n  ) +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso.\n\n\n\n\nOutra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com uma linha de densidade sobreposta.\nNote como o os variações mensais do IPCA estão concentradas entre 0,25 e 0,5 e como os “outliers” à direita são mais comuns do que os “outliers” à esquerda. Isto é, eventos de alta inflação são mais frequentes do que de desinflação.\nConhecer a distribução dos dados costuma ser uma informação relevante para a previsão da variável.\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(\n    aes(y = ..density..),\n    bins = 38,\n    fill = \"#264653\",\n    color = \"white\"\n  ) +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(\n  filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n  aes(x = month, y = year, fill = acum12m)\n) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;%\n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;%\n  mutate(\n    ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n    ipca_group = factor(ipca_group)\n  )\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos, aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0,\n    point_colour = NA\n  ) +\n\n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(\n    plot.background = element_rect(fill = \"gray80\", color = NA),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(\n  xlim = range(ipca$ref.date),\n  unit_on_axis = \"months\",\n  period = \"year\",\n  period_per_loop = 1,\n  padding = unit(2, \"cm\")\n)\n#vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(\n  1:12,\n  label = TRUE,\n  abbr = FALSE,\n  locale = \"pt_BR\"\n))\nfor (i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i] / 180 * pi, (s$max_radius + 1) * 1.05)\n  grid.text(\n    month_name[i],\n    x = foo[1, 1],\n    y = foo[1, 2],\n    default.unit = \"native\",\n    rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90),\n    gp = gpar(fontsize = 10)\n  )\n}"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#pacotes",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#pacotes",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(ragg)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#importando-os-dados",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#importando-os-dados",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Importo os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(\n  id = 433,\n  first.date = as.Date(\"1998-01-01\")\n)\n\nipca &lt;- subset(ipca, ref.date &lt;= as.Date(\"2022-12-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(\n    as.Date(\"1998-01-01\"),\n    as.Date(\"2022-12-01\"),\n    \"1 month\"\n  )\n)\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda\n)\n\n\nipca &lt;- ipca %&gt;%\n  inner_join(grid) %&gt;%\n  fill() %&gt;%\n  mutate(\n    month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n    year = year(ref.date),\n    value = value / 100,\n    acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n    acum12m = acum12m * 100\n  ) %&gt;%\n  left_join(ipca_meta) %&gt;%\n  mutate(deviation = acum12m - meta)"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#inflação",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#inflação",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "O cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\", x = NULL, y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#no-longo-prazo",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#no-longo-prazo",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Uma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca, linetype = 2) +\n  geom_text(\n    data = dftext,\n    aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n    family = \"Helvetica\",\n    size = 3\n  ) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(\n    title = \"Inflação Acumulada em 12 Meses\",\n    x = NULL,\n    y = \"%\",\n    caption = \"Fonte: BCB\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;%\n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;%\n  pivot_longer(cols = -ref.date) %&gt;%\n  na.omit()\n\nggplot() +\n  geom_line(\n    data = filter(historico_meta, name == \"acum12m\"),\n    aes(x = ref.date, y = value),\n    color = \"#e63946\"\n  ) +\n  geom_line(\n    data = filter(historico_meta, name != \"acum12m\"),\n    aes(x = ref.date, y = value, color = name)\n  ) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n    labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")\n  ) +\n  labs(\n    title = \"Histórico de Metas de Inflação\",\n    y = \"% (acum. 12 meses)\",\n    x = NULL,\n    caption = \"Fonte: BCB\"\n  ) +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso."
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#enxergando-a-distribuição",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#enxergando-a-distribuição",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Outra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com uma linha de densidade sobreposta.\nNote como o os variações mensais do IPCA estão concentradas entre 0,25 e 0,5 e como os “outliers” à direita são mais comuns do que os “outliers” à esquerda. Isto é, eventos de alta inflação são mais frequentes do que de desinflação.\nConhecer a distribução dos dados costuma ser uma informação relevante para a previsão da variável.\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(\n    aes(y = ..density..),\n    bins = 38,\n    fill = \"#264653\",\n    color = \"white\"\n  ) +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(\n  filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n  aes(x = month, y = year, fill = acum12m)\n) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;%\n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;%\n  mutate(\n    ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n    ipca_group = factor(ipca_group)\n  )\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos, aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0,\n    point_colour = NA\n  ) +\n\n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(\n    plot.background = element_rect(fill = \"gray80\", color = NA),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(\n  xlim = range(ipca$ref.date),\n  unit_on_axis = \"months\",\n  period = \"year\",\n  period_per_loop = 1,\n  padding = unit(2, \"cm\")\n)\n#vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(\n  1:12,\n  label = TRUE,\n  abbr = FALSE,\n  locale = \"pt_BR\"\n))\nfor (i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i] / 180 * pi, (s$max_radius + 1) * 1.05)\n  grid.text(\n    month_name[i],\n    x = foo[1, 1],\n    y = foo[1, 2],\n    default.unit = \"native\",\n    rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90),\n    gp = gpar(fontsize = 10)\n  )\n}"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html",
    "href": "posts/general-posts/2024-02-media-movel/index.html",
    "title": "Médias móveis",
    "section": "",
    "text": "Uma média móvel, como o nome sugere, calcula a média de uma série temporal em janelas móveis. Tipicamente, a média móvel serve como uma estimativa da tendência da série; também é bastante comum usar a média móvel para identificar ciclos ou padrões numa série. Aplicar uma média móvel numa série de tempo \\(y_t\\) produz uma nova série \\(z_t\\):\n\\[\nz_t = \\sum_{j = -k}^{k}a_{j}y_{t+j}\n\\] No caso mais simples, k = 1, temos:\n\\[\nz_{t} = a_1y_{t-1} + a_2 y_{t} + a_3 y_{t+1}\n\\]\nSupondo que os pesos devem ser todos iguais e somar um temos a média móvel simples abaixo:\n\\[\nz_{t} = \\frac{1}{3}y_{t-1} +\\frac{1}{3} y_{t} + \\frac{1}{3} y_{t+1}\n\\]\nO exemplo acima, mostra uma média móvel simétrica de ordem 3, onde todos os pesos são iguais. Cada ponto na nova série \\(z_t\\) é uma média entre os valores vizinhos da série original. Tipicamente, valores próximos uns aos outros no tempo costumam ser similares; na prática, isto torna o filtro de médias móveis bastante suave.\nEvidentemente, não é possível calcular este filtro no início e no final da série \\(y_t\\). Isto implica que a série \\(z_t\\) tem menos observações do que a série original \\(y_t\\). Isto, de fato, é um ponto negativo quando se usa filtros de médias móveis.\nMédias móveis simétricas sempre tem um número ímpar de termos: no exemplo acima, \\(k=1\\) resultou num filtro com três termos; se tivéssemos usado \\(k=2\\) teríamos cinco termos e assim por diante. É possível fazer médias móveis não-simétricas variando a janela temporal. A equação abaixo mostra um filtro que soma as últimas duas observações e tira a média junto com a observação atual e a próxima observação.\n\\[\nz_{t} = \\frac{1}{4} y_{t-2} + \\frac{1}{4} y_{t-1} + \\frac{1}{4} y_{t} + \\frac{1}{4} y_{t+1}\n\\]\nNão existe forte “contraindicação” sobre o uso de médias móveis não-simétricas. Vale notar, contudo, que é relativamente fácil transformar uma média móvel não-simétrica em uma média móvel simétrica. Vamos tirar uma média móvel sobre a média móvel acima:\n\\[\n\\begin{align}\nx_{t} & = \\frac{1}{2}z_{t} + \\frac{1}{2}z_{t+1} \\\\\nx_{t} & = \\frac{1}{2}(\\frac{1}{4}(y_{t-2} + y_{t-1} + y_{t} + y_{t+1}) + \\frac{1}{4}(y_{t-1} + y_{t} + y_{t+1} + y_{t+2})) \\\\\nx_{t} & = \\frac{1}{8} y_{t-2} + \\frac{1}{4} y_{t-1} + \\frac{1}{4} y_{t} + \\frac{1}{4} y_{t+1} + \\frac{1}{8} y_{t+2}\n\\end{align}\n\\]\nA série final é uma média móvel simétrica com menor peso nas pontas. Esta é uma média móvel de ordem 2x4. Este tipo de filtro também é conhecido como média móvel ponderada.\nNa primeira demonstração acima, definimos que todos \\(a_j\\) teriam o mesmo valor. Isto não é necessário. Imagine que queremos um filtro que olha somente para o passado e atribui pesos decrescentes à medida que a observação se afasta no tempo. No caso de uma janela com apenas dois períodos teríamos algo da forma:\n\\[\nz_{t} = \\frac{1}{2}y_{t} + \\frac{1}{3}y_{t-1} + \\frac{1}{6}y_{t-2}\n\\]\nNote que os pesos somam 1 e vão decaindo no tempo. De forma mais geral,\n\\[\n\\text{WMA}_M = \\frac{ny_{t} + (n-1)y_{t-1} + \\dots + y_{M-n+1}}{\\frac{n(n+1)}{2}}\n\\]\nPode-se definir uma média móvel ponderada com pesos que apresentam algum tipo de decaimento. O filtro acima também é conhecido como média móvel linearmente ponderada, pois os pesos decaem linearmente. Outra opção seria usar pesos que apresentam decaimento exponencial como\n\\[\nz_{t} = \\alpha y_{t} + \\alpha(1-\\alpha) y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2}\n\\]\nTomando o valor \\(\\alpha = \\frac{1}{2}\\) temos que:\n\\[\nz_{t} = \\frac{1}{2}y_t + \\frac{1}{4}y_{t-1} + \\frac{1}{8} y_{t-2}\n\\]\nComparando os termos da expressão acima com a média móvel ponderada anterior, vemos que os termos do passado agora tem menor peso. De maneira mais geral, o filtro de médias móveis exponencial é dado por\n\\[\nz_t = \\alpha \\sum_{j = 0}^{\\infty}(1-\\alpha)^j y_{t-j}\n\\]\n\n\nPara montar um exemplo de médias móveis vamos importar uma das rubricas da balança de pagamentos do Brasil. O gráfico abaixo mostra a série anual da rubrica “passe de atletas”1.\n\nlibrary(rbcb)\nlibrary(ggplot2)\nlibrary(forecast)\n\ntheme_series = theme_bw(base_size = 10, base_family = \"sans\") +\n    theme(\n      legend.position = \"top\",\n      panel.grid.minor = element_blank(),\n      axis.text.x = element_text(angle = 90),\n      axis.title.x = element_blank()\n      )\n\natletas = rbcb::get_series(23617, as = \"ts\")\n\nautoplot(atletas) + theme_series\n\n\n\n\n\n\n\n\nA tabela abaixo mostra o cálculo de três filtros de médias móveis aplicados na série.\n\nma3 = stats::filter(atletas, filter = rep(1/3, 3), method = \"convolution\")\nma5 = stats::filter(atletas, filter = rep(1/5, 5), method = \"convolution\")\nma11 = stats::filter(atletas, filter = rep(1/11, 11), method = \"convolution\")\n\nmts = ts.intersect(atletas, ma3, ma5, ma11)\n\ntbl_ma = data.frame(\n  ano = as.numeric(time(mts)),\n  zoo::coredata(mts)\n)\n\n\n\n\n\n\n\n\n\nano\natletas\nma3\nma5\nma11\n\n\n\n\n1995\n18.0\n—\n—\n—\n\n\n1996\n46.6\n49.5\n—\n—\n\n\n1997\n84.0\n60.3\n51.9\n—\n\n\n1998\n50.4\n65.0\n73.6\n—\n\n\n1999\n60.7\n79.2\n85.0\n—\n\n\n2000\n126.4\n96.8\n80.2\n80.4\n\n\n2001\n103.4\n96.6\n86.5\n89.2\n\n\n2002\n60.0\n81.8\n97.8\n106.5\n\n\n2003\n82.1\n86.4\n99.6\n113.9\n\n\n2004\n117.0\n111.5\n101.9\n125.8\n\n\n2005\n135.4\n122.4\n137.3\n138.4\n\n\n2006\n114.9\n162.5\n154.0\n143.8\n\n\n2007\n237.1\n172.5\n166.9\n145.1\n\n\n2008\n165.5\n194.8\n179.7\n162.4\n\n\n2009\n181.7\n182.1\n193.7\n170.2\n\n\n2010\n199.1\n188.7\n170.0\n179.2\n\n\n2011\n185.2\n167.5\n186.9\n183.4\n\n\n2012\n118.3\n184.5\n184.1\n193.9\n\n\n2013\n250.0\n178.8\n187.6\n201.5\n\n\n2014\n168.0\n211.4\n186.8\n210.9\n\n\n2015\n216.3\n188.5\n209.3\n214.3\n\n\n2016\n181.3\n209.5\n223.2\n210.2\n\n\n2017\n230.8\n244.0\n243.6\n208.6\n\n\n2018\n319.8\n273.5\n244.1\n218.9\n\n\n2019\n269.8\n269.5\n238.6\n221.8\n\n\n2020\n218.9\n214.2\n226.0\n—\n\n\n2021\n153.9\n180.1\n208.4\n—\n\n\n2022\n167.6\n184.4\n210.8\n—\n\n\n2023\n231.8\n227.1\n—\n—\n\n\n2024\n281.9\n—\n—\n—\n\n\n\n\n\n\n\nO gráfico abaixo mostra o ajuste de cada um dos filtros.\n\nautoplot(mts) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\nNum caso aplicado, seria interessante ter uma maneira simples de escalar o processo acima para múltiplas séries de tempo. Para calcular médias móveis com grande velocidade, vamos usar o pacote RcppRoll, onde cpp significa C++, a linguagem subjacente do pacote. A função roll_mean() aplica uma média móvel sobre uma série.\n\nlibrary(RcppRoll)\nroll_mean(atletas, n = 3)\n\n [1]  49.53333  60.33333  65.03333  79.16667  96.83333  96.60000  81.83333\n [8]  86.36667 111.50000 122.43333 162.46667 172.50000 194.76667 182.10000\n[15] 188.66667 167.53333 184.50000 178.76667 211.43333 188.53333 209.46667\n[22] 243.96667 273.46667 269.50000 214.20000 180.13333 184.43333 227.10000\n\n\nObjetos ts foram criados especificamente para armazenar séries de tempo, têm muitas praticidades, e são pré-carregados no R. A vasta maioria dos dados, e análises de dados, segue o paradigma de bases retangulares, de data.frame s por assim dizer. Neste sentido, o ts ou mts acaba sendo pouco prático2 quando se trabalha com múltiplas séries de tempo ou até mesmo com séries de tempo com altas frequências.\nNo exemplo abaixo vamos importar 9 séries do Índice de Produção Industrial (IPI) e aplicar uma média móvel 2x12 em cada uma delas. O código abaixo mostra como importar as séries e empilhá-las num único tibble. Para manipular os dados vamos utilizar o popular pacote dplyr3.\n\nlibrary(dplyr)\n\ncode = 28503:28511\nseries = get_series(code)\nseries = lapply(series, setNames, c(\"date\", \"value\"))\nnames(series) = code\nseries = bind_rows(series, .id = \"series_id\")\n\nggplot(series, aes(date, value)) +\n  geom_line() +\n  facet_wrap(vars(series_id)) +\n  theme_series\n\n\n\n\n\n\n\n\nÉ relativamente simples aplicar o filtro sobre cada uma das séries. O gráfico mostra o resultado final.\n\nseries = series |&gt;\n  group_by(series_id) |&gt;\n  mutate(\n    trend = roll_mean(value, n = 12, fill = NA),\n    trend = roll_mean(trend, n = 2, fill = NA)\n  )\n\nggplot(series, aes(date)) +\n  geom_line(aes(y = value), alpha = 0.8, color = \"#126782\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#023047\") +\n  facet_wrap(vars(series_id)) +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\nMédias móveis fornecem uma estimativa simples da tendência de uma série. Neste sentido, é comum utilizar o filtro de médias móveis para decompor uma série. O código abaixo usa a base USMacroG para importar algumas séries macroeconômicas trimestrais dos EUA no período 1947-19974. Vamos modelar a tendência das séries usando uma janela de quatro anos (dois anos para trás e dois anos para frente), ou seja, uma média móvel de ordem 25.\n\n\nCode\nlibrary(AER)\nlibrary(tidyr)\ndata(\"USMacroG\")\n\nmacro = tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nsubmacro = macro |&gt;\n  filter(date &gt;= as.Date(\"1947-01-01\"), date &lt;= as.Date(\"1997-01-01\")) |&gt;\n  mutate(across(gdp:m1, log))\n\nmacro_trend = submacro |&gt;\n  select(date, gdp, consumption, invest, m1, inflation, unemp) |&gt;\n  pivot_longer(col = -date, names_to = \"name_series\") |&gt;\n  group_by(name_series) |&gt;\n  mutate(\n    trend = roll_mean(value, n = 25, fill = NA),\n    detrend = value - trend) |&gt;\n  ungroup()\n\nggplot(macro_trend, aes(date)) +\n  geom_line(aes(y = value), alpha = 0.5) +\n  geom_line(aes(y = trend), lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico das séries livres de tendência é apresentado abaixo. Note que as séries não foram dessazonalisadas, portanto, elas ainda apresentam oscilação sazonal.\n\nggplot(macro_trend, aes(date, detrend)) +\n  geom_line(lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\nApesar de simples, o filtro de médias móveis é bastante utilizado e tem boas propriedades estatísticas. O gráfico abaixo compara o ajuste de um filtro de médias móveis com o filtro Baxter-King5, que é bastante mais sofisticado. Como se vê, ambos os filtros chegam em resultados muito similares.\n\n\nCode\nlibrary(mFilter)\n\ntbl_unemp = select(macro, date, unemp)\nbk = mFilter::bkfilter(tbl_unemp$unemp, 6, 32, nfix = 12)\n\ntbl_unemp = tbl_unemp |&gt;\n  mutate(\n    ma = roll_mean(unemp, n = 25, fill = NA, align = \"center\"),\n    bk = mFilter::bkfilter(unemp, 6, 32, nfix = 12)$trend\n  ) |&gt;\n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(tbl_unemp, aes(date, value, color = series)) +\n  geom_line(data = filter(tbl_unemp, series == \"unemp\")) +\n  geom_line(data = filter(tbl_unemp, series != \"unemp\"), lwd = 0.9) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#FFD166\", \"#0CB0A9\", \"#073B4C\"),\n    labels = c(\"Baxter-King\", \"MA25\", \"Original\")\n  ) +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\n\nO filtro de médias móveis exponenciais ou EWMA (exponentially weighted moving average) é bastante popular em finanças. Na prática, ele é um caso específico de suvização exponencial6. Tecnicamente, seria possível implementá-lo usando stats::filter fornecendo os pesos adequados. O código abaixo replica o primeiro exemplo, da série “passe de atletas”.\n\newma_wgt = function(alpha = 0.5, order = 3) { rev((1 - alpha)^(seq(1, order))) }\n\newma3 = stats::filter(atletas, filter = c(1/8, 1/4, 1/2), sides = 1L)\newma5 = stats::filter(atletas, filter = ewma_wgt(order = 5), sides = 1L)\newma11 = stats::filter(atletas, filter = ewma_wgt(order = 11), sides = 1L)\n\n\nmts = ts.intersect(atletas, ewma3, ewma5, ewma11)\n\nautoplot(mts) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series\n\n\n\n\n\n\n\n\nO exemplo acima é um tanto quanto artificial. Tipicamente, o filtro EWMA é utilizado em séries financeiras de alta frequência. O código abaixo usa o pacote TTR (Technical Trading Rules), especializado em funções para finanças, para computar o EWMA de uma série de preço. Para tornar o exemplo mais simples vamos aproveitar a base ttrc do próprio pacote.\n\n\nCode\nlibrary(TTR)\ndata(ttrc)\n\nttrc = ttrc |&gt;\n  rename_with(tolower) |&gt;\n  dplyr::filter(date &gt;= as.Date(\"2004-01-01\")) |&gt;\n  mutate(\n    trend07 = EMA(close, n = 7),\n    trend30 = EMA(close, n = 30),\n    trend50 = EMA(close, n = 50)\n    ) |&gt;\n  select(date, close, starts_with(\"trend\")) |&gt;\n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(ttrc, aes(x = date, y = value, color = series)) +\n  geom_line(data = dplyr::filter(ttrc, series == \"close\"), alpha = 0.8) +\n  geom_line(data = dplyr::filter(ttrc, series != \"close\"), lwd = 0.5) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"EWMA7\", \"EWMA25\", \"EWMA50\")) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#exemplo-simples",
    "href": "posts/general-posts/2024-02-media-movel/index.html#exemplo-simples",
    "title": "Médias móveis",
    "section": "",
    "text": "Para montar um exemplo de médias móveis vamos importar uma das rubricas da balança de pagamentos do Brasil. O gráfico abaixo mostra a série anual da rubrica “passe de atletas”1.\n\nlibrary(rbcb)\nlibrary(ggplot2)\nlibrary(forecast)\n\ntheme_series = theme_bw(base_size = 10, base_family = \"sans\") +\n    theme(\n      legend.position = \"top\",\n      panel.grid.minor = element_blank(),\n      axis.text.x = element_text(angle = 90),\n      axis.title.x = element_blank()\n      )\n\natletas = rbcb::get_series(23617, as = \"ts\")\n\nautoplot(atletas) + theme_series\n\n\n\n\n\n\n\n\nA tabela abaixo mostra o cálculo de três filtros de médias móveis aplicados na série.\n\nma3 = stats::filter(atletas, filter = rep(1/3, 3), method = \"convolution\")\nma5 = stats::filter(atletas, filter = rep(1/5, 5), method = \"convolution\")\nma11 = stats::filter(atletas, filter = rep(1/11, 11), method = \"convolution\")\n\nmts = ts.intersect(atletas, ma3, ma5, ma11)\n\ntbl_ma = data.frame(\n  ano = as.numeric(time(mts)),\n  zoo::coredata(mts)\n)\n\n\n\n\n\n\n\n\n\nano\natletas\nma3\nma5\nma11\n\n\n\n\n1995\n18.0\n—\n—\n—\n\n\n1996\n46.6\n49.5\n—\n—\n\n\n1997\n84.0\n60.3\n51.9\n—\n\n\n1998\n50.4\n65.0\n73.6\n—\n\n\n1999\n60.7\n79.2\n85.0\n—\n\n\n2000\n126.4\n96.8\n80.2\n80.4\n\n\n2001\n103.4\n96.6\n86.5\n89.2\n\n\n2002\n60.0\n81.8\n97.8\n106.5\n\n\n2003\n82.1\n86.4\n99.6\n113.9\n\n\n2004\n117.0\n111.5\n101.9\n125.8\n\n\n2005\n135.4\n122.4\n137.3\n138.4\n\n\n2006\n114.9\n162.5\n154.0\n143.8\n\n\n2007\n237.1\n172.5\n166.9\n145.1\n\n\n2008\n165.5\n194.8\n179.7\n162.4\n\n\n2009\n181.7\n182.1\n193.7\n170.2\n\n\n2010\n199.1\n188.7\n170.0\n179.2\n\n\n2011\n185.2\n167.5\n186.9\n183.4\n\n\n2012\n118.3\n184.5\n184.1\n193.9\n\n\n2013\n250.0\n178.8\n187.6\n201.5\n\n\n2014\n168.0\n211.4\n186.8\n210.9\n\n\n2015\n216.3\n188.5\n209.3\n214.3\n\n\n2016\n181.3\n209.5\n223.2\n210.2\n\n\n2017\n230.8\n244.0\n243.6\n208.6\n\n\n2018\n319.8\n273.5\n244.1\n218.9\n\n\n2019\n269.8\n269.5\n238.6\n221.8\n\n\n2020\n218.9\n214.2\n226.0\n—\n\n\n2021\n153.9\n180.1\n208.4\n—\n\n\n2022\n167.6\n184.4\n210.8\n—\n\n\n2023\n231.8\n227.1\n—\n—\n\n\n2024\n281.9\n—\n—\n—\n\n\n\n\n\n\n\nO gráfico abaixo mostra o ajuste de cada um dos filtros.\n\nautoplot(mts) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#maior-eficiência",
    "href": "posts/general-posts/2024-02-media-movel/index.html#maior-eficiência",
    "title": "Médias móveis",
    "section": "",
    "text": "Num caso aplicado, seria interessante ter uma maneira simples de escalar o processo acima para múltiplas séries de tempo. Para calcular médias móveis com grande velocidade, vamos usar o pacote RcppRoll, onde cpp significa C++, a linguagem subjacente do pacote. A função roll_mean() aplica uma média móvel sobre uma série.\n\nlibrary(RcppRoll)\nroll_mean(atletas, n = 3)\n\n [1]  49.53333  60.33333  65.03333  79.16667  96.83333  96.60000  81.83333\n [8]  86.36667 111.50000 122.43333 162.46667 172.50000 194.76667 182.10000\n[15] 188.66667 167.53333 184.50000 178.76667 211.43333 188.53333 209.46667\n[22] 243.96667 273.46667 269.50000 214.20000 180.13333 184.43333 227.10000\n\n\nObjetos ts foram criados especificamente para armazenar séries de tempo, têm muitas praticidades, e são pré-carregados no R. A vasta maioria dos dados, e análises de dados, segue o paradigma de bases retangulares, de data.frame s por assim dizer. Neste sentido, o ts ou mts acaba sendo pouco prático2 quando se trabalha com múltiplas séries de tempo ou até mesmo com séries de tempo com altas frequências.\nNo exemplo abaixo vamos importar 9 séries do Índice de Produção Industrial (IPI) e aplicar uma média móvel 2x12 em cada uma delas. O código abaixo mostra como importar as séries e empilhá-las num único tibble. Para manipular os dados vamos utilizar o popular pacote dplyr3.\n\nlibrary(dplyr)\n\ncode = 28503:28511\nseries = get_series(code)\nseries = lapply(series, setNames, c(\"date\", \"value\"))\nnames(series) = code\nseries = bind_rows(series, .id = \"series_id\")\n\nggplot(series, aes(date, value)) +\n  geom_line() +\n  facet_wrap(vars(series_id)) +\n  theme_series\n\n\n\n\n\n\n\n\nÉ relativamente simples aplicar o filtro sobre cada uma das séries. O gráfico mostra o resultado final.\n\nseries = series |&gt;\n  group_by(series_id) |&gt;\n  mutate(\n    trend = roll_mean(value, n = 12, fill = NA),\n    trend = roll_mean(trend, n = 2, fill = NA)\n  )\n\nggplot(series, aes(date)) +\n  geom_line(aes(y = value), alpha = 0.8, color = \"#126782\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#023047\") +\n  facet_wrap(vars(series_id)) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#extraindo-tendência",
    "href": "posts/general-posts/2024-02-media-movel/index.html#extraindo-tendência",
    "title": "Médias móveis",
    "section": "",
    "text": "Médias móveis fornecem uma estimativa simples da tendência de uma série. Neste sentido, é comum utilizar o filtro de médias móveis para decompor uma série. O código abaixo usa a base USMacroG para importar algumas séries macroeconômicas trimestrais dos EUA no período 1947-19974. Vamos modelar a tendência das séries usando uma janela de quatro anos (dois anos para trás e dois anos para frente), ou seja, uma média móvel de ordem 25.\n\n\nCode\nlibrary(AER)\nlibrary(tidyr)\ndata(\"USMacroG\")\n\nmacro = tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nsubmacro = macro |&gt;\n  filter(date &gt;= as.Date(\"1947-01-01\"), date &lt;= as.Date(\"1997-01-01\")) |&gt;\n  mutate(across(gdp:m1, log))\n\nmacro_trend = submacro |&gt;\n  select(date, gdp, consumption, invest, m1, inflation, unemp) |&gt;\n  pivot_longer(col = -date, names_to = \"name_series\") |&gt;\n  group_by(name_series) |&gt;\n  mutate(\n    trend = roll_mean(value, n = 25, fill = NA),\n    detrend = value - trend) |&gt;\n  ungroup()\n\nggplot(macro_trend, aes(date)) +\n  geom_line(aes(y = value), alpha = 0.5) +\n  geom_line(aes(y = trend), lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico das séries livres de tendência é apresentado abaixo. Note que as séries não foram dessazonalisadas, portanto, elas ainda apresentam oscilação sazonal.\n\nggplot(macro_trend, aes(date, detrend)) +\n  geom_line(lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#filtros-complexos",
    "href": "posts/general-posts/2024-02-media-movel/index.html#filtros-complexos",
    "title": "Médias móveis",
    "section": "",
    "text": "Apesar de simples, o filtro de médias móveis é bastante utilizado e tem boas propriedades estatísticas. O gráfico abaixo compara o ajuste de um filtro de médias móveis com o filtro Baxter-King5, que é bastante mais sofisticado. Como se vê, ambos os filtros chegam em resultados muito similares.\n\n\nCode\nlibrary(mFilter)\n\ntbl_unemp = select(macro, date, unemp)\nbk = mFilter::bkfilter(tbl_unemp$unemp, 6, 32, nfix = 12)\n\ntbl_unemp = tbl_unemp |&gt;\n  mutate(\n    ma = roll_mean(unemp, n = 25, fill = NA, align = \"center\"),\n    bk = mFilter::bkfilter(unemp, 6, 32, nfix = 12)$trend\n  ) |&gt;\n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(tbl_unemp, aes(date, value, color = series)) +\n  geom_line(data = filter(tbl_unemp, series == \"unemp\")) +\n  geom_line(data = filter(tbl_unemp, series != \"unemp\"), lwd = 0.9) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#FFD166\", \"#0CB0A9\", \"#073B4C\"),\n    labels = c(\"Baxter-King\", \"MA25\", \"Original\")\n  ) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#médias-móveis-exponenciais",
    "href": "posts/general-posts/2024-02-media-movel/index.html#médias-móveis-exponenciais",
    "title": "Médias móveis",
    "section": "",
    "text": "O filtro de médias móveis exponenciais ou EWMA (exponentially weighted moving average) é bastante popular em finanças. Na prática, ele é um caso específico de suvização exponencial6. Tecnicamente, seria possível implementá-lo usando stats::filter fornecendo os pesos adequados. O código abaixo replica o primeiro exemplo, da série “passe de atletas”.\n\newma_wgt = function(alpha = 0.5, order = 3) { rev((1 - alpha)^(seq(1, order))) }\n\newma3 = stats::filter(atletas, filter = c(1/8, 1/4, 1/2), sides = 1L)\newma5 = stats::filter(atletas, filter = ewma_wgt(order = 5), sides = 1L)\newma11 = stats::filter(atletas, filter = ewma_wgt(order = 11), sides = 1L)\n\n\nmts = ts.intersect(atletas, ewma3, ewma5, ewma11)\n\nautoplot(mts) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series\n\n\n\n\n\n\n\n\nO exemplo acima é um tanto quanto artificial. Tipicamente, o filtro EWMA é utilizado em séries financeiras de alta frequência. O código abaixo usa o pacote TTR (Technical Trading Rules), especializado em funções para finanças, para computar o EWMA de uma série de preço. Para tornar o exemplo mais simples vamos aproveitar a base ttrc do próprio pacote.\n\n\nCode\nlibrary(TTR)\ndata(ttrc)\n\nttrc = ttrc |&gt;\n  rename_with(tolower) |&gt;\n  dplyr::filter(date &gt;= as.Date(\"2004-01-01\")) |&gt;\n  mutate(\n    trend07 = EMA(close, n = 7),\n    trend30 = EMA(close, n = 30),\n    trend50 = EMA(close, n = 50)\n    ) |&gt;\n  select(date, close, starts_with(\"trend\")) |&gt;\n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(ttrc, aes(x = date, y = value, color = series)) +\n  geom_line(data = dplyr::filter(ttrc, series == \"close\"), alpha = 0.8) +\n  geom_line(data = dplyr::filter(ttrc, series != \"close\"), lwd = 0.5) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"EWMA7\", \"EWMA25\", \"EWMA50\")) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#footnotes",
    "href": "posts/general-posts/2024-02-media-movel/index.html#footnotes",
    "title": "Médias móveis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNome completo: “Transfer rights on sporting club players - annual - net”.↩︎\nExistem, na verdade, uma infinitude de classes de objetos para lidar com séries temporais no R. Para uma referência veja o pacote tsbox que funciona como uma pedra de Rosetta entre esses diferentes tipos de objetos.↩︎\nApesar de muito bom, o dplyr tem um problema chato quando se trabalha com séries de tempo. Há um conflito entre as funções dplyr::filter e stats::filter, que se utiliza para calcular uma média móvel. É muito comum carregar o pacote e esquecer deste detalhe. Para resolver os conflitos basta sempre declarar a função usando o operador “quatro pontos”, ::.↩︎\nPara a definição das variáveis, consulte ?USMacroG↩︎\nM. Baxter and R.G. King. Measuring business cycles: Approximate bandpass filters. The Review of Economics and Statistics, 81(4):575-93, 1999.↩︎\nPara uma boa introdução a suvização exponencial usando o pacote forecast consulte Forecasting: Principles and Practice (2ed).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "O tópico de housing affordability entrou em pauta nos últimos anos à medida que o preço dos imóveis cresceu rapidamente na última década em boa parte do mundo desenvolvido. Como mostrei num post anterior, este não é o caso do Brasil: os preços dos imóveis em termos reais “andaram de lado” nos últimos 13 anos. Em termos reais, os preços dos imóveis no final de 2023 estão um pouco abaixo do que estavam em 2010.\nDiversos fatores impactam o preço de imóveis no longo prazo. Num post anterior discuti o fator demográfico, e como o crescimento populacional é pouco correlacionado com os aumentos de preços observados nos últimos anos. O aumento de preços é mais relacionado com a manutenção prolongada de uma baixa taxa de juros na década de 2010 em conjunção com a baixa construção de novas unidades de moradia.\n\n\n\n\n\nMesmo com preços reais sob controle, a acessibilidade à moradia no Brasil é bastante ruim. A Fundação João Pinheiro reporta aumento do déficit habitacional, ano a ano. Em São Paulo, um levantamento recente registrou mais de 50 mil pessoas sem moradia. Em 2022, o Censo da População em Situação de Rua indicou que havia mais de 30 mil pessoas morando nas ruas da cidade.\nA acessibilidade à moradia é função de três fatores centrais:\n\nPreços dos imóveis.\nRenda das famílias.\nCondições de financiamento.\n\nO preço real dos imóveis diminuiu na última década. A renda real das famílias aumentou pouco, mas teve um crescimento na faixa de 20-25% em termos reais, segundo dados da PNADC de 2012 a 2023.\n\n\nO gráfico abaixo mostra a evolução do preço dos imóveis (IVG-R) e da renda média das famílias em comparação com a inflação (nível geral de preços) no Brasil. As três séries mostram a evolução nominal das variáveis e os valores estão indexados em 2010 para facilitar a comparação. Os valores de renda interpolam as séries de PNAD, PNADC e Censo para chegar numa estimativa suavizada da renda familiar habitual mensal.\nEm linhas gerais, o preço dos imóveis para de crescer em 2014-15 com a Crise Econômica e fica estagnado - em termos nominais - nos próximos anos. Isto é, a partir de 2015 o preço dos imóveis começa a cair em termos reais. A renda média, em contrapartida, tem uma tendência de crescimento estável e consistentemente consegue acompanhar a inflação no período observado. Assim, pode-se concluir - ainda que superficialmente - que o affordability deve ter melhorado, visto que os preços dos imóveis cresceram menos do que renda média.\n\n\n\n\n\n\n\n\nO cenário em São Paulo é similar. A série de preços mais extensa que se tem disponível é a do FipeZap, indicada em vermelho no gráfico abaixo. Novamente, as variáveis estão em termos nominais e indexadas em seus valores médios em 2010. A dinâmica de preços em São Paulo é muito similar à dinâmica nacional; a diferença é que os preços sobem mais durante o “boom” e caem menos durante o período de estagnação. A renda das famílias cresce acima da inflação média, indicando um ganho real de renda.\nO gráfico abaixo faz parte do artigo Acesso à moradia em São Paulo escrito por mim em conjunto com outros colegas, publicado na LARES 2021. Como se vê, no longo prazo, a renda conseguiu alcançar o preço dos imóveis, indicando um certo equilíbrio entre renda e preço.\n\n\n\n\n\n\n\n\nHá dois indicadores básicos, que mensuram a acessibilidade financeira à moradia: (1) o price-income ratio (PIR); e (2) o Housing Affordability Index (HAI). Abaixo apresento formalmente os dois indicadores.\n\n\nO PIR é uma razão simples entre o preço médio/mediano dos imóveis e a renda média/mediana anual das famílias:\n\\[\n\\text{PIR} = \\frac{\\text{Precos}}{\\text{Renda Anual}}\n\\]\nO PIR indica, grosso modo, a quantidade de “anos de trabalho” que uma família típica precisa investir de renda para comprar um imóvel típico. A principal vantagem do PIR é a sua simplicidade de cálculo, o que facilita comparações entre diferentes regiões. A principal desvantagem do PIR é de ignorar as condições de financiamento disponíveis para a população. Supondo que a renda anual média das famílias seja \\(R\\$30.000\\) e que o preço médio dos imóveis seja \\(R\\$270.000\\). Então o valor do PIR seria:\n\\[\n\\text{PIR} = \\frac{R\\$270.000}{R\\$30.000} = 9\n\\]\n\n\n\nO HAI é um indicador que possui diversas definições. Uma definição simples é a razão abaixo:\n\\[\n\\text{HAI} = \\frac{\\text{Parcela Máxima}}{\\text{Parcela Típica}}\\times100\n\\]\nonde a “parcela máxima de financiamento” é o valor teórico máximo que uma família típica estaria elegível num financiamento típico. Em outras palavras, suponha que a renda média familiar seja de \\(R\\$4.000\\). Considerando um financiamento a uma taxa de juros \\(r\\) num prazo de financiamento de \\(\\tau\\) períodos e um comprometimento máximo de renda \\(\\gamma\\) chega-se num valor \\(p_{max}\\) que representa o imóvel mais caro que o banco estaria disposto a financiar para esta família. Associado a este \\(p_{max}\\) existe um \\(\\text{pmt}_{max}\\) que é a parcela mais cara que a família poderia “aguentar”.\nComo regra de bolso, considera-se que a parcela de financiamento do imóvel não deve superar 30% da renda bruta familiar mensal. Assim, a parcela máxima para uma família que recebe \\(R\\$4.000\\) deveria ser de \\(R\\$1.200\\). Num típico financiamento SAC1, um imóvel de \\(R\\$270.000\\) teria uma parcela inicial próxima de \\(R\\$2.000\\). Assim, o valor do HAI seria:\n\\[\n\\text{HAI} = \\frac{1.200}{2.000}\\times100 = 60\n\\]\nO HAI mensura o valor da parcela de financiamento do imóvel típico em relação à renda mensal típica. Em outras palavras, este indicador verifica se o financiamento do imóvel típico de uma região “cabe no bolso” da família típica. Um valor próximo de 100 indica que a família com renda média consegue comprar o imóvel com preço médio."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#quadro-geral",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#quadro-geral",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "O gráfico abaixo mostra a evolução do preço dos imóveis (IVG-R) e da renda média das famílias em comparação com a inflação (nível geral de preços) no Brasil. As três séries mostram a evolução nominal das variáveis e os valores estão indexados em 2010 para facilitar a comparação. Os valores de renda interpolam as séries de PNAD, PNADC e Censo para chegar numa estimativa suavizada da renda familiar habitual mensal.\nEm linhas gerais, o preço dos imóveis para de crescer em 2014-15 com a Crise Econômica e fica estagnado - em termos nominais - nos próximos anos. Isto é, a partir de 2015 o preço dos imóveis começa a cair em termos reais. A renda média, em contrapartida, tem uma tendência de crescimento estável e consistentemente consegue acompanhar a inflação no período observado. Assim, pode-se concluir - ainda que superficialmente - que o affordability deve ter melhorado, visto que os preços dos imóveis cresceram menos do que renda média."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#são-paulo",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#são-paulo",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "O cenário em São Paulo é similar. A série de preços mais extensa que se tem disponível é a do FipeZap, indicada em vermelho no gráfico abaixo. Novamente, as variáveis estão em termos nominais e indexadas em seus valores médios em 2010. A dinâmica de preços em São Paulo é muito similar à dinâmica nacional; a diferença é que os preços sobem mais durante o “boom” e caem menos durante o período de estagnação. A renda das famílias cresce acima da inflação média, indicando um ganho real de renda.\nO gráfico abaixo faz parte do artigo Acesso à moradia em São Paulo escrito por mim em conjunto com outros colegas, publicado na LARES 2021. Como se vê, no longo prazo, a renda conseguiu alcançar o preço dos imóveis, indicando um certo equilíbrio entre renda e preço."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#quantificando",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#quantificando",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "Há dois indicadores básicos, que mensuram a acessibilidade financeira à moradia: (1) o price-income ratio (PIR); e (2) o Housing Affordability Index (HAI). Abaixo apresento formalmente os dois indicadores.\n\n\nO PIR é uma razão simples entre o preço médio/mediano dos imóveis e a renda média/mediana anual das famílias:\n\\[\n\\text{PIR} = \\frac{\\text{Precos}}{\\text{Renda Anual}}\n\\]\nO PIR indica, grosso modo, a quantidade de “anos de trabalho” que uma família típica precisa investir de renda para comprar um imóvel típico. A principal vantagem do PIR é a sua simplicidade de cálculo, o que facilita comparações entre diferentes regiões. A principal desvantagem do PIR é de ignorar as condições de financiamento disponíveis para a população. Supondo que a renda anual média das famílias seja \\(R\\$30.000\\) e que o preço médio dos imóveis seja \\(R\\$270.000\\). Então o valor do PIR seria:\n\\[\n\\text{PIR} = \\frac{R\\$270.000}{R\\$30.000} = 9\n\\]\n\n\n\nO HAI é um indicador que possui diversas definições. Uma definição simples é a razão abaixo:\n\\[\n\\text{HAI} = \\frac{\\text{Parcela Máxima}}{\\text{Parcela Típica}}\\times100\n\\]\nonde a “parcela máxima de financiamento” é o valor teórico máximo que uma família típica estaria elegível num financiamento típico. Em outras palavras, suponha que a renda média familiar seja de \\(R\\$4.000\\). Considerando um financiamento a uma taxa de juros \\(r\\) num prazo de financiamento de \\(\\tau\\) períodos e um comprometimento máximo de renda \\(\\gamma\\) chega-se num valor \\(p_{max}\\) que representa o imóvel mais caro que o banco estaria disposto a financiar para esta família. Associado a este \\(p_{max}\\) existe um \\(\\text{pmt}_{max}\\) que é a parcela mais cara que a família poderia “aguentar”.\nComo regra de bolso, considera-se que a parcela de financiamento do imóvel não deve superar 30% da renda bruta familiar mensal. Assim, a parcela máxima para uma família que recebe \\(R\\$4.000\\) deveria ser de \\(R\\$1.200\\). Num típico financiamento SAC1, um imóvel de \\(R\\$270.000\\) teria uma parcela inicial próxima de \\(R\\$2.000\\). Assim, o valor do HAI seria:\n\\[\n\\text{HAI} = \\frac{1.200}{2.000}\\times100 = 60\n\\]\nO HAI mensura o valor da parcela de financiamento do imóvel típico em relação à renda mensal típica. Em outras palavras, este indicador verifica se o financiamento do imóvel típico de uma região “cabe no bolso” da família típica. Um valor próximo de 100 indica que a família com renda média consegue comprar o imóvel com preço médio."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#pir-1",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#pir-1",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "PIR",
    "text": "PIR\nNa literatura internacional, valores de PIR na faixa de 5-10 indicam baixa acessibilidade financeira. Estudos feitos em países em desenvolvimento e subdesenvolvidos, contudo, apontam valores de PIR muito mais elevados do que este intervalo, devido tanto à baixa renda média da população como também às condições limitadas de acesso ao crédito. Neste sentido, talvez uma comparação mais justa seja com as megacidades chinesas que tem PIR na faixa de 10-15.\n\n\n\n\n\n\n\n\n\nO Jardim Europa foi a região menos acessível de São Paulo, nesta análise. O preço mediano do metro quadrado na região está em torno de 14 mil. Considerando um imóvel de 100 m2 temos um preço mediano na faixa 1,4 milhão. A renda mínima necessária para estar elegível a um financiamento deste imóvel seria em torno de 35 mil, considerando um comprometimento de 30% num sistema SAC e uma taxa de 10%. Evidentemente, este exemplo é bastante artificial quando se considera que esta região possui imóveis com metragem consideravelmente mais elevadas do que esta.\nPode-se calcular indiretamente qual seria um valor do PIR máximo para cada região. Suponha que a renda mensal típica considerada seja em torno de \\(R\\$4.500\\). Então para um típico financiamento no sistema SAC a 10% de juros e 360 meses, o imóvel mais caro que esta família pode financiar teria preço próximo a 180 mil. Este valor resulta num PIR próximo de 32.\n\\[\n\\text{PIR}_{max} = \\frac{180000}{12\\times 4500} = 3.33\n\\]\nGrosso modo, isto significa que qualquer imóvel que tenha preço 3 vezes superior à renda anual bruta familiar é inacessível. Para São Paulo, isto indica que a cidade inteira é inacessível para uma família que recebe em torno de 54 mil por ano.\nPara exemplificar o impacto do crédito sobre a acessibilidade, vale notar que uma redução dos juros para 5% e uma extensão do prazo para 35 anos (420 meses) aumentaria o preço máximo do imóvel para 300 mil. Isto aumentaria o PIR máximo para 5.5, valor próximo ao threshold de países desenvolvidos."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#hai-1",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#hai-1",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "HAI",
    "text": "HAI\nO HAI é um indicador mais interessante do que o PIR pois ele incorpora as condições de financiamento diretamente à métrica de acessibilidade. Assim como o PIR, o HAI aponta um grave problema de acessibilidade à moradia em São Paulo."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#referências",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#referências",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "Referências",
    "text": "Referências\n\nVinícius Oike Reginatto & Fernando F.A. Souza & Lucas Hernandes Porto & Rafael Butt Ferna Farias, 2021. “Acesso à moradia em São Paulo: visão geral e mensuração,” LARES lares-2021-4drb, Latin American Real Estate Society (LARES).\nLi, Y. S., Li, A. H., Wang, Z. F., & Wu, Q. (2019). Analysis on housing affordability of urban residents in Mainland China based on multiple indexes: taking 35 cities as examples. Annals of Data Science, 6, 305-319.\nQian, H., Ma, X., Wang, Q., & Liu, C. (2015). Temporal and Spatial Variation of Housing Affordability in China. In Proceedings of the 19th International Symposium on Advancement of Construction Management and Real Estate(pp. 595-605). Springer Berlin Heidelberg.\nAcolin, A., & Green, R. K. (2017). Measuring housing affordability in São Paulo metropolitan region: Incorporating location. Cities, 62, 41-49."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#footnotes",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#footnotes",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConsiderou-se um financiamento SAC de um imóvel de 270.000 a uma taxa de juros de 10% a.a. financiada em 360 meses com LTV de 70% (i.e., entrada de 81.000). O valor da primeira parcela é de 2032,11.↩︎\nA multiplicação por 12 é uma aproximação. Pode-se também considerar a multiplicação por 13, considerando valor do décimo-terceiro. Idealmente, usa-se a renda familiar anual bruta.↩︎"
  },
  {
    "objectID": "posts/shiny-apps/atlas-brasil.html",
    "href": "posts/shiny-apps/atlas-brasil.html",
    "title": "Atlas Brasil",
    "section": "",
    "text": "Sobre o aplicativo\nLink para o aplicativo\nEste aplicativo ajuda a visualizar uma série de indicadores socioeconômicos e demográficos nas regiões metropolitanas do Brasil. Os dados são do Atlas Brasil e o aplicativo foi inteiramente construído usando {shiny}. O código do aplicativo e alguns detalhes sobre a sua construção estão disponíveis no repositório do GitHub (em inglês).\nFiz este aplicativo em inglês então deixo o about abaixo.\n\n\nAbout the app\nThe Atlas of Human Development is a comprehensive collection of development indicators in Brazil. It provides access to information that reveals socioeconomic realities and inequalities. The data is compiled from IBGE’s decennial Census and yearly PNAD/C survey. It is the result of a collaborative effort between PNUD (UN), IPEA, and FJP.\nThe interactive map displays both regions and UDHs (human development units) for the major metropolitan regions of Brazil in 2000 and 2010. Income values have been adjusted for inflation up to January 2023. The map options allow for different forms of aggregation and color palettes. The ranking tool ranks metro regions and includes more recent data from PNAD. Finally, the download data tool provides a convenient way to download all of the data used in this app.\nThe construction of this app required extensive data cleaning, classification, and standardization. I chose a smaller subset of variables to keep the app manageable and to avoid overwhelming the user with options. For reference, the complete Atlas UDH dataset contains almost 230 variables. This smaller subset of variables also allowed me to better integrate the different Atlas datasets. In the future, I will provide more details about this process on my blog."
  },
  {
    "objectID": "press.html",
    "href": "press.html",
    "title": "Imprensa",
    "section": "",
    "text": "Em certas ocasiões, sou convidado para participar de matérias sobre mercado imobiliário na imprensa. Em outros casos, estudos que fiz ganham repercussão nos meios de comunicação. Aqui destaco algumas das publicações mais relevantes.\n\n\n\n[BBC] Nômades digitais e aluguel em dólar: por que moradores estão sendo expulsos de seus bairros na América Latina\n[BBC] Cuánto se han disparado los alquileres en las mayores ciudades de América Latina y qué posibilidades hay de que bajen\n[BBC] O que explica disparada de aluguéis nas maiores cidades da América Latina?\n[BBC] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[CBN] Entenda o movimento de alta dos aluguéis e de queda no valor de compra de imóveis\n[Folha de SP] Aluguel de apartamento com 1 quarto cai em SP, diz pesquisa\n[InvestNews] Aluguel em SP e RJ tem a maior alta nos últimos 3 anos, segundo Quinto Andar\n[Forbes] Aluguéis em SP sobem mais do que a inflação no 1ºtrimestre\n[G1] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[G1] Mercado de imóveis de luxo cresce no Brasil e preços passam dos R$ 40 milhões; veja apartamentos por dentro\n[G1 RS] Microapartamentos: aluguel médio de imóvel com até 30m² em Porto Alegre é de R$ 860\n[Rede Minas TV] A BUSCA POR MICROAPARTAMENTOS É REFLEXO DE UMA NOVA DINÂMICA SOCIAL\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[Veja SP] São Paulo teve aumento de 15,5% no aluguel em 2022\n\n\n\n\n\n[Abecip] Rio é a 6ª capital mais cara para comprar imóvel na América Latina\n[El Economista] CDMX quiere ser el hogar de los nómadas digitales con ayuda de Airbnb\n[Estadão] Preço de Aluguel de Microapartamentos em SP bate recorde após 6a alta seguida\n[Folha de SP] Tamanho mínimo de casa vai de 8 a 25 metros quadrados em capitais brasileiras\n[G1] Comprar imóvel é mais barato no Brasil em relação à América Latina, mas valor ainda pesa no bolso das famílias, diz pesquisa\n[Imobi Report] Em alta ou em queda? Entenda o momento dos microapartamentos\n[Real Estate Market] CDMX y Buenos Aires tienen el alquiler más caro en LATAM\n[R7] Porto Alegre, Curitiba, Salvador e BH são as cidades mais baratas para morar na América Latina\n[Nacion MX] ESTUDIO EN LATAM NOS DA PANORAMA DE REAL ESTATE EN MÉXICO\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[QA] Mercado residencial na América Latina: QuintoAndar faz estudo inédito em 12 cidades de seis países\n[QA] Tendências do mercado imobiliário: o que a economia nos aponta?\n\n\n\n\n\n[Folha de SP] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP, diz pesquisa\n[Abecip] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP\n[G1 SP] Casal com dois filhos e renda mediana em SP só consegue comprar imóvel com mais de 40m² longe do Centro"
  },
  {
    "objectID": "press.html#section",
    "href": "press.html#section",
    "title": "Imprensa",
    "section": "",
    "text": "[BBC] Nômades digitais e aluguel em dólar: por que moradores estão sendo expulsos de seus bairros na América Latina\n[BBC] Cuánto se han disparado los alquileres en las mayores ciudades de América Latina y qué posibilidades hay de que bajen\n[BBC] O que explica disparada de aluguéis nas maiores cidades da América Latina?\n[BBC] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[CBN] Entenda o movimento de alta dos aluguéis e de queda no valor de compra de imóveis\n[Folha de SP] Aluguel de apartamento com 1 quarto cai em SP, diz pesquisa\n[InvestNews] Aluguel em SP e RJ tem a maior alta nos últimos 3 anos, segundo Quinto Andar\n[Forbes] Aluguéis em SP sobem mais do que a inflação no 1ºtrimestre\n[G1] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[G1] Mercado de imóveis de luxo cresce no Brasil e preços passam dos R$ 40 milhões; veja apartamentos por dentro\n[G1 RS] Microapartamentos: aluguel médio de imóvel com até 30m² em Porto Alegre é de R$ 860\n[Rede Minas TV] A BUSCA POR MICROAPARTAMENTOS É REFLEXO DE UMA NOVA DINÂMICA SOCIAL\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[Veja SP] São Paulo teve aumento de 15,5% no aluguel em 2022"
  },
  {
    "objectID": "press.html#section-1",
    "href": "press.html#section-1",
    "title": "Imprensa",
    "section": "",
    "text": "[Abecip] Rio é a 6ª capital mais cara para comprar imóvel na América Latina\n[El Economista] CDMX quiere ser el hogar de los nómadas digitales con ayuda de Airbnb\n[Estadão] Preço de Aluguel de Microapartamentos em SP bate recorde após 6a alta seguida\n[Folha de SP] Tamanho mínimo de casa vai de 8 a 25 metros quadrados em capitais brasileiras\n[G1] Comprar imóvel é mais barato no Brasil em relação à América Latina, mas valor ainda pesa no bolso das famílias, diz pesquisa\n[Imobi Report] Em alta ou em queda? Entenda o momento dos microapartamentos\n[Real Estate Market] CDMX y Buenos Aires tienen el alquiler más caro en LATAM\n[R7] Porto Alegre, Curitiba, Salvador e BH são as cidades mais baratas para morar na América Latina\n[Nacion MX] ESTUDIO EN LATAM NOS DA PANORAMA DE REAL ESTATE EN MÉXICO\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[QA] Mercado residencial na América Latina: QuintoAndar faz estudo inédito em 12 cidades de seis países\n[QA] Tendências do mercado imobiliário: o que a economia nos aponta?"
  },
  {
    "objectID": "press.html#section-2",
    "href": "press.html#section-2",
    "title": "Imprensa",
    "section": "",
    "text": "[Folha de SP] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP, diz pesquisa\n[Abecip] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP\n[G1 SP] Casal com dois filhos e renda mediana em SP só consegue comprar imóvel com mais de 40m² longe do Centro"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "My name is Vinícius Reginatto. I hold a degree in Economics from UFRGS, recognized as Brazil’s leading university by INEP, and a Master’s degree in Economics from the University of São Paulo (USP), consistently ranked as the top university in Latin America.\nWith a strong quantitative foundation, I focus on areas such as spatial analysis, econometrics, and time-series forcasting. I am proficient in Python, R, Quarto, Shiny, SQL, and a range of widely used machine learning algorithms. Additionally, I have developed R packages and Shiny applications to address specific analytical challenges. My professional experience spans data science and economic consulting.\nOver the years, I have worked on economic consulting projects, focused on urban intervention initiatives—such as transit-oriented development (TOD)—and real estate market analysis. In these roles, I collaborated remotely with large, multidisciplinary, and multilingual teams to deliver impactful results."
  },
  {
    "objectID": "cv.html#presentation",
    "href": "cv.html#presentation",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "My name is Vinícius Reginatto. I hold a degree in Economics from UFRGS, recognized as Brazil’s leading university by INEP, and a Master’s degree in Economics from the University of São Paulo (USP), consistently ranked as the top university in Latin America.\nWith a strong quantitative foundation, I focus on areas such as spatial analysis, econometrics, and time-series forcasting. I am proficient in Python, R, Quarto, Shiny, SQL, and a range of widely used machine learning algorithms. Additionally, I have developed R packages and Shiny applications to address specific analytical challenges. My professional experience spans data science and economic consulting.\nOver the years, I have worked on economic consulting projects, focused on urban intervention initiatives—such as transit-oriented development (TOD)—and real estate market analysis. In these roles, I collaborated remotely with large, multidisciplinary, and multilingual teams to deliver impactful results."
  },
  {
    "objectID": "cv.html#short-cv",
    "href": "cv.html#short-cv",
    "title": "Vinicius Oike Reginatto",
    "section": "Short CV",
    "text": "Short CV\nEducation\nUniversity of São Paulo (USP) | São Paulo, Brazil\nMsC Economics | Jan 2017 - Aug 2019\nFederal University of RS | Porto Alegre, Brazil\nBA Economics | Jan 2012 - Dec 2016\nWork Experience\nHeartman House | Associate Consultant | May 2024-\nQuintoAndar | Data Scientist | 2022-2024\nUrbit | Data Scientist | 2019-2022\nFreelance Economic Consultant | 2019-\nLanguages\nPortuguese (fluent/native), English (fluent, C2), Spanish (Full professional proficiency)\nProgramming Languages\nR, Python, SQL, Quarto, RMarkdown."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Vinicius Oike Reginatto",
    "section": "Education",
    "text": "Education\n\n\nMsC in Economics\nUniversity of São Paulo | São Paulo, SP. MSc Economics1. Macroeconomics, growth theory, time series.\n\nAssistant teacher: (1) Mathematical Economics (dynamic optimization); (2) econometrics; (3) time series econometrics.\nDissertation work in dynamic optimization (calculus of variations and optimal control) and the history of economics.\n\n\n\nBA in Economics\nFederal University of Rio Grande do Sul2 | Porto Alegre, RS Bachelor (B.A.) Economics. Mathematical Economics."
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "Vinicius Oike Reginatto",
    "section": "Work Experience",
    "text": "Work Experience\n\n\nHeartman House Consultants | Associate Consultant\n2024-: Associate Consultant - Heartman House\n\nDeveloped a comprehensive framework on the Attention Economy and its implications for the real estate market, analyzing key trends and major players, including iBuyers and digital brokerages.\n\n\n\nContributed to the restructuring of a client in the education sector by:\n\nAuditing critical databases to uncover and address revenue collection inefficiencies.\nImproving data workflows, enhancing operational efficiency and accuracy.\n\n\n\n\nQuintoAndar | Data Scientist & Spokesperson\n2022-2024: Economist and Data Scientist — QuintoAndar (proptech startup)\nQuintoAndar is Brazil’s leading proptech unicorn, redefining how people rent, buy, and manage properties. As one of the founding members of DataHouse, QuintoAndar’s data hub, I played a key role in establishing the company as a leader in the residential real estate sector.\n\nResponsible for real estate reports and indices that helped establish QuintoAndar as the leading proptech brand in Brazil. Leader in SoV among real estate players.\nRedesigned and maintained the QuintoAndar ImovelWeb Rent Price Index.\nBuilt a centralized database to streamline and support market analysis and reporting. Produced quarterly macroeconomic reports on the state of Brazilian real estate.\nProvided insights as a recognized real estate expert, guiding strategic market initiatives.\n\n\n\nUrbit | Data Scientist\n2019-2022: Data Scientist — Urbit (market intelligence startup)\nAt Urbit I helped develop proprietary software to deliver data-driven insights to developers and urban planners.\n\nDeveloped real estate indices, successfully used by clients to enhance mortgage pipelines, property developments, and urban planning projects.\nProduced automated, on-demand real estate reports tailored to specific locations.\nManaged large spatial databases using PostgreSQL, R, and Python.\n\n\n\nEconomic Consultant\nAdvised multilateral organizations, government agencies, and private clients on urban planning, infrastructure, and real estate development through economic modeling and data-driven strategies.\n\nConducted in-depth analysis of urban and socioeconomic data (e.g., income, mobility, housing conditions) to inform policy and development decisions.\nSupported major transit-oriented development projects, including the TIC train (São Paulo-Campinas) and metro/BRT initiatives in Rio de Janeiro and Belo Horizonte.\nDeveloped econometric models and delivered economic and demographic forecasts for long-term infrastructure planning and private equity investment strategies.\nCreated internal indices (e.g., House Price Index) to provide actionable insights for market analysis and decision-making.\nContributed to zoning reform in São Paulo by presenting findings on housing affordability and advocating for public housing policies.\n\n\n\nAssistant Teacher\nDuring my MsC I worked as private tutor in econometrics and also as an assistant teacher in undergraduate and graduate courses:\n\nCourses: (1) Mathematical Economics (dynamic optimization); (2) econometrics (core); (3) time series econometrics.\n\n\n\nInternships\n2014-12/2014: Internship at Secretary of Treasury of Rio Grande do Sul"
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Vinicius Oike Reginatto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Economics Graduate course has been evaluated with a grade 7 (the highest grade), since the inception of the Capes’ plurianual review, indicating its high standard of international performance.↩︎\nRanked by INEP (Ministry of Education) as the best public university in Brazil since 2012 and during 2012-2014 also as the best university in Brazil. Commonly ranked in the Top 10 of all Brazilian universities in both national and international rankings.↩︎"
  },
  {
    "objectID": "tutorial-tidyverse.html",
    "href": "tutorial-tidyverse.html",
    "title": "Datascience: tidyverse",
    "section": "",
    "text": "A filosofia do tidyverse\n\n\nO tidyverse é um conjunto de pacotes focados em limpeza e extração de dados, que se tornou extremamente popular. O sucesso do tidyverse se deve à sua consistência em torno de princípios básicos. Neste post, discuto o estado atual do tidyverse e o que me parece ser sua filosofia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: select\n\n\nNeste post ensino abordagens diferentes para selecionar colunas de maneira eficiente. Apresento também as novidades que o dplyr trouxe nos últimos anos como as funções tidyselectors, que ajudam a selecionar colunas com base em padrões lógicos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: rename\n\n\nNeste post ensino abordagens diferentes para renomear colunas de maneira eficiente. Apresento também algumas das inovações que o pacote dplyr lançou nos últimos anos como as funções auxiliares all_of e any_of.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: filter\n\n\nNeste post ensino abordagens diferentes para filtrar linhas de uma tabela de maneira eficiente. Apresento também algumas das inovações que o pacote dplyr lançou nos últimos anos como as funções auxiliares if_any e if_all.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: mutate\n\n\nNeste post ensino abordagens diferentes para criar colunas de maneira eficiente. Apresento também as novidades que o dplyr trouxe nos últimos anos como a função across e novo argumento .by.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: summarise\n\n\nNeste post ensino abordagens diferentes para agregar dados de maneira eficiente. Mostro como aplicar funções sobre diversas colunas de uma base de dados ao mesmo tempo. Apresento também as novidades que o dplyr trouxe nos últimos anos como as funções tidyselectors, que ajudam a selecionar colunas com base em padrões lógicos.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/general-posts/2023-10-brazil-growth-context/index.html",
    "href": "posts/general-posts/2023-10-brazil-growth-context/index.html",
    "title": "Crescimento do Brasil em contexto",
    "section": "",
    "text": "Falar de crescimento econômico não é nada fácil.\nAs pessoas mantêm uma visão estática sobre o grupos dos páises “ricos” e dos países “pobres”, a despeito das enormes mudanças de renda que observamos nos últimos 40-60 anos. As pessoas tendem a acreditar que países centrais como Reino Unido, França, Estados Unidos e Alemanha continuam sendo os mais ricos1, abrindo algumas raras exceções, por exemplo, para os países produtores de petróleo do Oriente Médio, que tem valores inflacionados de PIB per capita.\nQue as pessoas são ignorantes sobre o mundo não é novidade. O historiador Hans Rolsing criou sua carreira mostrando o quão pouco as pessoas sabem sobre o mundo. Ainda assim não custa notar:\n\nTaiwan e Coreia do Sul já ultrapassaram Japão, França e Reino Unido. A Cingapura ultrapassou o PIB per capita dos EUA já na metade dos anos 2000.\nEstônia já é mais rica do que Grécia, Portugal e Rússia. Letônia, que no começo dos anos 90 tinha PIB per capita de ~US$2500, atualmente está próxima de ~US$30.000, ultrapassando Grécia, Chile e Turquia.\nApesar de aparentemente estar num estado perpétuo de crise econômica, a Argentina continua tendo um dos maiores PIB per capita da América do Sul e continua (bastante) à frente do Brasil.\nChina e Índia continuam com PIB per capita similar ao de países latinoamericanos e estão (por enquanto) atrás do Brasil e do México."
  },
  {
    "objectID": "posts/general-posts/2023-10-brazil-growth-context/index.html#footnotes",
    "href": "posts/general-posts/2023-10-brazil-growth-context/index.html#footnotes",
    "title": "Crescimento do Brasil em contexto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTalvez até a forma como estuda-se história e geografia no colégio reforce este tipo de visão.↩︎\nTradução literal de: low-income, lower-middle income, upper-middle income e upper income. Para mais informações consulte WB↩︎"
  },
  {
    "objectID": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html",
    "href": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html",
    "title": "O novo tidyverse: filter",
    "section": "",
    "text": "O tidyverse é uma coleção poderosa de pacotes, voltados para a manipulação e limpeza de dados. Num outro post, discuti alguns aspectos gerais da filosofia destes pacotes que incluem a sua consistência sintática e o uso de pipes. A filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes” que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático.\nO tidyverse está em constante expansão, novas funcionalidades são criadas para melhorar a performance e capabilidade de suas funções. Assim, é importante atualizar nosso conhecimento destes pacotes periodicamente. Nesta série de posts vou focar nas funções principais dos pacotes dplyr e tidyr, voltados para a limpeza de dados."
  },
  {
    "objectID": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html#o-básico",
    "href": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html#o-básico",
    "title": "O novo tidyverse: filter",
    "section": "O básico",
    "text": "O básico\nOs pacotes utilizados neste tutorial são listados abaixo.\n\nlibrary(dplyr)\nlibrary(readr)\n\nPara praticar as funções vamos utilizar uma tabela que traz informações sobre as cidades do Brasil.\n\ndat &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/cities_brazil.csv\"\n)\n\n# dat &lt;- select(dat, 1:7, population, population_growth, pib)\n\nA função filter é talvez uma das que menos mudou ao longo do desenvolvimento do pacote dplyr. A função serve para filtrar as linhas de um data.frame segundo alguma condição lógica.\n\nfiltered_dat &lt;- filter(dat, population_growth &lt; 0)\n\nnrow(filtered_dat)\n\n[1] 2399\n\n\nOs principais operadores lógicos no R:\n\n“Maior que”, “Menor que”: &gt;, &lt;, &gt;=, &lt;=\nE/ou: &, |\n“Negação”: !\n“Igual a”: ==\n“Dentro de”: %in%\n\nAs funções is_* também são bastante importantes; em particular a função is.na() é útil para encontrar ou remover observações ausentes.\nO exemplo abaixo mostra como filtrar linhas baseado num string. Note que quando se usa múltiplos strings é preciso usar o %in%.\n\nfilter(dat, name_muni == \"São Paulo\")\nfilter(dat, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\"))\n\ncities &lt;- c(\"São Paulo\", \"Rio de Janeiro\")\nfilter(dat, name_muni %in% cities)\n\n\nfilter(dat, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\")) |&gt;\n  print_table()\n\nPara negar a igualdade, basta usar o operador !. No caso do operador %in% há duas maneiras válidas de negá-lo: pode-se colocar o ! no começo da expressão ou colocar a expressão inteira dentro de um parêntesis. Eu tendo a preferir a segunda sintaxe.\n\n#&gt; Remove todas as cidades da região Sudeste\nfilter(dat, name_region != \"Sudeste\")\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(dat, !name_region %in% c(\"Sudeste\", \"Norte\"))\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(dat, !(name_region %in% c(\"Sudeste\", \"Norte\")))\n\nEm geral, pode-se omitir o operador E (&), já que se pode concatenar várias condições lógicas dentro uma mesma chamada para a função filter, separando as condições por vírgulas. Esta sintaxe costuma ser preferida pois ela é mais eficiente do que chamar a função a função filter múltiplas vezes. Além disso, a escrita do código fica mais limpa, pois é fácil separar as condições em linhas distintas. As três versões do código abaixo geram o mesmo resultado.\n\n# Mais eficiente e mais fácil de ler\nd1 &lt;- dat |&gt;\n  filter(\n    name_region == \"Nordeste\",\n    !(name_state %in% c(\"Pernambuco\", \"Piauí\")),\n    !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n  )\n# Igualmente eficiente, leitura fica um pouco pior\nd2 &lt;- dat |&gt;\n  filter(\n    name_region == \"Nordeste\" &\n      !(name_state %in% c(\"Pernambuco\", \"Piauí\")) &\n      !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n  )\n\n# Menos eficiente\nd3 &lt;- dat |&gt;\n  filter(name_region == \"Nordeste\") |&gt;\n  filter(!(name_state %in% c(\"Pernambuco\", \"Piauí\"))) |&gt;\n  filter(!(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\")))\n\nall.equal(d1, d2)\nall.equal(d2, d3)\nall.equal(d3, d1)\n\nRelações de grandeza funcionam naturalmente com números. A tabela abaixo mostra todos os municípios com mais do que um milhão de habitantes.\n\nfilter(dat, population &gt; 1e6)\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npopulation\npopulation_density\n\n\n\n\nSão Paulo\nSP\n11.451.245\n7.528\n\n\nRio de Janeiro\nRJ\n6.211.423\n5.175\n\n\nBrasília\nDF\n2.817.068\n489\n\n\nFortaleza\nCE\n2.428.678\n7.775\n\n\nSalvador\nBA\n2.418.005\n3.487\n\n\nBelo Horizonte\nMG\n2.315.560\n6.988\n\n\nManaus\nAM\n2.063.547\n181\n\n\nCuritiba\nPR\n1.773.733\n4.079\n\n\nRecife\nPE\n1.488.920\n6.804\n\n\nGoiânia\nGO\n1.437.237\n1.971\n\n\nPorto Alegre\nRS\n1.332.570\n2.690\n\n\nBelém\nPA\n1.303.389\n1.230\n\n\nGuarulhos\nSP\n1.291.784\n4.054\n\n\nCampinas\nSP\n1.138.309\n1.433\n\n\nSão Luís\nMA\n1.037.775\n1.780\n\n\n\n\n\n\n\nTambém pode-se usar alguma função que retorne um valor numérico. Nos exemplos abaixo filtra-se apenas os municípios com PIB acima da média e os municípios no top 1% da distribuição do PIB.\n\nfilter(dat, pib &gt; mean(pib))\nfilter(dat, pib &gt; quantile(pib, probs = 0.99))\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npib\npib_share_uf\n\n\n\n\nSão Paulo\nSP\n748.759.007\n31\n\n\nRio de Janeiro\nRJ\n331.279.902\n44\n\n\nBrasília\nDF\n265.847.334\n100\n\n\nBelo Horizonte\nMG\n97.509.893\n14\n\n\nManaus\nAM\n91.768.773\n79\n\n\nCuritiba\nPR\n88.308.728\n18\n\n\nOsasco\nSP\n76.311.814\n3\n\n\nPorto Alegre\nRS\n76.074.563\n16\n\n\nGuarulhos\nSP\n65.849.311\n3\n\n\nCampinas\nSP\n65.419.717\n3\n\n\nFortaleza\nCE\n65.160.893\n39\n\n\nSalvador\nBA\n58.938.115\n19\n\n\nGoiânia\nGO\n51.961.311\n23\n\n\nBarueri\nSP\n51.254.572\n2\n\n\nJundiaí\nSP\n51.235.050\n2\n\n\nRecife\nPE\n50.311.002\n26\n\n\nSão Bernardo do Campo\nSP\n48.614.342\n2\n\n\nDuque de Caxias\nRJ\n47.153.673\n6\n\n\nNiterói\nRJ\n40.949.495\n5\n\n\nSão José dos Campos\nSP\n39.148.012\n2\n\n\nPaulínia\nSP\n38.572.766\n2\n\n\nParauapebas\nPA\n38.014.863\n18\n\n\nUberlândia\nMG\n37.631.537\n6\n\n\nSorocaba\nSP\n36.723.769\n2\n\n\nJoinville\nSC\n36.391.912\n10\n\n\nMaricá\nRJ\n35.618.327\n5\n\n\nRibeirão Preto\nSP\n35.218.869\n1\n\n\nItajaí\nSC\n33.084.145\n9\n\n\nSão Luís\nMA\n33.074.010\n31\n\n\nBelém\nPA\n30.835.763\n14\n\n\nCampo Grande\nMS\n30.121.789\n25\n\n\nContagem\nMG\n29.558.094\n4\n\n\nSanto André\nSP\n29.440.477\n1\n\n\nPiracicaba\nSP\n27.172.817\n1\n\n\nCuiabá\nMT\n26.528.839\n15\n\n\nBetim\nMG\n26.185.005\n4\n\n\nCaxias do Sul\nRS\n25.965.161\n6\n\n\nCamaçari\nBA\n25.697.266\n8\n\n\nVitória\nES\n25.473.898\n18\n\n\nSerra\nES\n25.079.657\n18\n\n\nCampos dos Goytacazes\nRJ\n23.841.837\n3\n\n\nMaceió\nAL\n22.872.756\n36\n\n\nNatal\nRN\n22.729.773\n32\n\n\nCanaã dos Carajás\nPA\n22.522.725\n10\n\n\nSantos\nSP\n22.073.535\n1\n\n\nSão José dos Pinhais\nPR\n21.975.612\n4\n\n\nLondrina\nPR\n21.729.852\n4\n\n\nTeresina\nPI\n21.578.875\n38\n\n\nFlorianópolis\nSC\n21.312.447\n6\n\n\nCajamar\nSP\n20.798.646\n1\n\n\nJoão Pessoa\nPB\n20.766.551\n30\n\n\nMaringá\nPR\n20.005.630\n4\n\n\nAraucária\nPR\n19.724.416\n4\n\n\nPorto Velho\nRO\n19.448.762\n38\n\n\nSão Gonçalo\nRJ\n19.002.883\n3\n\n\nSão José do Rio Preto\nSP\n18.694.213\n1"
  },
  {
    "objectID": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html#grupos",
    "href": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html#grupos",
    "title": "O novo tidyverse: filter",
    "section": "Grupos",
    "text": "Grupos\nA função de filtro segue uma regra lógica que é aplicada sobre a tabela como um todo. É possível filtrar dentro de grupos usando o argumento .by = \"nome_do_grupo\".\nNo código abaixo, novamente filtra-se os municípios com PIB acima da média. No segundo exemplo, contudo, este filtro é aplicado dentro de cada região, segundo a coluna/grupo name_region. A regra lógica pib &gt; mean(pib) é aplicada dentro de cada região, isto é, filtra-se todos os municípios que têm PIB superior à média do PIB da sua região.\n\ndat |&gt; filter(pib &gt; mean(pib))\ndat |&gt; filter(pib &gt; mean(pib), .by = \"name_region\")\n\nVale notar que a a sintaxe .by = \"grupo\" ainda está em fase experimental. Ela oferece um substituto mais sucinto à antiga sintaxe que usava a função group_by() com a vantagem de sempre aplicar a função ungroup() ao final do processo, isto é, o resultado final da função acima será uma tabela sem grupos. O código acima é equivalente ao código abaixo.\n\ndat |&gt;\n  group_by(name_region) |&gt;\n  filter(pib &gt; mean(pib)) |&gt;\n  ungroup()\n\nEste outro exemplo enfatiza como o resultado da função filter muda quando é aplicada em diferentes grupos.\n\ndat |&gt; filter(pib == max(pib))\ndat |&gt; filter(pib == max(pib), .by = \"name_state\")\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npib\npib_share_uf\n\n\n\n\nPorto Velho\nRO\n19.448.762\n38\n\n\nRio Branco\nAC\n9.579.592\n58\n\n\nManaus\nAM\n91.768.773\n79\n\n\nBoa Vista\nRR\n11.826.207\n74\n\n\nParauapebas\nPA\n38.014.863\n18\n\n\nMacapá\nAP\n11.735.557\n64\n\n\nPalmas\nTO\n9.940.091\n23\n\n\nSão Luís\nMA\n33.074.010\n31\n\n\nTeresina\nPI\n21.578.875\n38\n\n\nFortaleza\nCE\n65.160.893\n39\n\n\nNatal\nRN\n22.729.773\n32\n\n\nJoão Pessoa\nPB\n20.766.551\n30\n\n\nRecife\nPE\n50.311.002\n26\n\n\nMaceió\nAL\n22.872.756\n36\n\n\nAracaju\nSE\n16.447.105\n36\n\n\nSalvador\nBA\n58.938.115\n19\n\n\nBelo Horizonte\nMG\n97.509.893\n14\n\n\nVitória\nES\n25.473.898\n18\n\n\nRio de Janeiro\nRJ\n331.279.902\n44\n\n\nSão Paulo\nSP\n748.759.007\n31\n\n\nCuritiba\nPR\n88.308.728\n18\n\n\nJoinville\nSC\n36.391.912\n10\n\n\nPorto Alegre\nRS\n76.074.563\n16\n\n\nCampo Grande\nMS\n30.121.789\n25\n\n\nCuiabá\nMT\n26.528.839\n15\n\n\nGoiânia\nGO\n51.961.311\n23\n\n\nBrasília\nDF\n265.847.334\n100"
  },
  {
    "objectID": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html#if_any-e-if_all",
    "href": "posts/tutorial-tidyverse/2024-01-tidyverse-filter/index.html#if_any-e-if_all",
    "title": "O novo tidyverse: filter",
    "section": "if_any e if_all",
    "text": "if_any e if_all\nA função filter não funciona em conjunção com a função across(). Esta função foi desenvolvida para funcionar apenas com mutate e summarise e aplica uma mesma regra/função sobre múltiplas colunas.\nJá a função filter recebeu duas funções auxiliares: if_any e if_all. Elas seguem o mesmo padrão das funções base any e all. Estas funções servem para agregar condições lógicas. A função any, por exemplo, testa múltiplas condições lógicas e retorna um único TRUE se houver ao menos um TRUE entre as condições lógicas. Já a função all retorna um único TRUE se absolutamente todas as condições lógicas testadas também retornaram TRUE.\nA função if_any aplica uma mesma regra em múltiplas colunas e retorna todas as linhas que atendem esta regra. No exemplo abaixo\n\ndat |&gt; filter(if_any(starts_with(\"pib\"), ~ . &gt; 100000))\n\nO exemplo seguinte é mais interessante. Neste caso, todas as variáveis numéricas da tabela são normalizadas (por região) e retorna-se apenas os municípios onde o valor de cada coluna é superior a 1. Como as variáveis estão normalizadas isto é equivalente a retornar os municípios que estão 1 desvio-padrão acima da média da sua região em todos os atributos numéricos considerados.\n\ndat |&gt;\n  select(-contains(\"code\")) |&gt;\n  select(where(~ all(.x &gt; 0))) |&gt;\n  mutate(\n    across(where(is.numeric), ~ as.numeric(scale(log(.x)))),\n    .by = \"name_region\"\n  ) |&gt;\n  filter(if_all(everything(), ~ . &gt; 1))\n\n# A tibble: 2 × 15\n  name_muni            name_state abbrev_state name_region population city_area\n  &lt;chr&gt;                &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 Paranaguá            Paraná     PR           Sul               2.32      1.13\n2 São José dos Pinhais Paraná     PR           Sul               3.00      1.28\n  population_density households dwellers_per_household   pib pib_taxes\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1               1.52       2.24                   2.25  2.80      2.81\n2               2.10       2.97                   1.04  3.27      3.31\n  pib_added_value pib_industrial pib_services pib_govmt_services\n            &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n1            2.76           2.36         2.71               2.57\n2            3.18           2.76         2.92               3.17\n\n\nO último exemplo é similar ao anterior. As variáveis numéricas novamente são normalizadas mas desta vez busca-se somente os municípios que estão 3 desvios-padrão, acima da média do seu estado, ou na população ou no PIB.\n\ndat |&gt;\n  select(-contains(\"code\")) |&gt;\n  select(where(~ all(.x &gt; 0))) |&gt;\n  mutate(\n    across(where(is.numeric), ~ as.numeric(scale(log(.x)))),\n    .by = \"name_state\"\n  ) |&gt;\n  filter(if_any(c(population, pib), ~ . &gt; 3))\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npopulation\npib\npopulation_density\npib_services\ncity_area\n\n\n\n\nPorto Velho\nRO\n3,249\n3,508\n1,129\n2,954\n2,622\n\n\nRio Branco\nAC\n3,254\n3,337\n2,525\n3,112\n0,628\n\n\nManaus\nAM\n5,214\n5,439\n3,420\n5,136\n-0,239\n\n\nBoa Vista\nRR\n3,299\n3,310\n3,059\n3,172\n-0,709\n\n\nBelém\nPA\n4,041\n3,410\n3,122\n3,670\n-0,672\n\n\nCanaã dos Carajás\nPA\n0,950\n3,151\n0,511\n2,260\n0,069\n\n\nParauapebas\nPA\n2,305\n3,582\n0,816\n2,795\n0,602\n\n\nAraguaína\nTO\n3,837\n3,459\n2,184\n3,492\n1,201\n\n\nGurupi\nTO\n3,062\n2,857\n2,255\n3,018\n0,381\n\n\nPalmas\nTO\n4,467\n4,166\n3,269\n4,073\n0,580\n\n\nBalsas\nMA\n2,045\n3,190\n-0,898\n3,117\n2,899\n\n\nImperatriz\nMA\n3,236\n3,599\n2,353\n3,654\n0,373\n\n\nSão José de Ribamar\nMA\n3,103\n2,412\n4,270\n2,681\n-1,894\n\n\nSão Luís\nMA\n4,845\n5,096\n4,541\n4,903\n-0,581\n\n\nParnaíba\nPI\n3,676\n3,413\n3,580\n3,577\n-0,564\n\n\nPicos\nPI\n2,881\n3,018\n2,643\n3,340\n-0,266\n\n\nTeresina\nPI\n5,667\n5,526\n4,092\n5,417\n0,676\n\n\nUruçuí\nPI\n1,464\n3,101\n-1,185\n2,756\n2,604\n\n\nCaucaia\nCE\n3,041\n2,955\n1,882\n2,814\n0,878\n\n\nFortaleza\nCE\n5,212\n4,963\n5,137\n4,922\n-0,641\n\n\nMaracanaú\nCE\n2,569\n3,239\n3,900\n3,146\n-1,851\n\n\nParnamirim\nRN\n3,428\n3,265\n4,138\n3,371\n-0,674\n\n\nMossoró\nRN\n3,475\n3,434\n1,213\n3,512\n2,658\n\n\nNatal\nRN\n4,538\n4,421\n4,968\n4,488\n-0,323\n\n\nCabedelo\nPB\n2,206\n3,090\n3,771\n3,125\n-2,247\n\n\nCampina Grande\nPB\n4,163\n4,287\n2,668\n4,093\n1,407\n\n\nJoão Pessoa\nPB\n4,893\n4,951\n4,328\n4,721\n0,137\n\n\nSanta Rita\nPB\n3,070\n3,029\n1,490\n2,768\n1,645\n\n\nIpojuca\nPE\n1,478\n3,168\n0,696\n2,718\n0,530\n\n\nJaboatão dos Guararapes\nPE\n3,469\n3,148\n2,836\n3,085\n-0,142\n\n\nRecife\nPE\n4,360\n4,261\n3,673\n4,202\n-0,303\n\n\nArapiraca\nAL\n3,021\n2,913\n3,084\n3,187\n0,645\n\n\nMaceió\nAL\n4,588\n4,318\n4,478\n4,537\n1,190\n\n\nAracaju\nSE\n3,638\n3,676\n4,258\n3,886\n-0,094\n\n\nCamaçari\nBA\n3,357\n4,191\n2,547\n3,774\n0,001\n\n\nFeira de Santana\nBA\n4,229\n3,722\n2,744\n3,800\n0,506\n\n\nJuazeiro\nBA\n3,068\n2,623\n0,357\n2,778\n2,135\n\n\nLuís Eduardo Magalhães\nBA\n2,123\n3,040\n0,108\n3,046\n1,628\n\n\nSalvador\nBA\n5,882\n4,927\n4,579\n5,007\n-0,122\n\n\nSão Francisco do Conde\nBA\n0,853\n3,510\n1,627\n2,947\n-1,059\n\n\nVitória da Conquista\nBA\n3,615\n3,054\n1,439\n3,230\n1,414\n\n\nBelo Horizonte\nMG\n5,103\n4,613\n5,077\n4,620\n-0,186\n\n\nBetim\nMG\n3,489\n3,638\n3,490\n3,365\n-0,148\n\n\nContagem\nMG\n3,874\n3,728\n4,373\n3,712\n-0,715\n\n\nExtrema\nMG\n1,581\n3,027\n1,961\n3,055\n-0,487\n\n\nGovernador Valadares\nMG\n3,049\n2,630\n1,342\n2,768\n1,768\n\n\nIpatinga\nMG\n2,935\n3,004\n3,619\n2,862\n-0,882\n\n\nJuiz de Fora\nMG\n3,744\n3,312\n2,450\n3,412\n1,280\n\n\nMontes Claros\nMG\n3,495\n2,900\n1,386\n2,955\n2,195\n\n\nNova Lima\nMG\n2,270\n3,072\n2,118\n2,792\n0,073\n\n\nRibeirão das Neves\nMG\n3,282\n2,320\n4,007\n2,358\n-0,944\n\n\nUberaba\nMG\n3,304\n3,326\n0,995\n3,208\n2,426\n\n\nUberlândia\nMG\n4,003\n3,907\n1,752\n3,795\n2,332\n\n\nSerra\nES\n3,040\n2,923\n2,726\n2,891\n0,284\n\n\nRio de Janeiro\nRJ\n3,591\n3,414\n2,301\n3,286\n1,393\n\n\nGuarulhos\nSP\n3,055\n2,881\n2,875\n2,802\n0,155\n\n\nSão Paulo\nSP\n4,578\n4,338\n3,294\n4,267\n2,022\n\n\nAraucária\nPR\n2,360\n3,294\n2,508\n2,752\n0,310\n\n\nCascavel\nPR\n3,108\n3,016\n1,810\n2,989\n2,310\n\n\nCuritiba\nPR\n4,576\n4,554\n5,174\n4,384\n0,210\n\n\nFoz do Iguaçu\nPR\n2,930\n3,212\n2,899\n2,666\n0,660\n\n\nLondrina\nPR\n3,531\n3,376\n2,550\n3,365\n1,995\n\n\nMaringá\nPR\n3,255\n3,306\n3,514\n3,330\n0,361\n\n\nPonta Grossa\nPR\n3,135\n3,183\n1,859\n2,982\n2,286\n\n\nSão José dos Pinhais\nPR\n3,058\n3,385\n2,585\n3,081\n1,249\n\n\nFlorianópolis\nSC\n3,322\n3,046\n2,472\n3,052\n1,339\n\n\nItajaí\nSC\n2,731\n3,372\n2,587\n3,097\n0,268\n\n\nJoinville\nSC\n3,436\n3,443\n2,155\n3,134\n1,988\n\n\nCanoas\nRS\n3,136\n3,231\n3,730\n2,942\n-0,689\n\n\nCaxias do Sul\nRS\n3,370\n3,492\n1,901\n3,237\n1,694\n\n\nPelotas\nRS\n3,083\n2,722\n1,637\n2,718\n1,669\n\n\nPorto Alegre\nRS\n4,230\n4,316\n3,739\n4,169\n0,561\n\n\nCampo Grande\nMS\n4,131\n3,470\n2,785\n3,555\n1,112\n\n\nCuiabá\nMT\n3,653\n2,967\n3,573\n3,093\n0,040\n\n\nAnápolis\nGO\n3,070\n3,060\n2,746\n3,038\n0,151\n\n\nAparecida de Goiânia\nGO\n3,297\n3,039\n3,874\n3,094\n-0,966\n\n\nGoiânia\nGO\n4,110\n3,976\n3,908\n4,062\n-0,081"
  },
  {
    "objectID": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html",
    "href": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html",
    "title": "A filosofia do tidyverse",
    "section": "",
    "text": "O tidyverse é um metapacote, ou conjunto de pacotes. Pode-se pensar no tidyverse como uma família de pacotes, unidos por uma filosofia comum; grosso modo, os pacotes que compõem o tidyverse tem o mesmo objetivo: facilitar a manipulação de dados. Estes pacotes criaram uma nova forma de se escrever código, que substitui boa parte ou mesmo todas as funções base do R. Atualmente, o tidyverse parece estar se consolidando como a variante dominante do R. De fato, a maior parte dos pacotes do tidyverse consta na lista dos mais baixados no repositório CRAN.\nO tidyverse é intimamente ligado com o estatístico Hadley Wickham, criador ou co-criador da maioria dos seus pacotes, e autor do influente artigo em que ele define o que é “tidy” data. Ele também é autor de diversos livros didáticos como R for Data Science e ggplot2: Elegant graphics for Data Analysis, que ajudaram a popularizar o tidyverse.\nWickham também tem posição de liderança dentro da Posit (antigamente conhecida como RStudio), que mantém o GUI mais popular de R e que patrocina inúmeras atividades vinculadas com o aprendizado de R, que costumam enfatizar os pacotes do tidyverse. De fato, tornou-se lugar comum começar a se ensinar R pelo tidyverse como se vê pela prevalência de cursos no Coursera ou Datacamp.\nQuando se olha para a curta história do tidyverse é difícil explicar o porquê do seu enorme sucesso, mas é fato que este conjunto de pacotes se tornou um dialeto dominante dentro da comunidade do R. As funções do tidyverse tem algumas vantagens importantes sobre o base-R.\n\n\nAs funções do tidyverse possuem uma característica ausente na maior parte das funções base do R: consistência. As funções do tidyverse oferecem consistência sintática: o nome da funções segue certas convenções e a ordem dos argumentos segue regras previsíveis.\nUm exemplo imediato é o pacote stringr, que serve para manipulação de strings. Todas as funções deste pacote começam com prefixo str_ e seus argumentos seguem a lógica: string e pattern como em str_detect(string, pattern)1. Além disso, as funções são mais otimizadas em relação às funções base do R.\nO purrr faz algo similar, ao simplificar a família de funções apply em diversas funções map_*. Neste caso, além da consistência sintática, as funções map_* também garantem a consistência do output, em termos da classe do objeto que é retornado como resultado da função. Isto é uma grande vantagem, especialmente quando comparado com a função sapply que “simplifica” o output de maneiras às vezes imprevisíveis.\nEm termos de eficiência, o tidyverse costuma ganhar das funções equivalentes em base-R. O dplyr/tidyr, de maneira geral, garante manipulações de dados muito mais velozes2, assim como o readr importa dados mais rapidamente3. As funções map também tem paralelos simples na família future_ do pacote furrr, que permite usar processamento paralelo no R.\n\n\n\nHá muito material de apoio para tidyverse: livros, materiais didáticos, posts em blogs, respostas em fóruns, etc. Como citado acima, o próprio Posit produz inúmeros materiais didáticos e livros que ajudam a aprender e a ensinar tidyverse. Na medida em que o tidyverse consolida-se como o dialeto dominante isto tende a se tornar um ciclo virtuoso.\nO R é uma linguagem bastante versátil, que reúne pesquisadores de campos distintos. Recentemente, parece haver uma convergência para o tidyverse. O campo de séries de tempo, por exemplo, agora tem o tidyquant, fable e modeltime que utilizam os princípios do tidyverse. Com o tempo, deve-se observar movimentos similares de outros campos.\n\n\n\nO tidyverse oferece funções que se aplicam a cada uma das etapas de uma análise de dados. Neste sentido, ele vai de ponta-a-ponta, cobrindo importação, limpeza, modelagem e visualização de dados. A natureza autocontida do tidyverse é bastante atraente pois oferece um caminho seguro para novatos no R, especialmente para quem tem interesse em ciência de dados.\n\n\n\nO conhecimento no R muitas vezes é bastante horizontal. Cada pacote novo traz funções diferentes, que funcionam de novas maneiras e este conhecimento adquirido nem sempre se traduz para outras tarefas. Já sintaxe do dplyr é bastante geral, pode ser utilizada em vários contextos. O dbplyr, por exemplo, é um backend para databases (como BigQuery, PostgreSQL, etc.) que usa a sintaxe do dplyr como frontend. O mesmo acontece com dtplyr/tidytable que permite usar a sintaxe do dplyr junto com a eficiência do data.table. Até para dados complexos já existe o pacote srvyr que usa o survey como backend.\n\n\n\nEste último ponto é bastante mais contencioso. Eu acredito que o tidyverse é mais fácil do que base-R. Eu tenho um conhecimento razoável de base-R e avançado tanto de tidyverse como de data.table. Na minha opinião, a lógica do tidyverse de usar o nome das colunas de um data.frame como objetos é muito poderosa e intuitiva. Não só torna o código mais legível como também evita uma sintaxe carregada com operadores estranhos como $.\nA integração com pipes também simplifica muito o workflow da análise de dados. Com o tempo, a leitura de um código em pipes torna-se natural. Por fim, fazer funções com tidyverse também é muito fácil. Especialmente no caso de funções simples, a sintaxe {x} e !!x facilita bastante e, de maneira geral, considero mais simples programar usando princípios “tidy” do que programar usando base-R."
  },
  {
    "objectID": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html#o-tidyverse-em-números",
    "href": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html#o-tidyverse-em-números",
    "title": "A filosofia do tidyverse",
    "section": "O tidyverse em números",
    "text": "O tidyverse em números\nOlhando para as estatísticas do CRAN, vê-se que os pacotes do tidyverse são muito relevantes dentro do ecossistema. Os dados compilados abaixo mostram o retrato dos pacotes do CRAN em 05 de setembro de 2023, quando havia cerca de 19.800 pacotes ativos.\nAtualmente, cerca de um terço dos pacotes no CRAN dependem de algum dos pacotes core do tidyverse. O crescimento desta razão tem sido crescente: de todos os pacotes ativos em 2023, 40% dependem diretamente do tidyverse. Note que no gráfico abaixo, o ano de publicação reflete o ano da versão mais recente de cada pacote. Assim, pacotes ativos cuja última atualização foi anterior a 2016 dificilmente vão possuir alguma dependência com os pacotes do tidyverse já que a maioria deles não existia nesta época.\n\n\n\n\n\n\n\n\n\nVale lembrar que há três tipos de “dependência” entre pacotes no R: imports, depends e suggests. Tipicamente, se um pacote A usa algumas funções de outro pacote B, então o pacote A importa (imports) o pacote B. Isto é, ele assume que o usuário tenha o pacote B instalado. Já a relação depends é mais estrita: se um pacote A depends de um pacote C então os pacotes são carregados conjuntamente quando se chama library()4. Por fim se um pacote A usa um pacote D, em algum contexto específico, mas não requer que o usuário tenha o pacote D instalado, então o pacote A suggests o pacote D5.\nOlhando os dados por pacote vê-se que o ggplot2 e dplyr são os mais populares."
  },
  {
    "objectID": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html#a-filosofia-do-tidyverse",
    "href": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html#a-filosofia-do-tidyverse",
    "title": "A filosofia do tidyverse",
    "section": "A filosofia do tidyverse",
    "text": "A filosofia do tidyverse\nA filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes”6 que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático. A tarefa de renomear colunas, criar variáveis e calcular uma média nos grupos torna-se “linear” no mesmo sentido em que uma frase com sujeito-verbo-objeto é linear.\n\nPipes\nO pipe, essencialmente, carrega o resultado de uma função adiante numa cadeia de comandos: objeto |&gt; função1 |&gt; função2 |&gt; função3. Isto tem duas vantagens: primeiro, evita que você use funções compostas que são lidas “de dentro para fora” como exp(mean(log(x))); e, segundo, dispensa a criação de objetos intermediários “inúteis” que estão ali somente para segurar um valor que não vai ser utilizado mais adiante.\n\nmodel &lt;- lm(log(AirPassengers) ~ time(AirPassengers))\n\n#&gt; Função composta\nmean(exp(fitted(model)))\n#&gt; Usando pipes\nmodel |&gt; fitted() |&gt; exp() |&gt; mean()\n#&gt; Usando objetos intermediários\nx1 &lt;- fitted(model)\nx2 &lt;- exp(x1)\nx3 &lt;- mean(x2)\n\nNum contexto de manipulação de dados pode-se ter algo como o código abaixo.\n\ntab_vendas_cidade &lt;- dados |&gt;\n  #&gt; Renomeia colunas\n  rename(date = data, city = cidade, variable = vendas, value = valor) |&gt;\n  #&gt; Transforma colunas\n  mutate(\n    value = value / 1000,\n    date = readr::parse_date(\n      date,\n      format = \"%Y-%b%-d\",\n      locale = readr::locale(\"pt\")\n    ),\n    year = lubridate::year(date)\n  ) |&gt;\n  #&gt; Agrupa pela coluna year e calcula algumas estatísticas\n  group_by(year) |&gt;\n  summarise(\n    total = sum(value),\n    count = n()\n  )\n\nEm base-R o mesmo código ficaria algo como o descrito abaixo.\n\nnames(dados) &lt;- c(\"date\", \"city\", \"variable\", \"value\")\n\ndados$value &lt;- dados$value / 1000\ndados$date &lt;- readr::parse_date(\n  dados$date,\n  format = \"%Y-%b%-d\",\n  locale = readr::locale(\"pt\")\n)\ndados$year &lt;- lubridate::year(dados$date)\n\ntab_vendas_cidade &lt;- tapply(\n  dados$value,\n  dados$city,\n  \\(x) {\n    data.frame(total = sum(x), count = length(x))\n  }\n)\n\nHá um tempo atrás argumentava-se contra o uso de “pipes”, pois estes dificultavam a tarefa de encontrar bugs no código. Isto continua sendo parcialmente verdade, mas as funções do tidyvserse atualmente têm mensagens de erro bastante ricas e permitem encontrar a fonte do erro com relativa facilidade. Ainda assim, não se recomenda encadear funções em excesso, i.e., pipes com 10 funções ou mais7.\n\n\nFunções\nOutra filosofia do tidyverse é de que tarefas rotineiras devem ser transformadas em funções específicas. Neste sentido, os pacotes dplyr, tidyr e afins são recheados de funções, às vezes com nomes muito semelhantes e com usos redundantes. As funções starts_with e ends_with, por exemplo, são casos específicos da função matches. Há funções que permitem até duas formas de grafia como summarise e summarize. Outras como slice_min e slice_max são convenientes mas são literalmente: arrange + slice.\nSomando somente os dois principais pacotes, dplyr e tidyr, há 360 funções disponíveis. Contraste isto com o data.table que permite fazer 95% das transformações de dados somente com dt[i, j, by = c(), .SDcols = cols].\nMesmo as funções base do R costumam ser mais sucintas do que códigos em tidyverse. No exemplo abaixo, a função tapply consegue o mesmo resultado que o código mais extenso feito com dplyr.\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(avg = mean(cyl))\n\nAs vantagens do tidyverse se tornam mais evidentes com o tempo. De fato, o pacote permite abstrações muito poderosas, e eventualmente, pode-se fazer um código centenas de vezes mais sucinto combinando as suas funções. Em outros casos, as funções do tidyverse são simplesmente muito convenientes.\nTome a starts_with, por exemplo, que seleciona as colunas que começam de uma certa forma. Suponha uma tabela simples em que há múltiplas colunas cujos nomes começam com a letra “x”. O código em tidyverse é muito mais limpo que o código em base-R.\n\ndf &lt;- df |&gt;\n  select(date, starts_with(\"x\"))\n\ndf &lt;- df[, c(\"date\", names(df)[grep(\"^x\", names(df))])]\ndf &lt;- df[, c(\"date\", names(df)[stringr::str_detect(names(df), \"^x\")])]\n\nO exemplo abaixo é inspirado neste post, que mostra como calcular lags de uma série de tempo que esteja em um data.frame. Calcular defasagens de uma série de tempo é uma tarefa um pouco árdua quando se usa somente funções base. O código abaixo mostra não somente a elegância do tidyverse mas também a facilidade em se criar funções a partir do tidyverse.\n\ncalculate_lags &lt;- function(df, var, lags) {\n  map_lag &lt;- lags |&gt; map(~ partial(lag, n = .x))\n  out &lt;- df |&gt;\n    mutate(\n      across(.cols = {{ var }}, .fns = map_lag, .names = \"{.col}_lag{lags}\")\n    )\n\n  return(out)\n}\n\ndf &lt;- data.frame(\n  date = time(AirPassengers),\n  value = as.numeric(AirPassengers)\n)\n\ndf |&gt; calculate_lags(value, 1:3) |&gt; head()\n#       date value value_lag1 value_lag2 value_lag3\n# 1 1949.000   112         NA         NA         NA\n# 2 1949.083   118        112         NA         NA\n# 3 1949.167   132        118        112         NA\n# 4 1949.250   129        132        118        112\n# 5 1949.333   121        129        132        118\n# 6 1949.417   135        121        129        132\n\n\n\nDesvantagens\nO lado negativo da abordagem “gramatical” é que para não-falantes de inglês muitas destas vantagens são despercebidas8 e o resultado é somente um código “verborrágico”, cheio de funções. Além disso, pode-se argumentar que há ambiguidades inerentes na linguagem. A função filter, por exemplo, é utilizada para filtrar as linhas de um data.frame, mas podia, igualmente, chamar-se select, que selecionaria as linhas de um data.frame. A função select, contudo, é usada para selecionar as colunas de um data.frame.\nUm fato particularmente irritante do tidyverse é a frequência com que os pacotes mudam. Na maior parte das vezes, as mudanças são positivas, mas isto faz com que o código escrito em tidyverse não seja sustentável ao longo do tempo.\nEu demorei um bom tempo para entender as funções tidyr::gather e tidyr::spread e, atualmente, ambas foram descontinuadas e substituídas pelas funções pivot_longer e pivot_wider9. As funções mutate_if, mutate_at e similares do dplyr foram todas suprimidas pela sinataxe mais geral do across. A função tidyr::separate agora está sendo substituída por separate_wider_position e separate_wider_delim.\nMesmo um código bem escrito há poucos anos atrás tem grandes chances de não funcionar mais porque as funções foram alteradas ou descontinuadas. Em 2021, Wickham discutiu este problema abertamente numa palestra. Desde então, o tidyverse tem melhorado a sua política de manutenção de funções.\nA velocidade e eficiência das funções do tidyverse pode ser um problema, mas atualmente existem diversas boas soluções como o já citado tidytable. Particularmente, são raras as situações em que a velocidade do tidyverse me incomoda.\nAtualmente, parece haver um consenso crescente de que a melhor forma de começar a aprender R é começando pelo tidyverse; esta visão não é livre de críticos como de Norm Matloff, professor de estatística da UC Davis. Essencialmente, Matloff considera que o tidyverse é muito complexo para iniciantes: há muitas funções para se aprender e o incentivo à programação funcional torna o código muito abstrato. O tidyverse também esconde o uso do base-R e não ensina operadores básicos como [[ e $. Matloff também considera que “pipes” prejudicam o aprendizado pois dificultam a tarefa de encontrar a fonte dos erros no código."
  },
  {
    "objectID": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html#footnotes",
    "href": "posts/tutorial-tidyverse/2023-09-tidyverse/index.html#footnotes",
    "title": "A filosofia do tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs funções base gsub, grep e grepl, por exemplo seguem o padrão pattern, string. Já a função strsub usa o padrão string, pattern. Para mais diferenças entre as funções base para manipulação de texto e o stringr consulte este material.↩︎\nExistem diversos benchmarks que atestam os ganhos do dplyr em relação ao base-R. Veja, por exemplo, este comparativo. Apesar disto, o dplyr é menos eficiente que seu concorrente data.table. Existem algumas alternativas como dtplyr e, mais recentemente, tidytable, que fornecem a velocidade do data.table com a sintaxe do dplyr.↩︎\nSimilarmente ao dplyr, o readr também perde para seu concorrente data.table::fread. Contudo, o pacote vroom oferece uma alternativa mais veloz ao readr dentro do universo tidyverse.↩︎\nO pacote ggforce, por exemplo, depends do ggplot2. Isto significa que library(ggforce) automaticamente carrega o pacote ggplot2.↩︎\nEm geral, os pacotes sugeridos (suggests) são listados para os desenvolvedores do pacote ou utilizados para testes e exemplos. Você provavelmente já deve ter visto algum exemplo que começa com: if (require(\"pacote\")) { … }. O pacote nnet, por exemplo, não depende nem importa outros pacotes, mas utiliza o pacote MASS em seus exemplos; assim, o pacote nnet suggests o pacote MASS. Também existem casos onde um pacote tem mais capacidades ou melhor performance quando os pacotes sugeridos estão instalados; nestes casos, recomenda-se instalar o pacote com install.packages(dependencies = TRUE)`.↩︎\nPara saber mais sobre pipes e a diferença entre o novo pipe nativo |&gt; e o pipe |&gt; do magrittr veja meu post sobre o assunto.↩︎\nPara mais sobre pipes consulte o meu post sobre o assunto.↩︎\nNo fundo, isto é ainda mais um incentivo para aprender inglês.↩︎\nTecnicamente, elas foram “superseded”, ou suplatandas. Isto significa que elas continuam existindo exatamente da forma como sempre existiram e que não receberão mais atualizações.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-age-index/index.html",
    "href": "posts/general-posts/2024-04-wz-age-index/index.html",
    "title": "Índice de Envelhecimento no Brasil",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(patchwork)\nfont_add_google(\"Roboto Mono\", \"Roboto Mono\")\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\ncities_age &lt;- readr::read_rds(\n  here::here(\"static/data/census_aging_index_city.rds\")\n)\n\nlabels &lt;- c(\"Less than 25\", \"25 to 50\", \"50 to 75\", \"75 to 100\", \"100 or more\")\n\nmap2022 &lt;-\n  ggplot(cities_age) +\n  geom_sf(aes(fill = age_index_2022, color = age_index_2022)) +\n  scale_fill_fermenter(\n    name = \"\",\n    breaks = seq(0, 125, 25),\n    palette = \"Spectral\"\n  ) +\n  scale_color_fermenter(\n    name = \"\",\n    breaks = seq(0, 125, 25),\n    palette = \"Spectral\"\n  ) +\n  coord_sf(xlim = c(NA, -34.469802)) +\n  labs(\n    title = \"Aging Index in Brazil (2022)\",\n    subtitle = stringr::str_wrap(\n      \"The aging index shows the ratio of the elderly population (65 years or more) per 100 individuals in relation to the young population (14 years of younger).\",\n      80\n    )\n  ) +\n  ggthemes::theme_map(base_family = \"Roboto Condensed\") +\n  theme(\n    #plot.title = element_text(hjust = 0.5, size = 22),\n    #plot.subtitle = element_text(hjust = 0.5, size = 14),\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.5, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 38, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n  )\n\ntbl_age_index &lt;- tibble(\n  year = c(1991, 2000, 2010, 2022),\n  age_index = c(13.90, 16.18, 24.31, 55.24)\n)\n\nsparkline &lt;- ggplot(tbl_age_index, aes(year, age_index)) +\n  geom_hline(yintercept = 0) +\n  geom_point(color = \"#02818a\", size = 2) +\n  geom_line(color = \"#02818a\", lwd = 1) +\n  geom_text(\n    aes(label = age_index),\n    nudge_y = c(5, 5, 5, 5),\n    nudge_x = c(0, 0, -2, -1),\n    family = \"Roboto Condensed\",\n    size = 4\n  ) +\n  scale_x_continuous(breaks = c(1991, 2000, 2010, 2022)) +\n  labs(\n    title = \"Brazil is getting older\",\n    subtitle = \"Average Age Index (country)\",\n    x = NULL,\n    y = \"Age Index\"\n  ) +\n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.y = element_blank()\n  )\n\nfinal_map &lt;- map2022 + inset_element(sparkline, 0, 0.05, 0.4, 0.45)\n\n\n\nEnvelhecimento no Brasil\nO mapa abaixo mostra o índice de envelhecimento nas cidades brasileiras. Este índice compara a população idosa (65 anos ou mais) com a população jovem (14 anos ou menos). Os dados são do mais recente Censo do IBGE. Vale lembrar que valores maiores do que 100 indicam que há uma proporção maior de idosos do que jovens numa determinada cidade.\nO futuro demográfico do Brasil, em grande parte, já é conhecido. Assim como no resto do mundo, a combinação de queda de taxa de fecundidade e aumento de expectativa de vida implica no envelhecimento da população. Pelo índice de envelhecimento vê-se como esta tendência já é realidade em boa parte do território brasileiro.\nDe modo geral, as regiões mais envelhecidas são o Sul e Sudeste. O estado de Santa Catarina é uma exceção neste sentido. Algumas partes do Centro Oeste e do Nordeste também já apresentam índices de envelhecimento mais elevados do que a média do país.\n\n\n\n\n\n\n\n\n\n\nDados: IBGE, Censos Demográficos (1991, 2000, 2010, 2022)\nTipografia: Roboto Condensed\nPaleta: Spectral (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2025-01-top-posts-2024/index.html",
    "href": "posts/general-posts/2025-01-top-posts-2024/index.html",
    "title": "Melhores Posts de 2024",
    "section": "",
    "text": "Escrevi 47 posts em 2024, meu ano mais produtivo até agora. Comecei o ano tentando voltar às raízes do blog: tutoriais de R e posts sobre mercado imobiliário.\n\n\nEscrevi uma série completa sobre as inovações do tidyverse, focados nas principais funções do pacote {dplyr} (select, mutate, filter, rename). Séries de tempo também voltaram a ter um espaço de destaque. Primeiro, um post bastante completo sobre a extração de tendência e sazonalidade, um assunto importante que costuma ser pouco discutido nos cursos de econometria. Também escrevi dois posts técnicos sobre filtros lineares: médias móveis e o filtro HP. Neste último post também comentei sobre o recente filtro de Hamilton. Escrevi também sobre o algoritmo de Gradient Descent, post muito similar ao de otimização numérica de alguns anos atrás.\nNa segunda metade do ano, lancei tutoriais mais voltados para análises espaciais e visualização de dados. Mostrei como usar webscrape para geolocalizar todas as lojas da Starbucks e da The Coffee no Brasil; também mostrei como coletar dados do Google Maps e como criar mapas interativos usando leaflet. Fiz um post mostrando como importar dados de pdfs e depois apliquei isto para importar a série histórica de passageiros da linha-4 do metrô de São Paulo.\n\n\n\nDiscuti o mercado imobiliário brasileiro e de São Paulo em diversos posts. Publiquei uma análise abrangente sobre os atuais índices de preços imobiliários no Brasil e também uma análise histórica sobre a dinâmica do preço dos imóveis no país. Inspirado pelas mudanças do mercado no pós-pandemia, escrevi sobre o mercado de aluguel residencial e como este se descolou do mercado de venda após 2020. Voltei ao tema da acessibilidade à moradia em São Paulo num post mais enxuto (este post é muito inspirado neste outro texto, publicado na LARES em 2021).\n\n\n\nEste ano publiquei muitos mapas. Destaquei abaixo alguns dos principais. Primeiro, temos vários mapas sobre São Paulo\n\n\n\nO mapa de distribuição de renda combina os hexágonos H3 com a malha de setores censtiários do IBGE.\nO mapa da acessibilidade à moradia é uma atualização de meu trabalho anterior sobre o assunto.\n\n\n\n\n\n\n\n\n\n\n\n\n\nO mapa do IDH de São Paulo por regiões foi feito usando dados do Atlas Brasil. Este mapa também foi destaque no RGraphGallery, site de visualização de dados, que coleta as melhores visualizações usando R.\nO mapa dos domicílios foi feito em grid usando a base inédita do Cadastro Nacional de Endereços para Fins Estatísticos do IBGE.\n\n\n\n\n\n\n\n\n\n\nFiz também vários mapas sobre o Brasil."
  },
  {
    "objectID": "posts/general-posts/2025-01-top-posts-2024/index.html#o-ano-em-retrospectiva",
    "href": "posts/general-posts/2025-01-top-posts-2024/index.html#o-ano-em-retrospectiva",
    "title": "Melhores Posts de 2024",
    "section": "",
    "text": "Escrevi 47 posts em 2024, meu ano mais produtivo até agora. Comecei o ano tentando voltar às raízes do blog: tutoriais de R e posts sobre mercado imobiliário.\n\n\nEscrevi uma série completa sobre as inovações do tidyverse, focados nas principais funções do pacote {dplyr} (select, mutate, filter, rename). Séries de tempo também voltaram a ter um espaço de destaque. Primeiro, um post bastante completo sobre a extração de tendência e sazonalidade, um assunto importante que costuma ser pouco discutido nos cursos de econometria. Também escrevi dois posts técnicos sobre filtros lineares: médias móveis e o filtro HP. Neste último post também comentei sobre o recente filtro de Hamilton. Escrevi também sobre o algoritmo de Gradient Descent, post muito similar ao de otimização numérica de alguns anos atrás.\nNa segunda metade do ano, lancei tutoriais mais voltados para análises espaciais e visualização de dados. Mostrei como usar webscrape para geolocalizar todas as lojas da Starbucks e da The Coffee no Brasil; também mostrei como coletar dados do Google Maps e como criar mapas interativos usando leaflet. Fiz um post mostrando como importar dados de pdfs e depois apliquei isto para importar a série histórica de passageiros da linha-4 do metrô de São Paulo.\n\n\n\nDiscuti o mercado imobiliário brasileiro e de São Paulo em diversos posts. Publiquei uma análise abrangente sobre os atuais índices de preços imobiliários no Brasil e também uma análise histórica sobre a dinâmica do preço dos imóveis no país. Inspirado pelas mudanças do mercado no pós-pandemia, escrevi sobre o mercado de aluguel residencial e como este se descolou do mercado de venda após 2020. Voltei ao tema da acessibilidade à moradia em São Paulo num post mais enxuto (este post é muito inspirado neste outro texto, publicado na LARES em 2021).\n\n\n\nEste ano publiquei muitos mapas. Destaquei abaixo alguns dos principais. Primeiro, temos vários mapas sobre São Paulo\n\n\n\nO mapa de distribuição de renda combina os hexágonos H3 com a malha de setores censtiários do IBGE.\nO mapa da acessibilidade à moradia é uma atualização de meu trabalho anterior sobre o assunto.\n\n\n\n\n\n\n\n\n\n\n\n\n\nO mapa do IDH de São Paulo por regiões foi feito usando dados do Atlas Brasil. Este mapa também foi destaque no RGraphGallery, site de visualização de dados, que coleta as melhores visualizações usando R.\nO mapa dos domicílios foi feito em grid usando a base inédita do Cadastro Nacional de Endereços para Fins Estatísticos do IBGE.\n\n\n\n\n\n\n\n\n\n\nFiz também vários mapas sobre o Brasil."
  },
  {
    "objectID": "posts/general-posts/2025-01-top-posts-2024/index.html#melhores-posts",
    "href": "posts/general-posts/2025-01-top-posts-2024/index.html#melhores-posts",
    "title": "Melhores Posts de 2024",
    "section": "Melhores Posts",
    "text": "Melhores Posts\n\nAcesso a Hospitais e Leitos em São Paulo\nNeste post, mapeei a acessibilidade a hospitais e leitos em São Paulo. Para avaliar quantitativamente o nível de acessibilidade montei uma métrica bastante simples: o tempo mínimo necessário que se leva para chegar no hospital/leito mais próximo, considerando um deslocamento de bicicleta. //// \n\n\nCarros e Renda em São Paulo\nEste post analisa a relação entre a posse de automóveis e a renda domiciliar usando dados da Pesquisa Origem e Destino do Metrô de 2017. Um modelo de escolha discreta revela um impacto significativo da renda na decisão de possuir um carro. Idade, educação e filhos também são fatores importantes e que aumentam a probabilidade do domicílio ter um automóvel. O único fator encontrado que reduz a probabilidade do domicílio ter um automóvel é ele ser chefiado por uma mulher.\n\n\n\n\n\n\n\nÍndices de Preços Imobiliários no Brasil\nNeste post discuti a teoria sobre índices de preços imobiliários e apresento os principais índices disponíveis no Brasil.\n\n\n\n\n\n\n\nFinding all The Coffee Shops\n\n\n\n\n\n\n\nAnalfabetismo no Brasil"
  },
  {
    "objectID": "posts/general-posts/2025-01-top-posts-2024/index.html#posts-mais-compartilhados",
    "href": "posts/general-posts/2025-01-top-posts-2024/index.html#posts-mais-compartilhados",
    "title": "Melhores Posts de 2024",
    "section": "Posts mais compartilhados",
    "text": "Posts mais compartilhados\n\nLinha 4-Amarela do Metrô\nGDP in Brazil\nExpectativa de Vida em São Paulo\nDistribuição de Renda em São Paulo\nÍndices de Preços Imobiliários no Brasil"
  },
  {
    "objectID": "posts/general-posts/2025-01-top-posts-2024/index.html#tutoriais",
    "href": "posts/general-posts/2025-01-top-posts-2024/index.html#tutoriais",
    "title": "Melhores Posts de 2024",
    "section": "Tutoriais",
    "text": "Tutoriais\n\nTidyverse e R\n\nTidyverse: select\nTidyverse: filter\nTidyverse: mutate\nTidyverse: rename\nWebscraping Line-4 Metro (inglês)\nImportando dados em PDF no R\n\n\n\nSéries de Tempo\n\nGradient Descent\nFiltro HP e Filtro de Hamilton\nMédias Móveis\nTendência e Sazonalidade\n\n\n\nAnálise espacial e Mapas\n\nEncontrando Todos os Starbucks do Brasil\nMapas Interativos com Leaflet e R\nEnriquecendo e Coletando Dados do Google Maps\nFinding All Starbucks in Brazil (inglês)"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "",
    "text": "Neste post, vou mapear a acessibilidade a hospitais e leitos em São Paulo. Para avaliar quantitativamente o nível de acessibilidade vou montar uma métrica bastante simples: o tempo mínimo necessário que se leva para chegar no hospital/leito mais próximo, considerando um deslocamento de bicicleta1.\nPara tornar o problema tratável, divido a cidade em hexágonos, no padrão H3, em resolução 9. Esta resolução tem um tamanho aproximado de 1km2 e estratifica a cidade em cerca de 15 mil subáreas. Seria possível reduzir o número de hexágonos cruzando este grid com dados de população do Censo. Contudo, como os dados do Censo a nível de setor censitário estão consideravelmente defasados, utilizo o grid inteiro.\nPara cada hospital/leito, contruo isócronas de 10, 15, 20, 25 e 30 minutos, considerando um deslocamento de bicicleta. A partir desta métrica simples, pode-se construir refinamentos como ponderar o número de leitos acessíveis em relação a população que reside em cada região. Evidentemente, pode-se considerar também outros modos de transporte e intervalos de tempo."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#dados",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#dados",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Dados",
    "text": "Dados\nAs bases de dados são de livre acesso no site dados.gov.br. Especificamente, vamos usar a base de Cadastros de CNES e de Hospitais e Leitos. Normalmente, eu importaria estes dados usando webscrapping, mas neste caso acho mais simples baixar os arquivos manualmente. Como são apenas duas tabelas que precisam ser baixadas, acho que seria um exagero fazer um rotina para importar os dados via scraping. Além disso, olhando o código-fonte da página, o download é feito via uma URL dinâmica, o que exigiria o uso do RSublime, ou, alternativamente, o uso do BeautifulSoup em Python2.\n\ncnes = fread(\"cnes_estabelecimentos.csv\")\nleitos = fread(\"Leitos_2024.csv\", encoding = \"Latin-1\")"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#limpeza",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#limpeza",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Limpeza",
    "text": "Limpeza\nOs códigos abaixo mostram a rotina de limpeza necessária. Como os dados brutos são relativamente bem estruturados não é necessário muito esforço neste passo. O primeiro código abaixo seleciona apenas as colunas necessárias da base de Cadastro de CNES para São Paulo.\n\n# Simplifica e limpa o nome das colunas\ncnes = janitor::clean_names(cnes)\n# Vetor para renomear e selecionar as colunas\nsel_cols &lt;- c(\"code_cnes\", \"lng\", \"lat\")\n# Altera o nome das colunas\nsetnames(cnes, c(\"co_cnes\", \"nu_longitude\", \"nu_latitude\"), sel_cols)\n# Seleciona as colunas apenas para Sao Paulo\ncnes &lt;- cnes[co_ibge == 355030, ..sel_cols]\n\nO segundo código abaixo mostra os passos para limpar a base de hospitais e leitos. Por fim, as duas bases são combinadas via o identificador code_cnes.\n\n# Vetor para renomear as colunas\n\n# Remove o prefixo do nome das colunas\nnew_names &lt;- str_remove_all(names(leitos), \"^[A-Z]{2}_\")\n# Cria novos nomes paras as colunas\nnew_names &lt;- janitor::make_clean_names(new_names)\nsetnames(leitos, names(leitos), new_names)\nsetnames(leitos, c(\"cnes\", \"endereco\"), c(\"code_cnes\", \"numero\"))\n\n# Vetor para selecionar colunas\nsel_cols &lt;- c(\n  \"code_cnes\",\n  \"nome_estabelecimento\",\n  \"logradouro\",\n  \"numero\",\n  \"bairro\",\n  \"cep\",\n  \"leitos_existentes\"\n)\n\nleitos &lt;- leitos[uf == \"SP\" & municipio == \"SAO PAULO\", ..sel_cols]\n\ndat &lt;- merge(leitos, cnes, by = \"code_cnes\")"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#isócronas",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#isócronas",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Isócronas",
    "text": "Isócronas\nIsócronas são polígonos que representam áreas de alcance dentro de um período de tempo pré-determinado. Isto é, para um determinado ponto no espaço \\(s_{i}\\) a isócrona \\(I(s_{i}, m, t)\\) representa todos os pontos que se pode chegar, partindo de \\(s_{i}\\), usando o modo de transporte \\(m\\) em \\(t\\) minutos. Para este exemplo considero apenas deslocamentos de bicicleta em intervalos de 10, 15, 20, 25, e 30 minutos.\nO código abaixo exemplifica a construção de uma isócrona em torno de um hospital.\n\niso_test = osrmIsochrone(\n  dat[1, .(lng, lat)],\n  breaks = seq(from = 10, to = 30, by = 5),\n  osrm.profile = \"bike\"\n)\n\nmapview(iso_test, zcol = \"isomax\")\n\n\n\n\n\nAo todo, temos 225 pontos que representam um estabelecimento único. Ao todo, calcula-se 1125 isócronas.\n\nlocations &lt;- unique(dat[, .(code_cnes, lng, lat)])\nnrow(locations)\n\n[1] 225\n\n\n\ngeo_locations &lt;- st_as_sf(\n  locations[, .(lng, lat)],\n  coords = c(\"lng\", \"lat\"),\n  crs = 4326\n)\n\nmapview(geo_locations)\n\n\n\n\n\nO código abaixo importa as isócronas para todos os pontos. Note que este código leva um tempo considerável para rodar.\n\nget_isochrone = function(dat) {\n  iso = osrm::osrmIsochrone(\n    dat[, .(lng, lat)],\n    breaks = seq(from = 10, to = 30, by = 5),\n    osrm.profile = \"bike\"\n  )\n\n  return(iso)\n}\n\nlocations &lt;- split(locations, by = \"code_cnes\")\ngeo_locations &lt;- lapply(locations, get_isochrone)"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#grid",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#grid",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Grid",
    "text": "Grid\nPara montar o grid, uso o pacote h3jsr que implementa as funções da biblioteca H3 da Uber dentro do R (via JavaScript). Importo o shapefile da cidade de São Paulo do IBGE via pacote geobr.\nVale notar que é possível conseguir um grid H3 em resolução 9 diretamente do pacote aopdata::read_grid(), que oferece os dados do projeto de Acesso a Oportunidades do IPEA. Este atalho, contudo, está disponível apenas para as cidades que entraram no estudo. Assim, o código abaixo é mais geral, pois funciona para qualquer cidade do Brasil.\n\nlibrary(h3jsr)\nlibrary(geobr)\n\nspo = read_municipality(3550308, simplified = FALSE, showProgress = FALSE)\ngrid = polygon_to_cells(spo, res = 9, simple = FALSE)\n\npolyfill = polygon_to_cells(spo, res = 9, simple = FALSE)\nindex_h3 = unlist(str_split(unlist(polyfill$h3_addresses), \", \"))\n\ngrid = data.frame(\n  id_hex = index_h3\n)\n\nh3grid = cell_to_polygon(grid)\nh3grid = st_as_sf(h3grid, crs = 4326)\nh3grid$id_hex = index_h3\n\nFazendo a interseção espacial entre o grid H3 e as isócronas, calcula-se para cada hexágono o tempo mínimo necessário para chegar no hospital/leito mais próximo.\n\nisocronas &lt;- dplyr::bind_rows(geo_locations, .id = \"code_cnes\")\n\n\nmatch_h3_destination = function(dest) {\n  iso = isocronas |&gt;\n    dplyr::filter(code_cnes == dest)\n\n  hex_codes = lapply(1:5, \\(i) {\n    idhex = dplyr::filter(iso, id == i) |&gt;\n      h3jsr::polygon_to_cells(res = 9) |&gt;\n      unlist()\n\n    return(data.frame(id_hex = idhex))\n  })\n\n  names(hex_codes) = as.character(c(10, 15, 20, 25, 30))\n\n  h3iso = rbindlist(hex_codes, idcol = \"isomax\")\n}\n\ncnes_codes &lt;- unique(isocronas$code_cnes)\n\nod_table &lt;- parallel::mclapply(cnes_codes, match_h3_destination)\n\nAgora, para cada hexágono, encontra-se o leito/hospital mais próximo\n\n# Consolida tabela que faz o match das origens e destinos (hospitais)\nnames(od_table) &lt;- cnes_codes\nod &lt;- rbindlist(od_table, idcol = \"code_cnes\")\n# Encontra o hospital mais próximo para cada ponto\nod[, isomax := as.numeric(isomax)]\nmin_table &lt;- od[, .SD[which.min(isomax)], by = \"id_hex\"]\n\n# Grid H3\ndtgrid &lt;- setDT(st_drop_geometry(h3grid))\ntimetable &lt;- merge(dtgrid, min_table, by = \"id_hex\", all.x = TRUE)\n\n# Junta os dados com o grid\nacesso_saude &lt;- dplyr::left_join(h3grid, timetable, by = \"id_hex\")\n# Troca NAs por &gt; 30\nacesso_saude &lt;- acesso_saude |&gt;\n  dplyr::mutate(\n    isomax = ifelse(is.na(isomax), 30, isomax)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#resultado",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#resultado",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Resultado",
    "text": "Resultado\nO mapa abaixo mostra o resultado final. De maneira geral, a área central da cidade, o Centro Expandido, está relativamente bem atendido de hospitais numa distância de 10 a 15 minutos até o hospital mais próximo. As áreas periféricas da cidade tem indicadores bastante piores, contudo, vale notar que estas áreas potencialmente podem ser atendidas por hospitais em municípios vizinhos, que fazem conurbação com São Paulo.\nComo comentado inicialmente, esta métrica é bastante rudimentar. Uma melhoria interessante seria ponderar o número total de leitos disponíveis em relação a população atendida em cada região.\n\n\nCode\nggplot(acesso_saude) +\n  geom_sf(aes(fill = isomax, color = isomax)) +\n  scale_fill_distiller(\n    name = \"\",\n    palette = \"RdBu\",\n    labels = c(\"Até 10 min.\", \"15 min.\", \"20 min\", \"25 min\", \"30 min. ou mais\")\n  ) +\n  scale_color_distiller(\n    name = \"\",\n    palette = \"RdBu\",\n    labels = c(\"Até 10 min.\", \"15 min.\", \"20 min\", \"25 min\", \"30 min. ou mais\")\n  ) +\n  labs(\n    title = \"Acesso a Hospitais\",\n    subtitle = \"Tempo mínimo necessário para chegar ao hospital/leito mais próximo.\",\n    caption = \"Fonte: Open Source Routing Machine (osrm), Dados Abertos (Ministério da Saúde).\"\n  ) +\n  ggthemes::theme_map() +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.75, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 18, hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#footnotes",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#footnotes",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSeria possível também fazer o deslocamento de carro, mas, na minha experiência, o deslocamento de bicicleta aproxima bem o deslocamento de carro com trânsito. As isócronas de deslocamento de carro em São Paulo - sem ajustes - costumam ser bastante “otimistas” com relação às possibilidades de deslocamento.↩︎\nCaso fosse necessário seria possível combinar a rotina de importação em Python com a rotina de análise em R usando reticulate.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "Uma tarefa rotineira em pesquisa econômica é de encontrar tendências e ciclos em séries de tempo macroeconômicas. Dada uma série de tempo \\(y_t\\), tenta-se encontrar alguma decomposição que resulte em:\n\\[\ny_t = \\text{cycle}_t + \\text{trend}_t + \\text{remainder}_t\n\\]\nDiferentes teorias e abordagens foram levantadas para extair as tendências de curto e longo prazo de séries.\nDe maneira geral, há dois tipos de tendências: (1) tendências determinísticas e (2) tendências estocásticas. A forma mais simples de tendência determinística é de uma tendência temporal linear. Algo na forma\n\\[\ny_t = \\alpha_{0} + \\alpha_{1}t + u_{t}\n\\]\nNaturalmente, também é possível propor polinômios de ordem mais elevada para modelar a tendência acima. Contudo, não é usual usar polinômios maiores do que de grau 3, isto é,\n\\[\ny_t = \\alpha_{0} + \\alpha_{1}t + \\alpha_{2}t^2 + \\alpha_{3}t^3 + u_{t}\n\\]\nJá o exemplo mais simples de série com tendência estocástica é um random-walk\n\\[\ny_t = y_{t-1} + u_{t}\n\\]\nA depender do tipo de série que se considera, pode fazer sentido remover diferenças em ordens específicas. Na modelagem SARIMA, por exemplo, tira-se “diferenças sazonais”. No caso de uma série trimestral, por exemplo, pode fazer sentido tirar uma diferença trimestral\n\\[\nz_{t} = y_{t} - y_{t-4}\n\\]\n\n\nO gráfico abaixo mostra a produção de automóveis no Brasil entre 1993 e 2007. Os dados são da Anfavea e baixados no R via API do Banco Central. A série não foi ajustada sazonalmente.\n\n\nCode\nggplot(subcarros, aes(x = date, y = lcar)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  labs(\n    subtitle = \"Produção de automóveis e comerciais leves\",\n    x = NULL,\n    y = \"Unidades (log)\",\n    caption = \"Fonte: BCB\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra as séries sem tendência e o gráfico de autocorrelação do resíduo do ajuste. Note que é possível perceber visualmente a presença de uma componente sazonal que permanece na série. Os resíduos do ajuste com polinômio de terceiro grau ainda apresentam bastante autocorrelação, sugerindo que a série não é tendência-estacionária. O ajuste na primeira diferença parece ser mais adequado neste caso.\n\n\nCode\np1 &lt;- ggplot(subcarros, aes(x = date, y = resid_tt)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  labs(x = NULL, title = \"Tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np2 &lt;- ggplot(subcarros, aes(x = date, y = resid_sto)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  labs(x = NULL, title = \"Tendência estocástica I(1)\") +\n  theme_bw()\n\np3 &lt;- ggAcf(ts(subcarros$resid_tt, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np4 &lt;- ggAcf(ts(subcarros$resid_sto, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência I(1)\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\n\n\n\nUma abordagem bastante comum para encontrar a tendência/ciclo em séries macroeconômicas é o filtro Hodrick-Prescott (HP). O filtro HP separa uma série em suas componentes de tendência e ciclo. Sua formulação matemática baseia-se na minimização de uma função objetivo que busca encontrar uma estimativa suave da tendência subjacente, ao mesmo tempo em que penaliza variações abruptas.\n\n\nDada uma série temporal \\(y_t\\), o objetivo é encontrar uma tendência \\(g_t\\) que minimize a seguinte função objetivo:\n\\[\n\\min_{g_t} \\left[ \\sum_{t=1}^{T} \\left( y_t - g_t \\right)^2 + \\lambda \\sum_{t=2}^{T-1} \\left( g_{t+1} - 2g_t + g_{t-1} \\right)^2 \\right]\n\\]\nO primeiro termo representa a soma dos quadrados dos resíduos entre a série observada e a estimativa da tendência, enquanto o segundo termo é uma penalidade que desencoraja variações abruptas na tendência. O filtro HP suaviza a série observada, permitindo identificar movimentos de longo prazo enquanto remove flutuações de curto prazo. O parâmetro \\(\\lambda\\) desempenha um papel crucial na determinação do nível de suavização da tendência, com valores maiores resultando em tendências mais suaves.\nNote que se \\(\\lambda = 0\\) o valor ótimo de \\(g_{t}\\) é simplesmente \\(y_{t}\\). Quando \\(\\lambda \\to \\infty\\) o componente \\(g_{t}\\) se aproxima de uma tendência temporal linear. A escolha de \\(\\lambda\\) não é simples e é usual amparar-se em regras de bolso. Para séries trimestrais, costuma-se usar \\(\\lambda = 1600\\); para séries mensais \\(\\lambda = 14400\\)1.\nApesar de ter sido desenvolvida para séries macroeconômicas, não há problema, em princípio, em aplicar o filtro HP a qualquer tipo de série. O gráfico abaixo mostra o resultado da aplicação do filtro HP à série de produção de veículos.\n\n\nCode\ny &lt;- ts(log(subcarros$value), frequency = 12)\nhpy &lt;- hpfilter(y, freq = 14400, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\nggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(subtitle = \"Filtro HP: Série e Tendência\", x = NULL, y = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nO passeio aleatório é um processo estocástico comum em economia. Variáveis importantes como preços futuros, preços de ações, preços de petróleo, conusmo, inflação, e a taxa de crescimento do estoque de moeda podem ser modeladas como passeios aleatórios.\nSeja um passeio aleatório \\((1-L)y_{t} = u_{t}\\) com \\(T\\) observações e frequência trimestral, onde \\(L\\) é o operador defasagem usual: \\(Lx_{t} = x_{t-1}\\). Segundo Hamilton (2017), Quando se aplica o filtro HP sobre \\(y_{t}\\) encontra-se o seguinte termo de ciclo, \\(c_t\\)\n\\[\nc_{t} = \\frac{\\lambda(1-L)^3}{F(L)}u_{t+2}\n\\]\nUsando \\(\\lambda = 1600\\), a sugestão usual para séries trimestrais,\n\\[\nc_t = 89.72 \\{ - q_{0,t+2} + \\sum^{\\infty}_{j=0} (0.8941)^j[\\cos(0.1117j)+8.916\\sin(0.1117j)](q_{1,t+2-j} + q_{2,t+2+j}) \\}\n\\]\nonde\n\n\\(q_{0t} = \\varepsilon_t - 3\\varepsilon_{t-1} + 3\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\\(q_{1t} = \\varepsilon_t - 3.79\\varepsilon_{t-1} + 5.37\\varepsilon_{t-2} - 3.37\\varepsilon_{t-3} + 0.79\\varepsilon_{t-4}\\)\n\\(q_{2t} = -0.79\\varepsilon_{t+1} + 3.37\\varepsilon_t - 5.37\\varepsilon_{t-1} + 3.79\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\nque é uma expressão bastante longa. O importante a se notar é que o termo \\(c_{t}\\) tem uma estrutura recursiva em função das defasagens de \\(\\varepsilon_t\\). Note que a série subjacente é simplesmente uma soma de ruídos aleatórios sem nenhum padrão:\n\\[\n\\begin{align}\n(1-L)y_{t} & = \\varepsilon_{t} \\\\\ny_{t} & = \\frac{\\varepsilon_{t}}{(1-L)} \\\\\ny_{t} & = \\sum_{i = 0}^{\\infty}e_{i}\n\\end{align}\n\\]\nmas o ciclo desta série tem uma estrutura artifical, que foi acrescentada pelo uso do filtro HP. Em linhas gerais, a exposição acima resume o argumento de Hamilton contra o uso indiscriminado do filtro HP em séries macroeconômicas não-estacionárias. O filtro HP acaba “criando” uma estrutura nos dados que não existia previamente.\n\n\nCode\nset.seed(1984)\ny &lt;- ts(cumsum(rnorm(100)), frequency = 4)\n\nhpy &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  cycle = as.numeric(hpy$cycle),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\np1 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(\n    \"Filtro HP aplicado no Random Walk\",\n    subtitle = \"Série e Tendência\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_bw()\n\np2 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = cycle), lwd = 0.8, color = \"#023047\") +\n  labs(x = NULL, y = NULL, subtitle = \"Ciclo\") +\n  theme_bw()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\nPara um exemplo mais aplicado, vamos usar a base USMacroG do pacote {AER}. O código abaixo aplica o filtro HP sobre o logaritmo das séries trimestrais do PIB, consumo, investimento e M1. As séries são todas completas com 204 observações entre 1950 e 2000.\nO gráfico mostra o ajuste do filtro HP em cada uma das quatro séries.\n\n\nCode\nlibrary(AER)\ndata(\"USMacroG\")\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro &lt;- macro |&gt;\n  mutate(across(gdp:m1, log))\n\nnest_macro &lt;- macro |&gt;\n  select(date, gdp, consumption, invest, m1) |&gt;\n  pivot_longer(col = -date, names_to = \"name_series\") |&gt;\n  group_by(name_series) |&gt;\n  nest()\n\nmacro_hp &lt;- nest_macro |&gt;\n  mutate(\n    hpy = map(data, \\(x) {\n      y &lt;- ts(x$value, frequency = 4, start = c(1950, 1))\n      hp &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n\n      tibble(\n        cycle = as.numeric(hp$cycle),\n        trend = as.numeric(hp$trend)\n      )\n    })\n  ) |&gt;\n  unnest(c(data, hpy)) |&gt;\n  ungroup()\n\np1 &lt;- ggplot(macro_hp, aes(x = date, y = value)) +\n  geom_line(aes(y = value), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  facet_wrap(~name_series, scales = \"free_y\", ncol = 1) +\n  labs(x = NULL, y = NULL) +\n  theme_bw()\n\np2 &lt;- ggplot(macro_hp, aes(x = date, y = cycle)) +\n  geom_line(lwd = 0.8, color = \"#023047\") +\n  geom_hline(yintercept = 0) +\n  facet_wrap(~name_series, ncol = 1) +\n  labs(x = NULL, y = NULL) +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nA análise das autocorrelações mostra que há bastante autocorrelação cruzada entre as séries.\n\n\nCode\nmacro_detrend &lt;- macro_hp |&gt;\n  mutate(detrend = value - trend) |&gt;\n  pivot_wider(\n    id_cols = \"date\",\n    names_from = \"name_series\",\n    values_from = \"detrend\"\n  ) |&gt;\n  select(-date)\n\nggAcf(ts(macro_detrend, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: séries com filtro HP\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSe tivéssemos assumido, contudo, que as séries são I(1), não haveria muita correlação cruzada entre as séries.\n\n\nCode\nmacro_d1 &lt;- macro |&gt;\n  select(gdp, consumption, invest, m1) |&gt;\n  mutate(across(everything(), ~ c(NA, diff(.x))))\n\n\nggAcf(ts(macro_d1, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: primeira diferença das séries\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo alternativa ao filtro HP Hamilton (2017) sugere um procedimento bastante simples. Seja \\(y_t\\) uma série não-estacionária. Então fazemos uma regressão linear de \\(y_{t+h}\\) contra os \\(p\\) valores mais recentes.\n\\[\ny_{t+h} = \\beta_0 + \\beta_{1} y_{t} + \\beta_{2} y_{t-1} + \\beta_{3} y_{t-2} + \\beta_{4} y_{t-3} + u_{t+h}\n\\]\nPara o caso específico de dados trimestrais, ele sugere uma formulação do tipo:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nOu, alternativamente, um modelo ainda mais simples, em função apenas de \\(y_{t-8}\\) na forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + u_{t+h}\n\\]\nDe modo geral, Hamilton argumenta que este filtro garante a estacionaridade de séries, sendo elas tendência-estacionárias ou diferença-estacionárias. Mais detalhes sobre o filtro podem ser verificados no Working Paper.\n\n\nO primeiro passo necessário é preparar os dados. Novamente, vamos trabalhar com as séries num padrão retangular, usando um tibble. A base de dados utilizada é a USMacroG, descrita acima; neste caso, vamos adicionar as séries de gasto do governo e inflação. Isto é, temos agora seis séries: PIB, consumo, investimento, gasto do governo, inflação e M1. Todas as variáveis, exceto pela taxa de inflação, serão transformadas usando log, seguindo o paper de Hamilton.\nO código abaixo tem um trecho interessante onde se usa uma combinação de map e partial para criar várias colunas com os valores defasados das variáveis.\n\ndata(USMacroG)\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro_long &lt;- macro |&gt;\n  select(date, gdp, consumption, invest, m1, government, inflation) |&gt;\n  mutate(across(gdp:government, ~ log10(.x) * 100)) |&gt;\n  pivot_longer(-date, names_to = \"name_series\")\n\nmacro_lags &lt;- macro_long |&gt;\n  group_by(name_series) |&gt;\n  mutate(\n    across(value, map(1:12, ~ partial(dplyr::lag, n = .x)), .names = \"l{.fn}\")\n  )\n\n\n\n\nHamilton sugere duas maneiras de tirar a tendência dos dados: usando uma regressão linear e tirando uma diferença simples. No caso de séries trimestrais, o autor sugere \\(p=4\\) e \\(h=8\\). Assim, temos de estimar um modelo linear da forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nE a série livre de tendência, \\(z_t\\) será dada por:\n\\[\nz_{t} = y_{t} - \\hat{\\beta_0} - \\hat{\\beta_1}y_{t-8} - \\hat{\\beta_2}y_{t-9} - \\hat{\\beta_3}y_{t-10} - \\hat{\\beta_4}y_{t-11}\n\\]\nNo segundo “modelo”, vamos simplesmente tirar uma diferença na forma:\n\\[\nz_{t} = y_{t} - y_{t-8}\n\\]\nO primeiro modelo será chamado de “regression”, enquanto o segundo modelo será chamado de “Random Walk”. O código abaixo faz a regressão em todas as séries e grava os resultados do fit e os resíduos de todos os modelos.\n\nmacro_models &lt;- macro_lags |&gt;\n  mutate(resid_rw = value - l8) |&gt;\n  nest() |&gt;\n  mutate(\n    model_reg = map(data, \\(d) lm(value ~ l8 + l9 + l10 + l11, data = d)),\n    trend = map(model_reg, fitted),\n    resid = map(model_reg, residuals)\n  )\n\n\n\n\nIdealmente, deve-se verificar o comportamento dos resíduos do modelo de regressão. Neste caso, estes resíduos são considerados o componente cíclico da série original. O painel abaixo mostra os resíduos da primeira série, do PIB. O gráfico de cima é simplesmente o gráfico do resíduo. No gráfico inferior-esquerdo temos o gráfico de autocorrelação da série e no gráfico inferior-direito temos o historgrama dos resíduos com a densidade da distribuição normal superimposta.\nVale lembrar que não estamos buscando limpar a série totalmente de autocorrelação, como na modelagem ARIMA.\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 417.09, df = 10, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\nNULL\n\n\nEvidentemente, não é fácil inspecionar os resíduos de todos os modelos, especialmente quando o número de séries é muito grande. O painel abaixo mostra o gráfico de autocorrelação entre as seis séries consideradas. Na diagonal principal temos o gráfico do resíduo da própria série; nos demais gráficos, mostra-se a autocorrelação cruzada entre os resíduos.\n\n\n\n\n\n\n\n\n\n\n\n\nO código abaixo junta todas as séries para facilitar a sua visualização. Como há várias séries e muita repetição, crio uma função plot_series para facilitar a construção dos painéis.\n\n\nCode\nm1 &lt;- macro_models |&gt;\n  ungroup() |&gt;\n  select(name_series, trend, resid) |&gt;\n  unnest(cols = c(\"trend\", \"resid\")) |&gt;\n  mutate(id = as.numeric(names(trend)), .before = everything())\n\nmacro_trend &lt;- macro_models |&gt;\n  select(name_series, data) |&gt;\n  unnest(cols = c(data)) |&gt;\n  mutate(id = vctrs::vec_group_id(date), .before = everything()) |&gt;\n  ungroup()\n\nmacro_trend &lt;- macro_trend |&gt;\n  left_join(m1, by = c(\"id\", \"name_series\"))\n\nseries_trend &lt;- macro_trend |&gt;\n  select(date, name_series, trend, resid, resid_rw) |&gt;\n  pivot_longer(cols = -c(date, name_series), names_to = \"name_decomp\") |&gt;\n  mutate(\n    decomp = if_else(str_detect(name_decomp, \"trend\"), \"trend\", \"resid\")\n  ) |&gt;\n  filter(!is.na(value))\n\nplot_series &lt;- function(x, ylim = c(NA, NA)) {\n  p1 &lt;- ggplot() +\n    geom_line(\n      data = dplyr::filter(series_trend, name_series == x, decomp == \"trend\"),\n      aes(x = date, y = value),\n      lwd = 0.8,\n      color = \"steelblue\"\n    ) +\n    labs(x = NULL, y = NULL) +\n    ggtitle(glue::glue(\"Series: {x}\")) +\n    theme_bw() +\n    theme(plot.title = element_text(size = 10))\n\n  p2 &lt;- ggplot() +\n    geom_hline(yintercept = 0) +\n    geom_line(\n      data = dplyr::filter(series_trend, name_series == x, decomp == \"resid\"),\n      aes(x = date, y = value, color = name_decomp),\n      lwd = 0.8\n    ) +\n    scale_x_date(\n      breaks = seq(\n        as.Date(\"1950-01-01\"),\n        as.Date(\"2000-01-01\"),\n        by = \"10 year\"\n      ),\n      date_labels = \"%Y\"\n    ) +\n    scale_y_continuous(limits = ylim) +\n    scale_color_manual(\n      name = \"\",\n      values = c(\"#003049\", \"#d62828\"),\n      labels = c(\"Regression\", \"Random Walk\")\n    ) +\n    labs(x = NULL, y = NULL) +\n    ggtitle(glue::glue(\"Cyclical Component: {x}\")) +\n    theme_bw() +\n    theme(legend.position = \"top\", plot.title = element_text(size = 10))\n\n  return(list(trend = p1, cycle = p2))\n}\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de PIB e de consumo.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de investimento e gasto do governo.\n\n\n\n\n\n\n\n\n\nFinalmente, o gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de M1 e de inflação.\n\n\n\n\n\n\n\n\n\nO filtro proposto por Hamilton tem diversas características interessantes. A simplicidade e intuição do filtro, em especial, são pontos atrativos. Um paper recente, de Viv Hall e Peter Thompson (2021), contudo, não encontrou grandes vantagens deste filtro em relação aos tradicionais filtro HP e filtro Baxter-King. Os autores testam os filtros usando um amplo conjunto de séries macroeconômicas da Nova Zelândia. De maneira geral, o filtro de Hamilton gerou séries com maior volatilidade e pior capacidade preditiva. Considerando a literatura de filtros, parece improvável, de fato, que um filtro tão simples possa ter uma melhor performance do que filtros no domínio da frequência."
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#produção-de-automóveis",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#produção-de-automóveis",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "O gráfico abaixo mostra a produção de automóveis no Brasil entre 1993 e 2007. Os dados são da Anfavea e baixados no R via API do Banco Central. A série não foi ajustada sazonalmente.\n\n\nCode\nggplot(subcarros, aes(x = date, y = lcar)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  labs(\n    subtitle = \"Produção de automóveis e comerciais leves\",\n    x = NULL,\n    y = \"Unidades (log)\",\n    caption = \"Fonte: BCB\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra as séries sem tendência e o gráfico de autocorrelação do resíduo do ajuste. Note que é possível perceber visualmente a presença de uma componente sazonal que permanece na série. Os resíduos do ajuste com polinômio de terceiro grau ainda apresentam bastante autocorrelação, sugerindo que a série não é tendência-estacionária. O ajuste na primeira diferença parece ser mais adequado neste caso.\n\n\nCode\np1 &lt;- ggplot(subcarros, aes(x = date, y = resid_tt)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  labs(x = NULL, title = \"Tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np2 &lt;- ggplot(subcarros, aes(x = date, y = resid_sto)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  labs(x = NULL, title = \"Tendência estocástica I(1)\") +\n  theme_bw()\n\np3 &lt;- ggAcf(ts(subcarros$resid_tt, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np4 &lt;- ggAcf(ts(subcarros$resid_sto, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência I(1)\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#filtro-hp",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#filtro-hp",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "Uma abordagem bastante comum para encontrar a tendência/ciclo em séries macroeconômicas é o filtro Hodrick-Prescott (HP). O filtro HP separa uma série em suas componentes de tendência e ciclo. Sua formulação matemática baseia-se na minimização de uma função objetivo que busca encontrar uma estimativa suave da tendência subjacente, ao mesmo tempo em que penaliza variações abruptas.\n\n\nDada uma série temporal \\(y_t\\), o objetivo é encontrar uma tendência \\(g_t\\) que minimize a seguinte função objetivo:\n\\[\n\\min_{g_t} \\left[ \\sum_{t=1}^{T} \\left( y_t - g_t \\right)^2 + \\lambda \\sum_{t=2}^{T-1} \\left( g_{t+1} - 2g_t + g_{t-1} \\right)^2 \\right]\n\\]\nO primeiro termo representa a soma dos quadrados dos resíduos entre a série observada e a estimativa da tendência, enquanto o segundo termo é uma penalidade que desencoraja variações abruptas na tendência. O filtro HP suaviza a série observada, permitindo identificar movimentos de longo prazo enquanto remove flutuações de curto prazo. O parâmetro \\(\\lambda\\) desempenha um papel crucial na determinação do nível de suavização da tendência, com valores maiores resultando em tendências mais suaves.\nNote que se \\(\\lambda = 0\\) o valor ótimo de \\(g_{t}\\) é simplesmente \\(y_{t}\\). Quando \\(\\lambda \\to \\infty\\) o componente \\(g_{t}\\) se aproxima de uma tendência temporal linear. A escolha de \\(\\lambda\\) não é simples e é usual amparar-se em regras de bolso. Para séries trimestrais, costuma-se usar \\(\\lambda = 1600\\); para séries mensais \\(\\lambda = 14400\\)1.\nApesar de ter sido desenvolvida para séries macroeconômicas, não há problema, em princípio, em aplicar o filtro HP a qualquer tipo de série. O gráfico abaixo mostra o resultado da aplicação do filtro HP à série de produção de veículos.\n\n\nCode\ny &lt;- ts(log(subcarros$value), frequency = 12)\nhpy &lt;- hpfilter(y, freq = 14400, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\nggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(subtitle = \"Filtro HP: Série e Tendência\", x = NULL, y = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nO passeio aleatório é um processo estocástico comum em economia. Variáveis importantes como preços futuros, preços de ações, preços de petróleo, conusmo, inflação, e a taxa de crescimento do estoque de moeda podem ser modeladas como passeios aleatórios.\nSeja um passeio aleatório \\((1-L)y_{t} = u_{t}\\) com \\(T\\) observações e frequência trimestral, onde \\(L\\) é o operador defasagem usual: \\(Lx_{t} = x_{t-1}\\). Segundo Hamilton (2017), Quando se aplica o filtro HP sobre \\(y_{t}\\) encontra-se o seguinte termo de ciclo, \\(c_t\\)\n\\[\nc_{t} = \\frac{\\lambda(1-L)^3}{F(L)}u_{t+2}\n\\]\nUsando \\(\\lambda = 1600\\), a sugestão usual para séries trimestrais,\n\\[\nc_t = 89.72 \\{ - q_{0,t+2} + \\sum^{\\infty}_{j=0} (0.8941)^j[\\cos(0.1117j)+8.916\\sin(0.1117j)](q_{1,t+2-j} + q_{2,t+2+j}) \\}\n\\]\nonde\n\n\\(q_{0t} = \\varepsilon_t - 3\\varepsilon_{t-1} + 3\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\\(q_{1t} = \\varepsilon_t - 3.79\\varepsilon_{t-1} + 5.37\\varepsilon_{t-2} - 3.37\\varepsilon_{t-3} + 0.79\\varepsilon_{t-4}\\)\n\\(q_{2t} = -0.79\\varepsilon_{t+1} + 3.37\\varepsilon_t - 5.37\\varepsilon_{t-1} + 3.79\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\nque é uma expressão bastante longa. O importante a se notar é que o termo \\(c_{t}\\) tem uma estrutura recursiva em função das defasagens de \\(\\varepsilon_t\\). Note que a série subjacente é simplesmente uma soma de ruídos aleatórios sem nenhum padrão:\n\\[\n\\begin{align}\n(1-L)y_{t} & = \\varepsilon_{t} \\\\\ny_{t} & = \\frac{\\varepsilon_{t}}{(1-L)} \\\\\ny_{t} & = \\sum_{i = 0}^{\\infty}e_{i}\n\\end{align}\n\\]\nmas o ciclo desta série tem uma estrutura artifical, que foi acrescentada pelo uso do filtro HP. Em linhas gerais, a exposição acima resume o argumento de Hamilton contra o uso indiscriminado do filtro HP em séries macroeconômicas não-estacionárias. O filtro HP acaba “criando” uma estrutura nos dados que não existia previamente.\n\n\nCode\nset.seed(1984)\ny &lt;- ts(cumsum(rnorm(100)), frequency = 4)\n\nhpy &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  cycle = as.numeric(hpy$cycle),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\np1 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(\n    \"Filtro HP aplicado no Random Walk\",\n    subtitle = \"Série e Tendência\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_bw()\n\np2 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = cycle), lwd = 0.8, color = \"#023047\") +\n  labs(x = NULL, y = NULL, subtitle = \"Ciclo\") +\n  theme_bw()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\nPara um exemplo mais aplicado, vamos usar a base USMacroG do pacote {AER}. O código abaixo aplica o filtro HP sobre o logaritmo das séries trimestrais do PIB, consumo, investimento e M1. As séries são todas completas com 204 observações entre 1950 e 2000.\nO gráfico mostra o ajuste do filtro HP em cada uma das quatro séries.\n\n\nCode\nlibrary(AER)\ndata(\"USMacroG\")\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro &lt;- macro |&gt;\n  mutate(across(gdp:m1, log))\n\nnest_macro &lt;- macro |&gt;\n  select(date, gdp, consumption, invest, m1) |&gt;\n  pivot_longer(col = -date, names_to = \"name_series\") |&gt;\n  group_by(name_series) |&gt;\n  nest()\n\nmacro_hp &lt;- nest_macro |&gt;\n  mutate(\n    hpy = map(data, \\(x) {\n      y &lt;- ts(x$value, frequency = 4, start = c(1950, 1))\n      hp &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n\n      tibble(\n        cycle = as.numeric(hp$cycle),\n        trend = as.numeric(hp$trend)\n      )\n    })\n  ) |&gt;\n  unnest(c(data, hpy)) |&gt;\n  ungroup()\n\np1 &lt;- ggplot(macro_hp, aes(x = date, y = value)) +\n  geom_line(aes(y = value), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  facet_wrap(~name_series, scales = \"free_y\", ncol = 1) +\n  labs(x = NULL, y = NULL) +\n  theme_bw()\n\np2 &lt;- ggplot(macro_hp, aes(x = date, y = cycle)) +\n  geom_line(lwd = 0.8, color = \"#023047\") +\n  geom_hline(yintercept = 0) +\n  facet_wrap(~name_series, ncol = 1) +\n  labs(x = NULL, y = NULL) +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nA análise das autocorrelações mostra que há bastante autocorrelação cruzada entre as séries.\n\n\nCode\nmacro_detrend &lt;- macro_hp |&gt;\n  mutate(detrend = value - trend) |&gt;\n  pivot_wider(\n    id_cols = \"date\",\n    names_from = \"name_series\",\n    values_from = \"detrend\"\n  ) |&gt;\n  select(-date)\n\nggAcf(ts(macro_detrend, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: séries com filtro HP\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSe tivéssemos assumido, contudo, que as séries são I(1), não haveria muita correlação cruzada entre as séries.\n\n\nCode\nmacro_d1 &lt;- macro |&gt;\n  select(gdp, consumption, invest, m1) |&gt;\n  mutate(across(everything(), ~ c(NA, diff(.x))))\n\n\nggAcf(ts(macro_d1, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: primeira diferença das séries\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#hamilton",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#hamilton",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "Como alternativa ao filtro HP Hamilton (2017) sugere um procedimento bastante simples. Seja \\(y_t\\) uma série não-estacionária. Então fazemos uma regressão linear de \\(y_{t+h}\\) contra os \\(p\\) valores mais recentes.\n\\[\ny_{t+h} = \\beta_0 + \\beta_{1} y_{t} + \\beta_{2} y_{t-1} + \\beta_{3} y_{t-2} + \\beta_{4} y_{t-3} + u_{t+h}\n\\]\nPara o caso específico de dados trimestrais, ele sugere uma formulação do tipo:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nOu, alternativamente, um modelo ainda mais simples, em função apenas de \\(y_{t-8}\\) na forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + u_{t+h}\n\\]\nDe modo geral, Hamilton argumenta que este filtro garante a estacionaridade de séries, sendo elas tendência-estacionárias ou diferença-estacionárias. Mais detalhes sobre o filtro podem ser verificados no Working Paper.\n\n\nO primeiro passo necessário é preparar os dados. Novamente, vamos trabalhar com as séries num padrão retangular, usando um tibble. A base de dados utilizada é a USMacroG, descrita acima; neste caso, vamos adicionar as séries de gasto do governo e inflação. Isto é, temos agora seis séries: PIB, consumo, investimento, gasto do governo, inflação e M1. Todas as variáveis, exceto pela taxa de inflação, serão transformadas usando log, seguindo o paper de Hamilton.\nO código abaixo tem um trecho interessante onde se usa uma combinação de map e partial para criar várias colunas com os valores defasados das variáveis.\n\ndata(USMacroG)\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro_long &lt;- macro |&gt;\n  select(date, gdp, consumption, invest, m1, government, inflation) |&gt;\n  mutate(across(gdp:government, ~ log10(.x) * 100)) |&gt;\n  pivot_longer(-date, names_to = \"name_series\")\n\nmacro_lags &lt;- macro_long |&gt;\n  group_by(name_series) |&gt;\n  mutate(\n    across(value, map(1:12, ~ partial(dplyr::lag, n = .x)), .names = \"l{.fn}\")\n  )\n\n\n\n\nHamilton sugere duas maneiras de tirar a tendência dos dados: usando uma regressão linear e tirando uma diferença simples. No caso de séries trimestrais, o autor sugere \\(p=4\\) e \\(h=8\\). Assim, temos de estimar um modelo linear da forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nE a série livre de tendência, \\(z_t\\) será dada por:\n\\[\nz_{t} = y_{t} - \\hat{\\beta_0} - \\hat{\\beta_1}y_{t-8} - \\hat{\\beta_2}y_{t-9} - \\hat{\\beta_3}y_{t-10} - \\hat{\\beta_4}y_{t-11}\n\\]\nNo segundo “modelo”, vamos simplesmente tirar uma diferença na forma:\n\\[\nz_{t} = y_{t} - y_{t-8}\n\\]\nO primeiro modelo será chamado de “regression”, enquanto o segundo modelo será chamado de “Random Walk”. O código abaixo faz a regressão em todas as séries e grava os resultados do fit e os resíduos de todos os modelos.\n\nmacro_models &lt;- macro_lags |&gt;\n  mutate(resid_rw = value - l8) |&gt;\n  nest() |&gt;\n  mutate(\n    model_reg = map(data, \\(d) lm(value ~ l8 + l9 + l10 + l11, data = d)),\n    trend = map(model_reg, fitted),\n    resid = map(model_reg, residuals)\n  )\n\n\n\n\nIdealmente, deve-se verificar o comportamento dos resíduos do modelo de regressão. Neste caso, estes resíduos são considerados o componente cíclico da série original. O painel abaixo mostra os resíduos da primeira série, do PIB. O gráfico de cima é simplesmente o gráfico do resíduo. No gráfico inferior-esquerdo temos o gráfico de autocorrelação da série e no gráfico inferior-direito temos o historgrama dos resíduos com a densidade da distribuição normal superimposta.\nVale lembrar que não estamos buscando limpar a série totalmente de autocorrelação, como na modelagem ARIMA.\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 417.09, df = 10, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\nNULL\n\n\nEvidentemente, não é fácil inspecionar os resíduos de todos os modelos, especialmente quando o número de séries é muito grande. O painel abaixo mostra o gráfico de autocorrelação entre as seis séries consideradas. Na diagonal principal temos o gráfico do resíduo da própria série; nos demais gráficos, mostra-se a autocorrelação cruzada entre os resíduos.\n\n\n\n\n\n\n\n\n\n\n\n\nO código abaixo junta todas as séries para facilitar a sua visualização. Como há várias séries e muita repetição, crio uma função plot_series para facilitar a construção dos painéis.\n\n\nCode\nm1 &lt;- macro_models |&gt;\n  ungroup() |&gt;\n  select(name_series, trend, resid) |&gt;\n  unnest(cols = c(\"trend\", \"resid\")) |&gt;\n  mutate(id = as.numeric(names(trend)), .before = everything())\n\nmacro_trend &lt;- macro_models |&gt;\n  select(name_series, data) |&gt;\n  unnest(cols = c(data)) |&gt;\n  mutate(id = vctrs::vec_group_id(date), .before = everything()) |&gt;\n  ungroup()\n\nmacro_trend &lt;- macro_trend |&gt;\n  left_join(m1, by = c(\"id\", \"name_series\"))\n\nseries_trend &lt;- macro_trend |&gt;\n  select(date, name_series, trend, resid, resid_rw) |&gt;\n  pivot_longer(cols = -c(date, name_series), names_to = \"name_decomp\") |&gt;\n  mutate(\n    decomp = if_else(str_detect(name_decomp, \"trend\"), \"trend\", \"resid\")\n  ) |&gt;\n  filter(!is.na(value))\n\nplot_series &lt;- function(x, ylim = c(NA, NA)) {\n  p1 &lt;- ggplot() +\n    geom_line(\n      data = dplyr::filter(series_trend, name_series == x, decomp == \"trend\"),\n      aes(x = date, y = value),\n      lwd = 0.8,\n      color = \"steelblue\"\n    ) +\n    labs(x = NULL, y = NULL) +\n    ggtitle(glue::glue(\"Series: {x}\")) +\n    theme_bw() +\n    theme(plot.title = element_text(size = 10))\n\n  p2 &lt;- ggplot() +\n    geom_hline(yintercept = 0) +\n    geom_line(\n      data = dplyr::filter(series_trend, name_series == x, decomp == \"resid\"),\n      aes(x = date, y = value, color = name_decomp),\n      lwd = 0.8\n    ) +\n    scale_x_date(\n      breaks = seq(\n        as.Date(\"1950-01-01\"),\n        as.Date(\"2000-01-01\"),\n        by = \"10 year\"\n      ),\n      date_labels = \"%Y\"\n    ) +\n    scale_y_continuous(limits = ylim) +\n    scale_color_manual(\n      name = \"\",\n      values = c(\"#003049\", \"#d62828\"),\n      labels = c(\"Regression\", \"Random Walk\")\n    ) +\n    labs(x = NULL, y = NULL) +\n    ggtitle(glue::glue(\"Cyclical Component: {x}\")) +\n    theme_bw() +\n    theme(legend.position = \"top\", plot.title = element_text(size = 10))\n\n  return(list(trend = p1, cycle = p2))\n}\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de PIB e de consumo.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de investimento e gasto do governo.\n\n\n\n\n\n\n\n\n\nFinalmente, o gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de M1 e de inflação.\n\n\n\n\n\n\n\n\n\nO filtro proposto por Hamilton tem diversas características interessantes. A simplicidade e intuição do filtro, em especial, são pontos atrativos. Um paper recente, de Viv Hall e Peter Thompson (2021), contudo, não encontrou grandes vantagens deste filtro em relação aos tradicionais filtro HP e filtro Baxter-King. Os autores testam os filtros usando um amplo conjunto de séries macroeconômicas da Nova Zelândia. De maneira geral, o filtro de Hamilton gerou séries com maior volatilidade e pior capacidade preditiva. Considerando a literatura de filtros, parece improvável, de fato, que um filtro tão simples possa ter uma melhor performance do que filtros no domínio da frequência."
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#footnotes",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#footnotes",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara uma discussão sobre a escolha de \\(\\lambda\\) em séries macroeconômicas, veja Ravn e Uhlig (2002)↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/index.html",
    "href": "posts/general-posts/2023-12-wz-inflation/index.html",
    "title": "Weekly Viz: Brazilian Inflation",
    "section": "",
    "text": "Brazil has a long and painful story with inflation. Though technically Brazilian inflation only classified as a hyperinflation during a brief period in the early 1990s1, few countries had to endure such a long-standing period of prolonged and relentless inflation. The chart below shows the monthly percentage change in Brazil’s official consumer price index IPCA. I highlight some important economic and institutional events. Chiefs of the executive are also shaded in through different colors. It’s important to note that Figueiredo was not a constitutionally elected president, but rather the last leader of the Military Regime.\nThis visualization is heavily inspired by the graphic from the book Saga Brasileira: a longa luta de um povo por sua moeda2, by Miriam Leitão. Total inflation during the “hyperinflation” period surpassed 13 trillion percent! In July of 1994 the ingeniously designed Plano Real stabilizes Brazil’s economy and ends a long period of persistently high inflation. The Brazil post-Plano Real represents a paradigm shift, yet it is not entirely immune to inflation. In the nearly 30 years since the implementation of Plano Real, Brazil has witnessed a cumulative inflation of 560%.\nA nuance that is not captured by the graphic is the frequency of currency changes during this era3. Nearly every new economic plan brought about either a change in currency or a price/wage freeze. The dips in the plot coincide with theses moments, marking brief lapses of time when inflation seemed to magically subside, only to return with renewed vigor.\nOBS: to better see the plot: right-click &gt; “Open image in new tab”.\n\n\n\n\n\n\n\nEven after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Two significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the subsequent decade, the looming threat of inflation resurfaced. Facing a slowing economy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices were increased drastically causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer assumed office.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence came in early 2021. The Covid-19 inflation surge was tackled with inordinary high interest rates: in fact, for many months Brazil had the highest real interest rate in the world! After nearly a year of tight monetary policy, the Brazilian Central Bank started to cut interest rates as inflation converged to its long-run target."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/index.html#post-plano-real",
    "href": "posts/general-posts/2023-12-wz-inflation/index.html#post-plano-real",
    "title": "Weekly Viz: Brazilian Inflation",
    "section": "",
    "text": "Even after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Two significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the subsequent decade, the looming threat of inflation resurfaced. Facing a slowing economy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices were increased drastically causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer assumed office.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence came in early 2021. The Covid-19 inflation surge was tackled with inordinary high interest rates: in fact, for many months Brazil had the highest real interest rate in the world! After nearly a year of tight monetary policy, the Brazilian Central Bank started to cut interest rates as inflation converged to its long-run target."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/index.html#footnotes",
    "href": "posts/general-posts/2023-12-wz-inflation/index.html#footnotes",
    "title": "Weekly Viz: Brazilian Inflation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Hyperinflation_in_Brazil↩︎\nLoosely translated: “The Brazilian Saga: the long struggle of a people for its currency”.↩︎\nBefore 1986, Brazil’s currency was the Cruzeiro, though it had been “updated” several times to accommodate inflation. After the Cruzado Plan, the official currency became the “Cruzado”. In 1989, it transitioned to the “Cruzado Novo” (New Cruzado), following the Verão Plan. With the Collor Plan, in 1990, came a new currency that borrowed the old “Cruzeiro” name. Ultimately, in the lead-up to the Real, Brazil introduced a “virtual currency” named URV and the Cruzeiro was renamed as “Cruzeiro Real”.↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html",
    "title": "OLS com matrizes",
    "section": "",
    "text": "Uma forma instrutiva de entender o modelo de regressão linear é expressando ele em forma matricial. Os cursos introdutórios de econometria costumam omitir esta abordagem e expressam todas as derivações usando somatórios, deixando a abordagem matricial para cursos mais avançados. É bastante simples computar uma regressão usando apenas matrizes no R.\nDe fato, um dos objetos fundamentais do R é a e muitas das operações matriciais (decomposições, inversa, transposta, etc.) já estão implementadas em funções base. Uma var \\(k\\) .\nNeste post vou mostrar como fazer uma regressão linear usando somente matrizes no R. Além disso, vou computar algumas estatísticas típicas (t, F)\nO modelo linear é da forma\n\\[\ny_{t} = x^\\intercal_{t} \\beta + e_{t}\n\\]\nonde \\(x^\\intercal\\) é o vetor transposto de \\(x\\) . É importante sempre ter em mente a dimensão destes vetores. O vetor \\(y_{t}\\) é \\(n\\times1\\) onde \\(n\\) representa o número de observações na amostra. O vetor \\(\\beta\\) é \\(k\\times1\\) onde \\(k\\) é o número de regressores (ou variáveis explicativas). Como há \\(n\\) observações para cada uma dos \\(k\\) regressores, \\(x\\) é \\(k\\times1\\) ; o detalhe é que \\(x = (1 \\, \\,x_{1} \\, \\dots \\,x_{k-1})\\) , onde cada \\(x_{i}\\) é \\(n\\times 1\\) e \\(1\\) é um vetor de uns \\(n\\times1\\) . Finalmente, \\(e_{t}\\) é \\(n\\times1\\) . Temos então que:\n\\[\n\\begin{pmatrix}\ny_{1} \\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}=\n\\begin{pmatrix}\n1\\\\\nx_{1}\\\\\n\\vdots\\\\\nx_{k-1}\n\\end{pmatrix}^\\intercal\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k-1}\n\\end{pmatrix}+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n\\]\nonde:\n\\[\n\\begin{bmatrix}\nx_{1} & = (x_{11} & x_{12} & x_{13} & \\dots & x_{1n})\\\\\nx_{2} & = (x_{21} & x_{22} & x_{23} & \\dots & x_{2n})\\\\\nx_{3} & = (x_{31} & x_{32} & x_{33} & \\dots & x_{3n})\\\\\n\\vdots\\\\\nx_{k-1} & = (x_{(k-1)1} & x_{(k-1)2} & x_{(k-1)3} & \\dots & x_{(k-1)n})\n\\end{bmatrix}\n\\]\nQueremos encontrar o vetor \\(\\hat{\\beta}\\) que minimiza o a soma do quadrado dos erros, isto é, que minimiza\n\\[\nS(\\beta) = \\sum_{t = 1}^{T}(y_{t} - x^\\intercal_{t}\\beta)^{2}\n\\]\nEncontramos o ponto crítico derivando a expressão acima e igualando-a a zero. O resultado é o conhecido estimador de mínimos quadrados:\n\\[\n\\hat{\\beta} = \\left ( \\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} \\right )^{-1} \\sum_{t = 1}^{T}x_{t}y_{t}\n\\]\nPara reescrever as equações acima usando matrizes usamos os seguintes fatos:\n\\[\n\\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} = X^\\intercal X\n\\]\nonde \\(X\\) é uma matriz \\(n \\times k\\)\n\\[\n\\sum_{t = 1}^{T}x_{t}y_{t} = X^\\intercal y\n\\]\nonde \\(X^\\intercal y\\) é \\(k \\times 1\\) . Lembre-se que uma hipótese do modelo linear é de que \\(X\\) é uma matriz de posto completo, logo \\(X^\\intercal X\\) possui inversa e podemos escrever:\n\\[\n\\hat{\\beta} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "title": "OLS com matrizes",
    "section": "Exemplo: salário (Wooldridge)",
    "text": "Exemplo: salário (Wooldridge)\nComo exemplo vou usar um exemplo clássico de livro texto de econometria: uma regressão de salário (rendimento) contra algumas variáveis explicativas convencionais: anos de educação, sexo, anos de experiência, etc. As bases de dados do livro Introductory Econometrics estão disponíveis no pacote wooldridge. O código abaixo carrega a base de dados .\n\nlibrary(wooldridge)\n# Carrega a base\ndata(wage2)\n# Remove os valores ausentes (NAs)\nsal &lt;- na.omit(wage2)\n\n\n\n\n\n\n\n\nwage\nhours\nIQ\nKWW\neduc\nexper\ntenure\nage\nmarried\nblack\nsouth\nurban\nsibs\nbrthord\nmeduc\nfeduc\nlwage\n\n\n\n\n1\n769\n40\n93\n35\n12\n11\n2\n31\n1\n0\n0\n1\n1\n2\n8\n8\n6.645091\n\n\n3\n825\n40\n108\n46\n14\n11\n9\n33\n1\n0\n0\n1\n1\n2\n14\n14\n6.715383\n\n\n4\n650\n40\n96\n32\n12\n13\n7\n32\n1\n0\n0\n1\n4\n3\n12\n12\n6.476973\n\n\n5\n562\n40\n74\n27\n11\n14\n5\n34\n1\n0\n0\n1\n10\n6\n6\n11\n6.331502\n\n\n7\n600\n40\n91\n24\n10\n13\n0\n30\n0\n0\n0\n1\n1\n2\n8\n8\n6.396930\n\n\n9\n1154\n45\n111\n37\n15\n13\n1\n36\n1\n0\n0\n0\n2\n3\n14\n5\n7.050990\n\n\n10\n1000\n40\n95\n44\n12\n16\n16\n36\n1\n0\n0\n1\n1\n1\n12\n11\n6.907755\n\n\n11\n930\n43\n132\n44\n18\n8\n13\n38\n1\n0\n0\n0\n1\n1\n13\n14\n6.835185\n\n\n14\n1318\n38\n119\n24\n16\n7\n2\n28\n1\n0\n0\n1\n3\n1\n10\n10\n7.183871\n\n\n15\n1792\n40\n118\n47\n16\n9\n9\n34\n1\n0\n0\n1\n1\n1\n12\n12\n7.491087\n\n\n\n\n\n\nA base traz 663 observações de 17 variáveis. A função str é útil para entender a estrutura dos dados.\n\n# Dimensão da base (# linhas  # colunas)\ndim(sal)\n\n[1] 663  17\n\n# Descrição da base\nstr(sal)\n\n'data.frame':   663 obs. of  17 variables:\n $ wage   : int  769 825 650 562 600 1154 1000 930 1318 1792 ...\n $ hours  : int  40 40 40 40 40 45 40 43 38 40 ...\n $ IQ     : int  93 108 96 74 91 111 95 132 119 118 ...\n $ KWW    : int  35 46 32 27 24 37 44 44 24 47 ...\n $ educ   : int  12 14 12 11 10 15 12 18 16 16 ...\n $ exper  : int  11 11 13 14 13 13 16 8 7 9 ...\n $ tenure : int  2 9 7 5 0 1 16 13 2 9 ...\n $ age    : int  31 33 32 34 30 36 36 38 28 34 ...\n $ married: int  1 1 1 1 0 1 1 1 1 1 ...\n $ black  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ south  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urban  : int  1 1 1 1 1 0 1 0 1 1 ...\n $ sibs   : int  1 1 4 10 1 2 1 1 3 1 ...\n $ brthord: int  2 2 3 6 2 3 1 1 1 1 ...\n $ meduc  : int  8 14 12 6 8 14 12 13 10 12 ...\n $ feduc  : int  8 14 12 11 8 5 11 14 10 12 ...\n $ lwage  : num  6.65 6.72 6.48 6.33 6.4 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n - attr(*, \"na.action\")= 'omit' Named int [1:272] 2 6 8 12 13 19 20 21 31 36 ...\n  ..- attr(*, \"names\")= chr [1:272] \"2\" \"6\" \"8\" \"12\" ...\n\n\nO modelo proposto é o abaixo:\n\\[\n\\text{lwage}_{t} = \\beta_{0} + \\beta_{1}\\text{educ}_{t} + \\beta_{2}\\text{exper}_{t} + \\beta_{3}\\text{exper}^{2}_{t} + \\beta_{4}\\text{tenure}_{t} + \\beta{5}\\text{married}_{t} + u_{t}\n\\]\nonde:\n\nlwage = logaritmo natural do salário\neduc = anos de educação\nexper = anos de experiência (trabalhando)\ntenure = anos trabalhando com o empregador atual\nmarried = dummy (1 = casado, 0 = não-casado)\n\nHá 6 coeficientes para estimar logo \\(k = 6\\) . Além disso, como há \\(663\\) observações temos que \\(n = 663\\) . A matriz de “dados” é da forma:\n\\[\nX = \\begin{bmatrix}\n1 & 12 & 11 & 121 & 2 & 1\\\\\\\\\n1 & 14 & 11 & 121 & 9 & 1\\\\\\\\\n1 & 12 & 13 & 169 & 7 & 1\\\\\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\n1 & 13 & 10 & 100 & 3 & 1\n\\end{bmatrix}_{663\\times 6}\n\\]\nO código abaixo monta a matriz \\(X\\) acima. As funções head e tail podem ser usadas para verificar as primeiras e últimas linhas da matriz. Vale esclarecer dois pontos sobre o código a ser usado neste post. Primeiro, quando se cria um objeto usando o operador &lt;- pode-se forçar o R a imprimir o seu valor colocando a expressão entre parêntesis. Por exemplo, teste (x &lt;- 3). O segundo ponto é que o operador de multiplicação matricial é %*%.\n\n# Define alguns valores úteis: \n## N = número de observações\n## k = número de regressores\n## const = vetor com 1^\\intercals (uns)\nN &lt;- 663; k &lt;- 6; const &lt;- rep(1, 663)\n# Monta a matriz de observações da regressão\nX &lt;- cbind(const, sal$educ, sal$exper, sal$exper^2, sal$tenure, sal$married)\nX &lt;- as.matrix(X)\n# Define o nome das colunas da matriz de observações\ncolnames(X) &lt;- c(\"const\", \"educ\", \"exper\", \"exper2\", \"tenure\", \"married\")\n# Função para verificar as primeiras linhas da matriz X\nhead(X)\n\n     const educ exper exper2 tenure married\n[1,]     1   12    11    121      2       1\n[2,]     1   14    11    121      9       1\n[3,]     1   12    13    169      7       1\n[4,]     1   11    14    196      5       1\n[5,]     1   10    13    169      0       0\n[6,]     1   15    13    169      1       1\n\n# Função para verificar as últimas linhas da matriz X\ntail(X)\n\n       const educ exper exper2 tenure married\n[658,]     1   12     9     81      2       1\n[659,]     1   16     8     64     10       1\n[660,]     1   12    11    121      3       1\n[661,]     1   12     9     81      3       1\n[662,]     1   16    10    100      9       1\n[663,]     1   13    10    100      3       1\n\n\nLembrando que o problema de mínimos quadrados é de encontrar os valores de \\(\\beta\\) que minimizam a soma dos erros ao quadrado.\n\\[\n\\underset{\\beta}{\\text{Min }} e^\\intercal e\n\\]\nAbrindo mais a expressão acima:\n\\[\n\\begin{align}\n  e^\\intercal e & = (y - X\\beta )^\\intercal(y - X\\beta ) \\\\\\\\\n      & = y^\\intercal y - y^\\intercal X\\beta - \\beta ^\\intercal X^\\intercal y + \\beta ^\\intercal X^\\intercal X \\beta \\\\\\\\\n      & = y^\\intercal y - 2 y^\\intercal X \\beta + \\beta^\\intercal X^\\intercal X \\beta\n\\end{align}\n\\]\nDerivando em relação a \\(\\beta\\) e igualando a zero chega-se no estimador de MQO\n\\[\n\\beta_{\\text{MQO}} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]\nO código abaixo computa \\(\\beta_{\\text{MQO}}\\) . Note que os parêntesis por fora da expressão forçam o R a imprimir o valor do objeto. Além disso, como estamos multiplicando matrizes/vetores usamos %*%.\n\n# Define o vetor y (log do salário)\ny &lt;- sal$lwage\n# Computa a estimativa para os betas\n(beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y)\n\n                [,1]\nconst   5.3563606713\neduc    0.0767258959\nexper   0.0104985672\nexper2  0.0002881339\ntenure  0.0091039254\nmarried 0.2002468574\n\n\nOs valores estimados dos betas são reportados na tabela abaixo.\n\ntabela &lt;- as.data.frame(round(beta, 4))\ncolnames(tabela) &lt;- c(\"Coeficiente estimado\")\nround(beta, 4) %&gt;%\n  kable(align = \"c\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\nconst\n5.3564\n\n\neduc\n0.0767\n\n\nexper\n0.0105\n\n\nexper2\n0.0003\n\n\ntenure\n0.0091\n\n\nmarried\n0.2002"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html#resíduo-e-variância",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html#resíduo-e-variância",
    "title": "OLS com matrizes",
    "section": "Resíduo e variância",
    "text": "Resíduo e variância\nO resíduo do modelo é simplesmente a diferença entre o observado \\(y_{t}\\) e o estimado \\(\\hat{y_{t}}\\) . Isto é,\n\\[\n\\hat{e}_{t} = y_{t} - \\hat{y}_{t} = y_{t} - x_{t}^\\intercal\\hat{\\beta}\n\\]\nou, de forma equivalente,\n\\[\n\\hat{e} = y - X\\hat{\\beta}\n\\]\n\n# Computa o resíduo da regressão\nu_hat &lt;- y - X %*% beta\n\nUsando o histograma pode-se visualizar a distribuição dos resíduos.\n\nhist(u_hat, breaks = 30, freq = FALSE, main = \"Histograma dos resíduos\")\n\n\n\n\n\n\n\n\nO estimador da variância é dado por:\n\\[\n\\hat{\\sigma}^{2} = \\frac{1}{N-k}\\sum_{t = 1}^{N}\\hat{e}_{t}^{2}\n\\]\nSubstituindo os valores calculados acima chegamos em:\n\\[\n\\hat{\\sigma}^{2} = \\frac{\\hat{e}^\\intercal\\hat{e}}{N-k} = 0.1403927\n\\]"
  },
  {
    "objectID": "posts/general-posts/2023-09-wz-unemployment/index.html",
    "href": "posts/general-posts/2023-09-wz-unemployment/index.html",
    "title": "Brazil in Charts: Unemployment",
    "section": "",
    "text": "After a slow start in 2021, Brazilian unemployment registered a remarkable drop, boosted by soaring commodity prices and generous fiscal expansion. Throughout 2020 and up until the 2022 elections, the Federal Government sustained several subsidy programs that initially accounted for up to 10% of total GDP. Even after the election, government is still running at a significant deficit, as the promised budget cuts fail to be approved.\nThe job market numbers are even more impressive when one accounts for the very high real interest rates that prevailed through 2021 and 20221. Despite a global slowdown in the markets, Brazilian GDP surprised in the first quarter of 2023 due to an impressive performance of the agricultural sector. Agricultural related activities grew 18,8% YoY and led to a 4% increase in GDP YoY2.\n\n\n\n\n\n\n\n\n\nThe drop in unemployment has been seen across all Brazilian states. The largest drop happened in the northern state of Roraíma, which saw unemployment drop 10 percentage points. Santa Catarina had the lowest unemployment rate in 2019-Q2 and now has the third lowest rate at 3.5%.\n\n\n\n\n\n\n\n\n\nOverall, the Mid-Western states of Mato Grosso and Mato Grosso do Sul are at historically low unemployment rates. Both states have strong ties to the primary sector and have benefited from recent record-breaking soybean crops. The states in the South and Southeast all have below average unemployment rates with the exception of Rio de Janeiro. Rio’s economy is still struggling after repeated blows; the aftermath of the 2014 World Cup has been a continuing string of crisis: the 2015-17 recession, Lavajato, and the Covid-19 Pandemic have all aggravated a pre-existing economic stagnation."
  },
  {
    "objectID": "posts/general-posts/2023-09-wz-unemployment/index.html#unemployment-rate-dropped-sharply-in-the-post-pandemic-period-but-still-above-pre-recession-levels",
    "href": "posts/general-posts/2023-09-wz-unemployment/index.html#unemployment-rate-dropped-sharply-in-the-post-pandemic-period-but-still-above-pre-recession-levels",
    "title": "Brazil in Charts: Unemployment",
    "section": "",
    "text": "After a slow start in 2021, Brazilian unemployment registered a remarkable drop, boosted by soaring commodity prices and generous fiscal expansion. Throughout 2020 and up until the 2022 elections, the Federal Government sustained several subsidy programs that initially accounted for up to 10% of total GDP. Even after the election, government is still running at a significant deficit, as the promised budget cuts fail to be approved.\nThe job market numbers are even more impressive when one accounts for the very high real interest rates that prevailed through 2021 and 20221. Despite a global slowdown in the markets, Brazilian GDP surprised in the first quarter of 2023 due to an impressive performance of the agricultural sector. Agricultural related activities grew 18,8% YoY and led to a 4% increase in GDP YoY2.\n\n\n\n\n\n\n\n\n\nThe drop in unemployment has been seen across all Brazilian states. The largest drop happened in the northern state of Roraíma, which saw unemployment drop 10 percentage points. Santa Catarina had the lowest unemployment rate in 2019-Q2 and now has the third lowest rate at 3.5%.\n\n\n\n\n\n\n\n\n\nOverall, the Mid-Western states of Mato Grosso and Mato Grosso do Sul are at historically low unemployment rates. Both states have strong ties to the primary sector and have benefited from recent record-breaking soybean crops. The states in the South and Southeast all have below average unemployment rates with the exception of Rio de Janeiro. Rio’s economy is still struggling after repeated blows; the aftermath of the 2014 World Cup has been a continuing string of crisis: the 2015-17 recession, Lavajato, and the Covid-19 Pandemic have all aggravated a pre-existing economic stagnation."
  },
  {
    "objectID": "posts/general-posts/2023-09-wz-unemployment/index.html#footnotes",
    "href": "posts/general-posts/2023-09-wz-unemployment/index.html#footnotes",
    "title": "Brazil in Charts: Unemployment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.nytimes.com/interactive/2022/06/16/business/economy/global-interest-rate-increases.html↩︎\nhttps://agenciadenoticias.ibge.gov.br/agencia-sala-de-imprensa/2013-agencia-de-noticias/releases/37029-pib-cresce-1-9-no-1-trimestre-de-2023↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-10-unemployment/index.html",
    "href": "posts/general-posts/2023-10-unemployment/index.html",
    "title": "Brazil: Unemployment",
    "section": "",
    "text": "Unemployment\nBrazilian unemployment has fallen sharply since the end of the pandemic and is below pre-pandemic values. In this Chart of the Week I highlight the decrease in unemployment across all Brazilian states. The northern state of Roraíma registered the biggest drop in the past three years: the rate of unemployment fell from 15.1% to 5.1%.\nIt should be noted that the Brazilian labor market has a large degree of informality. This overall unemployment rate\n\n\n  code_state trimestre unemp       date abbrev_state name_state code_region\n1         14    202302   5.1 2023-04-01           RR    Roraima           1\n  name_region\n1       Norte"
  },
  {
    "objectID": "posts/general-posts/2023-10-wz-census/index.html",
    "href": "posts/general-posts/2023-10-wz-census/index.html",
    "title": "Weekly Viz - Brazilian Census",
    "section": "",
    "text": "This week I show the population growth rate of cities in Brazil using the recent 2022 Census data. Overall, a large share of cities registered negative growth rates in the past 12 years: 43.28% of cities experienced a decrease in population. The spatial patterns of population growth varied by each region. The southern region recorded a large share of cities with declining population (44.8%), mainly due to demographic factors: the region has the lowest fertility rate and the highest elder to youth ratio. The Southeast and the Midwest had the largest share of cities with growth: 61.1% and 64.9% respectively.\n\n\n\n\nBoth the north and northeastern cities benefit from relatively younger demographics. Adult population is still - by far - the largest demographic group and fertility rates are usually above the national average. These cities usually have the lowest share of above 65 years population. Some regions, and the state of Roraíma in particular, have also benefited from immigration from neighbor Venezuela.\n\n\n\n\n\n\n\n\n\n\n\n\nDespite favorable demographics, northeastern cities are aging rapidly and shrinking in size. Alagoas and Bahia registered some of the lowest growth rates at 0.02% and 0.07% respectively. The region, as a whole, struggles with safety issues, high unemployment, and stagnant income growth.\n\n\n\n\n\n\n\n\n\n\n\n\nThe southeast region concentrates the largest share of GDP and population in Brazil. Demographic indicators show a large elder population and very low fertility rates. Growth in São Paulo and Espírito Santo has been close to the national average while Rio de Janeiro and Minas Gerais lag behind. The latter state has the second lowest growth rate, 0.03%. As shown in the previous Brazil in Charts post, Rio de Janeiro has the highest unemployment rate among the southeastern states.\n\n\n\n\n\n\n\n\n\n\n\n\nThe southern region cities registered significant population losses. The southernmost state, Rio Grande do Sul, had a growth rate of only 0.14%. Economic stagnation, fertility rate decreases, and the rise of safety problems have also increased emigrations flows. Santa Catarina had the second highest growth of all states, 1.66%, falling only behind Roraíma, which received a huge influx of Venezuelan migrants. Santa Catarina boasts low unemployment, high income, and solid economic growth. The northeastern cities and the Chapecó metropolitan region, in particular, had some of the highest growth rates among large cities (over 100 thousand inhabitants).\n\n\n\n\n\n\n\n\n\n\n\n\nThe midwest cities exhibited significant population growth. Agriculture is still the most productive Brazilian sector and the Midwest accounts for the majority of the countries agricultural output. High income per capita and job opportunities make these cities attractive destinations for migrants.\nThe map can be deceptive, since some of the bigger municipalities actually have medium or small populations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe geometric growth rate expresses the change in a population, assuming that it increases or decreases at a fixed rate.\nLets assume that population at a given time, \\(P_{t}\\), increases at a fixed rate \\(R\\). Then, the relative population change/growth \\(\\frac{\\Delta P_{t}}{P_{t}}\\) is given by\n\\[\n\\frac{\\Delta P_{t}}{P_{t}} = \\frac{P_{t+1} - P_{t}}{P_{t}} = R\n\\]\nRearranging terms,\n\\[\nP_{t+1} = P_{t} + RP_{t} = (1 + R)P_{t}\n\\]\nwhich states that the population in a future period is simply the current population increased by \\(RP_{t}\\) where \\(R\\) is the geometric growth rate. Since this growth rate is constant, we can generalize that\n\\[\nP_{t+n} = (1+R)P_{t+n-1} = (1+R)^2P_{t+n-2} = \\dots = (1+R)^nP_{t}\n\\]\nThis means that the population in any given time in the future is equal to the current population increased by the compounded geometric rate of growth. This is likely an unrealistic assumption for long periods of time but is a decent approximation in short periods. This last equation also provides the definition for the geometric growth rate:\n\\[\nR = (\\frac{P_{t+n}}{P_{t}})^\\frac{1}{n} - 1\n\\]"
  },
  {
    "objectID": "posts/general-posts/2023-10-wz-census/index.html#brazilian-census-in-maps",
    "href": "posts/general-posts/2023-10-wz-census/index.html#brazilian-census-in-maps",
    "title": "Weekly Viz - Brazilian Census",
    "section": "",
    "text": "Both the north and northeastern cities benefit from relatively younger demographics. Adult population is still - by far - the largest demographic group and fertility rates are usually above the national average. These cities usually have the lowest share of above 65 years population. Some regions, and the state of Roraíma in particular, have also benefited from immigration from neighbor Venezuela.\n\n\n\n\n\n\n\n\n\n\n\n\nDespite favorable demographics, northeastern cities are aging rapidly and shrinking in size. Alagoas and Bahia registered some of the lowest growth rates at 0.02% and 0.07% respectively. The region, as a whole, struggles with safety issues, high unemployment, and stagnant income growth.\n\n\n\n\n\n\n\n\n\n\n\n\nThe southeast region concentrates the largest share of GDP and population in Brazil. Demographic indicators show a large elder population and very low fertility rates. Growth in São Paulo and Espírito Santo has been close to the national average while Rio de Janeiro and Minas Gerais lag behind. The latter state has the second lowest growth rate, 0.03%. As shown in the previous Brazil in Charts post, Rio de Janeiro has the highest unemployment rate among the southeastern states.\n\n\n\n\n\n\n\n\n\n\n\n\nThe southern region cities registered significant population losses. The southernmost state, Rio Grande do Sul, had a growth rate of only 0.14%. Economic stagnation, fertility rate decreases, and the rise of safety problems have also increased emigrations flows. Santa Catarina had the second highest growth of all states, 1.66%, falling only behind Roraíma, which received a huge influx of Venezuelan migrants. Santa Catarina boasts low unemployment, high income, and solid economic growth. The northeastern cities and the Chapecó metropolitan region, in particular, had some of the highest growth rates among large cities (over 100 thousand inhabitants).\n\n\n\n\n\n\n\n\n\n\n\n\nThe midwest cities exhibited significant population growth. Agriculture is still the most productive Brazilian sector and the Midwest accounts for the majority of the countries agricultural output. High income per capita and job opportunities make these cities attractive destinations for migrants.\nThe map can be deceptive, since some of the bigger municipalities actually have medium or small populations."
  },
  {
    "objectID": "posts/general-posts/2023-10-wz-census/index.html#what-is-the-geometric-growth-rate",
    "href": "posts/general-posts/2023-10-wz-census/index.html#what-is-the-geometric-growth-rate",
    "title": "Weekly Viz - Brazilian Census",
    "section": "",
    "text": "The geometric growth rate expresses the change in a population, assuming that it increases or decreases at a fixed rate.\nLets assume that population at a given time, \\(P_{t}\\), increases at a fixed rate \\(R\\). Then, the relative population change/growth \\(\\frac{\\Delta P_{t}}{P_{t}}\\) is given by\n\\[\n\\frac{\\Delta P_{t}}{P_{t}} = \\frac{P_{t+1} - P_{t}}{P_{t}} = R\n\\]\nRearranging terms,\n\\[\nP_{t+1} = P_{t} + RP_{t} = (1 + R)P_{t}\n\\]\nwhich states that the population in a future period is simply the current population increased by \\(RP_{t}\\) where \\(R\\) is the geometric growth rate. Since this growth rate is constant, we can generalize that\n\\[\nP_{t+n} = (1+R)P_{t+n-1} = (1+R)^2P_{t+n-2} = \\dots = (1+R)^nP_{t}\n\\]\nThis means that the population in any given time in the future is equal to the current population increased by the compounded geometric rate of growth. This is likely an unrealistic assumption for long periods of time but is a decent approximation in short periods. This last equation also provides the definition for the geometric growth rate:\n\\[\nR = (\\frac{P_{t+n}}{P_{t}})^\\frac{1}{n} - 1\n\\]"
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html",
    "href": "posts/general-posts/2023-10-censo-erros/index.html",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Houve certo rebuliço nas redes sociais, quando do lançamento dos dados mais recentes do Censo Demográfico de 2022. O fato carregado na maior parte das manchetes do Brasil foi a queda no número projetado da população brasileira. Até 2021, projetava-se que a população brasileira estivesse em torno de 213 milhões de habitantes, segundo a pesquisa Estimativas da População do IBGE. O número que o Censo trouxe foi de 203 milhões, ou seja, houve uma queda de 10 milhões de habitantes em relação ao previsto. Ainda que pareça grande, o “erro de projeção” foi de menos de 5%. Contudo, é importante entender por que este número foi subestimado.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMesmo antes da divulgação dos primeiros dados de população, a mais recente edição do Censo enfrentou diversos problemas. A eclosão da Pandemia Covid-19, em 2020, forçou o adiamento da pesquisa para o ano seguinte; em 2021, contudo, o orçamento do Censo, originalmente de R$2 bilhão foi reduzido em mais de 90%, efetivamente impossibilitando a sua execução. Inicialmente, previa-se que haveria cerca de 200 mil recenseadores em operação: na prática, houve menos da metade, em torno de 91 mil; além disso, a redução do orçamento comprometeu o treinamento destes funcionários. A situação chegou a tal ponto que se decretou uma medida provisória permitindo a contratação de recenseadores sem processo seletivo algum.\nO processo de coleta dos dados enfrentou diversos atrasos, sobretudo em função do número insuficiente de recenseadores em campo. A falta de verba também comprometeu o pagamento destes funcionários, que chegaram a organizar uma greve em setembro de 2022, em resposta às más condições de trabalho.\n\n\n\nA fraca divulgação do Censo na imprensa, junto com a politização da pesquisa e a propagação de fake news, reduziu a adesão da população à pesquisa. A taxa geral de não-resposta foi de 4,23%, mas chegou a 8,11% no estado de São Paulo; em alguns municípios como Santana de Parnaíba, na Região Metropolitana de São Paulo, esta taxa chegou a 16,8%1. O problema foi particularmente severo nos domicílios de alta renda e condomínios fechados; também há evidência de que cidades, com maior proporção de votos para Bolsonaro no 2o turno , tiveram menor adesão ao Censo.\n\n\n\nPor fim, como os dados de população de municípios têm um impacto direto sobre o repasse do Fundo de Participação de Municípios as prefeitura tem um incentivo perverso para distorcer os dados populacionais. De fato, o economista Leonardo Monastério verificou que isto já aconteceu em Censos anteriores e que o problema vem se agravando com o tempo. No histograma abaixo, retirado do trabalho original, vê-se que há quebras suspeitas nos valores da população que coincidem as faixas do FPM. Será interessante replicar o experimento para os dados atuais do Censo.\n\n\n\nReprodução de gráfico de Monasterio (2013)\n\n\n\n\n\nO Censo enfrentou diversos problemas:\n\nPandemia do Covid-19, cortes no orçamento e mudanças na presidência. Além de gerar atrasos na pesquisa, os cortes comprometeram a capacidade de contratação e treinamento de funcionários.\nFraca divulgação da pesquisa e menor adesão da população. Autoridades importantes do governo, como o próprio presidente da república à época minaram a credibilidade do IBGE publicamente.\n\nA ex-presidente do IBGE, Wasmália Bivar, avalia que o corte de orçamento comprometeu a pesquisa e que a subestimação da população era esperada. Já Roberto Olinto, também ex-presidente do instituto, pediu por uma auditoria do Censo, chegando a sugerir a possibilidade de ser necessário refazer a pesquisa. Apesar de todas as dificuldades, os representantes do IBGE defendem que o número divulgado é confiável e avaliam que o Censo foi um sucesso, quando se considera todos os desafios que foram enfrentados.\n\n\n\n\n\n\nA fórmula da dinâmica populacional é bastante simples. A população num determinado ano é igual à população do ano anterior somada da variação populacional. Esta variação populacional é o (1) número total de nascimentos, (2) óbitos, (3) imigrantes e emigrantes.\nFormalmente, seja a população no ano seguinte denotada por \\(P_{t+1}\\) e população no ano corrente, \\(P_{t}\\). A partir deste número, soma-se o total de nascimentos \\(B_{t, t+1}\\), subtrai-se o total de óbitos \\(D_{t, t+1}\\) e soma-se o fluxo líquido de migrantes \\(NM_{t, t+1}\\) . A equação abaixo resume estes fatos:\n\\[\nP_{t+1} = P_{t} + B_{t,t+1} - D_{t,t+1} + NM_{t,t+1}\n\\]\nO IBGE tem boas maneiras de estimar estes números:\n\nO número de nascimentos, \\(B_{t, t+1}\\), é uma função da taxa de fertilidade e do número de mulheres em idade fértil;\nO número de óbitos, \\(D_{t,t+1}\\), similarmente, pode ser estimado a partir de tábuas atuariais de mortalidade, discriminadas por grupos de idade e sexo;\nPor fim, ainda que seja difícil quantificar o fluxo migratório, \\(NM_{t,t+1}\\), ele tem pouco impacto no número final, no caso do Brasil2.\n\nA projeção da população, segundo a equação acima, é chamada de Método das Componentes Demográficas. Esta metodologia tem amplo respaldo teórico3 e é utilizada na pesquisa Projeções da População do IBGE. Na mais recente edição divulgada, de 2018, a projeção populacional do Brasil para 2022 é de 214,8 milhões de habitantes.\nO Método das Componentes Demográficas também é utilizado pela Organização das Nações Unidas (ONU). Na edição de 2019 do World Population Prospects, projetava-se que a população do Brasil deveria chegar a 219 milhões de habitantes em 2025. Este número é muito próximo ao projetado pelo Projeções da População, citado acima. Segundo os dados do Censo, contudo, seria necessário um crescimento de 16 milhões de habitantes para alcançar este resultado.\n\n\n\nUma maneira ainda mais simples de modelar a dinâmica da população é via uma equação diferencial que expressa o crescimento exponencial da população. Na prática, populações frequentemente exibem comportamentos deste tipo, crescendo ou decaindo exponencialmente a uma taxa fixa. Esta taxa fixa \\(r\\) pode ser interpretada como a taxa geométrica de crescimento (TGC) da população.\n\\[\nP_{t} = P_{0}\\times(1 + r)^{t}\n\\]\nSupondo, que a TGC observada entre os censos de 2000 e 2010, de 1,18%, permancesse constante durante os próximos doze anos, a população em 2022 deveria ser de 220 milhões. Usando uma estimativa mais conservadora de 0,97% (TGC entre 2009 e 2010) a população seria de 214 milhões - valor muito próximo ao previsto pelo mais complexo Método de Componentes Demográficas acima. Por fim, utilizando a estimativa da TGC do IBGE em 2010, de 0,88%, a população brasileira deveria crescer para 212 milhões. A TGC verifica pelo Censo foi de 0,52%, que resulta na população de 203 milhões verificada.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparando os dados da Projeção da População, de 2018, e do Censo Demográfico de 2022 podemos mensurar a discrepância entre os valores projetados e os efetivamente verificados. De maneira geral, as projeções superestimaram o crescimento populacional em todas as Unidades Federativas com exceção de Mato Grosso e Santa Catarina. Em alguns casos, como no Amapá a diferença foi de quase 18%: projetou-se uma população de 886 mil, mas verificou apenas 811 mil. Em termos absolutos, a maior diferença aconteceu no estado de São Paulo, onde o Censo encontrou um valor 2,5 milhões inferior ao projetado.\n\n\n\n\n\n\n\nNos termos da equação populacional acima, a divergência do Censo deve ser explicada por:\n\nSuperestimação do número de nascimentos. Isto pode ter ocorrido em função da queda de renda e aumento de desemprego causado pela recessão de 2015-174 e também pela Pandemia5. Também pode ser o caso de que a taxa de fecundidade do Brasil diminuiu mais rápido do que o esperado.\nSubestimação do número de óbitos. A Pandemia trouxe muito mais óbitos do que qualquer modelo demográfico razoável poderia prever.\nSubestimação do fluxo migratório para fora do país. Desde a crise econômica, aumentou o número de emigrantes no Brasil e taxa de brasileiros que não voltaram ao país também subiu.\n\nNo Brasil, os fluxos migratórios são pequenos, relativamente ao tamanho da população. Em 2021, por exemplo, o saldo líquido migratório brasileiro foi negativo em cerca de 300 mil pessoas6, equivalente a cerca de 0,1% da população. Assim, o número de nascimentos e óbitos deve explicar a maior parte da diferença entre o valor estimado pelo Censo e o valor projetado anteriormente.\n\n\n\nAs séries de nascimentos e óbitos, do Registro Civil, apontam que houve uma queda na tendência de nascimentos durante a última década7. Olhando para a tendência da série, vê-se que a crise econômica de 2015-17 parece ter tido impacto negativo no número de nascidos vivos. Mesmo depois da recuperação da crise, a série segue numa tendência de queda, que se acentua a partir da Pandemia.\nJá a série de óbitos segue uma tendência estável de crescimento desde 2003. Houve um ligeiro aumento dos óbitos em 2015, mas rapidamente viu-se um retorno da série à sua tendência de longo prazo. Por fim, é notável como a Pandemia teve um efeito severo sobre o número de óbitos no país.\n\n\n\n\n\n\n\n\n\nModelando o comportamento da série de óbitos, pode-se elaborar uma espécie de contrafactual: isto é, pode-se ter uma estimativa de como teria sido o número de óbitos caso na ausência da pandemia.\nNo gráfico abaixo, a curva amarela mostra os valores preditos da série junto com intervalo de confiança de 95%. O modelo é treinado com os dados de janeiro de 2003 a dezembro de 2019. Nota-se como os valores previstos divergem dos valores observados a partir de abril de 2020. Ao longo do período, o número acumulado de óbitos, acima do previsto pelo modelo, supera 600 mil.\n\n\n\n\n\n\n\n\n\nComparando as projeções de curto prazo do IBGE para nascimentos e óbitos, divulgadas em 2018, com os dados mais recentes do Registro Civil, divulgados em 2022, pode-se ver como o número de nascimentos foi superestimado e o número de óbitos, subestimado. Ainda assim, a divergência entre os números não é grande o suficiente para explicar a diferença de 10 milhões de habitantes verificada pelo Censo. Grosso modo, houve cerca de 400 mil mortes a mais do que o projetado e 800 mil nascimentos a menos.\n\n\n\n\n\n\n\n\n\nPode-se também usar os dados do Censo de 2010 e atualizar os valores populacionais, ano a ano, usando os nascimentos e óbitos do Registro Civil. O economista Francisco Faria, no blog do IBRE, simulou este cenário e encontrou populações de 208,5 ou 212,9 milhões, a depender se usa-se o valor original ou revisado do Censo de 2010. De qualquer modo, ambos os valores estão acima do valor de 203 milhões.\nRecentemente, o IBGE divulgou os resultados de Censo por grupos de idade o que nos permite fazer ainda mais uma comparação. Os valores projetados estão disponíveis em grupos quinquenais de idade-sexo até 80 anos. O primeiro gráfico abaixo mostra a pirâmide populacional observada no Censo de 2022; as colunas transparentes mostram o valor projetado em cada grupo: vê-se como o valor observado foi menor em praticamente todos os grupos.\nO gráfico da direita mostra a diferença percentual entre o valor projetado e o valor observado em cada grupo de idade. É interessante notar que as projeções, aparentemente, superestimaram o número de novos nascidos vivos. Projetou-se quase 15 milhões de recém-nascidos, de 0 a 4 anos, mas verificou-se algo em torno de 12.7 milhões. O número de jovens e de adultos também foi superestimado, enquanto a população de 55 a 74 anos observada ficou muito próxima da projetada.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNão há consenso ainda que explique a discrepância verificada entre as projeções do IBGE e o valor auferido no Censo. Certamente, a causa será um misto dos três motivos apresentados acima: nascimentos, óbitos e fluxos migratórios. Somente com o tempo será possível entender melhor o que houve com o Censo de 2022. Ainda que os dados do Censo possam apresentar algum erro, a tendência geral demográfica é certa, o Brasil:\n\nTem uma proporção cada vez maior de idosos em relação a jovens.\nApresenta taxa de crescimento populacional decrescente.\nEm alguns anos vai começar a observar quedas na sua população.\n\nEstes três fatos já são a realidade de países desenvolvidos como Itália, Coreia do Sul, Japão, Alemanha e tantos outros. Até agora nenhum país conseguiu reverter a queda da taxa de fecundidade, mesmo com generosos benefícios fiscais. Os poucos países ricos que conseguem manter algum crescimento populacional se beneficiam de grandes influxos de migrantes que, por sua vez, trazem desafios adicionais. O Brasil encontra-se numa posição delicada, pois ele chega ao fim do seu crescimento populacional com um PIB per capita ainda relativamente baixo; além disso, o Brasil, há muitos anos, é um país com fluxo migratório negativo, isto é, mais brasileiros emigram (saem) do país do que estrangeiros imigram para cá.\nNo próximo texto desta série vou explorar mais a fundo os dados por cidades e metrópoles. O envelhecimento da população e acúmulo de problemas urbanos em grandes cidades parece estar reduzindo o puxo destas metrópoles. Rio de Janeiro e Salvador, por exemplo, registraram perdas populacionais que, muito provavelmente, devem-se a fluxos migratórios internos.\nPor fim, vale terminar o texto relembrando que os funcionários do IBGE são extremamente competentes e plenamente capacitados. O Censo sofreu, infortuitamente, cortes de orçamento severos e teve de ser atrasado devido à pandemia. Os dados do Censo de 2022, assim como dos Censos anteriores, serão revisados futuramente e, muito provavelmente, as discrepâncias mencionadas neste texto serão dirmidas."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#o-contexto-da-pesquisa",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#o-contexto-da-pesquisa",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Mesmo antes da divulgação dos primeiros dados de população, a mais recente edição do Censo enfrentou diversos problemas. A eclosão da Pandemia Covid-19, em 2020, forçou o adiamento da pesquisa para o ano seguinte; em 2021, contudo, o orçamento do Censo, originalmente de R$2 bilhão foi reduzido em mais de 90%, efetivamente impossibilitando a sua execução. Inicialmente, previa-se que haveria cerca de 200 mil recenseadores em operação: na prática, houve menos da metade, em torno de 91 mil; além disso, a redução do orçamento comprometeu o treinamento destes funcionários. A situação chegou a tal ponto que se decretou uma medida provisória permitindo a contratação de recenseadores sem processo seletivo algum.\nO processo de coleta dos dados enfrentou diversos atrasos, sobretudo em função do número insuficiente de recenseadores em campo. A falta de verba também comprometeu o pagamento destes funcionários, que chegaram a organizar uma greve em setembro de 2022, em resposta às más condições de trabalho.\n\n\n\nA fraca divulgação do Censo na imprensa, junto com a politização da pesquisa e a propagação de fake news, reduziu a adesão da população à pesquisa. A taxa geral de não-resposta foi de 4,23%, mas chegou a 8,11% no estado de São Paulo; em alguns municípios como Santana de Parnaíba, na Região Metropolitana de São Paulo, esta taxa chegou a 16,8%1. O problema foi particularmente severo nos domicílios de alta renda e condomínios fechados; também há evidência de que cidades, com maior proporção de votos para Bolsonaro no 2o turno , tiveram menor adesão ao Censo.\n\n\n\nPor fim, como os dados de população de municípios têm um impacto direto sobre o repasse do Fundo de Participação de Municípios as prefeitura tem um incentivo perverso para distorcer os dados populacionais. De fato, o economista Leonardo Monastério verificou que isto já aconteceu em Censos anteriores e que o problema vem se agravando com o tempo. No histograma abaixo, retirado do trabalho original, vê-se que há quebras suspeitas nos valores da população que coincidem as faixas do FPM. Será interessante replicar o experimento para os dados atuais do Censo.\n\n\n\nReprodução de gráfico de Monasterio (2013)\n\n\n\n\n\nO Censo enfrentou diversos problemas:\n\nPandemia do Covid-19, cortes no orçamento e mudanças na presidência. Além de gerar atrasos na pesquisa, os cortes comprometeram a capacidade de contratação e treinamento de funcionários.\nFraca divulgação da pesquisa e menor adesão da população. Autoridades importantes do governo, como o próprio presidente da república à época minaram a credibilidade do IBGE publicamente.\n\nA ex-presidente do IBGE, Wasmália Bivar, avalia que o corte de orçamento comprometeu a pesquisa e que a subestimação da população era esperada. Já Roberto Olinto, também ex-presidente do instituto, pediu por uma auditoria do Censo, chegando a sugerir a possibilidade de ser necessário refazer a pesquisa. Apesar de todas as dificuldades, os representantes do IBGE defendem que o número divulgado é confiável e avaliam que o Censo foi um sucesso, quando se considera todos os desafios que foram enfrentados."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#a-matemática-da-população",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#a-matemática-da-população",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "A fórmula da dinâmica populacional é bastante simples. A população num determinado ano é igual à população do ano anterior somada da variação populacional. Esta variação populacional é o (1) número total de nascimentos, (2) óbitos, (3) imigrantes e emigrantes.\nFormalmente, seja a população no ano seguinte denotada por \\(P_{t+1}\\) e população no ano corrente, \\(P_{t}\\). A partir deste número, soma-se o total de nascimentos \\(B_{t, t+1}\\), subtrai-se o total de óbitos \\(D_{t, t+1}\\) e soma-se o fluxo líquido de migrantes \\(NM_{t, t+1}\\) . A equação abaixo resume estes fatos:\n\\[\nP_{t+1} = P_{t} + B_{t,t+1} - D_{t,t+1} + NM_{t,t+1}\n\\]\nO IBGE tem boas maneiras de estimar estes números:\n\nO número de nascimentos, \\(B_{t, t+1}\\), é uma função da taxa de fertilidade e do número de mulheres em idade fértil;\nO número de óbitos, \\(D_{t,t+1}\\), similarmente, pode ser estimado a partir de tábuas atuariais de mortalidade, discriminadas por grupos de idade e sexo;\nPor fim, ainda que seja difícil quantificar o fluxo migratório, \\(NM_{t,t+1}\\), ele tem pouco impacto no número final, no caso do Brasil2.\n\nA projeção da população, segundo a equação acima, é chamada de Método das Componentes Demográficas. Esta metodologia tem amplo respaldo teórico3 e é utilizada na pesquisa Projeções da População do IBGE. Na mais recente edição divulgada, de 2018, a projeção populacional do Brasil para 2022 é de 214,8 milhões de habitantes.\nO Método das Componentes Demográficas também é utilizado pela Organização das Nações Unidas (ONU). Na edição de 2019 do World Population Prospects, projetava-se que a população do Brasil deveria chegar a 219 milhões de habitantes em 2025. Este número é muito próximo ao projetado pelo Projeções da População, citado acima. Segundo os dados do Censo, contudo, seria necessário um crescimento de 16 milhões de habitantes para alcançar este resultado.\n\n\n\nUma maneira ainda mais simples de modelar a dinâmica da população é via uma equação diferencial que expressa o crescimento exponencial da população. Na prática, populações frequentemente exibem comportamentos deste tipo, crescendo ou decaindo exponencialmente a uma taxa fixa. Esta taxa fixa \\(r\\) pode ser interpretada como a taxa geométrica de crescimento (TGC) da população.\n\\[\nP_{t} = P_{0}\\times(1 + r)^{t}\n\\]\nSupondo, que a TGC observada entre os censos de 2000 e 2010, de 1,18%, permancesse constante durante os próximos doze anos, a população em 2022 deveria ser de 220 milhões. Usando uma estimativa mais conservadora de 0,97% (TGC entre 2009 e 2010) a população seria de 214 milhões - valor muito próximo ao previsto pelo mais complexo Método de Componentes Demográficas acima. Por fim, utilizando a estimativa da TGC do IBGE em 2010, de 0,88%, a população brasileira deveria crescer para 212 milhões. A TGC verifica pelo Censo foi de 0,52%, que resulta na população de 203 milhões verificada."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#a-discrepância-do-censo",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#a-discrepância-do-censo",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Comparando os dados da Projeção da População, de 2018, e do Censo Demográfico de 2022 podemos mensurar a discrepância entre os valores projetados e os efetivamente verificados. De maneira geral, as projeções superestimaram o crescimento populacional em todas as Unidades Federativas com exceção de Mato Grosso e Santa Catarina. Em alguns casos, como no Amapá a diferença foi de quase 18%: projetou-se uma população de 886 mil, mas verificou apenas 811 mil. Em termos absolutos, a maior diferença aconteceu no estado de São Paulo, onde o Censo encontrou um valor 2,5 milhões inferior ao projetado.\n\n\n\n\n\n\n\nNos termos da equação populacional acima, a divergência do Censo deve ser explicada por:\n\nSuperestimação do número de nascimentos. Isto pode ter ocorrido em função da queda de renda e aumento de desemprego causado pela recessão de 2015-174 e também pela Pandemia5. Também pode ser o caso de que a taxa de fecundidade do Brasil diminuiu mais rápido do que o esperado.\nSubestimação do número de óbitos. A Pandemia trouxe muito mais óbitos do que qualquer modelo demográfico razoável poderia prever.\nSubestimação do fluxo migratório para fora do país. Desde a crise econômica, aumentou o número de emigrantes no Brasil e taxa de brasileiros que não voltaram ao país também subiu.\n\nNo Brasil, os fluxos migratórios são pequenos, relativamente ao tamanho da população. Em 2021, por exemplo, o saldo líquido migratório brasileiro foi negativo em cerca de 300 mil pessoas6, equivalente a cerca de 0,1% da população. Assim, o número de nascimentos e óbitos deve explicar a maior parte da diferença entre o valor estimado pelo Censo e o valor projetado anteriormente.\n\n\n\nAs séries de nascimentos e óbitos, do Registro Civil, apontam que houve uma queda na tendência de nascimentos durante a última década7. Olhando para a tendência da série, vê-se que a crise econômica de 2015-17 parece ter tido impacto negativo no número de nascidos vivos. Mesmo depois da recuperação da crise, a série segue numa tendência de queda, que se acentua a partir da Pandemia.\nJá a série de óbitos segue uma tendência estável de crescimento desde 2003. Houve um ligeiro aumento dos óbitos em 2015, mas rapidamente viu-se um retorno da série à sua tendência de longo prazo. Por fim, é notável como a Pandemia teve um efeito severo sobre o número de óbitos no país.\n\n\n\n\n\n\n\n\n\nModelando o comportamento da série de óbitos, pode-se elaborar uma espécie de contrafactual: isto é, pode-se ter uma estimativa de como teria sido o número de óbitos caso na ausência da pandemia.\nNo gráfico abaixo, a curva amarela mostra os valores preditos da série junto com intervalo de confiança de 95%. O modelo é treinado com os dados de janeiro de 2003 a dezembro de 2019. Nota-se como os valores previstos divergem dos valores observados a partir de abril de 2020. Ao longo do período, o número acumulado de óbitos, acima do previsto pelo modelo, supera 600 mil.\n\n\n\n\n\n\n\n\n\nComparando as projeções de curto prazo do IBGE para nascimentos e óbitos, divulgadas em 2018, com os dados mais recentes do Registro Civil, divulgados em 2022, pode-se ver como o número de nascimentos foi superestimado e o número de óbitos, subestimado. Ainda assim, a divergência entre os números não é grande o suficiente para explicar a diferença de 10 milhões de habitantes verificada pelo Censo. Grosso modo, houve cerca de 400 mil mortes a mais do que o projetado e 800 mil nascimentos a menos.\n\n\n\n\n\n\n\n\n\nPode-se também usar os dados do Censo de 2010 e atualizar os valores populacionais, ano a ano, usando os nascimentos e óbitos do Registro Civil. O economista Francisco Faria, no blog do IBRE, simulou este cenário e encontrou populações de 208,5 ou 212,9 milhões, a depender se usa-se o valor original ou revisado do Censo de 2010. De qualquer modo, ambos os valores estão acima do valor de 203 milhões.\nRecentemente, o IBGE divulgou os resultados de Censo por grupos de idade o que nos permite fazer ainda mais uma comparação. Os valores projetados estão disponíveis em grupos quinquenais de idade-sexo até 80 anos. O primeiro gráfico abaixo mostra a pirâmide populacional observada no Censo de 2022; as colunas transparentes mostram o valor projetado em cada grupo: vê-se como o valor observado foi menor em praticamente todos os grupos.\nO gráfico da direita mostra a diferença percentual entre o valor projetado e o valor observado em cada grupo de idade. É interessante notar que as projeções, aparentemente, superestimaram o número de novos nascidos vivos. Projetou-se quase 15 milhões de recém-nascidos, de 0 a 4 anos, mas verificou-se algo em torno de 12.7 milhões. O número de jovens e de adultos também foi superestimado, enquanto a população de 55 a 74 anos observada ficou muito próxima da projetada."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#o-futuro-do-brasil",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#o-futuro-do-brasil",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Não há consenso ainda que explique a discrepância verificada entre as projeções do IBGE e o valor auferido no Censo. Certamente, a causa será um misto dos três motivos apresentados acima: nascimentos, óbitos e fluxos migratórios. Somente com o tempo será possível entender melhor o que houve com o Censo de 2022. Ainda que os dados do Censo possam apresentar algum erro, a tendência geral demográfica é certa, o Brasil:\n\nTem uma proporção cada vez maior de idosos em relação a jovens.\nApresenta taxa de crescimento populacional decrescente.\nEm alguns anos vai começar a observar quedas na sua população.\n\nEstes três fatos já são a realidade de países desenvolvidos como Itália, Coreia do Sul, Japão, Alemanha e tantos outros. Até agora nenhum país conseguiu reverter a queda da taxa de fecundidade, mesmo com generosos benefícios fiscais. Os poucos países ricos que conseguem manter algum crescimento populacional se beneficiam de grandes influxos de migrantes que, por sua vez, trazem desafios adicionais. O Brasil encontra-se numa posição delicada, pois ele chega ao fim do seu crescimento populacional com um PIB per capita ainda relativamente baixo; além disso, o Brasil, há muitos anos, é um país com fluxo migratório negativo, isto é, mais brasileiros emigram (saem) do país do que estrangeiros imigram para cá.\nNo próximo texto desta série vou explorar mais a fundo os dados por cidades e metrópoles. O envelhecimento da população e acúmulo de problemas urbanos em grandes cidades parece estar reduzindo o puxo destas metrópoles. Rio de Janeiro e Salvador, por exemplo, registraram perdas populacionais que, muito provavelmente, devem-se a fluxos migratórios internos.\nPor fim, vale terminar o texto relembrando que os funcionários do IBGE são extremamente competentes e plenamente capacitados. O Censo sofreu, infortuitamente, cortes de orçamento severos e teve de ser atrasado devido à pandemia. Os dados do Censo de 2022, assim como dos Censos anteriores, serão revisados futuramente e, muito provavelmente, as discrepâncias mencionadas neste texto serão dirmidas."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#footnotes",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#footnotes",
    "title": "Censo 2022: O que houve de errado?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecusas para entrevista não prejudicaram resultado final do Censo, diz IBGE (Folha SP)↩︎\nDe 2010 a 2020 houve um aumento de 24,4% no número de imigrantes no Brasil, resultando numa população de 1,3 milhão. Isto é equivalent a 0,6% da população total do Brasil. Apesar do Brasil ter hospedado imigrantes de diversos países ao longo do século XX, atualmente o país tem um fluxo migratório negativo (mais brasileiros saem do país do que estrangeiros entram) e uma população de imigrantes bastante reduzida.↩︎\nPara uma referência simples veja Census dos EUA.)↩︎\nExiste alguma evidência de que recessões econômicas diminuem as taxas de fecundidade, sobretudo em países em desenvolvimento. Veja, por exemplo, este artigo sobre a Colômbia e este sobre a Grécia.↩︎\nDe maneira geral, a Pandemia do Covid-19 teve um efeito negativo sobre as taxas de fecundidade e natalidade mundo afora. Em alguns países houve uma forte recuperação, mas em outros estas taxas seguem em níveis menores que os pré-pandêmicos. Link↩︎\nTaxa de brasileiros que saem do país e não voltam é a maior em 11 anos (Valor)↩︎\nA tendência das séries é estimada com uma média móvel centrada de 12 meses.↩︎"
  }
]