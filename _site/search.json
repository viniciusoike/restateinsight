[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Tutoriais\n\nggplot2: Do básico ao avançado\nVisualização de dados é a base fundamental de uma boa análise e pode ser o fator que separara um trabalho amador de uma entrega profissional. Esta série de posts ensina os fundamentos do ggplot2 desde o início.\n\n\n\nPosts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDemographic Pyramids in R\n\n\n\n\n\n\nggplot2\n\n\ndata-visualization\n\n\nenglish\n\n\ntutorial-R\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nFilm ratings over the decades: replicating a Nexo plot\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nreplication\n\n\n\nIn this tutorial, we delve into the world of cinema ratings over the decades, examining how films from various eras are rated on IMDb. We’ll replicate a plot originally published in Nexo, a Brazilian media outlet that produces fantastic data visualizations. With these techniques, you can now apply similar methods to analyze and present your own data sets.\n\n\n\n\n\nDec 9, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nPunchcard plots in R\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\ntutorial-R\n\n\n\nPunchcard plots are an alternative way to visualize data distributed across two categorical dimensions, such as days of the week and hours of the day. In this tutorial, you’ll learn how to create a punchcard plot using R and ggplot2. I show some of the basics of this plot and also how to create more sophisticated plots.\n\n\n\n\n\nDec 4, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nReplicating the Github Contributions plot in R using ggplot2\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\ndata-visualization\n\n\n\nIn this post I’ll show how to replicate the infamous GitHub contributions graphic using ggplot2\n\n\n\n\n\nDec 3, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nLinha-4 Amarela Metrô de São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nbrazil\n\n\nmaps\n\n\n\nNeste post, analiso o fluxo de passageiros na Linha 4-Amarela do metrô de São Paulo. A demanda permanece abaixo dos níveis pré-pandemia, mesmo após a abertura da estação Vila Sônia. O fluxo nos dias úteis está 10% abaixo dos níveis pré-pandêmicos, enquanto nos finais de semana a queda é de 3,6%. O impacto do aumento da tarifa em junho-24 é incerto. Estações com baldeações seguem sendo as mais movimentadas.\n\n\n\n\n\nJul 17, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nFinding All Starbucks in Brazil\n\n\n\n\n\n\nstarbucks\n\n\nweb-scrapping\n\n\ndata-science\n\n\nbrazil\n\n\ntutorial-R\n\n\nfinding\n\n\nenglish\n\n\n\nIn this post, I show how to find all Starbucks in Brazil using web scraping in R. I also show how to use the Google Places API to add ratings information for all Starbucks units. Web scraping is useful in market studies to acquire information that isn’t readily available.\n\n\n\n\n\nJul 14, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nLine-4 Metro\n\n\n\n\n\n\ndata-science\n\n\nfinding\n\n\nsao-paulo\n\n\nsubway\n\n\nweb-scraping\n\n\ntutorial-R\n\n\n\nIn this post I show how to webscrape all publicly available information on passenger flow from the Line-4 Metro in São Paulo. This post is part of a larger series where I gather all data on the subway lines in São Paulo.\n\n\n\n\n\nJul 10, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nLocating all The Coffee shops in Brazil\n\n\n\n\n\n\ndata-science\n\n\nweb-scrapping\n\n\nfinding-all\n\n\ntutorial-r\n\n\nbrasil\n\n\n\nIn this post, I show how to map every The Coffee shop in Brazil using web scraping and geocoding. I also show how to combine this information with Goolge Maps ratings and Census demographic data to produce valuable insights.\n\n\n\n\n\nJun 28, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCensus Tracts in Brazil\n\n\n\n\n\n\ncensus\n\n\nbrazil\n\n\nsao-paulo\n\n\ndata-science\n\n\ndata-visualization\n\n\nmaps\n\n\nleaflet\n\n\nggplot2\n\n\n\nCensus tracts are the smallest administrative divisions that provide socio-economic and demographic data. They are highly useful for statistical and spatial analysis, offering a robust and reliable set of information at a fine geographical scale.\n\n\n\n\n\nJun 14, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nFinding coffee shops in Brazil\n\n\n\n\n\n\ndata-science\n\n\nweb-scrape\n\n\ncoffee\n\n\nbrazil\n\n\nfinding\n\n\ntutorial-R\n\n\n\nIn this series of posts, I will explore the coffee shop scene across Brazil. We’ll focus on how we can use data to understand where these shops are located, their characteristics, and why they matter in the larger picture. All of this will be done using R and a few handy packages to web scrape addresses of coffee shops throughout the country.\n\n\n\n\n\nJun 1, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAnalfabetismo no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nbrazil\n\n\nmaps\n\n\n\nOs dados mais recentes do IBGE revelam um padrão geográfico marcante na taxa de analfabetismo no Brasil. Os municípios do nordeste do país apresentam taxas de analfabetismo quase 3x maiores do que no restante do país. O gender gap na taxa de alfabetização também é maior nas cidades do nordeste.\n\n\n\n\n\nMay 12, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nGenerations in Brazil\n\n\n\n\n\n\nbrazil\n\n\ndemographics\n\n\ndata-visualization\n\n\nggplot2\n\n\nenglish\n\n\n\nBrazil has an enormous ‘young’ population that will both boost its labor force in the years to come. Failures in the education system and unequal access to opportunities, however, may dampen this potential economic gain. Recent statistics indicate that Brazil has one of the largest NEET populations in the world.\n\n\n\n\n\nMay 5, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nGDP in Brazil\n\n\n\n\n\n\ndata-visualization\n\n\nbrazil\n\n\nmaps\n\n\nggplot2\n\n\nenglish\n\n\n\nA choropleth map showing the spatial distribution of Brazilian GDP by city. Colors represent the economic sector with the highest share of contribution to total city GDP. Data comes from the most recent National Accounts data from IBGE (2023).\n\n\n\n\n\nApr 20, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAdministrative and Statistical Divisions in Brazil\n\n\n\n\n\n\nbrazil\n\n\nmaps\n\n\ndata-science\n\n\ndata-visualization\n\n\nggplot2\n\n\nenglish\n\n\n\nIn this post I present the main administrative and statistical subdivisions of the Brazilian territory. All information comes from IBGE, the official statistical bureau. I also show how to easily interpolate data from these shapefiles to custom statistical grids such as Uber’s H3 hexagons.\n\n\n\n\n\nApr 19, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nThe Greatest Films of All-time: a data approach\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nmovies\n\n\nenglish\n\n\n\nIn this post I delve into the data to try to visualize patterns in movies rankings. Are movie critics biased towards a certain period of cinema? Are new movies overrated? Are they underrated? Movie rankings created by critics and the public show clear differences. Critics’ lists tend to be quite consistent and often favor older films. In contrast, rankings based on public online votes generally give more weight to recent films.\n\n\n\n\n\nApr 17, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nRadar Plots\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nreal-estate\n\n\nenglish\n\n\n\nRadar plots, or spider plots, are commonly used in dashboards and general data visualizations. Radar plots show a static visualization of the numeric values of several different categories of a same entity. In this post I explain how to make radar plots in R, discuss some of their shortcomings, and present some alternatives.\n\n\n\n\n\nApr 15, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nÍndice de Envelhecimento no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nggplot2\n\n\nbrasil\n\n\ndemografia\n\n\n\nUm mapa coroplético que apresenta o Índice de Envelhecimento por município do Brasil segundo os dados do mais recente Censo Demográfico do IBGE.\n\n\n\n\n\nApr 11, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nNascimentos e Óbitos no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nggplot2\n\n\nbrasil\n\n\ndemografia\n\n\n\nNeste post analiso os dados do mais recente Estatísticas do Registro Civil do IBGE. Esta base de dados estima o número total de nascidos vivos e de óbitos em cada município a cada ano. Os dados são bastante detalhados e permitem diversos tipos de análise.\n\n\n\n\n\nApr 10, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nOs Pecados da Visualização de Dados\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\ntutorial-R\n\n\n\nNeste post apresento algumas recomendações práticas para melhorar uma visualização. O gráfico original é de um relatório do PewResearch Center sobre a religiosidade nos países.\n\n\n\n\n\nApr 9, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nImportando dados em PDF no R\n\n\n\n\n\n\ndata-science\n\n\nweb-scraping\n\n\ntutorial-R\n\n\n\nNeste post vou mostrar uma solução para importar dados em formato PDF de maneira fácil e prática usando R. Arquivos PDF não são um típico formato de armazenamento de dados, como csv, PDFs são relatórios que combinam texto, imagens, tabelas, etc. Ainda assim, não é incomum receber tabelas de dados salvas dentro de arquivos PDF.\n\n\n\n\n\nApr 7, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nRazão de Dependência no Brasil\n\n\n\n\n\nUm mapa coroplético interativo que mostra as medida de Razão de Dependência no Brasil por estado. O mapa destaca tanto a Razão de Dependência Total, como a RD Jovem e RD Idosa.\n\n\n\n\n\nApr 4, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nDomicilios em Sao Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nggplot2\n\n\nsao-paulo\n\n\n\nUm mapa coroplético em formato de grid retangular 100 x 100m mostrando a densidade de domicílios particulares em São Paulo. Os dados são do Censo mais recente, de 2022.\n\n\n\n\n\nApr 2, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nMapas Interativos com Leaflet e R\n\n\n\n\n\n\nmapas\n\n\ndata-visualization\n\n\ntutorial-R\n\n\ndata-science\n\n\nleaflet\n\n\ndemografia\n\n\n\nNeste post mostro como fazer mapas interativos usando o pacote leaflet no R. O pacote é bastante flexível na construção de mapas, permitindo muitas opções de customização. Mostro como fazer mapas simples de pontos e também como fazer mapas coropléticos.\n\n\n\n\n\nMar 25, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nEncontrando todos os Starbucks do Brasil\n\n\n\n\n\n\nstarbucks\n\n\nweb-scrapping\n\n\ndata-science\n\n\nbrasil\n\n\ntutorial-R\n\n\n\nNeste post mostro como encontrar todos os Starbucks do Brasil usando webscrapping dentro do R\n\n\n\n\n\nMar 23, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nEnriquecendo e coletando do Google Maps\n\n\n\n\n\n\ndata-science\n\n\ngoogle-maps\n\n\ntutorial-R\n\n\nmapas\n\n\nleaflet\n\n\nstarbucks\n\n\n\nNeste post mostro como usar o Google Places API para importar informação do Google Maps dentro do R. O foco deste post será no Places, que encontra informações sobre estabelecimentos ou pontos de interesse. Também mostro como montar um mapa interativo com os resultados.\n\n\n\n\n\nMar 22, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCasas e Apartamentos\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nreal-estate\n\n\ndata-science\n\n\n\nNeste post exploro os dados recentes do Censo sobre tipos de domicílios nas cidades brasileiras.\n\n\n\n\n\nMar 19, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAcidentes de Trânsito em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\n\nNeste post exploro o padrão espacial dos acidentes de trânsito em São Paulo. O detalhamento dos dados nos permite enxergar padrões sazonais intradiários e intrasemanais. De maneira geral, o maior número de acidentes coincide com locais e regiões de grande tráfego de veículos.\n\n\n\n\n\nMar 15, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Affordability em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a distribuição do Price-Income-Ratio (PIR) em São Paulo por regiões. O PIR mede a acessibilidade financeira aos imóveis.\n\n\n\n\n\nMar 7, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAno de 2024 começa mais difícil\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nbrazil\n\n\neconomics\n\n\n\nSegundo os dados mais recentes do IBGE, o carry-over do PIB de 2023 para 2024 é de apenas 0,2%, menor valor da série histórica (excluindo anos de recessão).\n\n\n\n\n\nMar 4, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nRecessões no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nbrasil\n\n\neconomia\n\n\nggplot2\n\n\n\nNeste post, exploro as recessões econômicas mais recentes na histórica econômica do Brasil. Mostro como comparar as recessões de diferentes maneiras e construo uma tabela informativa.\n\n\n\n\n\nMar 1, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nÍndices de Preços Imobiliários no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nbrasil\n\n\nggplot2\n\n\nreal-estate\n\n\n\nNeste post discuto a teoria sobre índices de preços imobiliários e apresento os principais índices disponíveis no Brasil. Pela discussão teórica deve ficar claro as principais limitações de cada índice.\n\n\n\n\n\nFeb 25, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nPreços de Aluguel e de Venda de Imóveis\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nweekly-viz\n\n\nbrasil\n\n\nreal-estate\n\n\n\nOs mercados de aluguel e de venda de imóveis são interligados e deve existir algum tipo de equilíbrio entre eles. Neste post exploro macrotendências destes mercados usando a série do FipeZap. Os resultados indicam a possibilidade um novo ciclo de aluguel, uma quebra na tendência histórica entre os mercados.\n\n\n\n\n\nFeb 21, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nIDH por região em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a distribuição do IDH por regiões em São Paulo. Os dados são importados do projeto Atlas Brasil e estão a nível de UDH (Unidade de Desenvolvimento Humano).\n\n\n\n\n\nFeb 21, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nMédias móveis\n\n\n\n\n\n\ndata-science\n\n\neconomia\n\n\ntutorial-R\n\n\neconometria\n\n\ntime-series\n\n\n\nO filtro de médias móveis serve para suavizar séries de tempo e encontrar tendências nos dados. Este filtro é bastante simples e pode ser escalado com facilidade para lidar com múltiplas séries de tempo.\n\n\n\n\n\nFeb 20, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nEnergia Elétrica e Crescimento Econômico no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nweekly-viz\n\n\ntime-series\n\n\n\nA demanda por energia elétrica é geralmente entendida como uma proxy do nível de produção de um país. Na prática, ela funciona como uma proxy mensal do PIB. Neste visualização mostro como a evolução destas variáveis divergiu nos últimos anos.\n\n\n\n\n\nFeb 15, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nFiltro HP e Filtro de Hamilton\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\ntutorial-R\n\n\neconomia\n\n\n\nCiclos variam de periodicidade e há várias abordagens para decompor uma série de tempo, separando a tendência do ruído. Neste post, discuto brevemente sobre os tipos de tendências e, talvez, a metodologia mais comum para encontrar o ciclo de uma série: o filtro HP. Também discuto um recente paper de Hamilton (2017) que critica o uso do filtro HP e propõe um novo filtro baseado numa regressão linear simples.\n\n\n\n\n\nFeb 14, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAcessibilidade financeira à moradia em São Paulo\n\n\n\n\n\n\ndata-science\n\n\ndata-visualization\n\n\nsao-paulo\n\n\nreal-estate\n\n\n\nNeste post exploro em maiores detalhes indicadores de acessibilidade financeira à moradia. Apresento uma análise histórica a nível nacional e o retrato da desigualdade do acesso à moradia em São Paulo.\n\n\n\n\n\nFeb 10, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nDistribuição de Renda em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a distribuição de renda em São Paulo. Os dados são importados do projeto Acesso a Oportunidades do IPEA e apresentados no padrão H3 da Uber.\n\n\n\n\n\nFeb 7, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\neconometria\n\n\n\nO algoritmo de Gradient Descent é um otimizador amplamente utilizado em Machine Learning para minimizar funções de custo. Este otimizador simples é baseado na direção do gradiente da função. Neste post explico a matemática que sustenta este método e mostro como implementá-lo passo a passo.\n\n\n\n\n\nFeb 5, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nPreços de Imóveis no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\nweekly-viz\n\n\nbrasil\n\n\nreal-estate\n\n\n\nExiste uma impressão generalizada de que o preço dos imóveis no Brasil cresceu muito nos últimos anos. Neste post mostro como, em termos reais, o preço médio dos imóveis no Brasil andou de lado; na prática, os preços atuais estão abaixo dos valores observados em 2010.\n\n\n\n\n\nJan 28, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nTendência e Sazonalidade\n\n\n\n\n\n\ndata-science\n\n\neconomia\n\n\ntutorial-R\n\n\neconometria\n\n\ntime-series\n\n\n\nSéries de tempo apresentam diversos padrões sazonais. Em econometria, o interesse nem sempre está na sazonalidade, em si, mas na sua eliminação para chegar na tendência de uma série. Este post apresenta algumas maneiras para visualizar e modelar a sazonalidade de uma série.\n\n\n\n\n\nJan 23, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAcesso a Hospitais e Leitos em São Paulo\n\n\n\n\n\n\ndata-science\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\ntransporte\n\n\n\nNeste post, vou mapear a acessibilidade a hospitais e leitos em São Paulo. Para avaliar quantitativamente o nível de acessibilidade vou montar uma métrica bastante simples: o tempo mínimo necessário que se leva para chegar no hospital/leito mais próximo, considerando um deslocamento de bicicleta.\n\n\n\n\n\nJan 20, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCarros e Renda em Sao Paulo\n\n\n\n\n\n\ndata-science\n\n\nsao-paulo\n\n\ntransporte\n\n\n\nEste post analisa a relação entre a posse de automóveis e a renda domiciliar usando dados da Pesquisa Origem e Destino do Metrô de 2017. Um modelo de escolha discreta revela um impacto significativo da renda na decisão de possuir um carro. Idade, educação e filhos também são fatores importantes e que aumentam a probabilidade do domicílio ter um automóvel. O único fator encontrado que reduz a probabilidade do domicílio ter um automóvel é ele ser chefiado por uma mulher.\n\n\n\n\n\nJan 15, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: mutate\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\ntidyverse\n\n\n\nNeste post ensino abordagens diferentes para criar colunas de maneira eficiente. Apresento também as novidades que o dplyr trouxe nos últimos anos como a função across e novo argumento .by.\n\n\n\n\n\nJan 11, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: filter\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\ntidyverse\n\n\n\nNeste post ensino abordagens diferentes para filtrar linhas de uma tabela de maneira eficiente. Apresento também algumas das inovações que o pacote dplyr lançou nos últimos anos como as funções auxiliares if_any e if_all.\n\n\n\n\n\nJan 10, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: rename\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\ntidyverse\n\n\n\nNeste post ensino abordagens diferentes para renomear colunas de maneira eficiente. Apresento também algumas das inovações que o pacote dplyr lançou nos últimos anos como as funções auxiliares all_of e any_of.\n\n\n\n\n\nJan 8, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nO novo tidyverse: select\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\ntidyverse\n\n\n\nNeste post ensino abordagens diferentes para selecionar colunas de maneira eficiente. Apresento também as novidades que o dplyr trouxe nos últimos anos como as funções tidyselectors, que ajudam a selecionar colunas com base em padrões lógicos.\n\n\n\n\n\nJan 6, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nUruguay in numbers\n\n\n\n\n\n\ndata-visualization\n\n\nlatin-america\n\n\nggplot2\n\n\nenglish\n\n\n\nUruguay has earned acclaim as a model democracy, leading the Economist’s Democracy Index and holding the status of the least corrupt country in Latin America. With a GINI index around 40, it stands out for its commitment to social equality, contrasting sharply with Brazil’s index of 52-53. I dedicated a Saturday afternoon to delve into Uruguay’s history, culture, and contemporary landscape, documenting my learning journey.\n\n\n\n\n\nJan 5, 2024\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nBump Plots\n\n\n\n\n\n\nggplot2\n\n\ntutorial-R\n\n\n\nUm ‘bump chart’ mostra diferentes valores de uma variável em contextos distintos. É similar a um gráfico de tendências paralelas mas com linhas mais suaves. Neste post mostro como fazer este tipo de gráfico no R usando ggplot2.\n\n\n\n\n\nDec 26, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Brazilian Inflation\n\n\n\n\n\n\ndata-visualization\n\n\nweekly-viz\n\n\nenglish\n\n\nggplot2\n\n\nbrasil\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nReplicando gráficos\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\n\nNeste post mostro como replicar alguns gráficos de publicações usando apenas o ggplot2.\n\n\n\n\n\nNov 30, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Car Dependency in São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nsao-paulo\n\n\nggplot2\n\n\nweekly-viz\n\n\ntransporte\n\n\n\nSão Paulo boasts a predominantly clean energy matrix, with the city’s primary challenge lying in the transportation sector. A simple manner to capture car dependency is to measure the prevalence of car-free households. Using OD data across 329 zones I make simple visualizations that suggest that income constraints, rather than preferences, play a pivotal role in keeping car usage low. Notably, some affluent OD zones in close proximity to key business districts exhibit the lowest share of car-free households.\n\n\n\n\n\nNov 24, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCenso 2022: O que houve de errado?\n\n\n\n\n\n\nbrasil\n\n\ndemografia\n\n\ncenso\n\n\n\nHouve certo rebuliço nas redes sociais, quando do lançamento dos dados mais recentes do Censo Demográfico de 2022. O fato carregado na maior parte das manchetes do Brasil foi a queda na população brasileira. Até 2021, projetava-se que a população brasileira estivesse em torno de 213 milhões de habitantes. O número que o Censo trouxe foi de 203 milhões, queda de 10 milhões de habitantes em relação ao previsto.\n\n\n\n\n\nNov 17, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCarros em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\ntransporte\n\n\n\nUm mapa coroplético que apresenta o número de carros por domicílios em São Paulo. Enquanto o Centro Antigo tem algumas das taxas mais baixas, bairros centrais como Pacembu e Jardim Europa tem as taxas mais elevadas.\n\n\n\n\n\nNov 17, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nNascimentos no Brasil\n\n\n\n\n\n\ndata-science\n\n\ndata-visualization\n\n\nbrasil\n\n\ndemografia\n\n\ntime-series\n\n\n\nTende-se a pensar que a data de nascimento de um indivíduo é algo completamente aleatório. Neste post faço algumas visualizações para investigar o padrão de sazonalidade nos nascimentos e concluo com uma regressão simples que aponta a presença de sazonalidade.\n\n\n\n\n\nNov 13, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nEnsino Superior em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta o percentual de adultos com ensino superior. Vê-se nos dados o conhecido padrão da cidade: indicadores de ensino superior altos dentro do Centro Expandido da cidade, que vão decaindo gradativamente à medida que se aproxima da periferia.\n\n\n\n\n\nNov 10, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nExpectativa de Vida em São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\nsao-paulo\n\n\nggplot2\n\n\n\nUm mapa coroplético que apresenta a desigualdade na expectativa de vida nos distritos de São Paulo. Do Jardim Paulista até Anhanguera são 23 anos de diferença na idade média ao morrer. Enquanto no Jardim Paulista espera-se viver até 82 anos, em Anhanguera a expectativa de vida não chega aos 60 anos.\n\n\n\n\n\nNov 3, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Transportation in São Paulo\n\n\n\n\n\n\ndata-visualization\n\n\nweekly-viz\n\n\nenglish\n\n\nsao-paulo\n\n\ntransporte\n\n\n\nThis week, I delve into the transportation modes within the Greater São Paulo Region by analyzing data sourced from the Origin Destination Survey. The achievement of recently announced sustainability goals, targeting net-zero emissions by 2050, hinges significantly on transportation choices.\n\n\n\n\n\nNov 3, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nEnvelhecimento no Brasil\n\n\n\n\n\n\ndata-visualization\n\n\nggplot2\n\n\ncenso\n\n\ndemografia\n\n\nbrasil\n\n\n\nO futuro demográfico do Brasil, em grande parte, já é conhecido. Assim como no resto do mundo, a combinação de queda de taxa de fecundidade e aumento de expectativa de vida implica no envelhecimento da população. Neste post mostro a distribuição espacial do índice de envelhecimento no Brasil.\n\n\n\n\n\nOct 31, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz - Brazilian Census\n\n\n\n\n\n\ndata-visualization\n\n\nweekly-viz\n\n\nenglish\n\n\nbrasil\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nUm pacote com dados do mercado imobiliário\n\n\n\n\n\n\ntutorial-R\n\n\ndata-science\n\n\nreal-estate\n\n\n\nConsumir os dados do mercado imobiliário brasileiro não é tarefa fácil. Pensando em simplificar este processo eu criei um pacote do R, que importa e limpa diversas bases de dados relacionadas ao mercado residencial.\n\n\n\n\n\nOct 26, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nBrazil in Charts: Unemployment\n\n\n\n\n\n\ndata-visualization\n\n\nbrazil\n\n\nweekly-viz\n\n\nggplot2\n\n\nenglish\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nImportando arquivos, visualizando linhas\n\n\n\n\n\n\ntutorial-R\n\n\ndata-science\n\n\n\nToda análise de dados passa por tarefas de rotina: importar dados, trocar nomes de colunas, remover observações vazias, etc. Por que não facilitar a sua vida e tornar essas tarefas simples?\n\n\n\n\n\nOct 19, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCidades Brasil\n\n\n\n\n\n\ntutorial-R\n\n\nmapas\n\n\nggplot2\n\n\n\nEste tutorial ensina como fazer um mapa da altitude de ruas de uma cidade usando programação funcional. O princípio da programação funcional é de decompor uma tarefa complexa em funções modulares, que tornam o código mais eficiente e estável. Ao final do tutorial, será possível reproduzir o mapa de altitude usando somente uma linha de código.\n\n\n\n\n\nOct 15, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Nascimentos no Brasil\n\n\n\n\n\n\nggplot2\n\n\ndata-visualization\n\n\nbrasil\n\n\nweekly-viz\n\n\n\nNesta semana resolvi olhar os nascimentos no Brasil. Há menos pessoas nascendo? Em quais meses nascem mais bebês?\n\n\n\n\n\nOct 13, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nA filosofia do Tidyverse\n\n\n\n\n\n\ntidyverse\n\n\ndata-science\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nLife Satisfaction and GDP per capita\n\n\n\n\n\n\ndata-visualization\n\n\ntutorial-R\n\n\nggplot2\n\n\nenglish\n\n\n\nIn this post I make a step-by-step replication of a plot from OurWorldInData. The plot shows the correlation between average self-reported life satisfaction and GDP per capita.\n\n\n\n\n\nSep 22, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nPipes\n\n\n\n\n\n\ndata-science\n\n\ntutorial-R\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Ruas de Porto Alegre\n\n\n\n\n\n\nmapas\n\n\nggplot2\n\n\nweekly-viz\n\n\ndata-visualization\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nMapa de altitude de ruas de Brasília\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nShiny Dashboard: IDH municípios\n\n\n\n\n\n\nshiny\n\n\ndata-visualization\n\n\nbrasil\n\n\n\nNeste post apresento o Dashboard do IDH de municípios que fiz com base nos dados da Firjan. O Dashboard foi feito com Shiny.\n\n\n\n\n\nAug 25, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nWeekly Viz: Recife em mapas\n\n\n\n\n\n\nmapas\n\n\nggplot2\n\n\nweekly-viz\n\n\ndata-visualization\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nO impacto dos juros na demanda imobiliário\n\n\n\n\n\n\nreal-estate\n\n\neconomia\n\n\n\nA taxa de juros é talvez a variável macroeconômica mais importante para se observar quando se pensa em financiamento imobiliário. Neste post apresento o básico do financiamento habitacional e mostro como o aumento dos juros impactou a acessibilidade à moradia. Por fim, comento sobre a recente queda na taxa SELIC.\n\n\n\n\n\nAug 17, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nImportando dados do SIDRA\n\n\n\n\n\n\ndata-science\n\n\neconomia\n\n\ntutorial-R\n\n\n\nImportando dados abertos do IBGE via API usando o sidrar no R.\n\n\n\n\n\nAug 10, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nDefinindo objetos no R. = ou &lt;- ?\n\n\n\n\n\n\ntutorial-R\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nPacotes Essenciais R\n\n\n\n\n\n\ndata-science\n\n\neconometria\n\n\ntutorial-R\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizando uma única variável\n\n\n\n\n\n\ndata-visualization\n\n\ntutorial-R\n\n\nrepost\n\n\nggplot2\n\n\n\nAlgumas ideias soltas sobre como visualizar uma única variável usando vários tipos de gráficos distintos em ggplot2.\n\n\n\n\n\nJun 28, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nPreços de Imóveis e Demografia\n\n\n\n\n\n\nreal-estate\n\n\neconomia\n\n\n\nEm algum sentido, o preço dos imóveis reflete tendências demográficas de longo prazo: a mais simples delas é o crescimento populacional. Neste post analiso padrões de crescimento demográfico e de preços de imóveis.\n\n\n\n\n\nJun 20, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nSéries de Tempo no R\n\n\n\n\n\n\ntime-series\n\n\ntutorial-R\n\n\neconometria\n\n\n\nNeste post faço um panorama geral de como estimar um modelo SARIMA no R usando majoritariamente funções base. O R vem equipado com diversas funções úteis e poderosas para lidar com séries de tempo.\n\n\n\n\n\nJun 1, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nUsando fontes com showtext no R\n\n\n\n\n\n\ntutorial-R\n\n\n\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar. Neste post ensino como importar fontes no R para criar visualizações mais refinadas e profissionais.\n\n\n\n\n\nMay 10, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizando o IPCA\n\n\n\n\n\n\ndata-visualization\n\n\nbrasil\n\n\neconomia\n\n\ntutorial-R\n\n\nggplot2\n\n\n\nNeste post discuto e apresento diversas maneiras de visualizar o IPCA incluindo alguns gráficos peculiares para enxergar a distribuição dos valores da inflação.\n\n\n\n\n\nDec 1, 2022\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nARMA: um exemplo simples\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\nrepost\n\n\ntutorial-R\n\n\n\nNeste post mostro como modelar um ARMA simples no R usando o pacote {astsa}.\n\n\n\n\n\nMar 1, 2021\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nEMV no R\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\nOs estimadoes de máxima verossimilhança possuem várias boas propriedades. Neste post discuto tanto aspectos teóricos como aplicados, com exemplos, e faço algumas simulações para comprovar resultados assintóticos.\n\n\n\n\n\nFeb 1, 2020\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nAquecimento Global\n\n\n\n\n\n\ndata-visualization\n\n\ntutorial-R\n\n\nrepost\n\n\nggplot2\n\n\n\nCada listra nesta imagem representa a temperatura de um ano desde o 1850 até o presente. A mensagem é bastante clara: o planeta está cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Neste post discuto como reproduzir esta visualização usando R.\n\n\n\n\n\nJan 10, 2020\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nOLS com matrizes\n\n\n\n\n\n\neconometria\n\n\nrepost\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nRepost: Otimização numérica - métodos de Newton\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\n\n\n\n\n\nSep 28, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nCrescimento do PIB per capita no mundo\n\n\n\n\n\n\nrepost\n\n\ndata-visualization\n\n\nggplot2\n\n\ntutorial-R\n\n\n\nReproduzindo uma visualização do portal Nexo\n\n\n\n\n\nJun 1, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nRepost: Expectativa de vida e Crescimento Econômico\n\n\n\n\n\n\ndata-visualization\n\n\neconomia\n\n\nrepost\n\n\ntutorial-R\n\n\nggplot2\n\n\n\nNeste tutorial mostro como criar visualizações ricas mostrando correlações, usando os dados de crescimento econômico e expectativa de vida do Gapminder.\n\n\n\n\n\nMay 6, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nMQO - teoria assintótica\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\nNeste post discuto e apresento simulações de alguns resultados assintóticos de MQO. Estes resultados permitem entender a distribuição dos estimadores MQO e o comportamento dos coeficientes à medida que o tamanho da amostra aumenta. Compreender esses resultados é fundamental para realizar inferências estatísticas precisas na análise de regressão.\n\n\n\n\n\nMar 23, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nTeoria Assintótica - LGN e TCL\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\nSimulações de dois resultados centrais para a econometria: a Lei dos Grandes Números e o Teorema Central do Limite\n\n\n\n\n\nMar 23, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nRegressão Linear com Séries de Tempo\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\nrepost\n\n\ntutorial-R\n\n\n\nCursos de econometria em séries de tempo às vezes omitem o uso de MQO num contexto de séries de tempo. Neste post discuto um pouco da teoria e de aplicações no R.\n\n\n\n\n\nJan 1, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n\n\n\n\n\n\nSARIMA no R\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\nrepost\n\n\ntutorial-R\n\n\n\nUm tutorial conciso sobre como utilizar o modelo SARIMA para análise e previsão de séries temporais no R. Aprenda a identificar componentes sazonais, ajustar parâmetros e fazer previsões precisas com este guia passo a passo.\n\n\n\n\n\nJan 1, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Atlas Brasil\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDH dos municípios do Brasil\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemographic Pyramids in R\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\n\n\n\n\n\nFilm ratings over the decades: replicating a Nexo plot\n\n\n\n\n\n\n\n\nDec 9, 2024\n\n\n\n\n\n\n\nPunchcard plots in R\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\n\n\n\n\n\nReplicating the Github Contributions plot in R using ggplot2\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\nLinha-4 Amarela Metrô de São Paulo\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n\nFinding All Starbucks in Brazil\n\n\n\n\n\n\n\n\nJul 14, 2024\n\n\n\n\n\n\n\nLine-4 Metro\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\nLocating all The Coffee shops in Brazil\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\nCensus Tracts in Brazil\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\nFinding coffee shops in Brazil\n\n\n\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\nAnalfabetismo no Brasil\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\nGenerations in Brazil\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nGDP in Brazil\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\nAdministrative and Statistical Divisions in Brazil\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\nThe Greatest Films of All-time: a data approach\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\nRadar Plots\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\nÍndice de Envelhecimento no Brasil\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\nNascimentos e Óbitos no Brasil\n\n\n\n\n\n\n\n\nApr 10, 2024\n\n\n\n\n\n\n\nOs Pecados da Visualização de Dados\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\n\n\n\n\n\nImportando dados em PDF no R\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n\nRazão de Dependência no Brasil\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\nDomicilios em Sao Paulo\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\nMapas Interativos com Leaflet e R\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\n\n\n\n\n\nEncontrando todos os Starbucks do Brasil\n\n\n\n\n\n\n\n\nMar 23, 2024\n\n\n\n\n\n\n\nEnriquecendo e coletando do Google Maps\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n\nCasas e Apartamentos\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\nAcidentes de Trânsito em São Paulo\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\nHousing Affordability em São Paulo\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\nAno de 2024 começa mais difícil\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\n\n\n\n\n\nRecessões no Brasil\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\nÍndices de Preços Imobiliários no Brasil\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\nPreços de Aluguel e de Venda de Imóveis\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\nIDH por região em São Paulo\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\nMédias móveis\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\nEnergia Elétrica e Crescimento Econômico no Brasil\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\nFiltro HP e Filtro de Hamilton\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\nAcessibilidade financeira à moradia em São Paulo\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\nDistribuição de Renda em São Paulo\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\nGradient Descent\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n\nPreços de Imóveis no Brasil\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\nTendência e Sazonalidade\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\n\n\n\n\n\nAcesso a Hospitais e Leitos em São Paulo\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\nCarros e Renda em Sao Paulo\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\nO novo tidyverse: mutate\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\n\n\n\n\n\nO novo tidyverse: filter\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\n\n\n\n\n\nO novo tidyverse: rename\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n\nO novo tidyverse: select\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\n\n\n\n\n\nUruguay in numbers\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\nYou need a map - Parte 2\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\nYou need a map - Parte 1\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\n\n\n\n\n\nBump Plots\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\nWeekly Viz: Brazilian Inflation\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\nReplicando gráficos\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\nWeekly Viz: Car Dependency in São Paulo\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\n\n\n\n\n\nCenso 2022: O que houve de errado?\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\nCarros em São Paulo\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\nIndo além: mapas de clusters\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\nNascimentos no Brasil\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n\nEnsino Superior em São Paulo\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\nExpectativa de Vida em São Paulo\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\nWeekly Viz: Transportation in São Paulo\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n\nIndo além: empilhando áreas\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n\n\n\n\n\nEnvelhecimento no Brasil\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n\nWeekly Viz - Brazilian Census\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\nUm pacote com dados do mercado imobiliário\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n\n\n\n\n\nIndo além: facets\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\n\n\n\n\n\nBrazil in Charts: Unemployment\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\nImportando arquivos, visualizando linhas\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n\nIndo Além: Lollipops\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\nCidades Brasil\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\nWeekly Viz: Nascimentos no Brasil\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\nEstético: Tipografia e temas\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\nA filosofia do Tidyverse\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\n\n\n\n\n\nLife Satisfaction and GDP per capita\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\nPipes\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\nWeekly Viz: Ruas de Porto Alegre\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\nEstético: Escalas e Cores\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\nMapa de altitude de ruas de Brasília\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\n\n\n\n\n\nEstético: Destacando informação\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\n\nShiny Dashboard: IDH municípios\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\n\n\n\n\n\nWeekly Viz: Recife em mapas\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\n\n\n\n\n\nFundamentos: gráfico de linha\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\nO impacto dos juros na demanda imobiliário\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\n\n\n\n\n\nFundamentos: histograma\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n\nImportando dados do SIDRA\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\n\n\n\n\n\nFundamentos: gráfico de coluna\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\n\n\n\n\n\nFundamentos: gráfico de dispersão\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\n\nDefinindo objetos no R. = ou &lt;- ?\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\n\n\n\n\n\nApêndice: manipular para enxergar\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\n\n\n\n\n\nIntrodução\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\nPacotes Essenciais R\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\nVisualizando uma única variável\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\nPreços de Imóveis e Demografia\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\nSéries de Tempo no R\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\nUsando fontes com showtext no R\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\nVisualizando o IPCA\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\nARMA: um exemplo simples\n\n\n\n\n\n\n\n\nMar 1, 2021\n\n\n\n\n\n\n\nEMV no R\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\nAquecimento Global\n\n\n\n\n\n\n\n\nJan 10, 2020\n\n\n\n\n\n\n\nOLS com matrizes\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\n\nRepost: Otimização numérica - métodos de Newton\n\n\n\n\n\n\n\n\nSep 28, 2019\n\n\n\n\n\n\n\nCrescimento do PIB per capita no mundo\n\n\n\n\n\n\n\n\nJun 1, 2019\n\n\n\n\n\n\n\nRepost: Expectativa de vida e Crescimento Econômico\n\n\n\n\n\n\n\n\nMay 6, 2019\n\n\n\n\n\n\n\nMQO - teoria assintótica\n\n\n\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\nTeoria Assintótica - LGN e TCL\n\n\n\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\nRegressão Linear com Séries de Tempo\n\n\n\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\n\nSARIMA no R\n\n\n\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "On this website, you’ll find various posts about economics, data visualization, and data analysis. I also write several tutorials about R, the programming language in which I’m most proficient. Most of the posts and texts are under the Blog tab. I have some applications, developed with Shiny, under the Apps tab.\nThis site was originally launched in late 2019, using RMarkdown and the blogdown package. The current version was officially released in 2023, using Quarto, an integrated language for presenting data analyses, which combines R, Python, Julia, and several other programming languages.\n\n\n\nMy name is Vinícius Reginatto, I’m an economist and data scientist currently based in São Paulo, Brazil. I have been working as an economic consultant since 2019, focusing on urban economics and the real estate market. I’ve also worked at tech startups as a data scientist and spokesperson. Link to my CV."
  },
  {
    "objectID": "aboutme.html#mini-cv",
    "href": "aboutme.html#mini-cv",
    "title": "RealEstate Insight",
    "section": "Mini-CV",
    "text": "Mini-CV\nUniversity of São Paulo | São Paulo, SP. MSc Economics1. CAPES\nFederal University of Rio Grande do Sul2 | Porto Alegre, RS Bachelor (B.A.) Economics. Mathematical Economics.\nHolds a Master’s degree in Economics from the University of São Paulo, specialized in mathematical economics and time series. Working in data analysis and real estate market since 2019, with a focus on the residential market. Has worked on various consultancy projects related to the residential market and urban public policy (TOD).\nLanguages: English, Portuguese, and Spanish.\n\nSome of my work\n\nHousing Affordability in São Paulo: overview and measurement presented at the 2021 Latin American Real Estate Society.\nMicroapartamentos: mercado e tendências (2022)\nO Mercado Residencial na América Latina (2022)"
  },
  {
    "objectID": "aboutme.html#footnotes",
    "href": "aboutme.html#footnotes",
    "title": "Vinicius Oike Reginatto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Economics Graduate course has been evaluated with a grade 7 (the highest grade), since the inception of the Capes’ plurianual review, indicating its high standard of international performance.↩︎\nRanked by INEP (Ministry of Education) as the best public university in Brazil since 2012 and during 2012-2014 also as the best university in Brazil. Commonly ranked in the Top 10 of all Brazilian universities in both national and international rankings.↩︎\nI’ve delivered public presentations, meetings, and interviews in all these languages. You can see some of my highlighted interviews listed here.↩︎"
  },
  {
    "objectID": "posts/general-posts/arima-no-r/index.html",
    "href": "posts/general-posts/arima-no-r/index.html",
    "title": "Séries de Tempo no R",
    "section": "",
    "text": "Neste post vou explorar um pouco das funções base do R para montar um modelo SARIMA. O R vem “pré-equipado” com um robusto conjunto de funções para lidar com séries de tempo. Inclusive, como se verá, existe uma class específica de objeto para trabalhar com séries de tempo."
  },
  {
    "objectID": "posts/general-posts/arima-no-r/index.html#modelagem-sarima",
    "href": "posts/general-posts/arima-no-r/index.html#modelagem-sarima",
    "title": "Séries de Tempo no R",
    "section": "Modelagem SARIMA",
    "text": "Modelagem SARIMA\nAqui a ideia é experimentar com alguns modelos simples. Em especial, o modelo que Box & Jenkins sugerem para a série é de um SARIMA (0, 1, 1)(0, 1, 1)[12] da forma\n\\[\n(1 - \\Delta)(1 - \\Delta^{12})y_{t} = \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t_1} + \\Theta_{1}\\varepsilon_{t-12}\n\\]\nA metodologia correta para a análise seria primeiro fazer testes de raiz unitária para avaliar a estacionaridade da série. Mas só de olhar para as funções de autocorrelação e autocorrelação parcial, fica claro que há algum componente sazonal e que a série não é estacionária.\n\n\n\n\n\n\n\n\n\n\nTeste de raiz unitária\nApenas a título de exemplo, faço um teste Dickey-Fuller (ADF), bastante geral, com constante e tendência temporal linear. Para uma boa revisão metodológica de como aplicar testes de raiz unitária, em partiular o teste ADF, consulte o capítulo de séries não-estacionárias do livro Applied Econometric Time Series do Enders\nAqui, a escolha ótima do lag é feita usando o critério BIC (também conhecido como Critério de Schwarz). Não existe uma função que aplica o teste ADF no pacote base o R. A implementação é feita na função ur.df do pacote urca.\nA estatísitica de teste mais relevante é a tau3 e vê-se, surpreendentemente, que se rejeita a hipótese nula de raiz unitária. As estatísticas phi2 e phi3 são testes-F da significânica conjunta dos termos de constante e de tendência temporal. As estatísticas de teste são convencionais e seguem a notação do livro do Enders citado acima e também do clássico livro do Hamilton.\n\nlibrary(urca)\nadf_test &lt;- ur.df(y, type = \"trend\", selectlags = \"BIC\", lags = 13)\nsummary(adf_test)\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090139 -0.022382 -0.002417  0.021008  0.110003 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.8968049  0.3999031   2.243 0.026859 *  \nz.lag.1      -0.1809364  0.0842729  -2.147 0.033909 *  \ntt            0.0016886  0.0008609   1.961 0.052263 .  \nz.diff.lag1  -0.2588927  0.1134142  -2.283 0.024301 *  \nz.diff.lag2  -0.0986455  0.1070332  -0.922 0.358665    \nz.diff.lag3  -0.0379799  0.1045583  -0.363 0.717097    \nz.diff.lag4  -0.1392651  0.0981271  -1.419 0.158560    \nz.diff.lag5  -0.0283998  0.0963368  -0.295 0.768686    \nz.diff.lag6  -0.1326313  0.0889223  -1.492 0.138581    \nz.diff.lag7  -0.1096365  0.0865862  -1.266 0.208019    \nz.diff.lag8  -0.2348880  0.0829892  -2.830 0.005497 ** \nz.diff.lag9  -0.0926604  0.0843594  -1.098 0.274344    \nz.diff.lag10 -0.2053937  0.0789245  -2.602 0.010487 *  \nz.diff.lag11 -0.1081091  0.0786801  -1.374 0.172127    \nz.diff.lag12  0.6633101  0.0752086   8.820 1.54e-14 ***\nz.diff.lag13  0.3197783  0.0883636   3.619 0.000443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04011 on 114 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8621 \nF-statistic: 54.75 on 15 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -2.147 4.9781 3.4342 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n\n\nPela análise do correlograma do resíduo da regressão, fica claro que ainda há autocorrelação. Novamente, o mais correto seria aplicar o teste Ljung-Box sobre os resíduos para verificar a presença de autocorrelação conjunta nas primeiras k defasagens, mas este não é o foco deste post. Esta pequena digressão exemplifica como a aplicação destes testes em séries de tempo pode não ser tão direto/simples.\n\n\n\n\n\n\n\n\n\nOs gráficos abaixo mostram o correlograma da série após tirarmos a primeira diferença e a primeira diferença sazonal. A partir da análise destes correlogramas poderíamos inferir algumas especificações alternativas para modelos SARIMA e aí, poderíamos escolher o melhor modelo usando algum critério de informação.\nA metodologia Box & Jenkins de análise de séries de tempo tem, certamente, um pouco de arte e feeling. Não é tão imediato entender como devemos proceder e, na prática, faz sentido experimentar com vários modelos alternativos de ordem baixa como SARIMA(1, 1, 1)(0, 1, 1), SARIMA(2, 1, 0)(0, 1, 1), etc.\n\n\n\n\n\n\n\n\n\n\n\nOs modelos\nPara não perder muito tempo experimentando com vários modelos vou me ater a três modelos diferentes. Uma função bastante útil é a auto.arima do pacote forecast que faz a seleção automática do melhor modelo da classe SARIMA/ARIMA/ARMA.\nEu sei que o Schumway/Stoffer, autores do ótimo Time Series Analysis and Its Applications, tem um post crítico ao uso do auto.arima. Ainda assim, acho que a função tem seu mérito e costuma ser um bom ponto de partida para a sua análise. Quando temos poucas séries de tempo para analisar, podemos nos dar ao luxo de fazer a modelagem manualmente, mas quando há centenas de séries, é muito conveniente poder contar com o auto.arima.\nComo o auto.arima escolhe o mesmo modelo do Box & Jenkins eu experimento com uma especificação diferente. Novamente, a título de exemplo eu comparo ambos os modelos SARIMA com uma regressão linear simples que considera uma tendência temporal linear e uma série de dummies sazonais. O modelo é algo da forma\n\\[\ny_{t} = \\alpha_{0} + \\alpha_{1}t + \\sum_{i = 1}^{11}\\beta_{i}s_{i} + \\varepsilon_{t}\n\\]\nOnde \\(s_{i}\\) é uma variável indicadora igual a 1 se \\(t\\) corresponder ao mês \\(i\\) e igual a 0 caso contrário. Vale notar que não podemos ter uma dummy para todos os meses se não teríamos uma matriz de regressores com colinearidade perfeita!\nAqui vou contradizer um pouco o espírito do post novamente para usar o forecast. O ganho de conveniência vem na hora de fazer as previsões. Ainda assim, indico como estimar os mesmos modelos usando apenas funções base do R.\n\n# Usando o forecast\nlibrary(forecast)\nmodel1 &lt;- auto.arima(train)\nmodel2 &lt;- Arima(train, order = c(1, 1, 1), seasonal = c(1, 1, 0))\nmodel3 &lt;- tslm(train ~ trend + season)\n\n\n# Usando apenas funções base\nmodel2 &lt;- arima(\n  trains,\n  order = c(1, 1, 1),\n  seasonal = list(order = c(1, 1, 1), period = 12)\n)\n\n# Extrai uma tendência temporal linear \ntrend &lt;- time(train)\n# Cria variáveis dummies mensais\nseason &lt;- cycle(train)\nmodel3 &lt;- lm(train ~ trend + season)\n\nA saída dos modelos segue abaixo. As saídas dos modelos SARIMA não são muito interessantes. Em geral, não é muito comum avaliar nem a significância e nem o sinal dos coeficientes, já que eles não têm muito valor interpretativo. Uma coisa que fica evidente dos dois modelos abaixo é que o primeiro parece melhor ajustado aos dados pois tem valores menores em todos os critérios de informação considerados.\n\nmodel1\n\nSeries: train \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.3424  -0.5405\ns.e.   0.1009   0.0877\n\nsigma^2 = 0.001432:  log likelihood = 197.51\nAIC=-389.02   AICc=-388.78   BIC=-381\n\n\n\nmodel2\n\nSeries: train \nARIMA(1,1,1)(1,1,0)[12] \n\nCoefficients:\n         ar1      ma1     sar1\n      0.0668  -0.4518  -0.4426\ns.e.  0.3046   0.2825   0.0875\n\nsigma^2 = 0.001532:  log likelihood = 195.14\nAIC=-382.28   AICc=-381.89   BIC=-371.59\n\n\nJá o modelo de regressão linear tem uma saída mais interessante. Note que, por padrão, o primeiro mês foi omitido e seu efeito aparece no termo constante. Na tabela abaixo, vemos que há um efeito positivo e significativo, por exemplo, nos meses 6-8 (junho a agosto), que coincidem com o período de férias de verão no hemisfério norte. Já, em novembro (mês 11) parece haver uma queda na demanda por passagens aéreas.\nNote que o R quadrado da regressão é extremamente elevado e isso é um indício de que algo está errado. Isto, muito provavelmente é resultado da não-estacionaridade da série.\n\nbroom::tidy(model3)\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic   p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  4.71     0.0191    247.     2.81e-149\n 2 trend        0.0106   0.000145   73.2    3.09e- 93\n 3 season2     -0.0134   0.0245     -0.549  5.84e-  1\n 4 season3      0.120    0.0245      4.91   3.32e-  6\n 5 season4      0.0771   0.0245      3.15   2.14e-  3\n 6 season5      0.0675   0.0245      2.75   6.93e-  3\n 7 season6      0.191    0.0245      7.81   4.23e- 12\n 8 season7      0.288    0.0245     11.7    6.32e- 21\n 9 season8      0.278    0.0245     11.4    4.36e- 20\n10 season9      0.143    0.0245      5.82   6.13e-  8\n11 season10     0.00108  0.0245      0.0441 9.65e-  1\n12 season11    -0.141    0.0245     -5.77   7.97e-  8\n13 season12    -0.0248   0.0245     -1.01   3.14e-  1\n\n\nDe fato, olhando para a função de autocorrelação do resíduo do modelo de regressão linear, fica evidente que há autocorrelação. Uma forma de contornar isso seria de incluir um termo ARMA no termo de erro. Novamente, este não é o foco do post e vamos seguir normalmente.\n\nacf(resid(model3))"
  },
  {
    "objectID": "posts/general-posts/arima-no-r/index.html#comparando-as-previsões",
    "href": "posts/general-posts/arima-no-r/index.html#comparando-as-previsões",
    "title": "Séries de Tempo no R",
    "section": "Comparando as previsões",
    "text": "Comparando as previsões\nA maneira mais prática de trabalhar com vários modelos ao mesmo tempo é agregando eles em listas e aplicando funções nessas listas.\nAbaixo eu aplico a função forecast para gerar as previsões 24 períodos a frente nos três modelos. Depois, eu extraio somente a estimativa pontual de cada previsão.\n\nmodels &lt;- list(model1, model2, model3)\nyhat &lt;- lapply(models, forecast, h = 24)\nyhat_mean &lt;- lapply(yhat, function(x) x$mean)\n\nComparamos a performance de modelos de duas formas: (1) olhando para medidas de erro (o quão bem o modelo prevê os dados do test) e (2) olhando para critérios de informação (o quão bem o modelo se ajusta aos dados do train).\nOs critérios de informação têm todos uma interpretação bastante simples: quanto menor, melhor. Tipicamente, o AIC tende a escolher modelos sobreparametrizados enquanto o BIC tende a escolher modelos mais parcimoniosos.\nJá a comparação de medidas de erro não é tão simples. Pois ainda que um modelo tenha, por exemplo, um erro médio quadrático menor do que outro, não é claro se esta diferença é significante. Uma maneira de testar isso é via o teste Diebold-Mariano, que compara os erros de previsão de dois modelos. Implicitamente, contudo, ele assume que a diferença entre os erros de previsão é covariância-estacionária (também conhecido como estacionário de segunda ordem ou fracamente estacionário). Dependendo do contexto, esta pode ser uma hipótese mais ou menos razoável.\n\ncompute_error &lt;- function(model, test) {\n  y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    y - yhat\n}\n\ncompute_error_metrics &lt;- function(model, test) {\n\n    y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    error &lt;- y - yhat\n    \n    # Raiz do erro quadrado medio\n    rmse &lt;- sqrt(mean(error^2))\n    # Erro medio absoluto\n    mae &lt;- mean(abs(error))\n    # Erro medio percentual\n    mape &lt;- mean(abs(100 * error / y))\n    # Root mean squared scaled error\n    rmsse &lt;- sqrt(mean(error^2 / snaive(train)$mean))\n\n    # Devolve os resultados num list\n    list(rmse = rmse, mae = mae, mape = mape, rmsse = rmsse)\n    \n\n}\n\ncompute_ics &lt;- function(model) {\n\n    # Extrai criterios de informacao\n    aic  &lt;- AIC(model)\n    bic  &lt;- BIC(model)\n\n    # Devolve os resultados num list\n    list(aic = aic, bic = bic)\n\n} \n\nfcomparison &lt;- lapply(yhat, function(yhat) compute_error_metrics(yhat, test))\nicc &lt;- lapply(models, compute_ics)\n\ncomp_error &lt;- do.call(rbind.data.frame, fcomparison)\nrownames(comp_error) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\ncomp_ics &lt;- do.call(rbind.data.frame, icc)\nrownames(comp_ics) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\n\nA tabela abaixo mostra os critérios AIC e BIC para os três modelos. Em ambos os casos, o SARIMA(0, 1, 1)(0, 1, 1)[12] parece ser o escolhido. Este tipo de feliz coincidência não costuma acontecer frequentemente na prática, mas neste caso ambos os critérios apontam para o mesmo modelo.\n\ncomp_ics\n\n                    aic       bic\nAutoArima     -389.0155 -380.9970\nManual SARIMA -382.2807 -371.5894\nOLS           -342.2930 -303.2681\n\n\nNa comparação de medidas de erro, o SARIMA(0, 1, 1)(0, 1, 1)[12] realmente tem uma melhor performance, seguido pelo OLS e pelo SARIMA(1, 1, 1)(1, 1, 0)[12].\n\ncomp_error\n\n                    rmse        mae     mape      rmsse\nAutoArima     0.09593236 0.08959921 1.463477 0.03939512\nManual SARIMA 0.11688549 0.10780460 1.762201 0.04806577\nOLS           0.10333715 0.09384411 1.549261 0.04266214\n\n\nSerá que esta diferença é significante? Vamos comparar os modelos SARIMA. Pelo teste DM ela é sim. Lembre-se que o teste DM é, essencialmente, um teste Z de que \\(e_{1} - e_{2} = 0\\) ou \\(e_{1} = e_{2}\\), onde os valores são a média dos erros de previsão dos modelos.\n\nerrors &lt;- lapply(yhat, function(yhat) compute_error(yhat, test))\ne1 &lt;- errors[[1]]\ne2 &lt;- errors[[2]]\n\ndm.test(e1, e2, power = 2)\n\n\n    Diebold-Mariano Test\n\ndata:  e1e2\nDM = -4.4826, Forecast horizon = 1, Loss function power = 2, p-value =\n0.0001691\nalternative hypothesis: two.sided\n\n\nVale notar que o teste DM serve para comparar os erros de previsão de quaisquer modelos. Como o teste não faz qualquer hipótese sobre “de onde vem” os erros de previsão, ele pode ser utilizado livremente. Vale lembrar também que este teste não deve ser utilizado para escolher o melhor modelo, já que ele compara apenas a capacidade preditiva de dois modelos alternativos.\nOutro ponto, também complicado, é de qual a medida de erro que se deve escolher. O teste DM implicitamente usa o erro médio quadrático, mas há várias outras alternativas. Uma breve discussão pode ser vista aqui.\nPor fim, o gráfico abaixo mostra as previsões dos modelos alternativos contra a série real.\n\nplot(test, ylim = c(5.8, 6.5), col = \"#8ecae6\", lwd = 2, type = \"o\")\nlines(yhat_mean[[1]], col = \"#ffb703\")\nlines(yhat_mean[[2]], col = \"#fb8500\")\nlines(yhat_mean[[3]], col = \"#B86200\")\ngrid()\nlegend(\"topleft\", lty = 1,\n       legend = c(\"Test\", \"AutoArima\", \"Arima\", \"OLS\"),\n       col = c(\"#8ecae6\", \"#ffb703\", \"#fb8500\", \"#B86200\"))"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html",
    "href": "posts/general-posts/repost-ols-timeseries/index.html",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "",
    "text": "[Este post foi originalmente escrito no início de 2019. Muitos dos pacotes apresentados aqui evoluíram bastante, mas acredito que o post original ainda tenha bastante valor didático para quem está iniciando seus estudos em econometria e R.]\nOs cursos de econometria de séries de tempo, usualmente, começam pelo ensino de modelos lineares univariados para séries estacionárias. Estes modelos são da família ARMA e tentam representar uma série de tempo \\(y_{t}\\) em função de suas defasagens \\(y_{t-1}, y_{t-2}, \\dots, y_{t-n}\\) e de choques aleatórios (inovações) \\(\\epsilon_{t}, \\epsilon_{t-1}, \\epsilon_{t-2}, \\dots, y_{t-n}\\). Contudo, pode ser mais interessante relacionar duas séries de tempo \\(y_{t}\\) e \\(x_{t}\\) diferentes via um modelo linear. Em alguns casos isto pode ser equivalente a um VAR ou VARMA, mas o modelo linear tem a vantagem de ser mais simples de implementar e de interpretar. O tipo de modelo linear que estamos interessados é da forma\n\\[\n  y_{t} = \\beta_{0} + \\beta_{1}x_{t} + w_{t}\n\\]\nonde \\(y_{t}\\) é a série que queremos “explicar” em função da série \\(x_{t}\\). É evidente que podemos estender este modelo para incluir defasagens das variáveis \\(x_{t}\\) e \\(y_{t}\\), além de incluir outras séries, dummies, efeitos sazonais e tendências temporais.\nQuando se usa dados em forma de séries de tempo numa regressão linear, é bastante comum que se enfrente algum nível de autocorrelação nos resíduos. Uma das hipóteses do modelo “clássico” de regressão linear é de que as observações são i.i.d., isto é, que os dados são independentes e identicamente distribuídos. Isto obviamente não se aplica no contexto de séries de tempo (os dados não são independentes), então é preciso algum cuidado no uso de modelos de regressão linear. Neste sentido, o diagnósito dos resíduos é o mais importante passo para verificar a qualidade do modelo. Idealmente, os resíduos do modelo devem se comportar como ruído branco (i.e., não devem apresentar autocorrelação).\nOutro problema típico deste tipo de análise, chamado de “regressão espúria”, acontece quando se faz a regressão de séries não-estacionárias. Quaisquer duas séries com tendência serão fortemente linearmente relacionadas. Isto leva a uma regressão com \\(R^2\\) altíssimo e estatísticas-t muito significativas e a vários modelos sem sentido. Exemplos disto podem ser vistos neste site (em inglês).\nAinda assim, há casos em que podemos utilizar estas regressões para encontrar relações úteis. Em particular, se as séries forem cointegradas podemos usar uma metodologia Engle-Granger. Esta abordagem não será discutida neste post."
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-simples",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-simples",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo simples",
    "text": "Exemplo simples\nUm modelo para explicar \\(y\\) em função de seu valor defasado em um período, do valor contemporâneo de \\(x\\) e do valor defasado de \\(x\\) em um período.\n\\[\n  y_{t} = \\beta_{0} + \\beta_{1}y_{t - 1} + \\beta_{2}x_{t} + \\beta_{3}x_{t - 1} + w_{t}\n\\]\nNote que o modelo acima não seria muito útil para gerar previsões de \\(y_{t + 1}\\) pois ele exigiria conhecimento de \\(x_{t + 1}\\). Então, seria necessário primeiro prever o valor de \\(x_{t + 1}\\) para computar uma estimativa para \\(y_{t + 1}\\).\n\\[\n\\mathbb{E}(y_{t + 1} | \\mathbb{I}_{t}) = \\beta_{0} + \\beta_{1}y_{t} + \\beta_{2}x_{t + 1} + \\beta_{3}x_{t}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-índice-de-produção-industrial",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-índice-de-produção-industrial",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: Índice de Produção Industrial",
    "text": "Exemplo: Índice de Produção Industrial\nPara o exemplo abaixo uso o pacote GetBCBData para carregar a série do Índice de Produção Industrial (IPI).\n\n\nCode\n# Baixa os dados\nipi &lt;- gbcbd_get_series(21859, first.date = as.Date(\"2002-01-01\"))\n# Converte a série para ts\nprod &lt;- ts(ipi$value, start = c(2002, 01), frequency = 12)\n# Gráfico da série\nautoplot(prod) +\n  labs(title = \"Índice de Produção Industiral - geral (2012 = 100)\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nPode-se visualizar a relação linear entre valores correntes e defasados do IPI usando a função lag.plot. Na imagem abaixo, cada quadrado mostra um gráfico de dispersão dos valores do IPI em \\(t\\) contra os valores do IPI em \\(t-k\\). Alguns lags parecem não exibir muita relação como o lag 6. Já o primeiro e último lag parecem apresentar uma relação linear mais acentuada.\n\n\nCode\ngglagplot(prod, 12, do.lines = FALSE, colour = FALSE) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nComo exemplo, podemos propor o modelo abaixo para o IPI. A escolha dos lags aqui foi um tanto arbitrária e há métodos mais apropriados para escolhê-los.\n\\[\n  IPI_{t} = \\beta_{0} + \\beta_{1}IPI_{t - 1} + \\beta_{2}IPI_{t - 2} + \\beta_{3}IPI_{t - 4} + \\beta_{2}IPI_{t - 12}\n\\]\nPara estimar este modelo no R há um pequeno inconveniente da função lag que, na verdade, funciona como um operador foreward. Além disso, agora a função base lm (e por conseguinte, também a função tslm) se prova um tanto inconveniente, pois ela não funciona bem com variáveis defasadas. Para usar a função lm seria necessário primeiro “emparelhar” as diferentes defasagens da série, isto é, seria necessário criar um data.frame (ou ts) em que cada coluna mostra os valores das defasagens escolhidas. Por motivo de completude, deixo um código de exemplo que faz isto. Na prática, vale mais a pena escolher alguma outra função como dynlm::dynlm ou dyn::dyn$lm. O código abaixo usa o forecast::tslm, mas nos exemplos seguintes uso o dyn::dyn$lm.\n\n\nCode\n# Exemplo usando forecast::tslm (tb funcionaria com stats::lm)\ndf &lt;- ts.intersect(\n  prod, lag(prod, -1), lag(prod, -2), lag(prod, -4), lag(prod, -12),\n  dframe = TRUE\n  )\n\nfit &lt;- tslm(prod ~ ., data = df)\n\n\nPode-se contornar o problema da função lag definindo uma nova função, L, que funciona da maneira desejada. O código abaixo estima a regressão usando dyn::dyn$lm. A sintaxe dentro da função é praticamente idêntica à que vimos acima com as funções lm e tslm.\n\n\nCode\n# Define uma função L\nL &lt;- function(x, k) {lag(x, -k)}\nfit &lt;- dyn$lm(prod ~ L(prod, 1) + L(prod, 2) + L(prod, 4) + L(prod, 12))\n\n\nOs resultados da regressão acima estão resumidos na tabela abaixo.\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nipi\n\n\n\n\n\n\n\n\nipi [1]\n\n\n0.442***\n\n\n\n\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\nipi [4]\n\n\n0.139**\n\n\n\n\n\n\n(0.062)\n\n\n\n\n\n\n\n\n\n\nipi [8]\n\n\n-0.114***\n\n\n\n\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\nipi [12]\n\n\n0.446***\n\n\n\n\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\nconstante\n\n\n9.478**\n\n\n\n\n\n\n(3.960)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n245\n\n\n\n\nR2\n\n\n0.783\n\n\n\n\nAdjusted R2\n\n\n0.779\n\n\n\n\nResidual Std. Error\n\n\n5.304 (df = 240)\n\n\n\n\nF Statistic\n\n\n216.286*** (df = 4; 240)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nPodemos combinar a informação de outros indicadores industriais para adicionar informação potencialmente relevante para nossa regressão. Neste exemplo, uso outros indicadores industriais para encontrar aqueles que “ajudam a explicar” o indicador geral.\nO código abaixo importa uma série de indicadores industriais e junta todos eles num único objeto ts.\nOs códigos utilizados são diretamente copiados do sistema de séries temporais do BCB.\n\n\nCode\n# Codigos das series do BACEN\ncodigos_series = c(21859, 21861:21868)\n# Vetor com nomes para facilitar o uso dos dados\nnomes = c(\n  \"geral\", \"extrativa_mineral\", \"transformacao\", \"capital\", \"intermediarios\",\n  \"consumo\", \"consumo_duraveis\", \"semiduraveis_e_nao_duraveis\",\n  \"insumos_da_construcao_civil\"\n  )\n# Junta estes dados num data.frame que serve de dicionário (metadata)\ndicionario &lt;- data.frame(id.num = codigos_series, nome_serie = nomes)\n\n# Baixa todas as series\nseries &lt;- gbcbd_get_series(codigos_series, first.date = as.Date(\"2002-01-01\"))\n# Junta as séries com o dicionário\nseries &lt;- merge(series, dicionario, by = \"id.num\")\n# Converte para wide usando os nomes do dicionario como nome das colunas\nseries_wide &lt;- reshape2::dcast(series, ref.date ~ nome_serie, value.var = \"value\")\n# Convert para ts\nseries &lt;- ts(as.matrix(series_wide[, -1]), start = c(2002, 1), frequency = 12)\n\n# Visualizar todas as series\nautoplot(series) +\n  facet_wrap(vars(series)) +\n  scale_color_viridis_d() +\n  theme_light() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFeito o trabalho de importação dos dados podemos propor um modelo simples que toma o valor defasado das variáveis. Novamente a escolha das defasagens e dos regressores foi completamente arbitrária. O modelo estimado usa defasagens de outras séries para modelar o comportamento da série do índice de produção da indústria de tranformação\n\\[\n  Transf_{t} = \\beta_{0} + \\beta_{1}Durav_{t - 1} + \\beta_{2}Durav_{t - 12} + \\beta_{3}Capital_{t - 1} + \\beta_{4}Capital_{t - 6} + \\beta_{5}Transf_{t - 1} + \\alpha_{1}t + \\sum_{k = 2}^{12}\\alpha_{k}d_{k}\n\\]\n\n\nCode\nfit &lt;- dyn$lm(\n  transformacao ~ L(consumo_duraveis, 1) + L(consumo_duraveis, 12) +\n                  L(capital, 1) + L(capital, 6) +\n                  L(intermediarios, 12) +\n                  L(transformacao, 1) +\n                  time(transformacao) + as.factor(cycle(transformacao)),\n  data = series\n  )\n\nautoplot(series[, 1]) +\n  autolayer(fitted(fit)) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nOlhando apenas para as observações mais recentes vemos que, com exceção do período da pandemia, o ajuste aos dados é relativamente satisfatório.\n\n\nCode\n# Filtra apenas as observações mais recentes, após jan/2015\nprod_recente &lt;- window(prod, start = c(2015, 1))\n# Reestima o modelo\nsummary(fit &lt;- tslm(prod_recente ~ trend + season))\n\n\n\nCall:\ntslm(formula = prod_recente ~ trend + season)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.8428  -1.8730   0.5559   2.3254   8.8825 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 95.60468    1.73833  54.998  &lt; 2e-16 ***\ntrend       -0.04273    0.01593  -2.682 0.008745 ** \nseason2     -2.56839    2.19658  -1.169 0.245455    \nseason3      5.54101    2.19676   2.522 0.013456 *  \nseason4      0.17262    2.19705   0.079 0.937553    \nseason5      7.93757    2.19745   3.612 0.000505 ***\nseason6      8.27116    2.26418   3.653 0.000440 ***\nseason7     14.48889    2.26413   6.399 7.35e-09 ***\nseason8     17.80661    2.26418   7.864 8.76e-12 ***\nseason9     14.04934    2.26435   6.205 1.75e-08 ***\nseason10    16.35457    2.26463   7.222 1.75e-10 ***\nseason11     9.44729    2.26502   4.171 7.09e-05 ***\nseason12    -1.25998    2.26553  -0.556 0.579517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 88 degrees of freedom\nMultiple R-squared:  0.722, Adjusted R-squared:  0.6841 \nF-statistic: 19.05 on 12 and 88 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nautoplot(prod_recente) +\n  autolayer(fitted(fit)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-tendência-e-sazonalidade",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-tendência-e-sazonalidade",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: tendência e sazonalidade",
    "text": "Exemplo: tendência e sazonalidade\nÉ relativamente simples prever os valores futuros de modelos de tendência e sazonalidade determinísticas. Como o adjetivo “determinístico” sugere sabe-se de antemão todos os valores que esta série vai exibir. O exemplo abaixo estima um modelo simples para a demanda por passagens aéreas (voos internacionais).\nVale notar que não se costuma fazer previsões de longo prazo com este tipo de modelo, pois a hipótese de que a sazonalidade/tendência continua exatamente igual ao longo do tempo vai se tornando cada vez mais frágil. A curto prazo, contudo, pode ser razoável supor que este modelo linear simples ofereça uma boa aproximação da realidade.\n\n\nCode\nfit &lt;- tslm(AirPassengers ~ trend + season)\n\nautoplot(forecast(fit, h = 24), include = 24) +\n  theme_light()"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-previsão-de-cenário",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-previsão-de-cenário",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: previsão de cenário",
    "text": "Exemplo: previsão de cenário\nNo caso da regressão acima do IPCA, pode-se estimar o impacto de uma nova greve dos caminhoneiros. [Mal sabiamos que em 2020 teriamos um evento extraordinário…].\n\n\nCode\ndummies = cbind(greve_caminhao, greve_2013, precos_adm)\nfit &lt;- Arima(ipca, order = c(1, 0, 0), xreg = coredata(dummies))\ngreve_caminhao_2020 = c(rep(0, 9), 1, 0, 0)\nnovo_xreg = cbind(greve_caminhao = greve_caminhao_2020, greve_2013 = rep(0, 12), precos_adm = rep(0, 12))\n\nautoplot(forecast(fit, xreg = novo_xreg), include = 20) +\n  theme_light()"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-variáveis-defasadas",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-variáveis-defasadas",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: variáveis defasadas",
    "text": "Exemplo: variáveis defasadas\nPode ser um tanto difícil fazer previsões com modelos que usam informação de outras séries. Num modelo simples como \\(y_{t} = \\beta_{0} + \\beta_{1}x_{t - 1}\\) para prever valores futuros de \\(y_{t}\\) é preciso fazer previsõs para a série \\(x_{t}\\), pois, \\(y_{t + 2}\\) é função linear de \\(x_{t + 1}\\). Há muitas maneiras de abordar este problema e eu provavelmente vou discutir mais sobre as alternativas num post futuro. O exemplo abaixo mostra como usar informação disponível de outras séries\n\n\nCode\ndf_ajustada &lt;-\n  ts.intersect(transf = series[, \"transformacao\"],\n               lag(series[, \"consumo_duraveis\"], -1),\n               lag(series[, \"consumo_duraveis\"], -12),\n               lag(series[, \"capital\"], -1),\n               lag(series[, \"capital\"], -6),\n               lag(series[, \"intermediarios\"], -12),\n               lag(series[, \"transformacao\"], -1),\n               dframe = TRUE\n               )\nfit &lt;- tslm(transf ~ ., data = df_ajustada)\nsub &lt;- df_ajustada[(length(df_ajustada[, \"transf\"]) - 12):length(df_ajustada[, \"transf\"]), ]\n\n\n\n\nCode\nautoplot(forecast(fit, sub), include = 36) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nPode-se, como de costume, separar os dados em train e test para avaliar a qualidade das previsões dentro da amostra."
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html",
    "href": "posts/general-posts/definindo-objetos/index.html",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Há dois operadores para definir um objeto no R: = e &lt;-. A maior parte dos usuários parece preferir o último apesar dele parecer um tanto inconveniente. Em teclados antigos, havia uma tecla específica com o símbolo &lt;-, mas em teclados ABNT modernos ele exige três teclas para ser escrito.\nPara contornar este incômodo é comum criar um atalho no teclado para esse símbolo; o RStudio, por exemplo, tem um atalho usando a teclas Alt e - em conjunto. Mas ainda assim fica a questão: por que não utilizar o =? A resposta curta é que o símbolo &lt;- é a melhor e mais consistente forma de definir objetos R. Na prática, contudo, há poucas diferenças entre as expressões e elas dificilmente vão fazer alguma diferença. Podemos começar com um exemplo bastante simples para entender estas diferenças.\n\n\nO código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1\n\n\n\n\n\nEu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%.\n\n\n\nNo geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html#qual-a-diferença",
    "href": "posts/general-posts/definindo-objetos/index.html#qual-a-diferença",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "O código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1"
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html#mais-um-adendo",
    "href": "posts/general-posts/definindo-objetos/index.html#mais-um-adendo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Eu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%."
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html#resumo",
    "href": "posts/general-posts/definindo-objetos/index.html#resumo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "No geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html",
    "title": "Visualizando uma única variável",
    "section": "",
    "text": "Há algum tempo atrás tive o seguinte problema: como visualizar várias observações de uma única variável numérica num gráfico? Tentei algumas soluções óbvias, mas nenhuma pareceu funcionar muito bem. Neste post junto algumas das minhas tentativas.\nOs dados provêm do Mapa da Desigualdade 2019. Neste site pode-se baixar todos os dados além de baixar o relatório completo que apresenta informações socioeconômicas atualizadas para todos os distritos de São Paulo, compilando e sistematizando dados de diversas fontes públicas."
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#histograma",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#histograma",
    "title": "Visualizando uma única variável",
    "section": "Histograma",
    "text": "Histograma\nUma solução bastante clássica seria de fazer um histograma. Neste tipo de gráfico a dispersão fica clara, mas pode ser difícil de dar destaque para distritos específicos. O histograma conta a frequência de observações dentro de janelas de tamanho fixo. A princípio, a única dificuldade em usar o histograma está em definir a amplitude de cada um destes intervalos, mas, na prática, este problema não costuma ser muito complexo.\nAinda que o gráfico seja comumente usado, ele não é tão popular, sendo raramente visto em publicações de jornal, por exemplo.\n\n\nCode\nggplot(df, aes(x = expec_vida)) +\n  geom_histogram(bins = 11, colour = \"white\", fill = \"#08519c\") +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    data = df_aux_label,\n    aes(x = expec_vida,\n        y = c(15, 17, 19, 16, 12),\n        label = stringr::str_wrap(label_distrito, 5)),\n    colour = \"gray25\",\n    family = \"Roboto Light\",\n    size = 4\n  ) +\n  geom_segment(\n    data = df_aux_label,\n    aes(x = expec_vida,\n        xend = expec_vida,\n        y = 0,\n        yend = c(15, 17, 19, 15, 12) - 0.5),\n    colour = \"gray70\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 20, 4)) +\n  scale_x_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"Anos de idade\",\n    y = \"\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022.\"\n  ) +\n  theme_vini +\n  theme(panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-colunas",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-colunas",
    "title": "Visualizando uma única variável",
    "section": "Gráfico de colunas",
    "text": "Gráfico de colunas\nUm simples gráfico de colunas também poderia ser uma alternativa. Neste caso, como há muitas observações (distritos) diferentes, o gráfico acaba sobrecarregado e confuso.\n\n\nCode\nordered_df &lt;- df %&gt;%\n  mutate(\n    label_distrito = factor(label_distrito),\n    label_distrito = forcats::fct_reorder(label_distrito, expec_vida)\n  )\n\nggplot(ordered_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_col(colour = \"white\", fill = \"#08519c\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"Anos de idade\",\n    y = \"\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022.\"\n  ) +\n  theme_vini +\n  theme(\n    text = element_text(size = 6),\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    panel.grid.major.y = element_blank()\n    )\n\n\n\n\n\n\n\n\n\nOutra saída seria dividir os dados em grupos menores (e.g. alta, média-alta, média-baixa, baixa) e usar a função facet_wrap. O lado negativo disto, além de tornar o código mais complexo, é de acrescentar divisões nos dados que eventualmente são muito artificiais.\nAqui eu faço um divisão por quintil e crio uma denominação um tanto arbitrária para permitir a leitura dos dados.\n\n\nCode\nxl &lt;- c(\"Baixo\", \"Médio-Baixo\", \"Médio\", \"Médio-Alto\", \"Alto\")\n\nquintile_df &lt;- ordered_df %&gt;%\n  mutate(\n    life_group = ntile(expec_vida, 5),\n    life_group = factor(life_group, labels = xl)\n    )\n\nggplot(quintile_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_col(colour = \"white\", fill = \"#08519c\") +\n  coord_flip() +\n  facet_wrap(vars(life_group), scales = \"free_y\") +\n  labs(x = NULL, y = NULL, title = \"Expectativa de Vida\") +\n  theme_vini +\n  theme(\n    text = element_text(size = 8),\n    strip.text = element_text(size = 10),\n    panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#lolipop",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#lolipop",
    "title": "Visualizando uma única variável",
    "section": "Lolipop",
    "text": "Lolipop\nEste tipo de gráfico vem ganhando espaço mesmo em veículos de mídia populares por ser bastante simples. Ele é mais comumente usado para mostrar a evolução de uma variável em dois momentos do tempo, mas também pode-se usá-lo analogamente a um gráfico de colunas.\nInfelizmente, neste exemplo, ele vai sofrer do mesmo problema que o gráfico de colunas. A título de exemplo faço um gráfico deste estilo apenas para os distritos com as maiores e menores expectativas de vida.\n\n\nCode\n# Ordena a base de dados pela expectativa de vida\nordered_df &lt;- arrange(ordered_df, expec_vida)\n# Cria uma tabela com as primeiras 5 e últimas 5 linhas\ntop_df &lt;- rbind(head(ordered_df, 5), tail(ordered_df, 5))\n\nggplot(top_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_segment(aes(xend = label_distrito, yend = 55)) +\n  geom_point(colour = \"black\", fill = \"#08519c\", size = 4, shape = 21) +\n  coord_flip() +\n    labs(\n      y = \"Anos de vida\",\n      x = NULL,\n      title = \"Expectativa de vida\",\n      subtitle = \"Idade média ao morrer em 2022. Dados apenas dos distritos\\ncom maiores e menores expectativas de vida.\",\n      caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\"\n    ) +\n  theme_vini +\n  theme(panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-dispersão",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-dispersão",
    "title": "Visualizando uma única variável",
    "section": "Gráfico de dispersão",
    "text": "Gráfico de dispersão\nGráficos de dispersão apresentam a relação entre duas variáveis. Neste caso, podemos fazer alguns truques para inventar uma variável falsa que serve somente para que o R faça o gráfico.\nUma alternativa seria impor um expec_vida constante e arbitrário para uma das variáveis. Neste caso escolho x = 1 e escondo o eixo. No gráfico abaixo, cada distrito é um ponto sobre uma mesma linha. Para amenizar a sobreposição de observações uso alpha = 0.5 que acrescenta um pouco de transparência nas observações.\n\n\nCode\ndf_aux &lt;- tibble(\n  x = 1.025,\n  y = df_media$expec_vida,\n  label = paste0(\"Média = \", round(y, 1))\n)\n\nggplot(df, aes(x = 1, y = expec_vida)) +\n  geom_vline(xintercept = 1, colour = \"gray60\", alpha = 0.5) +\n  geom_point(\n    aes(colour = highlight, alpha = highlight, size = populacao_total),\n    shape = 21,\n    fill = \"#08519c\",\n  ) +\n  geom_text(\n    data = df_aux_label,\n    aes(x = c(0.95, 0.95, 1.05, 1.05, 1.05), y = expec_vida, label = label_distrito),\n    family = \"Roboto Light\",\n    size = 4\n  ) +\n  geom_segment(\n    data = df_aux_label,\n    colour = \"gray25\",\n    aes(\n      x = 1, xend = c(0.955, 0.955, 1.045, 1.045, 1.045),\n      y = expec_vida, yend = expec_vida\n    )\n  ) +\n  geom_segment(\n    data = tibble(x = 1, xend = 1.02, y = df_media$expec_vida, yend = y),\n    aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"gray25\",\n  ) +\n  geom_text(\n    data = df_aux,\n    aes(x = x, y = y, label = label),\n    family = \"Roboto Light\",\n    size = 3\n  ) +\n  coord_flip() +\n  scale_x_continuous(limits = c(0.94, 1.06)) +\n  scale_y_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  scale_alpha_manual(values = c(0.45, 0.8)) +\n  scale_size_continuous(range = c(1, 7.5)) + \n  scale_colour_manual(values = c(\"#08519c\", \"black\")) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"\",\n    y = \"Anos de idade\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022. Cada ponto representa um distrito de São Paulo.\"\n  ) +\n  theme_vini +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nUma solucão alternativa seria criar uma variável aleatória qualquer para servir como a variável “falsa”. Estranhamente, cria-se uma sensação melhor de dispersão e não há mais o problema de sobreposição. Contudo, o gráfico pode ser bastante confuso, pois dá a entender que estamos vendo a relação entre duas variáveis distintas, quando uma delas, na verdade, não representa coisa alguma.\nAinda assim, ideias similares foram implementadas pelo portal Nexo nesta postagem.\n\n\nCode\n# Cria uma variável aleatória qualquer seguindo uma Gaussiana\ndf &lt;- df %&gt;% mutate(x = rnorm(nrow(.)))\n\n# Para destacar o expec_vida médio\ndf_aux &lt;- tibble(\n  x = 2.25,\n  y = df_media$expec_vida + 0.75,\n  label = paste(\"Média =\", round(y - 0.75, 1))\n)\n\ndf_aux_label &lt;- df %&gt;%\n  mutate(label_distrito = if_else(highlight == 1L, label_distrito, \"\"))\n\nggplot(df, aes(x = x, y = expec_vida)) +\n  geom_jitter(aes(alpha = highlight, size = populacao_total),\n    shape = 21,\n    fill = \"#08519c\"\n  ) +\n  ggrepel::geom_text_repel(\n    data = df_aux_label,\n    aes(label = label_distrito),\n    force = 5,\n    family = \"Roboto Light\",\n    size = 3\n  ) +\n  geom_text(\n    data = df_aux,\n    aes(x = x, y = y, label = label),\n    hjust = -0.15\n  ) +\n  geom_hline(aes(yintercept = mean(expec_vida)), colour = \"gray70\", size = 1) +\n  coord_flip() +\n  scale_alpha_manual(values = c(0.65, 1)) +\n  scale_y_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  scale_x_continuous(limits = c(-3, 3)) +\n  labs(\n    title = \"Expectativa de Vida (idade média ao morrer)\",\n    x = \"\",\n    y = \"Anos de idade\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\"\n  ) +\n  guides(colour = FALSE, alpha = FALSE) +\n  theme_vini +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#mapa",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#mapa",
    "title": "Visualizando uma única variável",
    "section": "Mapa",
    "text": "Mapa\nDeixei a visualização mais óbvia para o final. Como a variável está distribuída espacialmente, pode-se fazer um simples mapa de São Paulo separado por distritos.\nAqui, eu uso o shapefile de distritos da Pesquisa Origem e Destino. Apesar de não haver um identificador comum entre as bases, foi relativamente simples fazer o join compatibilizando o nome dos distritos.\n\n\nCode\n# Importa o shapefile\ndistritos &lt;- st_read(\n  here::here(\"posts\", \"general-posts\", \"repost-mapa-desigualdade\", \"districts.gpkg\"),\n  quiet = TRUE\n  )\n\ndistritos &lt;- distritos %&gt;%\n  # Filtra apenas distritos de SP (capital)\n  filter(code_district &lt;= 96) %&gt;%\n  # Renomeia a coluna para facilitar o join\n  rename(distrito = name_district)\n\n# Verifica se ha distritos com nome diferente (Mooca)\nanti &lt;- anti_join(select(df, distrito), distritos, by = \"distrito\")\n# Altera a grafia para garantir o join\ndf &lt;- df %&gt;%\n  mutate(\n    distrito = if_else(distrito == \"Moóca\", \"Mooca\", distrito)\n  )\n# Junta os dados no sf\ndistritos &lt;- left_join(distritos, df, by = \"distrito\")\n\n\nO gráfico abaixo mostra a expectativa de vida em cada distrito na cidade.\n\n\nCode\nggplot(distritos) +\n  geom_sf(aes(fill = expec_vida), linewidth = 0.1) +\n  scale_fill_viridis_c(name = \"Expectativa\\nde Vida\") +\n  ggtitle(\"Expectativa de Vida por Distrito\") +\n  theme_void() +\n  theme(\n    legend.title = element_text(hjust = 0.5, size = 10),\n    legend.position = c(0.8, 0.3)\n  )\n\n\n\n\n\n\n\n\n\nOutra maneira de apresentar este dado é agrupando-o de alguma forma. Eu sou particularmente parcial ao algoritmo de Jenks.\n\n\nCode\n# Encontra os intervalos de cada grupo\nbreaks &lt;- classInt::classIntervals(distritos$expec_vida, n = 7, style = \"jenks\")\n# Classifica os valores em grupos\ndistritos &lt;- distritos %&gt;%\n  mutate(\n    jenks_group = cut(expec_vida, breaks = breaks$brks, include.lowest = TRUE)\n  )\n\nggplot(distritos, aes(fill = jenks_group)) +\n  geom_sf(linewidth = 0.1, color = \"gray80\") +\n  scale_fill_brewer(palette = \"BrBG\", name = \"Expectativa\\nde Vida\") +\n  ggtitle(\"Expectativa de Vida por Distrito\") +\n  theme_void() +\n  theme(\n    legend.title = element_text(hjust = 0.5, size = 10),\n    legend.position = c(0.8, 0.3)\n  )"
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html",
    "href": "posts/general-posts/tutorial-showtext/index.html",
    "title": "Usando fontes com showtext no R",
    "section": "",
    "text": "Criar boas visualizações é parte importante de qualquer análise de dados.\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar e o mesmo vale para visualizações com dados. A fonte do texto ajuda a transmitir informação e pode comunicar, por exemplo, maior sobriedade, profissionalismo, etc.\nO pacote showtext, desenvolvido por yixuan, facilita a importação e o uso de fontes em gráficos no R. O pacote funciona com uma variedade de extensões de fontes, não sendo limitado como o extrafont, por exemplo, a arquivos .ttf."
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html#base-r",
    "href": "posts/general-posts/tutorial-showtext/index.html#base-r",
    "title": "Usando fontes com showtext no R",
    "section": "Base R",
    "text": "Base R\nPara modificar a fonte dos elementos textuais dos gráficos feitos com o plot() é preciso ajustar o argumento family. Usando as funções base do R, este argumento aparece dentro da função par (que configura vários parâmetros dos gráficos).\nO código abaixo mostra como trocar a fonte do gráfico.\n\n# Define a fonte padrão do gráfico\npar(family = \"alice\")\n# Monta um scatter plot de exemplo\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30)\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nNote que todos os objetos textuais (título, legenda, etc.) são convertidos para a mesma fonte. Caso se queira fontes diferentes para estes elementos é preciso especificá-los adequadamente. Por exemplo, para trocar somente a fonte do título\ntitle(\"nome_do_titulo\", family = \"nome_fonte\")\n\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\", family = \"RobCond\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30, family = \"Montserrat\")\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nVale notar que, uma vez definida a fonte usando a função par, todos os gráficos subsequentes vão usar esta fonte. Para trocar a fonte é preciso usar a função par novamente. Além da fonte também é possível trocar a ênfase (e.g. face = c(\"bold\", \"italic\")) e também o tamanho da letra (e.g. cex.axis = 1.5, cex.main = 2)."
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html#ggplot2",
    "href": "posts/general-posts/tutorial-showtext/index.html#ggplot2",
    "title": "Usando fontes com showtext no R",
    "section": "ggplot2",
    "text": "ggplot2\nTambém é possível trocar a fonte de gráficos feitos com outros pacotes, como o ggplot2. O exemplo abaixo monta um gráfico similar ao que foi feito acima. Modifica-se a fonte dentro da função theme. Esta função é um tanto particular, então vale a pena discorrer um pouco sobre ela. Ela é basicamente usada para modificar elementos do gráfico. Há quatro elementos principais, dos quais só nos interessa um: o element_text. São seis os principais elementos textuais que pode-se modificar:\n\naxis.text - texto dos eixos (em geral, os números do eixo);\naxis.title - nome do eixo (e.g. “Milhas por galão” no exemplo acima);\nlegend.text - texto da legenda;\nlegend.title - título da legenda;\nplot.title - título do gráfico;\ntext - todos os acima.\n\nPode-se ser mais específico com o texto dos eixos usando axis.text.x e axis.text.y, por exemplo. O último dos elementos listados acima funciona como um “coringa”, ele serve para modificar de uma vez só todos os elementos textuais de um gráfico. No exemplo abaixo modifico somente o text.\n\nlibrary(ggplot2)\n\np &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE) +\n  labs(\n    x = \"Peso (ton.)\",\n    y = \"Milhas por galão\",\n    title = \"Eficiência e peso de carros\",\n    subtitle = \"Regressão entre o peso de diferentes carros e sua eficiência energética\",\n    caption = \"Fonte: Motor Trend US Magazine 1974\"\n    )\n\n\np + theme(text = element_text(family = \"Montserrat\", size = 10))\n\n\n\n\n\n\n\n\nO próximo exemplo mostra como modificar alguns dos diferentes elementos do gráfico. Aproveito a variável cyl (cilindradas) para diferenciar os carros em três grupos para que o gráfico agora tenha uma legenda.\n\np +\n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme(\n    # Modifica o texto (números) dos eixos x e y\n    axis.text = element_text(family = \"alice\"),\n    # Modifica o título do eixo (i.e. Milhas por galão)\n    axis.title = element_text(family = \"Montserrat\"),\n    # Modifica o título da legenda (Cilindros)\n    legend.title = element_text(family = \"HelveticaNeue\"),\n    # Modifica o texto da legenda (i.e. 4, 6, 8)\n    legend.text = element_text(family = \"HelveticaNeue\"),\n    # Modifica o título do gráfico\n    plot.title = element_text(family = \"RobCond\", size = 20)\n    )\n\n\n\n\n\n\n\n\nTalvez o jeito mais sensato de usar fontes com o ggplot2 seja primeiro especificar uma fonte padrão para o gráfico usando text e depois calibrar as exceções. Os elementos textuais como axis.title e legend.text copiam as propriedades definidas em text.\nNo exemplo abaixo defino que todos os elementos textuais ser escritos em Arial simples em tamanho 12 na cor \"gray20\". Depois disso defino que o título deve ter mais destaque com Arial em negrito (bold) num tamanho maior e numa cor mais escura. Por fim, defino que o rodapé do gráfico seja escrito em fonte menor e em itálico.\n\ntheme_custom &lt;- theme_light() +\n  theme(\n    # Modifica todos os elementos textuais do gráfico\n    text = element_text(family = \"Arial\", size = 12, color = \"gray20\"),\n    # Modifica apenas o título\n    plot.title = element_text(face = \"bold\", size = 14, color = \"gray10\"),\n    # Modifica apenas a nota no rodapé\n    plot.caption = element_text(face = \"italic\", size = 8)\n  )\n\np + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n\n\n\n\n\n\n\nPor último, também pode ser interessante usar fontes diferentes para representar dados diferentes. Isto é possível usando o argumento family dentro do aes. Da mesma forma, seria possível também representar grupos de dados diferentes com tamanhos de fontes diferentes ou mesmo destacar algum grupo específico com itálico.\n\nnomes &lt;- row.names(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_text(aes(label = nomes, family = c(\"Arial\", \"alice\", \"Montserrat\")[cyl]))"
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "href": "posts/general-posts/tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "title": "Usando fontes com showtext no R",
    "section": "Anexo: problemas com DPI e RMarkdown",
    "text": "Anexo: problemas com DPI e RMarkdown\nApesar de muito conveniente, o showtext não é inteiramente desprovido de problemas. Dois problemas que enfrento com alguma recorrência são diferenças de DPI na hora de exportar gráficos e problemas com RMarkdown.\nO problema com o RMarkdown é mais simples. Em versões antigas do RMarkdown e do showtext era necessário adicionar um argumento fig.showtext = TRUE em todos os chunks em que um gráfico usando showtext fosse renderizado. Alternativamente, podia-se modificar esta opção globalmente inserido o seguinte código no início do documento RMarkdown.\n\nknitr::opts_chunk$set(\n  fig.showtext = TRUE,\n  fig.retina = 1\n  )\n\nAcredito, mas não tenho certeza, de que este problema sumiu em versões mais recentes dos pacotes, pois com frequência eu esqueço de adicionar estes argumentos mas não encontro problemas na prática.\nO problema com o DPI na hora de exportar gráficos é mais complicado. Por problemas de DPI quero dizer quando o showtext “desenha” o texto num DPI diferente do ggplot2. O resultado é que o texto fica ou grande ou pequeno demais. Por padrão o showtext utiliza o DPI em 96.\nVamos montar um gráfico para ilustrar o problema.\n\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exportar o gráfico em alta resolução\nggsave(\"meu_grafico.jpeg\", grafico, dpi = 300)\n\nQuando vamos abrir o arquivo que foi exportado temos o resultado abaixo.\n\nComo a imagem foi salva com DPI mais elevado o texto fica menor do que deveria; em casos mais extremos o texto fica minúsculo a ponto de ser ilegível. Há duas formas de tentar contornar este problema: (1) reduzir o DPI dentro de ggsave; ou (2) modificar o DPI do showtext.\nVamos tentar a primeira solução: modificar o ggsave para o DPI padrão do showtext. Agora o texto está maior mas a proporção dos elementos está péssima! O resultado é pior do que o problema inicial.\n\n# Exportar a imagem num dpi menor\nggsave(\"meu_grafico_96.jpeg\", grafico, dpi = 96)\n\n\n\n\n\n\nA segunda solução é modificar as opções internas do showtext. Isto é bastante simples e pode ser feito com o showtext_opts(dpi = 300) e chamando novamente a função showtext_auto().\n\n# Ajusta o DPI do showtext\nshowtext_opts(dpi = 300)\n# \"Ativa\" o showtext novamente\nshowtext_auto()\n\n# Refaz o gráfico (isto é importante!)\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exporta um novo gráfico\nggsave(\"meu_grafico_300.jpeg\", grafico, dpi = 300)\n\nAgora o tamanho do texto está correto e a imagem como um todo está em alta resolução. Um problema é que a imagem ficou bastante grande, mas isto pode ser ajustado variando os argumentos width e height da função ggsave.\n\nVale notar que, a depender do seu sistema operacional, modificar o DPI padrão do showtext pode distorcer os gráficos dentro do R ou RStudio. Na prática o melhor workflow pode ser de modificar o DPI do showtext apenas no momento de exportar os gráficos."
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "",
    "text": "Este é um repost antigo que fiz ainda na época do mestrado em economia. Apesar de intuitivo o código dos loops abaixo pode ser muito ineficiente. De maneira geral, for-loops são melhores do que loops feitos com repeat; melhor ainda é montar funções e usar parallel::mclapply ou furrr::future_map. Vale notar que é sempre bom pré-alocar (ou pré-definir) vetores antes de um for-loop\n\n# Bom\nx &lt;- vector(\"numeric\", 1000)\n# Ruim\nx &lt;- c()"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html#lei-dos-grande-números",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html#lei-dos-grande-números",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "Lei dos Grande Números",
    "text": "Lei dos Grande Números\nA Lei dos Grandes Números (LGN) é um resultado assintótico bastante utilizado em econometria. Numa definição informal, a LGN nos diz que uma média amostral converge para para a média verdadeira dos dados. Isto é, se temos uma sequência de variáveis aleatórias \\(x_{1}, x_{2}, \\dots , x_{n}\\) independentes e identicamente distribuídas:\n\\[\\begin{equation}\n  \\frac{1}{n}\\sum_{i = 1}^{n} x_{1} \\to \\mathbb{E}(x)\n\\end{equation}\\]\nVamos criar uma amostra de cinco observações a partir de uma distribuição uniforme e tirar a média destas observações. Lembre-se que este distribuição depende de dois parâmetros, digamos \\(a\\) e \\(b\\). A esperança de uma uniforme é simplesmente \\(\\frac{b-a}{2}\\). Podemos repetir este processo 1000 vezes e fazer um histograma dos resultados.\nNo código abaixo cria-se um vetor \\(x = (x_{1}, x_{2}, \\dots)\\) genérico para armazenar valoes. O loop vai inserindo neste vetor a média de uma amostra de cinco observações a partir de uma distribuição uniforme com \\(a = 0\\) e \\(b = 5\\). A cada iteração do loop uma nova amostra é gerada e sua média é salva no vetor \\(x\\) na posição \\(x_{i}\\). Depois de gerar estes valores faz-se um histograma deles.\n\nx &lt;- vector(\"numeric\", length = 1000) # cria um vetor para armazenar os valores\n\nfor(i in 1:1000){ #loop para gerar os valores\n  # computa a media de uma amostra com 5 observacoes\n  x[i] &lt;- mean(runif(n = 5, min = 0, max = 5)) \n\n}\n# Histograma\nhist(x, main = \"Histograma da media das amostras para n = 5\", xlab = \"\")\n# Linha vertical\nabline(v = 2.5, col = \"red\")\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para diferentes tamanhos de amostra. O código abaixo simplesmente faz um loop do código acima; o loop de fora varia n.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\n\nfor (n in c(10, 50, 100, 200)) { # loop para os diferentes tamanhos de amostra\n  x &lt;- vector(\"numeric\", 1000)\n  for (i in 1:1000) { # mesmo loop que o anterior\n    \n    x[i] &lt;- mean(runif(n, 0, 5))\n    \n  }\n    # Plotando o histograma\n    hist(x, main = paste(\"Histograma para n = \", n, sep = \"\"),\n       xlab = \"\")\n    abline(v = 2.5, col = \"red\")\n}\n\n\n\n\n\n\n\n\nNote que as escalas dos gráficos são diferentes. Como era de se esperar, à medida que cresce o tamanho da amostra os valores vão se acumulando em torno da média verdadeira.\nOutra maneira de visualizar a LGN é fazendo o seguinte experimento: sorteie um número a partir de uma distribuição particular e grave seu valor. Agora sorteie dois números a partir da mesma distribuição, tire a média dos valores e grave o resultado. Agora faça o mesmo com três números, quatro números e assim por diante. O código abaixo faz isto para uma distribuição normal padrão.\n\nx &lt;- vector(\"numeric\", 200)\nfor (n in 1:200){\n\n    x[n] &lt;- mean(rnorm(n))\n\n}\n\nplot(x, type = \"l\", xlab = \"\", ylab = \"\")\nabline(h = 0, col = \"red\", lty = 2)"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html#teorema-central-do-limite",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html#teorema-central-do-limite",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "Teorema Central do Limite",
    "text": "Teorema Central do Limite\nO segundo resultado importante que se usa em econometria é o Teorema Central do Limite (TCL). Existem algumas variantes do TCL que usam diferentes hipóteses, mas, novamente sendo informal, o TCL diz que se tivermos uma amostra qualquer \\(x_{1}, x_{2}, \\dots , x_{n}\\), então \\(\\sqrt{n}\\frac{\\overline{x} - \\mu}{\\sigma}\\) segue uma distribuição normal padrão, onde \\(\\overline{x}\\) é a média amostral, \\(\\mathbb{E}(x) = \\mu\\) e \\(\\text{Var}(x) = \\sigma^{2}\\). Para visualizar este resultado podemos novamente fazer o experimento usando a distribuição uniforme.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 200)){ # loop para os diferentes tamanhos de amostra\n  for (i in 1:1000){ # mesmo loop que o anterior\n\n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n\n  x_normalizado &lt;- sqrt(n)*(x - 5) / sqrt(100/12) # transforma a variavel\n  # plota o histograma usando a densidade da frequencia de cada observacao\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\", breaks = 20)\n  # superimpoe uma curva normal padrao \n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)  \n}\n\n\n\n\n\n\n\n\nNote que este resultado vale para qualquer sequência de variáveis i.i.d (independentes e identicamente distribuídas). Considere, por exemplo, uma sequência de variáveis aleatórias independentes que segue uma distribuição beta.\n\\[\\begin{equation}\nf(x) = \\frac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\end{equation}\\]\nonde \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\). A esperança da distribuição beta é dada por\n\\[\\begin{equation}\n    \\mathbb{E}(x) = \\frac{\\alpha}{\\alpha + \\beta}\n\\end{equation}\\]\nUma distribuição beta depende de dois parâmetros. Usando a função dbeta podemos simular algumas pdfs.\n\n\n\n\n\n\n\n\n\nNote que uma implicação do TCL é que, se \\(x_{i}\\) for i.i.d. com esperança igual a \\(\\mu\\), então\n\\[\\begin{equation}\n    \\sqrt{N} \\left (  \\frac{1}{n}\\sum_{i = 1}^{N}x_{i} - \\mu \\right ) \\to \\text{N}(0, \\sigma^{2})\n\\end{equation}\\]\nIsto é, não precisamos saber qual a forma da variância da distribuição para aplicar o TCL. Os loops abaixo são essencialmente idênticos aos anteriores: a diferença é que desta vez os histogramas vão representar variáveis normais de média zero com variância \\(\\sigma^{2}\\), que é aproximadamente igual à variância da distribuição beta.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # loop para os diferentes tamanhos de amostra\n \n  for (i in 1:1000){ # mesmo loop que o anterior\n\n    x[i] &lt;- mean(rbeta(n, 2, 5))\n    \n  }\n\n  x_normalizado &lt;- sqrt(n)*(x - 2/7) # transforma a variavel\n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\") # plota o histograma usando a densidade da frequencia de cada observacao\n}\n\n\n\n\n\n\n\n\n\nDois casos anômalos\nO TCL nos diz: \\[\\begin{equation}\n    \\sqrt{n} \\left ( \\frac{\\overline{x} - \\mu}{\\sigma} \\right ) \\to N(0,1)\n\\end{equation}\\] O termo \\(\\sqrt{n}\\) é essencial para garantir este resultado. Qualquer transformação maior do que \\(\\sqrt{n}\\) faz a variância crescer indefinidamente; qualquer transformação menor do que \\(\\sqrt{n}\\) faz a variância diminuir indefinidamente, isto é, faz a distribuição colapsar num único ponto. Os dois códigos abaixo apresentam exemplos destes casos. O primeiro usa \\(n^{\\frac{3}{4}}\\), o segundo \\(n^{\\frac{1}{4}}\\).\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # mesmo loop que o anterior\n  for (i in 1:1000){ \n    \n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n  \n  x_normalizado &lt;- n^(3/4)*(x - 5)/sqrt(100/12) # muda apenas o expoente de n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\")\n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # mesmo loop que o anterior\n  for (i in 1:1000){ \n    \n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n  \n  x_normalizado &lt;- n^(1/4)*(x - 5)/sqrt(100/12) # muda apenas o expoente de n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\")\n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)\n}"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html",
    "title": "Pacotes Essenciais R",
    "section": "",
    "text": "R é uma das linguagens de programação mais populares para ciência de dados, estatística, economia e várias outras áreas quantitativas. O R já vem com alguns pacotes “imbutidos” que, em geral, são referidos como base R, são os pacotes que são instalados automaticamente junto com o R como o stats, utils, graphics, datasets entre outros.\nOu seja, já é possível importar dados e limpá-los, fazer análises estatísticas, gráficos e tabelas sem carregar nenhum pacote adicional.\nMas para o usufruir de todo o potencial que o R pode oferecer é essencial conhecer os melhores pacotes e as suas funções."
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#dplyr-x-data.table",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#dplyr-x-data.table",
    "title": "Pacotes Essenciais R",
    "section": "dplyr x data.table",
    "text": "dplyr x data.table\nO dplyr é bastante intuitivo, poderoso e há centenas de tutoriais e livros sobre ele. Ele serve para transformar os dados: criar colunas, filtrar linhas, reordernar dados, etc. Ele integra um “ecossistema” de pacotes chamado tidyverse, uma coleção de pacotes para processamento e visualização de dados, construídos em torno de uma filosofia comum.\nO data.table surgiu com o intuito de fazer o R funcionar melhor num mundo de big data. Seu desenvolvimento foi focado em ser o mais rápido, eficiente e sucinto possível, permitindo trabalhar com bases de dados muito grandes dentro do R (~100GB). Em termos de velocidade, o data.table ganha facilmente de praticamente todas as outras linguagens populares em data science. Ele é centena de vezes mais rápido que base R e também ganha com folga do seu competidor dplyr.\nSe o data.table é tão mais eficiente então como o dplyr se tornou mais popular do que ele?\nAcontece que o dplyr é melhor integrado com vários outros pacotes podersos do tidyverse. Também há muito material de apoio ao dplyr na forma de tutoriais, vídeos, livros, etc. disponíveis na internet. Quase todo curso de R ensina a usar o dplyr. Assim, a curva de aprendizado fica mais fácil.\nO data.table funciona muito bem com funções do base R como lapply, colMeans(), etc. e tem uma sintaxe concisa e flexível, onde o usuário acaba tendo bastante liberdade para criar as suas soluções. Já o dplyr vai pelo caminho de tentar facilitar ao máximo a vida do usuário criando várias funções com usos bastante específicos para otimizar pequenas tarefas do dia-a-dia do processamento de dados.\nOutra pequena desvantagem do data.table é que ele não funciona de primeira no Mac. Como ele usa OpenMP é preciso baixar outros programas e modificar algumas configurações no R o que pode ser bem trabalhoso e chato.\nNa comparação fica o seguinte:\n\n\n\n\n\n\n\ndplyr\ndata.table\n\n\n\n\nMais funções para aprender. Em geral, o código fica mais comprido\nSintaxe sucinta\n\n\nFunções melhor integradas com o tidyverse\nFunções melhor integradas com base R\n\n\nMuito mais veloz que o base R\nOpção mais veloz possível\n\n\nSintaxe mais intuitiva, mais material de apoio na internet, mais popular.\nMenos material de apoio disponível"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#tidyverse",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#tidyverse",
    "title": "Pacotes Essenciais R",
    "section": "tidyverse",
    "text": "tidyverse\nIndepedentemente da sua escolha entre dplyr e/ou data.table (recomendo usar os dois!) vale a pena explorar os demais pacotes do tidyverse.\n\ntidyr\nO tidyr acrescenta algumas importantes funcionalidades que faltam no dplyr. Talvez as duas funções mais importantes do pacote sejam pivot_longer e pivot_wider que servem para converter seus dados de transversais (wide) para longitudinais (long) e vice-versa. O tidyr também traz um novo tipo de objeto o tibble, uma versão moderna do data.frame.\nVale também explorar algumas funções muito úteis como separate, fill, complete, replace_na entre outras.\n\nlibrary(tidyr)\nlibrary(dplyr)\n# Exemplo de tibble\ndata &lt;- as_tibble(USPersonalExpenditure)\ndata &lt;- mutate(data, variable = rownames(USPersonalExpenditure))\ndata\n\n# A tibble: 5 × 6\n  `1940` `1945` `1950` `1955` `1960` variable           \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              \n1 22.2   44.5    59.6    73.2  86.8  Food and Tobacco   \n2 10.5   15.5    29      36.5  46.2  Household Operation\n3  3.53   5.76    9.71   14    21.1  Medical and Health \n4  1.04   1.98    2.45    3.4   5.4  Personal Care      \n5  0.341  0.974   1.8     2.6   3.64 Private Education  \n\n\n\n# Exemplo de dado longitudinal\nlong &lt;- pivot_longer(data, cols = !variable, names_to = \"year\")\nlong\n\n# A tibble: 25 × 3\n   variable            year  value\n   &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt;\n 1 Food and Tobacco    1940   22.2\n 2 Food and Tobacco    1945   44.5\n 3 Food and Tobacco    1950   59.6\n 4 Food and Tobacco    1955   73.2\n 5 Food and Tobacco    1960   86.8\n 6 Household Operation 1940   10.5\n 7 Household Operation 1945   15.5\n 8 Household Operation 1950   29  \n 9 Household Operation 1955   36.5\n10 Household Operation 1960   46.2\n# ℹ 15 more rows\n\n\n\n# Calcula a variação e apresenta de maneira transversal\nlong %&gt;%\n  group_by(variable) %&gt;%\n  mutate(variacao = (value / lag(value) - 1) * 100) %&gt;%\n  na.omit() %&gt;%\n  pivot_wider(\n    id_cols = \"variable\",\n    names_from = \"year\",\n    values_from = \"variacao\")\n\n# A tibble: 5 × 5\n# Groups:   variable [5]\n  variable            `1945` `1950` `1955` `1960`\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Food and Tobacco     100.    33.9   22.8   18.6\n2 Household Operation   47.6   87.1   25.9   26.6\n3 Medical and Health    63.2   68.6   44.2   50.7\n4 Personal Care         90.4   23.7   38.8   58.8\n5 Private Education    186.    84.8   44.4   40  \n\n\n\n\nstringr\nO stringr é um pacote voltado para facilitar a manipulação de vetores de texto (character). Todas as funções do pacote são convenientemente precedidas pelo prefixo str_ e a lista dos argumentos segue um padrão uniforme. O nome das funções também é bastante intuitivo.\nNovamente, há funções base como gsub, grep, strsplit, sub, entre outras, que servem para manipular vetores de string, mas essas funções carecem de “coesão interna” e em alguns casos elas podem retornar valores inesperados ou “erros silenciosos”.\nUma função muito divertida do pacote é a str_glue que facilita na hora de concatenar strings. Note como é simples incluir variáveis como parte do texto e inclusive fazer transformações nas variáveis.\n\nlibrary(stringr)\n\nnome &lt;- \"Vinicius Oike\"\nidade &lt;- 29\nnfav &lt;- runif(1, min = 0, max = 10)\n\nstr_glue(\"Olá, meu nome é {nome}, tenho {idade} anos. Ano que vem, terei {idade + 1} anos. Meu número favorito é {round(nfav)}.\")\n\nOlá, meu nome é Vinicius Oike, tenho 29 anos. Ano que vem, terei 30 anos. Meu número favorito é 3.\n\n\n\n\nlubridate\nO lubridate é um pacote exclusivamente focado em manipulação de vetores de datas e tudo relacionado ao tempo como variável. Variáveis de datas podem ser uma dor de cabeça tremenda no processo de limpeza de dados, pois há inúmeros formatos diferentes, que variam para cada país, fora os problemas potenciais de ano bissextos, fusos-horários diferentes, etc.\nEspecialmente para quem precisa trabalhar com séries de tempo ou dados em painel, o lubridate é um pacote é essencial.\nO pacote também facilita “operações aritméticas” com datas como no exemplo abaixo:\n\nlibrary(lubridate)\n\nstart &lt;- ymd(\"2020-01-15\")\nstart + years(1) + months(3)\n\n[1] \"2021-04-15\"\n\n\nEle também permite a extração de qualquer informação específica de uma data\n\nagora &lt;- ymd_hms(\"2022-06-10 20:36:15\")\n\nComo o dia da semana:\n\nwday(agora)\n\n[1] 6\n\n\nO trimestre:\n\nquarter(agora)\n\n[1] 2\n\n\nOu o mês (com a opção de ter a abreviação do mês já no padrão desejado!):\n\nmonth(agora, label = TRUE, locale = \"pt_BR\")\n\n[1] Jun\n12 Levels: Jan &lt; Fev &lt; Mar &lt; Abr &lt; Mai &lt; Jun &lt; Jul &lt; Ago &lt; Set &lt; ... &lt; Dez\n\n\n\n\ndbplyr\nO dbplyr é um pacote muito interessante que transforma o R numa “interface” para o SQL. O pacote traduz código escrito em dplyr para código em SQL. Assim, é possível acessar, transformar e baixar dados de uma Base de Dados em SQL, PostGreSQL, etc. diretamente no R.\nO código abaixo é um exemplo (bem artficial) que demonstra o funcionamento da pacote. A função show_query() não costuma ser usada na prática, mas ela serve para mostrar o que o pacote está fazendo.\n\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Abre um servidor SQL para servir de exemplo\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\n# Faz o update da tabela mtcars para esse servidor\ncopy_to(con, mtcars)\n# Acessa a tabela do servidor usando tbl\ndb &lt;- tbl(con, \"mtcars\")\n\n# Trabalha normalmente usando comandos do dplyr\ndb %&gt;%\n  filter(cyl %in% c(4, 6)) %&gt;%\n  mutate(disp = log(round(disp))) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(a1 = mean(disp, na.rm = TRUE), a2 = median(wt, na.rm = TRUE)) %&gt;%\n  arrange(desc(a1)) %&gt;%\n  # No lugar de show_query() use collect() para baixar os dados\n  show_query()\n\n&lt;SQL&gt;\nSELECT `cyl`, AVG(`disp`) AS `a1`, MEDIAN(`wt`) AS `a2`\nFROM (\n  SELECT\n    `mpg`,\n    `cyl`,\n    LOG(ROUND(`disp`, 0)) AS `disp`,\n    `hp`,\n    `drat`,\n    `wt`,\n    `qsec`,\n    `vs`,\n    `am`,\n    `gear`,\n    `carb`\n  FROM `mtcars`\n  WHERE (`cyl` IN (4.0, 6.0))\n)\nGROUP BY `cyl`\nORDER BY `a1` DESC"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#rmarkdown",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#rmarkdown",
    "title": "Pacotes Essenciais R",
    "section": "RMarkdown",
    "text": "RMarkdown\nUma análise de dados, em geral, vira um de dois produtos: um relatório/artigo ou um aplicativo interativo. Novamente o R tem funcionalidades incríveis para ambos os objetivos. Existe um tipo de arquivo chamado RMarkdown que junto com o pacote knitr permite compilar um arquivo misto de Markdown e R em formato pdf, html, doc ou ppt.\nEssa extensão é tão poderosa que não só é possível escrever um artigo científico com ela, mas também, um dashboard interativo, um livro inteiro, ou até um blog/site (este blog é escrito em RMarkdown).\nAtualmente o formato do código é ainda mais flexível, permitindo misturar linguagens como R, Python, SQL e outras num mesmo arquivo. Um resumo das funcionalidades pode ser visto aqui."
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#shiny",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#shiny",
    "title": "Pacotes Essenciais R",
    "section": "Shiny",
    "text": "Shiny\nO pacote shiny tem se tornado cada vez mais potente nos últimos anos. Inicialmente, ele servia para montar dashboards relativamente simples, que permitiam ao usuário mais liberdade para explorar e montar suas próprias análises. Atualmente, tanto o pacote como as suas extensões melhoraram muito o seu potencial.\nA galeria de apps do RStudio já está um pouco desatualizada, mas dá um sabor do que é possível fazer com shiny.\nAlguns exemplos:\n\nRadiant - Esta é, sem dúvida, uma das aplicações mais completas e impressionantes de Shiny, vale um post inteiro por si só.\nMonitor de Covid\nAnaálise do Perfil de Eleitor no Brasil"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#quarto",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#quarto",
    "title": "Pacotes Essenciais R",
    "section": "Quarto",
    "text": "Quarto\nRecentemente lançado, o quarto não é um pacote de R propriamente dito. O Quarto é como um RMarkdown turbinado, é um meio de publicar análises de dados com textos. De maneira geral o Quarto te permite:\n\nEscrever e rodar análises de dados em R, python e julia\nPublicar relatórios, dashboards, livros, sites, etc.\nPublicar artigos científicos utilizando equações, citações, etc.\n\nPara conhecer mais sobre o Quarto vale a pena checar o site. Como o Quarto foi desenvolvido pela Posit, que desenvolvou o RStudio a IDE mais popular de R, o Quarto e o R funcionam muito bem no RStudio."
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html",
    "href": "posts/general-posts/repost-sarima-no-r/index.html",
    "title": "SARIMA no R",
    "section": "",
    "text": "Neste post apresento como estimar um modelo SARIMA simples no R usando os pacotes astsa e forecast. O pacote astsa foi elaborado pelos autores do livro Time Series Analysis. Já o forecast foi desenvolvido por Rob. Hyndman, autor do livro Forecasting: Principles and Practice."
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html#identificação-e-tranformações",
    "href": "posts/general-posts/repost-sarima-no-r/index.html#identificação-e-tranformações",
    "title": "SARIMA no R",
    "section": "Identificação e tranformações",
    "text": "Identificação e tranformações\nComo a variância da série cresce ao longo do tempo aplico uma transformação log nos valores da série. Seja \\(y_{t}\\) nossa série. Então fazemos \\(ly_{t} \\equiv \\text{log}(y_{t}).\\)\n\n# Aplica transformação log (logaritmo natural)\nly &lt;- log(AirPassengers)\n# Gráfico\nautoplot(ly) +\n  geom_point(shape = 21) +\n  labs(x = \"\",\n       y = \"Passagens aéreas (log)\",\n       title = \"Demanda mensal de passagens aéreas internacionais\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nPara testar a acurácia do modelo vamos remover algumas das últimas observações. Estas serão testadas contra as previsões do modelo. Aqui sigo a nomenclatura de train (treino) e test (teste). O gráfico abaixo permite visualizar esta divisão, onde os valores em vermelho foram excluídos da nossa amostra.\n\ntrain &lt;- window(ly, end = c(1957, 12))\ntest &lt;- window(ly, start = c(1958, 1))\n\nautoplot(train) +\n  autolayer(test) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nTirando a primeira diferença da série removemos a sua tendência de crescimento. Fazemos \\(dly_{t} \\equiv (1-L)ly_{t} = ly_{t} - ly_{t-1}\\) usando a função diff(). Note pelo gráfico que ainda parece haver forte sazonalidade na série.\n\n# Tira a primeira diferença da série\ndly &lt;- diff(train)\n\nautoplot(dly) +\n  ggtitle(\"Primeira diferença do log da série\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nPodemos ver isto mais claramente na análise do correlograma da série diferenciada. No gráfico abaixo, os “lags” seguem a periodicidade da série, isto é, cada “lag” representa o equivalente a 12 meses. Parece haver uma forte correlação entre \\(dly_{t}\\) com \\(dly_{t-12}, dly_{t-24}, \\dots, dly_{t-12k}\\) com \\(k = 1, 2, \\dots\\).\n\n# Gráfico da FAC e FACP\nacf2(dly)\n\n\n\n\n\n\n\n\nAgora tiramos uma diferença sazonal de 12 meses. A série resultante, \\(sdly_{t}\\) fica: \\[\\begin{align}\n  sdly_{t} \\equiv (1-L^{12})dly_{t} & = (1-L^{12})(1-L)ly_{t} \\\\\n                                    & = (1 - L^{12} - L + L^{13})ly_{t} \\\\\n                                    & = ly_{t} - ly_{t-1} - ly_{t-12} + ly_{t-13}\n\\end{align}\\]\nAbaixo temos o correlograma de \\(sdly_{t}\\). A primeira defasagem é significativa tanto no ACF como no PACF. Além disso a 12ª defasagem também é significativa em ambos. Assim, vamos primeiro tentar um modelo de “ordem máxima” SARIMA\\((1,1,1)(1,1,1)[12]\\). A partir deste modelo, vamos tentar estimar outros de ordens mais baixas para evitar o problema de sobreparametrização.\n\n# Tira a primeira diferença sazonal da série\nsdly &lt;- diff(dly, 12)\n# Gráfico da FAC e FACP\nacf2(sdly)"
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html#estimação",
    "href": "posts/general-posts/repost-sarima-no-r/index.html#estimação",
    "title": "SARIMA no R",
    "section": "Estimação",
    "text": "Estimação\nA equação do modelo que estamos estimando é: \\[\n  (1 - \\phi L)(1 - \\Phi L^{12})(1 - L)(1 - L^{12})ly_{t} = (1 + \\theta L)(1 + \\Theta L^{12})\\epsilon_{t}\n\\] Vamos o usar o comando sarima do pacote astsa. Note que os resíduos do modelo parecem se comportar como ruído branco, indicando que nosso modelo está bem ajustado.\n\n(m1 &lt;- sarima(p = 1, d = 1, q = 1, P = 1, D = 1, Q = 1, S = 12, xdata = train))\n\ninitial  value -3.031632 \niter   2 value -3.170590\niter   3 value -3.246467\niter   4 value -3.246688\niter   5 value -3.247295\niter   6 value -3.247539\niter   7 value -3.247841\niter   8 value -3.248283\niter   9 value -3.248794\niter  10 value -3.249345\niter  11 value -3.249360\niter  12 value -3.249378\niter  13 value -3.249379\niter  14 value -3.249380\niter  15 value -3.249381\niter  16 value -3.249381\niter  17 value -3.249381\niter  17 value -3.249381\niter  17 value -3.249381\nfinal  value -3.249381 \nconverged\ninitial  value -3.248923 \niter   2 value -3.252561\niter   3 value -3.258267\niter   4 value -3.265627\niter   5 value -3.267071\niter   6 value -3.267585\niter   7 value -3.267745\niter   8 value -3.268029\niter   9 value -3.268401\niter  10 value -3.268494\niter  11 value -3.268499\niter  12 value -3.268506\niter  13 value -3.268511\niter  14 value -3.268511\niter  15 value -3.268511\niter  15 value -3.268511\niter  15 value -3.268511\nfinal  value -3.268511 \nconverged\n\n\n\n\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.2565  -0.6243  -0.0599  -0.5574\ns.e.  0.2759   0.2289   0.1765   0.1670\n\nsigma^2 estimated as 0.001367:  log likelihood = 175.71,  aic = -341.42\n\n$degrees_of_freedom\n[1] 91\n\n$ttable\n     Estimate     SE t.value p.value\nar1    0.2565 0.2759  0.9299  0.3549\nma1   -0.6243 0.2289 -2.7271  0.0077\nsar1  -0.0599 0.1765 -0.3392  0.7353\nsma1  -0.5574 0.1670 -3.3380  0.0012\n\n$AIC\n[1] -3.593882\n\n$AICc\n[1] -3.589203\n\n$BIC\n[1] -3.459467\n\n\nPara evitar o problema de sobreparametrização (overfitting) temos que tentar ajustar modelos de ordens similares, porém mais baixas. Este processo costuma ser iterativo, isto é, na base da tentativa e erro seguindo algum critério de informação (i.e.: AIC, AICc, BIC, etc.). Depois de tentar vários modelos diferentes chegamos, por exemplo, no SARIMA\\((0,1,1)(0,1,1)[12]\\). A equação do modelo pode ser expressa como: \\[\n  (1 - L)(1 - L^{12})ly_{t} = (1 + \\theta L)(1 + \\Theta L^{12})\\epsilon_{t}\n\\]\nNovamente, os resíduos do modelo indicam que ele está bem ajustado aos dados.\n\nm2 &lt;- sarima(p = 0, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12, xdata = train)\n\ninitial  value -3.047456 \niter   2 value -3.251231\niter   3 value -3.251514\niter   4 value -3.268396\niter   5 value -3.270049\niter   6 value -3.270196\niter   7 value -3.270197\niter   8 value -3.270198\niter   8 value -3.270198\niter   8 value -3.270198\nfinal  value -3.270198 \nconverged\ninitial  value -3.263453 \niter   2 value -3.264133\niter   3 value -3.264159\niter   4 value -3.264160\niter   4 value -3.264160\niter   4 value -3.264160\nfinal  value -3.264160 \nconverged\n\n\n\n\n\n\n\n\n\n\nm2$fit\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     sma1\n      -0.3864  -0.5885\ns.e.   0.1097   0.0927\n\nsigma^2 estimated as 0.001383:  log likelihood = 175.3,  aic = -344.59\n\n\nA estimativa tem a forma:\n\\[\n  (1 - L)(1 - L^{12})ly_{t} = (1 - 0.3864 L)(1 - 0.5885 L^{12})\\epsilon_{t}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html#previsão",
    "href": "posts/general-posts/repost-sarima-no-r/index.html#previsão",
    "title": "SARIMA no R",
    "section": "Previsão",
    "text": "Previsão\nPara computar as previsões do modelo usamos a função sarima.for. Aqui podemos comparar as previsões do modelo (construindo usando apenas as observações dentro de train) com as observações. Esta função automaticametne retorna um gráfico com as previsões fora da amostra.\n\npred &lt;- sarima.for(train, n.ahead = length(test),\n                   p = 0, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)\n\n\n\n\n\n\n\n\nPodemos construir um gráfico que inclui as observações reservadas no test usando as seguintes funções base do R.\n\nplot.ts(pred$pred, col = \"red\", ylim = c(5.7, 6.5),\n        ylab = \"\",\n        main = \"Previsão do modelo SARIMA(0, 1, 1)(0, 1, 1)[12]\")\nlines(test, type = \"o\")\nlegend(\"topleft\", legend = c(\"Previsto\", \"Observado\"), lty = 1,\n       pch = c(NA, 1), col = c(\"red\", \"black\"))\n\n\n\n\n\n\n\n\nComo comentado em outro post, pode-se produzir visualizações mais elegantes usando o ggplot2, mas o pacote não “conversa” bem com os objetos típicos de séries de tempo. Um jeito de contornar isto é usando a função autoplot() do pacote forecast. Abaixo, reestimo o modelo usando a função arima. Este passo é necessário para usar a função autoplot().\n\nm &lt;- arima(train, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 12))\n\nAgora podemos verificar a qualidade das nossas previsões. O gráfico abaixo foi construído funções base do R. A linha azul representa as previsões do modelo SARIMA especificado acima, enquanto que a linha vermelha representa as observações. As áreas sombreadas são intervalos de confiança: o mais escuro é de 80% e o mais claro de 95%.\n\nplot(forecast(m, h = length(test)))\nlines(test, col = \"black\")\nlegend(\"topleft\",\n       legend = c(\"Previsto\", \"Observado\"),\n       lty = 1, col = c(\"red\", \"black\"))\n\n\n\n\n\n\n\n\nUsando o autoplot temos o seguinte resultado:\n\nautoplot(forecast(m, h = length(test)), include = 50) +\n  autolayer(test) +\n  labs(x = \"\", y = \"\") + \n  theme_bw() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/general-posts/aquecimento-global/index.html",
    "href": "posts/general-posts/aquecimento-global/index.html",
    "title": "Aquecimento Global",
    "section": "",
    "text": "Uma recente edição da revista inglesa The Economist exibe uma série de listras coloridas em sua capa. Elas formam um degradê que vai de um azul escuro até um vermelho intenso. Cada listra representa a temperatura de um ano e a linha do tempo vai desde o 1850 até o presente. A mensagem é bastante clara: o planeta esta cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Esta imagem é creditada a Ed Hawkings, editor do Climate Lab Book.\nPara ser preciso, a imagem não plota a temperatura de cada ano, mas sim o quanto cada ano se desvia da temperatura média do período 1971-2000. Isto é, anos acima dessa média têm um valor positivo, valores abaixo dessa média, valores negativos. Esta é uma forma bastante comum de representar este tipo de dado climático. De imediato, quando vi a imagem me ocorreu que seria bastante simples reproduzir uma versão aproximada dela usando o R."
  },
  {
    "objectID": "posts/general-posts/aquecimento-global/index.html#o-código",
    "href": "posts/general-posts/aquecimento-global/index.html#o-código",
    "title": "Aquecimento Global",
    "section": "O código",
    "text": "O código\nO código necessário para gerar a imagem é bastante enxuto. Vou descrever em linhas gerais o que ele faz:\nPrimeiro carrego dois pacotes (linhas 1, 2), depois a série de temperatura (linha 3), faço algumas transformações nos dados (linhas 4, 5) e, por fim, ploto os dados (linhas 6, 7, 8). O resultado inicial já é bastante satisfatório e a partir destas poucas linhas de código pode-se chegar num resultado muito próximo ao da imagem original. Vale notar que a imagem fica um pouco diferente da original porque eu uso uma base de dados diferente.\n\n# Carrega pacotes\nlibrary(ggplot2)\nlibrary(astsa)\n# Carrega a base de dados 'globtemp'\ndata(\"globtemp\")\n# Converte o objeto para data.frame\ndf &lt;- data.frame(ano = as.numeric(time(globtemp)),\n temp = as.numeric(globtemp))\n\n# Monta o gráfico\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")"
  },
  {
    "objectID": "posts/general-posts/aquecimento-global/index.html#os-detalhes-do-código",
    "href": "posts/general-posts/aquecimento-global/index.html#os-detalhes-do-código",
    "title": "Aquecimento Global",
    "section": "Os detalhes do código",
    "text": "Os detalhes do código\nVou explicar cada linha de código para ser didático. O R funciona, grosso modo, como um repositório de pacotes: cada pacote contem funções e, às vezes, bases de dados. O primeiro pacote que carrego é o ggplot2. Ele serve para fazer visualizações de dados. O pacote astsa traz várias funções para fazer análise de séries de tempo, mas eu carrego ele somente para usar a base de dados globtemp, que traz informação sobre a temperatura anual da terra coletada pela NASA.\nO objeto globtemp é uma série de tempo (um objeto da classe ts), que tem alguns atributos especiais. Um deles pode ser acesado pela função time que extrai um vetor numérico com as datas desta série de tempo. No código abaixo mostro os primeiros dez valores do time(globtemp).\n\nclass(globtemp)\n\n[1] \"ts\"\n\ntime(globtemp)[1:10]\n\n [1] 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n\n\nPara extrair somente os valores da série, uso a função as.numeric, que converte o vetor de ts para numeric (numérico). Este tipo de função é bastante comum já que frequentemente é preciso trocar a classe de um objeto. O objetivo destes primeiros passos é de inserir as informações do globtemp num data.frame em que a data aparece na primeira coluna e os valores da série são armazenados na segunda coluna. O procedimento pode parecer um tanto trabalhoso (e acho que é mesmo), mas é o jeito. Um data.frame é como uma tabela com dados. Este é um objeto bastante típico em análise de dados e é necessário para usar a função ggplot que vai fazer o gráfico. Abaixo pode-se ver as primeiras linhas desta tabela.\n\nhead(df)\n\n   ano  temp\n1 1880 -0.20\n2 1881 -0.11\n3 1882 -0.10\n4 1883 -0.20\n5 1884 -0.28\n6 1885 -0.31\n\n\nAgora que tenho os dados no formato apropriado posso usar o ggplot. O argumento que pode ser um pouco confuso é o aes. Nele especifica-se quais dados serão mapeados no gráfico. Depois disso adicionamos um geom. Há vários tipos de geom (geom_line, geom_bar, geom_histogram, etc.) e cada um deles produz uma imagem diferente. O geom_tile faz um pequeno quadrado. Para que a função consiga desenhar o quadrado é preciso informar uma variável x e uma variável y. Além disso, também especifico fill = temp. O fill se refere à cor que vai preencher (fill) o quadrado. Como especifico fill = temp a cor do quadrado vai representar a variável temp (temperatura).\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile()\n\n\n\n\n\n\n\n\nO resultado é exatamente como o esperado, mas ainda é preciso mudar a escala de cores. Faço isto com o scale_fill_gradient2. Aqui cada termo tem um signficado: scale_fill pois estamos mudando a escala do fill (outra opção seria scale_color que muda a escala do color). scale_fill_gradient pois queremos um gradiente (degradê) de cores. Por fim, o 2 é adicionado no final pois queremos um escala que diferencie dois grupos distintos: temperaturas acima da média em vermelho, temperaturas abaixo da média em azul. A escala de cores é determinada pelos argumentos low, mid e high.\nOs valores negativos serão coloridos pelo low, os próximos de zero pelo mid e os valores grandes pelo high. Abaixo escrevo as cores em hexa-decimal, mas elas podem ser lidas, essencialmente, como: azul-escuro, cinza-azulado-claro e vermelho-escuro.\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")\n\n\n\n\n\n\n\n\nComo comentei acima, pode-se melhorar o gráfico acima adicionando outros elementos e detalhes. A versão final que fiz do gráfico fica no código abaixo.\n\n# Pacote para carregar fontes externas no R\n# Necessário para utilizar 'Georgia' no gráfico\nlibrary(extrafont)\n# Data.frames auxiliares para plotar as anotações de texto\ndf_aux_title &lt;- data.frame(x = 1930, y = 0, label = \"The Climate Issue\")\ndf_aux_anos &lt;- data.frame(\n  label = c(1880, 1920, 1960, 2000),\n  x = c(1890, 1925, 1960, 1995)\n  )\n\nggplot() +\n  geom_tile(data = df, aes(x = ano, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 6,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = 1950, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 11,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_gradientn(\n    colors = c(\"#213A82\", \"#3B60CE\", \"#8DA2E2\", \"#DE2E02\", \"#9d0208\")\n   ) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n    )"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html",
    "href": "posts/general-posts/ipca-visualizacao/index.html",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "A inflação voltou a ser uma pauta, não só no Brasil, mas no mundo todo, nos últimos meses. No Brasil, a combinação de câmbio desvalorizado, desajustes logísticos, crise hídrica e choques de preços externos, culminaram no maior nível de inflação desde 2002.\nMesmo em países avançados, os níveis de inflação estão em altas históricas. Nos Estados Unidos, por exemplo, o nível do CPI está no valor mais alto desde o final dos anos 1970.\nVisualizar a magnitude da inflação no Brasil pode ser um pouco desafiador. A série do IPCA é calculada desde 1979. O número de cidades avaliadas pelo índice cresceu no tempo: nos primeiros anos o índice contemplava Rio de Janeiro, Porto Alegre, Belo Horizonte, Recife, São Paulo, Brasília, Belém, Fortaleza, Salvador e Curitiba. Em 1991, Goiânia entra no índice e, mais recentemente, em 2014, Vitória e Campo Grande também entraram no cômputo do índice.\nMais importante do que a variação no número das cidades, é o período hiperinflacionário da década de 1980. Os números da inflação são incomparavelmente mais altos do que os atuais. Como regra, os cortes temporais mais relevantes para enxergar a inflação são Jul/94 (Plano Real), Jul/99 (Regime de Metas de Inflação), Mai/00 (Lei de Responsabilidade Fiscal) e Mai/03 (pós choque de 2002).\nNeste post vou mostrar o comportamento da inflação desde 1999.\n\n\n\n# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)\n\n\n\n\nImporto os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  right_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)\n\n\n\n\nO cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nUma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso.\n\n\n\n\nOutra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com um\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#pacotes",
    "href": "posts/general-posts/ipca-visualizacao/index.html#pacotes",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#importando-os-dados",
    "href": "posts/general-posts/ipca-visualizacao/index.html#importando-os-dados",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Importo os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  right_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#inflação",
    "href": "posts/general-posts/ipca-visualizacao/index.html#inflação",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "O cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#no-longo-prazo",
    "href": "posts/general-posts/ipca-visualizacao/index.html#no-longo-prazo",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Uma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso."
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#enxergando-a-distribuição",
    "href": "posts/general-posts/ipca-visualizacao/index.html#enxergando-a-distribuição",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Outra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com um\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html",
    "title": "Fundamentos: histograma",
    "section": "",
    "text": "Um histograma serve para visualizar a distribuição de um conjunto de dados. Ele consiste em colunas que representam a frequência de ocorrência de determinados valores nos dados: quanto mais alta for a coluna, mais frequente é uma observação. Isto permite ver a forma da distribuição dos dados e identificar padrões e tendências.\n\n\n\n\n\n\n\n\n\nHistogramas aparecem naturalmente na hora de visualizar, por exemplo:\n\nDistribuição de notas de alunos em testes padronizados.\nDistribuição da renda familiar na população de um país.\nDistribuição de preços de imóveis numa cidade.\nDistribuição da altura das pessoas.\nDistribuição de variáveis aleatórias em estatística.\n\nNeste post vamos entender como montar histogramas no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para visualizar a taxa de poupança nos EUA aos longo dos anos. Depois vamos trabalhar um exemplo mais complexo, analisando a distribuição do preço dos imóveis no Texas, EUA."
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#ggplot2",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#ggplot2",
    "title": "Fundamentos: histograma",
    "section": "ggplot2",
    "text": "ggplot2\nPara criar um histograma com o pacote ggplot2 no R, usamos a função geom_histogram().\nA estrutura de um gráfico do ggplot2 parte de três elementos básicos: (1) a base de dados, isto é, um objeto data.frame; (2) um mapeamento de variáveis, feito com auxílio da função aes(); e (3) a escolha da forma do gráfico, feito com as funções geom.\nO ggplot2 funciona adicionando camadas e elementos subsequentemente sobre um gráfico inicial. Cada elemento novo que adicionamos ao gráfico é somado usando o operador +.\nPara resumir o processo: começamos com a função ggplot() e vamos adicionando geoms, funções auxiliares que especificam a forma do gráfico. Este processo construtivo de adicionar elementos a um gráfico é o principal diferencial do ggplot.\nOu seja, temos três elementos essenciais:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEsta estrutura básica é esquematizada no pseudo-código abaixo.\n\nggplot(data = base_de_dados, aes(x = variavel_x)) +\n  geom_histogram()\n\nVamos montar um exemplo usando a base economics, que vem carregada junto com o pacote ggplot2. Esta base compila uma série de informações econômicas e demográficas no período julho/1967 a abril/2014 nos EUA. Para explorar os dados podemos usar a função head() que exibe as primieras linhas da tabela.\n\nhead(economics)\n\n\n\n\n\n\ndate\npce\npop\npsavert\nuempmed\nunemploy\n\n\n\n\n1967-07-01\n507\n198712\n13\n4\n2944\n\n\n1967-08-01\n510\n198911\n13\n5\n2945\n\n\n1967-09-01\n516\n199113\n12\n5\n2958\n\n\n1967-10-01\n512\n199311\n13\n5\n3143\n\n\n1967-11-01\n517\n199498\n13\n5\n3066\n\n\n1967-12-01\n525\n199657\n12\n5\n3018\n\n\n\n\n\n\n\nInicialmente, vamos nos focar na coluna psavert, que é a taxa de poupança individual, isto é, o percentual da renda que as famílias poupam. O código abaixo monta um histograma desta variável.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram()\n\nVamos decompor o código acima em partes. Primeiro temos que informar onde estão os nossos dados. Fazemos isto dentro da função ggplot() usando o argumento data = economics.\nDepois, precisamos indicar qual a variável (coluna) que queremos visualizar, isto é, indicar qual é a variável que deve ser mapeada em um elemento visual. Fazemos isto usando a função aes(x = psavert).\nPor fim, como queremos desenhar um gráfico de histograma escolhemos o geom_histogram(). Esta última função é adicionada (somada) à função inicial com o sinal de soma +.\nSegue abaixo o código comentado junto com o gráfico produzido. Vemos que, historicamente, a taxa de poupança gira entre 5% e 15% da renda pessoal.\n\n# Chamada inical da função ggplot\nggplot(\n  # Define a base de dados\n  data = economics,\n  # Escolhe qual a variável deve ser visualizda\n  aes(x = psavert)\n  ) +\n  # Escolhe o tipo de gráfico (histograma)\n  geom_histogram()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#elementos-estéticos",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#elementos-estéticos",
    "title": "Fundamentos: histograma",
    "section": "Elementos estéticos",
    "text": "Elementos estéticos\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nUm gráfico de histograma tem cinco elementos estéticos principais:\n\ncolor - Define a cor do contorno da coluna.\nfill - Define a cor que preenche a coluna.\nalpha - Define o nível de transparência das cores.\nbinwidth - Define a largura da coluna.\nbins - Define o número de colunas.\n\nOs dois últimos elementos são parâmetros estatísticos que são interpretados como estéticos neste contexto. Vamos explorar cada um destes elementos em exemplos abaixo.\nVale notar que o argumento x também é um elemento estético. Mais especificamente ele é um elemento estético variável, logo é mapeado com a função aes(), e é obrigatório (pois é exigido pela função geom_histogram())\n\nCores\nTemos duas opções principais de cores: color é a cor da linha do contorno da coluna e fill é a cor que preenche o interior da coluna. O código abaixo ilustra como utilizar estes argumentos dentro da função geom_histogram(). Note que ambos os elementos estéticos são constantes.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém podemos fazer referência a cores via código hexadecimal. No exemplo abaixo uso as cores \"#e76f51 (laranja-escuro) e \"#264653\" (azul-escuro).\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(color = \"#E76F51\", fill = \"#264653\")\n\n\n\n\n\n\n\n\n\n\nTransparência\nO parâmetro alpha controla o nível de transparência das cores. O valor dele deve estar sempre entre 0 e 1. Quanto mais próximo de 0, mais transparente será o gráfico final. Os gráficos abaixo mostram o efeito de alguns valores distintos de alpha.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.9)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.7)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.3)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\nColunas\nPodemos controlar o número de colunas do histograma de duas formas: (1) escolhendo o número via bins; (2) escolhendo o tamanho dos intervalos/colunas via binwidth.\nA escolha padrão da função geom_histogram() é definir bins = 30. Isto raramente resulta num gráfico ideal. O número ótimo de intervalos depende do tipo de dado que estamos visualizando.\nEm geral, um número muito pequeno resulta num gráfico agrupado demais, enquanto um número muito grande resulta num gráfico disperso demais. Em ambos os casos fica difícil enxergar o padrão nos dados.\nO código abaixo reduz o número de intervalos para 5. Note como as observações estão mais agrupadas.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 5)\n\n\n\n\n\n\n\n\nJá o código seguinte aumenta o número de intervalos para 70. Agora conseguimos identificar mais facilmente os outliers, mas as observações estão dispersas demais para conseguir enxergar algum tipo de padrão.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nPor fim, o gráfico abaixo tenta chegar num meio termo. Vemos que a taxa de poupança tem uma distribuição parecida com uma normal e possui alguns outliers tanto à esquerda como à direita.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nComo mencionado acima, podemos definir o tamanho dos intervalos usando binwidth. Como nossa variável está expressa em formato de percentual, podemos experimentar intervalos de tamanho unitário. O resultado, neste caso, é bastante satisfatório.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: histograma",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  labs(\n    # Título\n    title = \"Taxa de poupança pessoal nos EUA\",\n    # Subtítulo\n    subtitle = \"Distribuição da taxa de poupança, como proporção da renda disponível, no período 1967-2014.\",\n    # Nome do eixo-x\n    x = \"Taxa de poupança (%)\",\n    # Nome do eixo-y\n    y = \"Frequência\",\n    # Nota de rodapé\n    caption = \"Fonte: FREDR\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: histograma",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes(). Como expliquei acima, esta função “transforma” nossos dados em elementos visuais. Nos casos acima, ela mapeia a variável x nas colunas do histograma.\nMas também podemos mapear uma coluna para um elemento estético como o fill, por exemplo. O resultado é um gráfico em que a cor de cada coluna vai corresponder a uma variável da nossa base de dados.\nAgora, vamos utilizar a base de dados txhousing que compila informações do mercado imobiliário das principais cidades do estado do Texas, nos EUA. Como a base inclui mais de 40 cidades vamos restringi-la para apenas quatro cidades: Austin, Dallas, Houston e San Angelo. Vamos visualizar a distribuição da variável median que registra o valor mediano de venda mensal dos imóveis em cada cidade. A variável city indica o nome da cidade.\n\nhead(txhousing)\n\n\n\n\n\n\ncity\nyear\nmonth\nsales\nvolume\nmedian\nlistings\ninventory\ndate\n\n\n\n\nAbilene\n2000\n1\n72\n5380000\n71400\n701\n6\n2000\n\n\nAbilene\n2000\n2\n98\n6505000\n58700\n746\n7\n2000\n\n\nAbilene\n2000\n3\n130\n9285000\n58100\n784\n7\n2000\n\n\nAbilene\n2000\n4\n98\n9730000\n68600\n785\n7\n2000\n\n\nAbilene\n2000\n5\n141\n10590000\n67300\n794\n7\n2000\n\n\nAbilene\n2000\n6\n156\n13910000\n66900\n780\n7\n2000\n\n\n\n\n\n\n\nQueremos um gráfico em que cada cidade tenha uma cor diferente, então, a variável city deve aparecer dentro da função aes(). O código abaixo primeiro organiza os dados e depois monta o gráfico. Agora, cada cidade tem uma cor diferente e as colunas são “empilhadas” umas sobre as outras.\n\n# Cria um vetor com as cidades selecionadas\ncities &lt;- c(\"Austin\", \"Dallas\", \"Houston\", \"San Angelo\")\n# Seleciona apenas as linhas que contêm informações sobre estas cidades\nsubtxhousing &lt;- subset(txhousing, city %in% cities)\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(aes(fill = city))\n\n\n\n\n\n\n\n\nNo gráfico acima, conseguimos ver, por exemplo, que o valor mais frequente de venda está em torno de 150 mil. Além disso, pode-se ver como os valores de venda em San Angelo costumam ser menores do que os valores de venda em Austin.\nPara ter maior controle sobre as cores e sobre a legenda usamos a função scale_fill_manual() e a função theme().\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(\n    # Mapeaia a variável city nas cores das colunas\n    aes(fill = city),\n    # Define o número de colunas\n    bins = 25,\n    # Define a cor (contorno) das colunas\n    color = \"white\") +\n  # Controla as cores e a legenda\n  scale_fill_manual(\n    # Título da legenda\n    name = \"Cidade\",\n    # Cores das colunas\n    values = c(\"#264653\", \"#2a9d8f\", \"#f4a261\", \"#e76f51\")\n  ) +\n  # Posiciona a legenda acima do gráfico\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#resumo",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#resumo",
    "title": "Fundamentos: histograma",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar alguns histogramas interessantes em poucas linhas de código. Em qualquer gráfico temos três elementos básicos:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos importantes:\n\nElementos estéticos podem ser constantes (números ou texto) ou variáveis (colunas da base de dados). Elementos variáveis precisam estar dentro da função aes().\nA escolha do número de colunas/intervalos depende do dado que queremos visualizar. Em geral, é preciso experimentar com números diferentes.\nSe o elemento fill for variável é preciso usar a função scale_fill_manual() para controlar as cores e a legenda de cores.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos."
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html",
    "title": "Fundamentos: gráfico de coluna",
    "section": "",
    "text": "Um gráfico do colunas é uma ferramenta de visualização poderosa e versátil. Tipicamente, cada coluna corresponde ao valor de uma classe.\n\n\n\n\n\n\n\n\n\nNeste post vamos aprender a montar gráficos de colunas usando o pacote ggplot2. Há duas funções para criar gráficos de colunas: o geom_bar() e geom_col(). A primeira função conta uma quantidade de ocorrências, é útil para resumir visualmente uma base de dados. Já a segunda função plota a altura da coluna segundo os valores nos dados, então é mais útil quando os dados já estão agregados. Estas diferenças ficarão mais claras nos exemplos abaixo."
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#ggplot2",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#ggplot2",
    "title": "Fundamentos: gráfico de coluna",
    "section": "ggplot2",
    "text": "ggplot2\nO pacote ggplot2 segue uma sintaxe bastante consistente, que permite “somar” elementos visuais sobre um mesmo gráfico. Isto permite que se crie uma infinidade de gráficos complexos a partir de elementos simples. Os elementos visuais são todos chamados por funções geom como as funções geom_bar (barra) e geom_col (coluna) citadas acima. Estes elementos são somados de maneira intuitiva usando o sinal de soma +.\nEssencialmente, temos os seguintes elementos principais:\n\ndata - Uma base de dados.\naes - Variáveis que são mapeadas em elementos visuais.\ngeom - Um objeto geométrico.\n\nEstes elementos são combinados numa sintaxe recorrente. A função ggplot tem apenas dois argumentos data e aes. Adicionamos uma função geom nesta chamada inicial como no exemplo abaixo. Note que usamos o geom_col mas poderíamos utilizar qualquer outra função como geom_point.\n\nggplot(data = dados, aes(x = varivel_x, y = variavel_y)) +\n  geom_col()\n\n\ngeom_bar\nA função geom_bar() exige apenas um argumento x que é o nome da variável que será “contada”. Esta função conta a quantidade de vezes que os elementos da variável x se repetem e plota este valor num gráfico de barras.\nVamos utilizar a base hprice1, que agrega o preço de venda de imóveis em Boston em 1990. Para inspecionar os dados utilizamos a função head.\n\nhead(hprice1)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810\n\n\nA coluna bdrms indica o número de dormitórios do imóvel. Podemos contar o número de imóveis pelo número de dormitórios usando a função geom_bar().\n\nggplot(data = hprice1, aes(x = factor(bdrms))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nVamos quebrar o código acima em detalhes. Primeiro, usamos a função ggplot() para declarar que queremos fazer um gráfico.\nColocamos os argumentos data e aes() dentro desta função. O argumento data deve ser o nome da nossa base de dados: neste caso, data = hprice1.\nA função aes() é a que transforma as variáveis (as colunas da base de dados) em elementos visuais. Neste caso ele vai transformar o número de dormitórios em algum elemento visual. Escolhemos aes(x = factor(bdrms)). Aqui, a função factor é opcional, mas fortemente recomendada. Ela força a variável bdrms a se comportar como uma variável categórica (ao invés de uma variável contínua).\nEspecificamos qual deve ser o elemento visual somando a função geom_bar() no código inicial. Esta função indica que queremos um gráfico de barras.\nUnindo todas estes elementos temos um código enxuto que plota o gráfico. Vemos que os imóveis de 3 e 4 dormitórios são os mais comuns.\n\nggplot(data = hprice1, aes(x = factor(bdrms))) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\ngeom_col\nA função geom_col desenha o mesmo tipo de gráfico que a função geom_bar, mas ela exige que sejam inputados dois argumentos: x e y. Geralmente, x é a classe ou categoria, enquanto que y é o valor que será transformado na “altura” da coluna.\nVamos montar um exemplo simples. Nosso objetivo é visualizar as medalhas olímpicas brasileiras na mais recente edição das Olimpíadas, em 2020 (que foi realizada apenas em 2021). Para isto, vamos inserir os dados diretamente usando a função data.frame(). Esta função permite criar uma base de dados manualmente. A sintaxe da função é bastante simples: primeiro declaramos o nome da coluna e depois os seus valores; vamos acrescentando colunas separando-as por vírgulas.\n\n# Exemplo de como usar a função data.frame\ndados &lt;- data.frame(\n  nome_1 = c(...),\n  nome_2 = c(...),\n  ...\n)\n\nNa última Olimpíada, o Brasil obteve 7 medalhas de ouro, 6 de prata e 8 de bronze. O código abaixo estrutura estes dados num data.frame. Aqui a função factor ajuda a organizar os dados, pois impõe uma ordem de grandeza na variável categórica medalha.\n\n# Cria a base de dados\nolimpiadas &lt;- data.frame(\n  medalha = factor(c(\"ouro\", \"prata\", \"bronze\"), levels = c(\"ouro\", \"prata\", \"bronze\")),\n  contagem = c(7, 6, 8)\n)\n# Exibe os dados\nolimpiadas\n\n\n\n\n\n\nmedalha\ncontagem\n\n\n\n\nouro\n7\n\n\nprata\n6\n\n\nbronze\n8\n\n\n\n\n\n\n\nMontamos um gráfico de colunas usando o código abaixo.\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNovamente, vamos decompor o código. Primeiro, usamos a função ggplot() para declarar que queremos fazer um gráfico. Precisamos informar: (1) uma base de dados, data; (2) duas variáveis que serão mapeadas aes(x, y); (3) um “elemento geométrico” geom.\nO argumento data deve ser o nome da nossa base de dados: neste caso, data = hprice1.\nA função aes() é o que transforma as variáveis (as colunas da base de dados) em elementos visuais. Escolhemos aes(x = medalha, y = contagem).\nA primeira variável é categórica e indica o tipo da medalha. Como especificamos a ordem de grandeza no código anterior, a função aes() sabe que ouro &gt; prata &gt; bronze.\nA segunda variável é numérica e indica quantas medalhas de cada tipo foram ganhas. Na prática, o aes() usa esta informação para definir a altura da coluna.\nEspecificamos qual deve ser o elemento visual somando a função geom_col() no código inicial. Esta função indica que queremos um gráfico de colunas."
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#características-estéticas",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#características-estéticas",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Características estéticas",
    "text": "Características estéticas\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nUm gráfico de colunas tem três elementos estéticos principais:\n\ncolor - Define a cor do contorno da coluna.\nfill - Define a cor que preenche a coluna.\nalpha - Define o nível de transparência das cores.\n\nVale notar que os argumento x e y também são elementos estéticos. Mais especificamente eles são elementos estéticos variáveis, logo são mapeado com a função aes(). No caso da função geom_bar() apenas x é obrigatório enquanto que a função geom_col() exige tanto x como y.\n\nCores\nTemos o controle de duas cores: do contorno da coluna (color) e da cor que preenche a coluna (fill). O exemplo abaixo ilustra como podemos modificar estes elementos estéticos. No exemplo usamos a cor steelblue para preencher a coluna e definimos um contorno escuro usando color = \"black\".\nUma lista completa de cores com nomes está disponível aqui. Também podemos especificar as cores usando código hexadecimal.\n\n# Exemplo chamando as cores por nomes\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(color = \"black\", fill = \"steelblue\")\n\n\n\n\n\n\n\n\n\n# Exemplo usando cores em hexadecimal\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(color = \"#E5E5E5\", fill = \"#2A9D8F\")\n\n\n\n\n\n\n\n\n\n\nAlpha\nO parâmetro alpha varia entre 0 e 1 e indica a transparências das cores. Quanto menor o valor de alpha, maior será a transparência no resultado final.\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(color = \"black\", fill = \"steelblue\", alpha = 0.3)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes. Como expliquei acima, esta função mapeia nossos dados em elementos visuais. No caso da função geom_col(), ela mapeia a variável x no eixo-x, representando a categoria e variável y no eixo-y é a altura da barra, que indica seu valor.\nMas podemos mapear as variáveis nos elementos estéticos: color, fill, alpha.\nO uso prático mais comum é de variar o elemento fill, a cor que preenche a coluna, segundo alguma variável nos dados.\nNosso gráfico de medalhas olímpicas ficaria mais intuitivo se as cores das colunas correspondessem às cores das medalhas. Vamos tentar construir este gráfico.\nComo queremos que a cor de cada barra seja difernete para cada tipo de medalha temos de incluir o argumento aes(fill = medalha) dentro de geom_col().\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = medalha))\n\n\n\n\n\n\n\n\nAgora o R mapeou uma cor diferente para cada valor distinto de medalha. Infelizmente, as cores padrão não nos ajudam neste caso. Podemos escolher estas cores manualmente usando a função scale_fill_manual(). O código abaixo faz este ajuste. As cores são inseridas dentro do argumento values em formato hexadecimal. Por fim, adicionamos a linha guides(fill = \"none\") para remover a legenda redundante.\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = medalha)) +\n  # Escolhe manualmente as cores das colunas\n  scale_fill_manual(values = c(\"#FFD700\", \"#C0C0C0\", \"#CD7F32\")) +\n  # Remove a legenda\n  guides(fill = \"none\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-quando-há-múltiplos-grupos",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-quando-há-múltiplos-grupos",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Usando cores quando há múltiplos grupos",
    "text": "Usando cores quando há múltiplos grupos\nQuando temos muitos grupos há duas opções para representar os dados: (1) empilhamos os diferentes grupos; ou (2) colocamos as colunas lado a lado.\nPara exemplificar, vamos aumentar nossa base de dados com medalhas olímpicas do Japão, Itália, Brasil e Nova Zelândia.\n\nolimp &lt;- data.frame(\n  pais = factor(\n    c(\"Japão\", \"Japão\", \"Japão\", \"Itália\", \"Itália\", \"Itália\", \"Brasil\",\n      \"Brasil\", \"Brasil\", \"Nova Zelândia\", \"Nova Zelândia\", \"Nova Zelândia\"),\n    levels = c(\"Japão\", \"Itália\", \"Brasil\", \"Nova Zelândia\")),\n  medalha = factor(\n    c(\"Ouro\", \"Prata\", \"Bronze\", \"Ouro\", \"Prata\", \"Bronze\", \"Ouro\", \"Prata\",\n      \"Bronze\", \"Ouro\", \"Prata\", \"Bronze\"),\n    levels = c(\"Ouro\", \"Prata\", \"Bronze\")),\n  contagem = c(20, 28, 23, 10, 10, 20, 7, 6, 8, 7, 6, 7)\n)\n\n\n\n\n\n\npais\nmedalha\ncontagem\n\n\n\n\nJapão\nOuro\n20\n\n\nJapão\nPrata\n28\n\n\nJapão\nBronze\n23\n\n\nItália\nOuro\n10\n\n\nItália\nPrata\n10\n\n\nItália\nBronze\n20\n\n\nBrasil\nOuro\n7\n\n\nBrasil\nPrata\n6\n\n\nBrasil\nBronze\n8\n\n\nNova Zelândia\nOuro\n7\n\n\nNova Zelândia\nPrata\n6\n\n\nNova Zelândia\nBronze\n7\n\n\n\n\n\n\n\nAgora, queremos que cada país seja representado por uma cor distinta. Vamos, então, inserir aes(fill = pais) dentro de geom_col(). Note que o padrão da função é de empilhar os resultados.\nOu seja, temos o número total de medalhas de ouro, prata e bronze, onde cada cor representa um país diferente.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais))\n\n\n\n\n\n\n\n\nSe quiseremos ter uma visualização que represente mais diretamente a performance de cada país temos que incluir um argumento adicional position = \"dodge\" dentro da função geom_col(). Agora fica mais evidente, por exemplo, que o Japão teve um número grande de medalhas de prata e que o Brasil e a Nova Zelândia tiveram desempenhos muito semelhantes - o Brasil ficou na frente por causa de uma medalha de bronze.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"dodge\")\n\n\n\n\n\n\n\n\nO padrão da função geom_col() é position = \"stack\", que empilha as observações. Novamente, podemos escolher manualmente as cores dos grupos usando a função scale_fill_manual()\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"dodge\") +\n  # Escolhe manualmente as cores das colunas\n  scale_fill_manual(\n    # Título da legenda (opcional)\n    name = \"País\",\n    # Valores das cores\n    values = c(\"#d62828\", \"#008C45\", \"#FFDF00\", \"#012169\"))\n\n\n\n\n\n\n\n\nVale notar que existem vários pacotes e funções com cores pré-definidas que simplificam o processo manual de escolher as cores. O exemplo mais simples é o scale_fill_brewer() que utiliza as paletas de cores do Color Brewer.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"dodge\") +\n  scale_fill_brewer(\n    # Título da legenda (opcional) - omite o título\n    name = \"\",\n    # Tipo (\"qual\" - qualitativo, \"div\" - divergente ou \"seq\" - sequencial)\n    type = \"qual\",\n    # Escolha da paleta\n    palette = 6\n    )\n\n\n\n\n\n\n\n\nUma última opção é de forçar o eixo-y a operar dentro do intervalo 0-1, isto é, fazer com que as barras somem 1 e representem a proporção de cada grupo. Fazemos isto utilizando o argumento position = \"fill\" dentro de geom_col().\nNo gráfico abaixo temos a participação relativa de cada país no total de medalhas. Note que este tipo de gráfico faz mais sentido quando temos a totalidade dos grupos.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"fill\")\n\n\n\n\n\n\n\n\nNo exemplo abaixo montamos uma nova base de dados com o número de medalhas por continente. Para reduzir a digitação manual utilizamos a função rep() que repete uma mesma palavra. Agora que temos a totalidade dos grupos faz mais sentido um gráfico de colunas que represente a proporção de cada grupo.\n\nolimp_continentes &lt;- data.frame(\n  continente = factor(\n    rep(c(\"África\", \"América\", \"Ásia\", \"Europa\", \"Oceania\"), each = 3),\n    levels = c(\"Europa\", \"Ásia\", \"América\", \"Oceania\", \"África\")\n  ),\n  medalha = factor(\n    rep(c(\"Ouro\", \"Prata\", \"Bronze\"), times = 5)\n  ),\n  contagem = c(11, 12, 14, 72, 70, 70, 92, 80, 98, 141, 164, 193, 26, 13, 29)\n)\n\n\nggplot(data = olimp_continentes, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = continente), position = \"fill\") +\n  scale_fill_brewer(name = \"Continente\", type = \"qual\", palette = 6)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(data = olimp_continentes, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = continente), position = \"fill\") +\n  scale_fill_brewer(type = \"qual\", palette = 6) +\n  labs(\n    title = \"Europa lidera Olimpíadas\",\n    subtitle = \"Medalhas obtidas por continente nas Olimpíadas de Tóquio 2020\",\n    x = \"Tipo da medalha\",\n    y = \"Share de medalhas\",\n    caption = \"Fonte: www.olympiandatabase.com\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#invertendo-os-eixos",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#invertendo-os-eixos",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Invertendo os eixos",
    "text": "Invertendo os eixos\nPara facilitar a visualização das categorias pode ser interessante inverter os eixos do gráfico. Para isto usa-se o coord_flip(). Vale notar que é possível inverter os eixos de qualquer tipo de gráfico do ggplot.\nPara este exemplo, vamos utilizar a base de dados gapminder. Este base compila dados de PIB per capita, população e expectativa de vida de vários países ao longo dos anos. Como o número de países é muito grande, vamos nos concentrar somente nos países do continente americano. Além disso, vamos olhar somente para as observações em 2007.\n\nlibrary(\"gapminder\")\ndata(\"gapminder\")\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n29\n8425333\n779\n\n\nAlbania\nEurope\n1952\n55\n1282697\n1601\n\n\nAlgeria\nAfrica\n1952\n43\n9279525\n2449\n\n\nAngola\nAfrica\n1952\n30\n4232095\n3521\n\n\nArgentina\nAmericas\n1952\n62\n17876956\n5911\n\n\nAustralia\nOceania\n1952\n69\n8691212\n10040\n\n\nAustria\nEurope\n1952\n67\n6927772\n6137\n\n\nBahrain\nAsia\n1952\n51\n120447\n9867\n\n\nBangladesh\nAsia\n1952\n37\n46886859\n684\n\n\nBelgium\nEurope\n1952\n68\n8730405\n8343\n\n\n\n\n\n\n\nO código abaixo seleciona as linhas relevantas.\n\n# Selecionar apenas os países das Américas em 2007\namericas &lt;- subset(gapminder, continent == \"Americas\" & year == 2007)\n\nVamos montar um gráfico de colunas para comparar a expectativa de vida nos países do continente americano.\n\nggplot(data = americas, aes(x = country, y = lifeExp)) +\n  geom_col() +\n  labs(title = \"Expectativa de Vida\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nPodemos melhorar o gráfico acima reordenando as colunas. Como comentei acima, podemos definir a ordem de uma variável categórica usando o argumento levels da função factor().\nImagine que você tem uma série de avaliações que podem ser “Bom”, “Médio” ou “Ruim” armazenadas num vetor chamado feedback. Para estruturar esta variável como factor é preciso definir qual a ordem destes valores. No exemplo abaixo define-se uma relação crescente: de “Ruim” até “Bom”.\n\nfeedback &lt;- c(\"Bom\", \"Bom\", \"Médio\", \"Ruim\", \"Médio\", \"Médio\")\nsatisfacao &lt;- factor(feedback, levels = c(\"Ruim\", \"Médio\", \"Bom\"))\n\nsatisfacao\n\n[1] Bom   Bom   Médio Ruim  Médio Médio\nLevels: Ruim Médio Bom\n\n\nNo nosso caso, queremos que a ordem do nome dos países seja a mesma que a ordem de grandeza da expectativa de vida. Para fazer isto usamos a função order(). O código abaixo pode parecer confuso à primeira vista, mas veremos maneiras de simplificá-lo em posts futuros.\n\nlvls &lt;- americas[[\"country\"]][order(americas[[\"lifeExp\"]])]\namericas[\"country_order\"] &lt;- factor(americas[[\"country\"]], levels = lvls)\n\nO gráfico de colunas agora está ordenado. Note que também modifico o título dos eixos para suprimir um deles definindo x = NULL.\n\nggplot(data = americas, aes(x = country_order, y = lifeExp)) +\n  geom_col() +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = NULL,\n    y = \"Anos\") +\n  coord_flip()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#estatísticas-descritivas",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#estatísticas-descritivas",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Estatísticas descritivas",
    "text": "Estatísticas descritivas\nO ggplot permite visualizar estatísticas descritivas simples diretamente, dispensando a manipulação dos dados. Voltando ao nosso exemplo inicial do preços de imóveis na base hprice1 podemos visualizar o preço mediano dos imóveis por número de dormitórios.\nPara fazer isto inserimos o argumento stat = \"summary_bin\" dentro da função geom_bar(). Além disso, precisamos adicionar a função desejada pelo argumento fun`.\n\n# Preço mediano por número de dormitórios\nggplot(data = hprice1, aes(x = factor(bdrms), y = price)) +\n  geom_bar(stat = \"summary_bin\", fun = median)\n\n\n\n\n\n\n\n\nDa mesma forma, podemos comparar o preço médio dos imóveis por número de dormitórios.\n\n# Preço médio por número de dormitórios\nggplot(data = hprice1, aes(x = factor(bdrms), y = price)) +\n  geom_bar(stat = \"summary_bin\", fun = mean)\n\n\n\n\n\n\n\n\nIsto pode ser utilizado para visulizar rapidamente o valor médio entre grupos distintos. Na base hprice1, a varíavel colonial indica o estilo arquitetônico do imóvel. Em particular se colonial = 1 o imóvel tem um estilo colonial (rústico). Caso contrário colonial = 0.\n\n# Preço médio comparando imóveis coloniais e não-coloniais\nggplot(data = hprice1, aes(x = factor(colonial), y = price)) +\n  geom_bar(stat = \"summary_bin\", fun = mean)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#resumo",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#resumo",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar gráficos de colunas/barras sofisticados usando poucas linhas de código. Em qualquer gráfico temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos importantes:\n\nA ordem das colunas é definida pelos níveis da variável x. Para reordenar as colunas é preciso definir uma nova ordem usando factor(x, levels = c(...)).\nAlém dos elementos estéticos, gráficos de colunas tem o argumento position que define o comportamento do gráfico.\nA função geom_bar() conta a ocorrência dos valores nos dados e pode ser utilizada também para informar outras estatísticas descritivas.\nA função geom_col() exige os argumentos x e y, onde y define a altura da coluna no gráfico.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos.\n\nggplot(data = hprice1, aes(x = factor(bdrms), y = price)) +\n  geom_bar(\n    stat = \"summary_bin\",\n    fun = median,\n    fill = \"#264653\") +\n  labs(\n    title = \"Preço mediano do imóvel por número de dormitórios\",\n    x = \"Número de dormitórios\",\n    y = \"Preço (USD milhares)\",\n    caption = \"Fonte: Wooldridge (Boston Globe)\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#footnotes",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#footnotes",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara mais informações sobre a ONG Gapminder veja o link.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "",
    "text": "O gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente qual a relação entre as duas variáveis escolhidas.\n\n\n\n\n\n\n\n\n\nEm geral, colocamos a variável explicativa (regressor) no eixo horizontal e a variável explicada (resposta) no eixo vertical.\nNeste post vamos entender como montar gráficos de dispersão no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para explorar uma base de preços de imóveis. Vamos entender como customizar o gráfico, variando as cores, os formatos e o tamanho dos círculos; além disso, vamos montar um gráfico de dispersão junto com uma linha de regressão."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#gráfico-de-dispersão",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#gráfico-de-dispersão",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "",
    "text": "O gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente qual a relação entre as duas variáveis escolhidas.\n\n\n\n\n\n\n\n\n\nEm geral, colocamos a variável explicativa (regressor) no eixo horizontal e a variável explicada (resposta) no eixo vertical.\nNeste post vamos entender como montar gráficos de dispersão no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para explorar uma base de preços de imóveis. Vamos entender como customizar o gráfico, variando as cores, os formatos e o tamanho dos círculos; além disso, vamos montar um gráfico de dispersão junto com uma linha de regressão."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#r",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#r",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "R",
    "text": "R\nO primeiro passo é instalar o pacote ggplot2. O R funciona como um repositório de pacotes: cada pacote é como uma família de funções. Em geral, cada pacote tem uma finalidade específica. O ggplot2 contém uma série de funções que permitem a construção de gráficos.\nO R tem um funcionalidade embutida que facilita o download e a instalação de pacotes. Usamos a função install.packages(\"nome_do_pacote\"). Então, para instalar o ggplot2 executamos o código abaixo.\nSe você estiver usando o R fora do RStudio é provável que a função abaixo solicite que você escolha um servidor a partir de uma lista. Escolha o que for mais próximo - geograficamente - de onde você está. No meu caso eu sempre utilizo o “Brazil (SP 1) [https] - University of Sao Paulo, Sao Paulo”. Se você usa o R dentro do RStudio pode ignorar este passo.\n\n# Instalar o pacote ggplot2 (se necessário)\ninstall.packages(\"ggplot2\")\n\nA cada vez que abrimos o R precisamos carregar os pacotes adicionais que instalamos previamente. Isto pode parecer trabalhoso à primeira vista, mas faz muito sentido: evita conflitos entre pacotes e é mais eficiente.\nPara carregar o ggplot2 usamos a função library (biblioteca).\n\n# Carrega o pacote ggplot2\nlibrary(ggplot2)\n\nEnquanto a maioria dos pactoes funciona como repositórios de funções alguns servem como repositórios de bases de dados. É o caso do pacote wooldridge que carrega as bases de dados utilizadas no livro Introductory Econometrics: A Modern Approach do economista Jeffrey Wooldridge.\n\n# Instalar o pacote ggplot2 (se necessário)\ninstall.packages(\"wooldridge\")\n# Carregar o pacote wooldridge\nlibrary(wooldridge)\n\nNos primeiros exemplos abaixo vamos trabalhar com a base de dados hrpice1 que coleta informações de preços de venda de imóveis na região metropolitana de Boston, nos EUA, em 1990. Para carregar a base usamos a função data().\n\ndata(\"hprice1\")\n\nVisualização e análise de dados são habilidades complementares. Aqui, vamos nos focar apenas nas habilidades visuais.\nA função head(), quando aplicada a um objeto data.frame mostra as primeiras linhas da tabela. Há muitas colunas mas vamos focar inicialmente na price que é o preço em milhares de dólares e na coluna sqrft que é o tamanho do imóvel em pés quadrados.\n\nhead(hprice1)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#ggplot2",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#ggplot2",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "ggplot2",
    "text": "ggplot2\nA estrutura de um gráfico do ggplot2 parte de três elementos básicos: (1) uma base de dados, isto é, um objeto data.frame; (2) um mapeamento de variáveis, feito com auxílio da função aes(); (3) a escolha da forma do gráfico, feito com as funções geom.\nO ggplot2 funciona adicionando camadas sobre um gráfico inicial.\nComeçamos com a função ggplot() e vamos adicionando geoms, funções auxiliares que especificam a forma do gráfico. Este processo construtivo de adicionar elementos a um gráfico é o principal diferencial do ggplot.\nOu seja, temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEsta estrutura básica é esquematizada no pseudo-código abaixo.\n\nggplot(data = base_de_dados, aes(x = variavel_x, y = variavel_y)) +\n  geom_point()\n\nNo nosso caso a base de dados é a hprice1 e as variáveis são sqrft e price.\nTemos que informar isto usando data = hprice e aes(x = sqrft, y = price).\nPor fim, como queremos um gráfico de dispersão escolhemos o geom_point(). Esta última chamada é adicionada à função inicial com o sinal de soma +.\nO código abaixo junta todos estes elementos e resulta num gráfico de dispersão entre o tamanho do imóvel (sqrft) e seu preço de venda (price).\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point()\n\n\n\n\n\n\n\n\nO gráfico acima ainda está bastante cru e podemos melhorá-lo de diversas formas. Ainda assim, ele já é interessante: revela uma relação crescente entre o tamanho do imóvel e do seu preço de venda."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#elementos-estéticos",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#elementos-estéticos",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Elementos estéticos",
    "text": "Elementos estéticos\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nSão quatro os principais elementos estéticos que podemos manipular no caso do geom_point:\n\ncolor - a cor do objeto\nalpha - a transparência da cor\nsize - o tamanho do objeto\nshape - o formato do objeto\n\nQuando executamos o código acima o valor destes parâmetros foi definido automaticamente. Podemos modificá-los chamando eles explicitamente.\n\nColor - cores\nA maneira mais simples de alterar as cores é chamando ela por nome. No exemplo usamos a cor steelblue. Uma lista completa de cores está disponível aqui.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém é possível escolher a cor via hexadecimal.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"#e76f51\")\n\n\n\n\n\n\n\n\n\n\nAlpha - transparência\nO parâmetro alpha controla o nível de transparência da cor. Este artifício costuma ser útil para evitar que muitos pontos fiquem sobrepostos (overplotting).\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\", alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize - tamanho\nPodemos manipular o tamanho dos pontos usando size e ajustando o valor numérico.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\", size = 5)\n\n\n\n\n\n\n\n\nAgora que os círculos estão maiores há mais casos de sobreposição. Uma solução para evitar isto é aplicar algum valor de alpha.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\", size = 5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nShape - formato\nPor padrão o formato do geom_point é um círculo mas há muitas outras opções. Para trocar o formato do objeto usamos shape = 2\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(shape = 2)\n\n\n\n\n\n\n\n\nO gráfico abaixo ilustra os principais tipos de formatos disponíveis.\n\n\n\n\n\n\n\n\n\nNote que em alguns casos como o 21 é possível controlar tanto a cor do contorno do círculo como também da cor de dentro.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(shape = 21, color = \"steelblue\", fill = \"orange\")\n\n\n\n\n\n\n\n\n\n\nCombinando todos os elementos\nO gráfico abaixo serve apenas para ilustrar o uso de todos os parâmetros. Naturalmente, o uso destes elementos estéticos deve favorecer o melhor entendimento do gráfico e não deve ser utilizado de forma gratuita.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(\n    shape = 21,\n    color = \"steelblue\",\n    fill = \"orange\",\n    size = 7,\n    alpha = 0.75)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes. Como expliquei acima, esta função “transforma” nossos dados em elementos visuais. Nos casos acima, ela mapeia as variáveis x e y nas suas respectivas coordenadas.\nMas podemos mapear as variáveis nos elementos estéticos: color, alpha, size, shape.\nVamos primeiro voltar à nossa base de dados. Olhando para as primeiras linhas vemos que há uma coluna chamada colonial que é uma variável binária que indica se o estilo arquitetônico do imóvel é colonial.\n\nhead(hprice1)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810\n\n\nPodemos plotar o mesmo gráfico de dispersão mas fazer com que a cor do círculo represente a variável colonial. No gráfico abaixo, os pontos em azul são imóveis com estilo colonial, enquanto que os pontos em vermelho (salmão) são os imóveis de outros estilos.\nVemos que parece haver uma tendência de que os imóveis coloniais vendem por valores mais elevados, pois os pontos azuis aparecem acima dos pontos vermelhos, mas há exceções. Isso sugere que imóveis de estilo colonial têm preço mais elevado do que imóveis de tamanho similar, mas construídos em estilos diferentes.\nEste exemplo mostra como a visualização ajuda a formar algumas hipóteses iniciais que depois podem ser verificadas usando modelos estatísticos.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(aes(color = factor(colonial)))\n\n\n\n\n\n\n\n\nDa mesma forma que modificamos a cor podemos modificar o tamanho segundo, por exemplo, o número de dormitórios.\nNo exemplo abaixo o tamanho do círculo é proporcional ao número de dormitórios.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(aes(size = bdrms), alpha = 0.5)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point() +\n  labs(\n    title = \"Quanto maior, mais caro\",\n    subtitle = \"Relação entre o preço do imóvel e sua área útil.\",\n    x = \"Área útil (pés quadrados)\",\n    y = \"Preço (USD milhares)\",\n    caption = \"Fonte: Wooldridge (Boston Globe)\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#incluindo-uma-linha-de-regressão",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#incluindo-uma-linha-de-regressão",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Incluindo uma linha de regressão",
    "text": "Incluindo uma linha de regressão\nComo falamos no início do post, parte da mágica do ggplot é ir “somando” objetos. Podemos desejar incluir, por exemplo, uma linha de regressão em cima do gráfico de dispersão. No caso de uma regressão linear simples, esta linha mostra a correlação linear entre a variável no eixo horizontal com a variável no eixo vertical.\nA função geom_smooth facilita a inclusão de linhas de regressão. Se não for fornecido argumento à função ela tentará uma aproximação LOESS. No exemplo abaixo eu escolho method = \"lm\" para que a função aproxime a relação linear (lm de linear model).\nNo nosso caso, a linha mostra a relação linear entre o preço do imóvel e a sua área útil. O argumento se = FALSE serve para omitir a estimativa do erro-padrão do coeficiente e deixar a visualização mais limpa.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nPode-se especificar uma forma particular para a regressão. No caso abaixo faço uma regressão polinomial de segunda ordem (quadrática) usando a função poly.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point() +\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nA curva quadrática parece ter um ajuste visual melhor aos dados."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#resumo",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#resumo",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar gráficos de dispersão sofisticados usando poucas linhas de código. Em qualquer gráfico temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"#e63946\", alpha = 0.75, size = 2) +\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Quanto maior, mais caro\",\n    subtitle = \"Relação quadrática entre o preço do imóvel e sua área útil.\",\n    x = \"Área útil (pés quadrados)\",\n    y = \"Preço (USD milhares)\",\n    caption = \"Fonte: Wooldridge (Boston Globe)\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/0-introduction.html",
    "href": "posts/ggplot2-tutorial/0-introduction.html",
    "title": "Introdução",
    "section": "",
    "text": "Nesta série, exploraremos uma das ferramentas mais poderosas e versáteis para visualização de dados - o pacote {ggplot2}.\nA visualização de dados desempenha um papel fundamental na compreensão e comunicação de dados. O pacote ggplot2, desenvolvido por Hadley Wickham, é amplamente reconhecido como uma das melhores opções para a criação de gráficos esteticamente elegantes e informativos: atualmente, o ggplot2 é utilizado nas melhores publicações acadêmicas e pelos melhores jornalistas de dados1.\nAbaixo seguem alguns aspectos fundamentais do ggplot2.\n\nGrammar of graphics: O ggplot2 cria gráficos seguindo uma abordagem consistente e “construtiva” baseada numa “gramática”. Essencialmente, montamos os gráficos somando elementos estéticos individualmente; isto significa que você pode criar visualizações complexas a partir de um pequeno conjunto de componentes gráficos. Este é um conceito fundamental e sua poderosa versatilidade ficará mais evidente com a prática.\nGráficos elegantes: O ggplot2 é permite criar gráficos de alta qualidade e visualmente atraentes, que vão imbuir seu trabalho de profissionalismo.\nPersonalização: O ggplot2 oferece uma ampla gama de opções de personalização, permitindo que você ajuste cada aspecto do seu gráfico. Pode-se modificar cores, fontes, tamanhos, e muito mais para atender às suas necessidades específicas.\nRecursos adicionais: a popularidade quase universal do ggplot2 significa que há diversos recursos riquíssimos para explorar. A lista oficial inclui mais de 100 pacotes que oferecem diversas funções que potencializam o ggplot2.\n\nAo longo desta série de tutoriais, abordaremos os conceitos fundamentais do ggplot2 e guiaremos você passo a passo na criação de visualizações incríveis. Desde a construção de gráficos básicos até técnicas mais avançadas, você aprenderá a aproveitar todo o potencial do ggplot2 para apresentar e analisar seus dados de forma impactante.\nSe você está animado para começar, vá aqui para acessar o primeiro tutorial da série.\nPara conhecer um pouco mais das potencialiades do ggplot2 veja, por exemplo:\n\nAs contribuições semanais de Cédric Scherer ao projeto {tidytuesday}. Estas visualizações foram feitas a partir de bases de dados variadas usando majoritariamente ggplot2 e o tidyverse\n\n\n\nA galeria de Thomas Lin Penderson, que usa ggplot2 e extensões para fazer arte generativa.\nA galeria com mais de 120 extensões (pacotes adicionais criados para apoiar o ggplot2) da Posit.\n\n\n\nO conteúdo destes posts segue uma filosofia básica: aprender a programar envolve prática e repetição. A melhor maneira de consumir este material é reescrevendo as linhas de código e executando elas.\nO material começa com quatro gráfico fundamentais. A apresentação do texto nestes posts é introdutória e supõe conhecimento nenhum de R. Os textos começam com a instalação do pacote ggplot2 e discutem brevemente o que é um pacote e como escrever linhas de código no R.\n\nGráfico de dispersão (ou scatterplot)\nGráfico de coluna/barra\nGráfico de histograma\nGráfico de linha\n\nAntes de introduzir outros tipos de gráfico, foco em alguns aspectos essenciais de gráficos. É importante que um gráfico seja o mais autoexplicativo possível. A maneira mais simples e efetiva de fazer isto é com títulos, legendas e caixas de texto; mas há também outras formas mais sutis de alcançar este objetivo: destacando certas áreas do gráfico ou usando cores chamativas.\n\nPlotando texto e destacando observações.\nEscalas, legendas e temas (scales, labels and themes).\nGuia básico de cores e temas.\n\nOs quatro gráficos acima costumam ser a base de qualquer análise e vão resolver o seu problema em 90% dos casos. Depois, introduzo alguns gráficos diferentes:\n\nFacet plots\nGráficos lollipop\nGráficos de área\nGráficos de tile (ou mapas de calor)\n\nPor fim, vou discutir algumas extensões populares do ggplot2 e mostrar algumas estratégias para produzir gráficos de qualidade em grande escala.\n\nExtensões: indo muito além do básico\nProdução: gráficos em grande escala\n\nPara produzir boas visualizações é preciso também conhecimento sobre manipulação e limpeza de dados. Nos primeiros posts, eu propositalmente evito ao máximo qualquer manipulação para focar somente nas funções de visualização. Contudo, à medida que o material vai avançando vou utilizando cada vez mais funções do pacote {tidyverse} que acredito ser o melhor para manipulação de dados. Para introduzir e revisar algumas informações importantes vou incluir também um post sobre isto.\n\nApêndice: manipular para enxergar\n\n\n\nCopiar e colar é uma opção tentadora, mas que pode atrapalhar seu aprendizado: se você sempre copia e cola você evita de cometer erros; e cometer erros é parte importante de aprender algo novo.\nConsidere o código abaixo. Se eu executo ele numa nova sessão de R eu encontro um erro.\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in ggplot(mtcars, aes(x = wt, y = MPG)): could not find function \"ggplot\"\n\n\nO retorno do comando acima indica que não foi possível encontrar a função ggplot. Isto acontece porque eu esqueci de carregar o pacote ggplot2. Para carregar o pacote eu rodo library(ggplot2). O comando agora retorna um novo erro.\n\nlibrary(ggplot2)\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'MPG' not found\n\n\nAgora temos um novo erro: o objeto MPG não foi encontrado. Talvez seja um erro de digitação. Vamos conferir o nome das colunas da base de dados mtcars usando a função names().\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nDe fato, após a verificação, vemos que a variável mpg é minúscula. Agora o comando funciona.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\nO processo iterativo de tentativa e erro é parte natural e rotineira da tarefa de programação. É importante entender que o seu código provavelmente não vai funcionar “de primeira” (e às vezes nem de segunda, nem de terceira…). É normal errar e devemos aproveitar o fato que as mensagens de erro do R costumam ser bastante instrutivas!"
  },
  {
    "objectID": "posts/ggplot2-tutorial/0-introduction.html#roteiro",
    "href": "posts/ggplot2-tutorial/0-introduction.html#roteiro",
    "title": "Introdução",
    "section": "",
    "text": "O conteúdo destes posts segue uma filosofia básica: aprender a programar envolve prática e repetição. A melhor maneira de consumir este material é reescrevendo as linhas de código e executando elas.\nO material começa com quatro gráfico fundamentais. A apresentação do texto nestes posts é introdutória e supõe conhecimento nenhum de R. Os textos começam com a instalação do pacote ggplot2 e discutem brevemente o que é um pacote e como escrever linhas de código no R.\n\nGráfico de dispersão (ou scatterplot)\nGráfico de coluna/barra\nGráfico de histograma\nGráfico de linha\n\nAntes de introduzir outros tipos de gráfico, foco em alguns aspectos essenciais de gráficos. É importante que um gráfico seja o mais autoexplicativo possível. A maneira mais simples e efetiva de fazer isto é com títulos, legendas e caixas de texto; mas há também outras formas mais sutis de alcançar este objetivo: destacando certas áreas do gráfico ou usando cores chamativas.\n\nPlotando texto e destacando observações.\nEscalas, legendas e temas (scales, labels and themes).\nGuia básico de cores e temas.\n\nOs quatro gráficos acima costumam ser a base de qualquer análise e vão resolver o seu problema em 90% dos casos. Depois, introduzo alguns gráficos diferentes:\n\nFacet plots\nGráficos lollipop\nGráficos de área\nGráficos de tile (ou mapas de calor)\n\nPor fim, vou discutir algumas extensões populares do ggplot2 e mostrar algumas estratégias para produzir gráficos de qualidade em grande escala.\n\nExtensões: indo muito além do básico\nProdução: gráficos em grande escala\n\nPara produzir boas visualizações é preciso também conhecimento sobre manipulação e limpeza de dados. Nos primeiros posts, eu propositalmente evito ao máximo qualquer manipulação para focar somente nas funções de visualização. Contudo, à medida que o material vai avançando vou utilizando cada vez mais funções do pacote {tidyverse} que acredito ser o melhor para manipulação de dados. Para introduzir e revisar algumas informações importantes vou incluir também um post sobre isto.\n\nApêndice: manipular para enxergar\n\n\n\nCopiar e colar é uma opção tentadora, mas que pode atrapalhar seu aprendizado: se você sempre copia e cola você evita de cometer erros; e cometer erros é parte importante de aprender algo novo.\nConsidere o código abaixo. Se eu executo ele numa nova sessão de R eu encontro um erro.\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in ggplot(mtcars, aes(x = wt, y = MPG)): could not find function \"ggplot\"\n\n\nO retorno do comando acima indica que não foi possível encontrar a função ggplot. Isto acontece porque eu esqueci de carregar o pacote ggplot2. Para carregar o pacote eu rodo library(ggplot2). O comando agora retorna um novo erro.\n\nlibrary(ggplot2)\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'MPG' not found\n\n\nAgora temos um novo erro: o objeto MPG não foi encontrado. Talvez seja um erro de digitação. Vamos conferir o nome das colunas da base de dados mtcars usando a função names().\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nDe fato, após a verificação, vemos que a variável mpg é minúscula. Agora o comando funciona.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\nO processo iterativo de tentativa e erro é parte natural e rotineira da tarefa de programação. É importante entender que o seu código provavelmente não vai funcionar “de primeira” (e às vezes nem de segunda, nem de terceira…). É normal errar e devemos aproveitar o fato que as mensagens de erro do R costumam ser bastante instrutivas!"
  },
  {
    "objectID": "posts/general-posts/repost-otimizacao-newton/index.html",
    "href": "posts/general-posts/repost-otimizacao-newton/index.html",
    "title": "Repost: Otimização numérica - métodos de Newton",
    "section": "",
    "text": "Métodos de otimização numérica são frequentemente necessários para estimar modelos econométricos e resolver problemas de máximo/mínimo em geral. Em particular, para encontrar estimadores de extremo — como estimadores do método generalizado de momentos (GMM) ou estimadores de máxima verossimilhança (MLE) — é necessário resolver um problema de otimização. Na maior parte dos casos, estes métodos rodam no background de funções prontas, mas não custa entender um pouco mais sobre como eles funcionam, até porque um dos métodos mais populares, o método de Newton (e suas variantes) é bastante simples e intuitivo.\n\n\nComo primeiro exemplo do método de Newton vou tomar emprestada uma sugestão deste tweet.\n\n\n\n\n\nO algoritmo descrito no tweet para encontrar \\(\\sqrt{y}\\) é basicamente o seguinte:\n\nEncontre um número \\(x\\) tal que \\(x^2 \\approx y\\) (ex: \\(y = 17\\), \\(4^2 = 16\\));\nCompute a diferença \\(d = y - x^2\\) (\\(d = 17 - 16 = 1\\));\nAtualize \\(x\\) pela diferença \\(d/2x\\) (\\(x = 4 + 1/8 = 4.125\\))\nRepita 2 e 3, com o novo valor de x.\n\nO código abaixo implementa os passos acima. Começo escolhendo um inteiro aleatório \\(y\\) entre \\(0\\) e \\(10^7\\). O desafio será encontrar \\(\\sqrt{y}\\). Usando o comando set.seed(1984) você poderá replicar os resultados abaixo. O número sorteado foi \\(6588047\\) e \\(\\sqrt{6588047} = 2566.719\\).\nPara demonstrar a poder do simples método acima vou começar com um chute inicial bem ruim, isto é, vou escolher um \\(x\\) inicial igual a \\(1\\). Como quero mostrar os passos intermediários do algoritmo vou criar um vetor numérico que salva todos os valores de x a cada iteração. Há diversas maneiras de escrever o loop abaixo e ao longo do post vou mostrar três tipos de loop (for-loop, repeat e while).\n\n# Para replicar os resultados\nset.seed(1984)\n# Sorteia um número inteiro\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\n# Cria um vetor x de comprimento 15\nx &lt;- vector(length = 20)\n# Primeiro valor de x = 1\nx[1] &lt;- 1\n# Loop\nfor (i in 1:19) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nPodemos ver graficamente como o algortimo se aproxima do valor verdadeiro de \\(\\sqrt{y}\\) rapidamente, mesmo partindo de um ponto inicial distante. Em vermelho, está o valor correto de \\(\\sqrt{y}\\).\n\nplot(2:20, x[2:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nComo os primeiros valores do gráfico acima são muito altos, faço um segundo gráfico usando somente os últimos valores que foram computados.\n\nplot(13:20, x[13:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nNote que depois de 16 repetições, a diferença entre \\(x\\) e \\(y\\) é tão próxima de zero que o R arredonda ela para 0.\n\nx - sqrt(y)\n\n [1] -2.565719e+03  3.291457e+06  1.644446e+06  8.209418e+05  4.091915e+05\n [6]  2.033204e+05  1.003928e+05  4.894506e+04  2.325311e+04  1.047078e+04\n[11]  4.204686e+03  1.305444e+03  2.200559e+02  8.688284e+00  1.465521e-02\n[16]  4.183812e-08  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n\n\nMas como funciona este algoritmo? Basicamente, estamos procurando a raiz de um polinômio. No caso, estamos buscando resolver \\(f(x) = x^2 - y = 0\\) (em que \\(y\\) é dado). A solução proposta pelo método acima é de aproximar este polinômio linearmente (usando a primeira derivada) e então encontrar a raiz desta aproximação. Para facilitar o exemplo vamos tomar \\(y = 2\\). Então temos o problema de encontrar a raiz do seguinte polinômio:\n\\[\nP(x) = x^2 - 2\n\\]\nVamos tomar o valor \\(x_{0} = 5\\) como chute inicial. A aproximação linear de \\(P(x)\\) é a derivada com respeito a \\(x\\), isto é, \\(2x\\). Avaliada no ponto \\(x_{0}\\), temos \\(P'(5) = 10\\). Assim, temos uma reta de inclinação \\(10\\) que passa no ponto \\((5, 23)\\). Com isso podemos traçar a reta vermelha no gráfico abaixo e calcular o ponto em que esta reta corta o eixo-x. A equação da reta pode ser escrita como:\n\\[\n\\frac{y_{2} - y_{1}}{x_{2} - x_{1}} = f'(x_{1})\n\\]\nComo queremos encontrar o ponto em que a curva corta o eixo-x estamos, na verdade, buscando o ponto \\((x_{2}, 0)\\). Logo, podemos substituir \\(y_{2} = 0\\) na expressão acima e resolver para \\(x_{2}\\):\n\\[\nx_{2} = x_{1} - \\frac{y_{1}}{f'(x_{1})} = x_{1} - \\frac{f(x_{1})}{f'(x_{1})}\n\\]\nA expressão acima é essência do método de Newton. Substituindo nossos valores temos que:\n\\[\n\\frac{23}{5 - x} = 10 \\longrightarrow x = 2.7\n\\]\nNo gráfico abaixo podemos ver como funciona este processo. Parte-se de um ponto inicial \\(x = 5\\). Encontra-se a reta tangente neste ponto (linha em vermelho) e computa-se o valor de \\(x\\) em que esta reta cruza o eixo-x (ponto vermelho). O processo repete-se agora tomando o ponto em vermelho como ponto de partida. De maneira geral, o processo de atualização segue a regra abaixo:\n\\[\n  x_{n + 1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}\n\\]\n\n\n\n\n\n\ny &lt;- 2\nx &lt;- vector(length = 10)\n# Primeiro valor de x = 1\nx[1] &lt;- 5\n# Loop\nfor (i in 1:9) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nO gráfico abaixo mostra os próximos passos do algoritmo. Após poucas repetições já estamos muito próximos da resposta correta. Note que isto acontece porque estamos trabalhando com uma função bastante simples.\n\n\n\n\n\nPode-se generalizar o problema acima facilmente para encontrar a n-ésima raiz de qualquer valor. No código abaixo isto pode ser feito variando o valor de n. O código abaixo também é uma variação em relação aos anteiores. Ele é um repeat-break, ao invés do for-loop que estávamos usando acima. Isto é, ele repete uma operação enquanto alguma condição for válida. Assim, podemos exigir que o algoritmo pare quando algum critério de convergência for atingido. No caso abaixo, especifico que o loop pare quando a distância entre os valores sucessivos de \\(x\\) for muito pequena (menor que \\(10^{-8}\\)) ou quando o número de iterações chegar a 100 repetições.\n\n# Dá pra melhorar um pouco o loop (usando repeat) e agora generalizar para n\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\nx &lt;- vector(length = 100)\nx[1] &lt;- (10^7 - 1) / 2\ni &lt;- 1\nn &lt;- 4\nrepeat {\n  x_chute &lt;- x_chute_novo\n  dist &lt;- y - x_chute^n\n  x_chute_novo &lt;- x_chute + dist / (n * x_chute^(n - 1))\n  x[i + 1] &lt;- x_chute_novo\n  # print(paste(\"iteration =\", i))\n  if (abs(x_chute_novo - x[i]) &lt; 10^(-8) | i &gt; 99) {\n    break\n  }\n  i &lt;- i + 1\n}\nplot(x[1:i + 1] - y^(1 / n), main = \"Distância entre valor computado e valor verdadeiro\", xlab = \"Contagem da iteração\", ylab = \"Distância\")"
  },
  {
    "objectID": "posts/general-posts/repost-otimizacao-newton/index.html#exemplo-simples",
    "href": "posts/general-posts/repost-otimizacao-newton/index.html#exemplo-simples",
    "title": "Repost: Otimização numérica - métodos de Newton",
    "section": "",
    "text": "Como primeiro exemplo do método de Newton vou tomar emprestada uma sugestão deste tweet.\n\n\n\n\n\nO algoritmo descrito no tweet para encontrar \\(\\sqrt{y}\\) é basicamente o seguinte:\n\nEncontre um número \\(x\\) tal que \\(x^2 \\approx y\\) (ex: \\(y = 17\\), \\(4^2 = 16\\));\nCompute a diferença \\(d = y - x^2\\) (\\(d = 17 - 16 = 1\\));\nAtualize \\(x\\) pela diferença \\(d/2x\\) (\\(x = 4 + 1/8 = 4.125\\))\nRepita 2 e 3, com o novo valor de x.\n\nO código abaixo implementa os passos acima. Começo escolhendo um inteiro aleatório \\(y\\) entre \\(0\\) e \\(10^7\\). O desafio será encontrar \\(\\sqrt{y}\\). Usando o comando set.seed(1984) você poderá replicar os resultados abaixo. O número sorteado foi \\(6588047\\) e \\(\\sqrt{6588047} = 2566.719\\).\nPara demonstrar a poder do simples método acima vou começar com um chute inicial bem ruim, isto é, vou escolher um \\(x\\) inicial igual a \\(1\\). Como quero mostrar os passos intermediários do algoritmo vou criar um vetor numérico que salva todos os valores de x a cada iteração. Há diversas maneiras de escrever o loop abaixo e ao longo do post vou mostrar três tipos de loop (for-loop, repeat e while).\n\n# Para replicar os resultados\nset.seed(1984)\n# Sorteia um número inteiro\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\n# Cria um vetor x de comprimento 15\nx &lt;- vector(length = 20)\n# Primeiro valor de x = 1\nx[1] &lt;- 1\n# Loop\nfor (i in 1:19) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nPodemos ver graficamente como o algortimo se aproxima do valor verdadeiro de \\(\\sqrt{y}\\) rapidamente, mesmo partindo de um ponto inicial distante. Em vermelho, está o valor correto de \\(\\sqrt{y}\\).\n\nplot(2:20, x[2:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nComo os primeiros valores do gráfico acima são muito altos, faço um segundo gráfico usando somente os últimos valores que foram computados.\n\nplot(13:20, x[13:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nNote que depois de 16 repetições, a diferença entre \\(x\\) e \\(y\\) é tão próxima de zero que o R arredonda ela para 0.\n\nx - sqrt(y)\n\n [1] -2.565719e+03  3.291457e+06  1.644446e+06  8.209418e+05  4.091915e+05\n [6]  2.033204e+05  1.003928e+05  4.894506e+04  2.325311e+04  1.047078e+04\n[11]  4.204686e+03  1.305444e+03  2.200559e+02  8.688284e+00  1.465521e-02\n[16]  4.183812e-08  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n\n\nMas como funciona este algoritmo? Basicamente, estamos procurando a raiz de um polinômio. No caso, estamos buscando resolver \\(f(x) = x^2 - y = 0\\) (em que \\(y\\) é dado). A solução proposta pelo método acima é de aproximar este polinômio linearmente (usando a primeira derivada) e então encontrar a raiz desta aproximação. Para facilitar o exemplo vamos tomar \\(y = 2\\). Então temos o problema de encontrar a raiz do seguinte polinômio:\n\\[\nP(x) = x^2 - 2\n\\]\nVamos tomar o valor \\(x_{0} = 5\\) como chute inicial. A aproximação linear de \\(P(x)\\) é a derivada com respeito a \\(x\\), isto é, \\(2x\\). Avaliada no ponto \\(x_{0}\\), temos \\(P'(5) = 10\\). Assim, temos uma reta de inclinação \\(10\\) que passa no ponto \\((5, 23)\\). Com isso podemos traçar a reta vermelha no gráfico abaixo e calcular o ponto em que esta reta corta o eixo-x. A equação da reta pode ser escrita como:\n\\[\n\\frac{y_{2} - y_{1}}{x_{2} - x_{1}} = f'(x_{1})\n\\]\nComo queremos encontrar o ponto em que a curva corta o eixo-x estamos, na verdade, buscando o ponto \\((x_{2}, 0)\\). Logo, podemos substituir \\(y_{2} = 0\\) na expressão acima e resolver para \\(x_{2}\\):\n\\[\nx_{2} = x_{1} - \\frac{y_{1}}{f'(x_{1})} = x_{1} - \\frac{f(x_{1})}{f'(x_{1})}\n\\]\nA expressão acima é essência do método de Newton. Substituindo nossos valores temos que:\n\\[\n\\frac{23}{5 - x} = 10 \\longrightarrow x = 2.7\n\\]\nNo gráfico abaixo podemos ver como funciona este processo. Parte-se de um ponto inicial \\(x = 5\\). Encontra-se a reta tangente neste ponto (linha em vermelho) e computa-se o valor de \\(x\\) em que esta reta cruza o eixo-x (ponto vermelho). O processo repete-se agora tomando o ponto em vermelho como ponto de partida. De maneira geral, o processo de atualização segue a regra abaixo:\n\\[\n  x_{n + 1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}\n\\]\n\n\n\n\n\n\ny &lt;- 2\nx &lt;- vector(length = 10)\n# Primeiro valor de x = 1\nx[1] &lt;- 5\n# Loop\nfor (i in 1:9) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nO gráfico abaixo mostra os próximos passos do algoritmo. Após poucas repetições já estamos muito próximos da resposta correta. Note que isto acontece porque estamos trabalhando com uma função bastante simples.\n\n\n\n\n\nPode-se generalizar o problema acima facilmente para encontrar a n-ésima raiz de qualquer valor. No código abaixo isto pode ser feito variando o valor de n. O código abaixo também é uma variação em relação aos anteiores. Ele é um repeat-break, ao invés do for-loop que estávamos usando acima. Isto é, ele repete uma operação enquanto alguma condição for válida. Assim, podemos exigir que o algoritmo pare quando algum critério de convergência for atingido. No caso abaixo, especifico que o loop pare quando a distância entre os valores sucessivos de \\(x\\) for muito pequena (menor que \\(10^{-8}\\)) ou quando o número de iterações chegar a 100 repetições.\n\n# Dá pra melhorar um pouco o loop (usando repeat) e agora generalizar para n\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\nx &lt;- vector(length = 100)\nx[1] &lt;- (10^7 - 1) / 2\ni &lt;- 1\nn &lt;- 4\nrepeat {\n  x_chute &lt;- x_chute_novo\n  dist &lt;- y - x_chute^n\n  x_chute_novo &lt;- x_chute + dist / (n * x_chute^(n - 1))\n  x[i + 1] &lt;- x_chute_novo\n  # print(paste(\"iteration =\", i))\n  if (abs(x_chute_novo - x[i]) &lt; 10^(-8) | i &gt; 99) {\n    break\n  }\n  i &lt;- i + 1\n}\nplot(x[1:i + 1] - y^(1 / n), main = \"Distância entre valor computado e valor verdadeiro\", xlab = \"Contagem da iteração\", ylab = \"Distância\")"
  },
  {
    "objectID": "posts/general-posts/repost-otimizacao-newton/index.html#na-prática-a-teoria-é-outra",
    "href": "posts/general-posts/repost-otimizacao-newton/index.html#na-prática-a-teoria-é-outra",
    "title": "Repost: Otimização numérica - métodos de Newton",
    "section": "Na prática, a teoria é outra",
    "text": "Na prática, a teoria é outra\nHá vários casos em que o otimizador de Newton falha em encontrar o valor desejado. Isto pode acontecer tanto porque a função viola alguma das hipóteses do método como também porque a função é complicada. A função abaixo, por exemplo, falha em convergir pois a sua derivada é descontínua no zero (há uma “quina” no zero). O resultado é que o método diverge para \\(\\infty\\).\n\\[\nf(x) = |x|^{1/3}\n\\]\n\n\n\n\n\nA tabela abaixo mostra os valores computados para as primeiras 10 iterações.\n\n\n\n\nIteração\nx\nf(x)\n\n\n\n\n1\n5\n1.709976\n\n\n2\n-10\n2.154435\n\n\n3\n-40\n3.419952\n\n\n4\n-160\n5.428835\n\n\n5\n-640\n8.617739\n\n\n6\n-2560\n13.679808\n\n\n7\n-10240\n21.715341\n\n\n8\n-40960\n34.470955\n\n\n9\n-163840\n54.719230\n\n\n10\n-655360\n86.861364\n\n\n\n\n\n\nOutro exemplo curioso em que o algoritmo falha em convergir é quando temos:\n\\[\nf(x) = \\left\\{\\begin{matrix}\n0 & \\text{se } x =0\\\\\nx + x^2 sin(\\frac{2}{x})) & \\text{se } x \\neq 0\n\\end{matrix}\\right.\n\\]\nAinda que a função seja contínua, temos uma descontinuidade na sua derivada e, como resultado, o otmizador fica osciliando entre valores sem nunca convergir para o zero.\n\n\n\n\n\nA tabela abaixo mostra os primeiros dez resultados:\n\n\n\n\n\nIteração\nx\nf(x)\n\n\n\n\n1\n5.0000000\n14.7354586\n\n\n2\n0.1719653\n0.1481520\n\n\n3\n0.4920907\n0.2990380\n\n\n4\n0.2819031\n0.3395412\n\n\n5\n-10.3200755\n-30.8312710\n\n\n6\n-0.0854544\n-0.0782425\n\n\n7\n-0.0171313\n-0.0169891\n\n\n8\n-0.0109142\n-0.0110166\n\n\n9\n-3.4206997\n-9.8789235\n\n\n10\n-0.2423414\n-0.2964612\n\n\n\n\n\n\n\nPode-se visualizar o comportamento do otimizador na animação abaixo. Mostro os primeiros 50 resultados. Note como o algoritmo fica pulando para todos os lados.\n\ntbl &lt;- data.frame(\n  x = x,\n  y = f(x),\n  label = 1:length(x)\n)\n\nggplot() +\n  geom_function(fun = ~ ifelse(.x == 0, 0, .x + .x^2 * sin(2 / .x))) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_point(data = tbl, aes(x = x, y = y), size = 2, color = \"firebrick\") +\n  geom_label(data = tbl, aes(x = x, y = y + 0.15, label = label)) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  transition_states(label) +\n  theme_light()\n\n\n\n\nEm geral, o método de Newton sofre problemas com:\n\nPontos inciais ruins. Às vezes uma escolha de ponto inicial ruim pode levar o otimizador a convergir para pontos diferentes. Em alguns casos, um ponto inicial ruim pode também levar o otimizador a ficar preso num ciclo.\n\nexemplo: \\(f(x) = x^3 - 2x + 2\\) com \\(x_{0} = 0\\). O otimizador fica preso entre \\(0\\) e \\(1\\).\n\n\n\n\n\nIteração\nx\nf(x)\n\n\n\n\n1\n0\n2\n\n\n2\n1\n1\n\n\n3\n0\n2\n\n\n4\n1\n1\n\n\n5\n0\n2\n\n\n6\n1\n1\n\n\n7\n0\n2\n\n\n8\n1\n1\n\n\n9\n0\n2\n\n\n10\n1\n1\n\n\n\n\n\n\n\n\nDescontinuidades. Descontinuidades tanto na raiz da função, como em outras partes da função (como também na sua derivada) podem levar a problemas sérios de convergência.\n\nexemplo: \\(f(x) = |x|^{1/3}\\) como apresentado acima.\n\nFunções difíceis. Algumas funções são simplesmente muito complicadas para o método de Newton. Em alguns casos, a convergência pode ser muito lenta e em outros o otimizador pode falhar completamente.\n\nexemplo: \\(f(x) = 7x - \\text{ln}(x)\\). A função tem mínimo global em \\(x = 1/7\\). Ainda assim, o método de Newton tem dificuldade em encontrar este valor (a não ser que o valor incial escolhido esteja muito próximo de \\(1/7\\)).\n\nf &lt;- function(x) {\n  7 * x - log(x)\n}\ngrad_f &lt;- function(x) {\n  7 - 1 / x\n}\nhess_f &lt;- function(x) {\n  1 / x^2\n}\ntheta &lt;- 5\nerror &lt;- 5\ni &lt;- 1\nx &lt;- vector(length = 50)\nx[1] &lt;- theta\nerror &lt;- 1\nwhile (error &gt; 10^(-8) & i &lt; 50) {\n  theta_0 &lt;- theta\n  G &lt;- grad_f(theta)\n  theta &lt;- theta_0 - grad_f(theta) / hess_f(theta)\n  x[i + 1] &lt;- theta\n  error &lt;- abs((theta - theta_0) / theta_0)\n  i &lt;- i + 1\n}"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "",
    "text": "Como desafio pessoal às vezes tento replicar gráficos que acho interessante. O portal Nexo, em particular, costuma ter lindas visualizações de dados. Vou tentar replicar os gráficos desta publicação. Como o foco desta postagem está na visualização e em mostrar exemplos de aplicações do ggplot2 vou omitir as (longas) manipulações de dados, deixando indicadas as fontes (com links) que usei. Numa postagem futura pretendo fazer um tutorial mais detalhado de como reproduzir estes gráficos.\n\n\n\n\nCode\nlibrary(\"readxl\")\nlibrary(\"here\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"dplyr\")\nlibrary(\"reshape2\")\nlibrary(\"kableExtra\")\n\n\n\n\n\nDados são do Maddison Project Database, disponíveis aqui\n\n\n\n\nPaís\nAno\nRegião\nPIB\nCrescimento\n\n\n\n\nAngola\n2005\nAfrica\n66351984\n1\n\n\nAngola\n2011\nAfrica\n123013536\n1\n\n\nAlbania\n2005\nEurope\n21452400\n1\n\n\nAlbania\n2006\nEurope\n22717995\n1\n\n\nAlbania\n2007\nEurope\n24080895\n1\n\n\nAlbania\n2008\nEurope\n25890410\n1\n\n\nAlbania\n2009\nEurope\n26754552\n1\n\n\nAlbania\n2010\nEurope\n27741824\n1\n\n\nAlbania\n2011\nEurope\n28452000\n1\n\n\nAlbania\n2012\nEurope\n28852736\n1"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#pacotes",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#pacotes",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "",
    "text": "Code\nlibrary(\"readxl\")\nlibrary(\"here\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"dplyr\")\nlibrary(\"reshape2\")\nlibrary(\"kableExtra\")"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#dados",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#dados",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "",
    "text": "Dados são do Maddison Project Database, disponíveis aqui\n\n\n\n\nPaís\nAno\nRegião\nPIB\nCrescimento\n\n\n\n\nAngola\n2005\nAfrica\n66351984\n1\n\n\nAngola\n2011\nAfrica\n123013536\n1\n\n\nAlbania\n2005\nEurope\n21452400\n1\n\n\nAlbania\n2006\nEurope\n22717995\n1\n\n\nAlbania\n2007\nEurope\n24080895\n1\n\n\nAlbania\n2008\nEurope\n25890410\n1\n\n\nAlbania\n2009\nEurope\n26754552\n1\n\n\nAlbania\n2010\nEurope\n27741824\n1\n\n\nAlbania\n2011\nEurope\n28452000\n1\n\n\nAlbania\n2012\nEurope\n28852736\n1"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-2",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-2",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "Gráfico 2",
    "text": "Gráfico 2\nEste gráfico conta o número absoluto de países que estava em recessão durante um determinado ano. Como o número de países dentro da amostra também cresce com o tempo, uma linha cinza foi adicionada para representar isto. Resolvi suprimir as flechas que aparecem na postagem original, pois a implementação disto no ggplot é muito trabalhosa e o resultado final não é tão bonito.\n\n\nCode\n# Primeiro: conta o número de países pela variável var_pib\n# (1 = expansão, 0 = recessão)\nrecession_countries &lt;- d %&gt;%\n  # Seleciona as observações a partir de 1900\n  filter(year &gt;= 1900) %&gt;%\n  # Agrupa os dados por ano e var_pib\n  group_by(year, var_pib) %&gt;%\n  # Soma os valores que não são NA por tipo (1 = expansão, 0 = recessão)\n  summarise(tipo = sum(!is.na(var_pib)))\n\n# Segundo: conta o número de países incluídos na amostra\nrecession_countries &lt;- recession_countries %&gt;%\n  group_by(year) %&gt;%\n  mutate(amostra = sum(tipo)) %&gt;%\n  ungroup()\n\n# Terceiro: remove as contagens de anos de expansão e converte os dados para longitudinal (melhor para plotar)\nrecession_countries &lt;- recession_countries %&gt;%\n  # Remove os valores que são referentes a anos de expansão\n  filter(var_pib != 1) %&gt;%\n  # Tranforma os dados em longitudinais por ano e 'var_pib'\n  # (tipo = recessão, amostra = observações válidas)\n  melt(id.vars = c(\"year\", \"var_pib\"))\n\n# Base de dados para os textos no gráfico\nhighlight &lt;- data.frame(\n  evento = c(\n    \"1ª GUERRA\\nMUNDIAL\", \"CRISE\\nDE 1929\", \"2ª GUERRA\\nMUNDIAL\",\n    \"CRISE DO\\nPETRÓLEO\", \"TRANSIÇÃO CAPITALISTA\\nDO LESTE EUROPEU\",\n    \"CRISE DE 2008\"),\n  year = c(1918, 1929, 1945, 1971, 1988, 2008),\n  y = c(52, 65, 35, 62, 78, 90)\n)\n\n\n\n\nCode\n# Obs: como plotamos dados de duas bases diferentes a sintaxe é diferente\n\nggplot() +\n  geom_line(\n    data = recession_countries, \n    aes(year, value, group = variable, colour = variable),\n    # Espessura da linha\n    linewidth = 1.2\n    ) +\n  # Destaques de texto\n  geom_text(\n    data = highlight,\n    aes(year, y, label = evento), \n    size = 3,\n    family = \"Gotham Rounded Light\",\n    vjust = \"center\",\n    hjust = \"center\") +\n  # Troca as cores das linhas e altera a legenda\n  scale_colour_manual(\n    name = NULL,\n    breaks = c(\"amostra\", \"tipo\"),\n    values = c(\"tomato\", \"gray50\"),\n    labels = c(\"países incluídos na amostra\", \"países com queda no PIB\")\n    ) +\n  scale_x_continuous(\n    breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n    expand = c(0.025,0.025)\n    ) +\n  # Define o título e subtítulo do gráfico e o título dos eixos\n  labs(\n    x = NULL,\n    y = \"número\\nde países\",\n    title = \"NÚMERO DE PAÍSES COM QUEDA NO PIB NO ANO\",\n    subtitle = \"Em valores absolutos\"\n    ) +\n  theme(\n    # Título e subtítulo\n    plot.title = element_text(family = \"Gotham Rounded Bold\"),\n    plot.subtitle = element_text(family = \"Gotham Rounded Light\", size = 14),\n    # Eixos\n    axis.text.x = element_text(family = \"Gotham Rounded Bold\", size = 10),\n    axis.text.y = element_text(family = \"Gotham Rounded Light\", size = 10),\n    axis.ticks.x = element_line(colour = \"grey70\"),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_text(angle = 360, face = \"bold\", size = 8),\n    # Legenda\n    legend.text = element_text(\n      family = \"Gotham Rounded Bold\",\n      size = 12,\n      vjust = .8,\n      hjust = 0\n      ),\n    legend.background = element_rect(fill = NA),\n    legend.key = element_rect(fill = \"white\"),\n    legend.position = c(0.17,0.75),\n    # Fundo do gráfico  \n    panel.background = element_rect(fill = \"white\"),\n    # Linhas de grade\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(colour = \"grey70\")\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-3",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-3",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "Gráfico 3",
    "text": "Gráfico 3\nEste último gráfico mostra o número relativo de países em recessão.\n\n\nCode\n# Calcula o share de países em recessão na amostra a cada ano\nshare_recession &lt;- d %&gt;% \n  filter(year &gt;= 1900) %&gt;%\n  # Agrupa os dados por ano e var_pib (indicadora)\n  group_by(year, var_pib) %&gt;%\n  # Remove as observações ausentes \n  filter(!is.na(var_pib)) %&gt;%\n  # Conta o número de casos\n  summarise(count = n()) %&gt;%\n  # Calcula a proporção dos casos acima (em porcentagem)\n  group_by(year) %&gt;%\n  mutate(freq = count / sum(count) * 100)\n\n# Tabela auxiliar para guardar os valores que vão ser plotados como\n# texto no gráfico\nhighlight &lt;- data.frame(\n  evento = c(\n    \"1ª GUERRA\\nMUNDIAL\", \"CRISE\\nDE 1929\", \"2ª GUERRA\\nMUNDIAL\",\n    \"CRISE DO\\nPETRÓLEO\", \"TRANSIÇÃO\\nCAPITALISTA\\nDO LESTE\\nEUROPEU\",\n    \"CRISE DE 2008\"),\n  year = c(1916, 1929, 1942, 1971, 1988, 2006),\n  y = c(70, 45, 75, 77, 81, 70)\n)\n\n\n\n\nCode\nggplot() +\n  geom_col(data = share_recession, aes(year, freq, fill = var_pib)) +\n  # Superimpõe linhas horizontais no gráfico\n  geom_hline(\n    yintercept = seq(0, 100, 25),\n    colour = \"gray70\",\n    alpha = .5,\n    size = .8\n    ) +\n  # Mapeia o texto no gráfico (geom_label permite escolher fill = \"salmon\")\n  geom_label(\n    data = highlight,\n    aes(year, y, label = evento),\n    colour = \"white\",\n    size = 3,\n    fill = \"salmon\",\n    family = \"Gotham Rounded Medium\")+\n  # Define título, subtítulo e o título dos eixos\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"NÚMERO DE PAÍSES COM QUEDA NO PIB\",\n    subtitle = \"Em percentual dos países incluídos na amostra\")+\n  scale_fill_discrete(\n    name = NULL,\n    breaks = c(1,0),\n    labels = c(\"PIB EM CRESCIMENTO\", \"PIB EM QUEDA\"))+\n  scale_x_continuous(\n    breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n    expand = c(0.025,0.025)\n    ) +\n  scale_y_continuous(\n    breaks = seq(0, 100, 25),\n    labels = c(0, 25, 50, 75, \"100%\")\n    )+\n  theme(\n    # Fundo do gráfico\n    panel.background = element_rect(fill = \"white\"),\n    # Linhas de grade\n    panel.grid = element_blank(),\n    # Legenda\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.justification='left',\n    legend.text = element_text(family = \"Gotham Rounded Bold\"),\n    # Título e subtítulo\n    plot.title = element_text(family = \"Gotham Rounded Bold\", size = 16),\n    plot.subtitle = element_text(family = \"Gotham Rounded Light\", size = 12),\n    # Eixos\n    axis.text.x = element_text(family = \"Gotham Rounded Bold\", size = 10),\n    axis.text.y = element_text(family = \"Gotham Rounded Light\", size = 10),\n    axis.ticks.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html",
    "title": "Importando dados do SIDRA",
    "section": "",
    "text": "O SIDRA, ou Sistema IBGE de Recuperação Automática, é um sistema automatizado de coleta de dados do IBGE, que disponbiliza tabelas das várias pesquisas feitas pelo IBGE. O sistema é centralizado no site. Encontrar pesquisas e dados pode não ser fácil e, em geral, exige conhecimento prévio sobre as pesquisas do IBGE. O site oferece uma página de ajuda.\nNeste post vou comentar mais especificamente sobre o pacote sidrar que dialoga diretamente com a API do SIDRA para importar tabelas diretamente para dentro do R. O pacote tem duas funções centrais:"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#variando-os-parâmetros-de-busca",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#variando-os-parâmetros-de-busca",
    "title": "Importando dados do SIDRA",
    "section": "Variando os parâmetros de busca",
    "text": "Variando os parâmetros de busca\nA função get_sidra() permite refinar a query que gera a tabela final que é importada dentro do R. Os principais argumentos são:\n\nperiod - escolhe a janela temporal dos dados.\nvariable - escolhe a variável a ser importada.\ngeo - nível geográfico dos dados (e.g. \"Brazil\", \"City\", \"Neighborhood\", etc.).\ngeo.filter - lista com filtros geográficos.\nclassific - classificação dos dados.\ncategory - escolhe quais categorias (do classific) importar.\n\nDe maneira geral, todos os parâmetros acima tem algumas nuances. Vamos explorá-los caso a caso"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-dados-de-vários-períodos",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-dados-de-vários-períodos",
    "title": "Importando dados do SIDRA",
    "section": "Importando dados de vários períodos",
    "text": "Importando dados de vários períodos\nPara selecionar múltiplos períodos podemos montar um vetor character com os períodos desejados. Para selecionar uma sequência de períodos deve-se montar um string único encadeando os períodos com o sinal de “-”. O código abaixo deve deixar isto mais claro.\nNote que se você rodar o código abaixo, provavelmente, vai enfrentar um problema de limite de query. A API do SIDRA restringe o número de linhas que um usuário pode chamar de uma só vez. Assim, para importar bases de dados maiores é preciso montar estratégias para particionar os dados.\n\n# Importa os dados de 2010 e de 2020\npop &lt;- get_sidra(6579, period = c(\"2010\", \"2020\"), geo = \"City\")\n\n# Importa todos os dados de 2010 até 2020 (erro: limite de requisição)\npop &lt;- get_sidra(6579, period = c(\"2010-2020\"), geo = \"City\")\n\nUm detalhe curioso é que a função não retorna erros quando o período extrapola os valores disponíveis nos dados. Assim a função abaixo retorna apenas os valores de 2021.\n\n# Importa todos os dados de 2021 até 2030\npop &lt;- get_sidra(6579, period = c(\"2021-2030\"), geo = \"City\")\n\nA mesma lógica acima vale para quando os dados são mensais ou trimestrais. Considere o exemplo da taxa de desocupação, levantada pela PNADC Trimestral. Note que os períodos estão no formato YYYYQQ.\n\ninfo_sidra(4099)\n\nVamos importar todos as observações da taxa de desocupação em 2012. Note que agora há múltiplas variáveis na mesma tabela e que o resultado já está em formato “long”.\n\nunemp &lt;- get_sidra(4099, period = \"201201-201204\")\n\n\nhead(unemp)\n\n  Nível Territorial (Código) Nível Territorial Unidade de Medida (Código)\n2                          1            Brasil                          2\n3                          1            Brasil                          2\n4                          1            Brasil                          2\n5                          1            Brasil                          2\n6                          1            Brasil                          2\n7                          1            Brasil                          2\n  Unidade de Medida Valor Brasil (Código) Brasil Trimestre (Código)\n2                 %   8.0               1 Brasil             201201\n3                 %   0.9               1 Brasil             201201\n4                 %  15.3               1 Brasil             201201\n5                 %   0.7               1 Brasil             201201\n6                 %  14.0               1 Brasil             201201\n7                 %   0.7               1 Brasil             201201\n          Trimestre Variável (Código)\n2 1º trimestre 2012              4099\n3 1º trimestre 2012              4103\n4 1º trimestre 2012              4114\n5 1º trimestre 2012              4115\n6 1º trimestre 2012              4116\n7 1º trimestre 2012              4117\n                                                                                                                                                                           Variável\n2                                                                                             Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n3                                                                   Coeficiente de variação - Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n4                           Taxa combinada de desocupação e de subocupação por insuficiência de horas trabalhadas, na semana de referência, das pessoas de 14 anos ou mais de idade\n5 Coeficiente de variação - Taxa combinada de desocupação e de subocupação por insuficiência de horas trabalhadas, na semana de referência, das pessoas de 14 anos ou mais de idade\n6                                                     Taxa combinada de desocupação e força de trabalho potencial, na semana de referência, das pessoas de 14 anos ou mais de idade\n7                           Coeficiente de variação - Taxa combinada de desocupação e força de trabalho potencial, na semana de referência, das pessoas de 14 anos ou mais de idade\n\n\nPara refinar a busca acima temos de usar o argumento variable.\n\nunemp &lt;- get_sidra(4099, period = \"201201-202203\", variable = 4099)\n\n\nhead(unemp)\n\n  Nível Territorial (Código) Nível Territorial Unidade de Medida (Código)\n2                          1            Brasil                          2\n3                          1            Brasil                          2\n4                          1            Brasil                          2\n5                          1            Brasil                          2\n6                          1            Brasil                          2\n7                          1            Brasil                          2\n  Unidade de Medida Valor Brasil (Código) Brasil Trimestre (Código)\n2                 %   8.0               1 Brasil             201201\n3                 %   7.6               1 Brasil             201202\n4                 %   7.1               1 Brasil             201203\n5                 %   6.9               1 Brasil             201204\n6                 %   8.1               1 Brasil             201301\n7                 %   7.5               1 Brasil             201302\n          Trimestre Variável (Código)\n2 1º trimestre 2012              4099\n3 2º trimestre 2012              4099\n4 3º trimestre 2012              4099\n5 4º trimestre 2012              4099\n6 1º trimestre 2013              4099\n7 2º trimestre 2013              4099\n                                                                               Variável\n2 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n3 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n4 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n5 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n6 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n7 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n\n\nAgora temos a série completa apenas da taxa de desocupação no Brasil. Podemos buscar também a taxa em cada uma das cidades do Brasil.\n\nunemp &lt;- get_sidra(\n  4099,\n  period = \"201201-202203\",\n  variable = 4099,\n  geo = \"City\"\n  )\n\n\n\n  Nível Territorial (Código) Nível Territorial Unidade de Medida (Código)\n1                          6         Município                          2\n2                          6         Município                          2\n3                          6         Município                          2\n4                          6         Município                          2\n5                          6         Município                          2\n6                          6         Município                          2\n  Unidade de Medida Valor Município (Código)        Município\n1                 %   9.0            1100205 Porto Velho - RO\n2                 %  10.4            1200401  Rio Branco - AC\n3                 %  12.9            1302603      Manaus - AM\n4                 %   9.3            1400100   Boa Vista - RR\n5                 %  10.5            1501402       Belém - PA\n6                 %  12.6            1600303      Macapá - AP\n  Trimestre (Código)         Trimestre Variável (Código)\n1             201201 1º trimestre 2012              4099\n2             201201 1º trimestre 2012              4099\n3             201201 1º trimestre 2012              4099\n4             201201 1º trimestre 2012              4099\n5             201201 1º trimestre 2012              4099\n6             201201 1º trimestre 2012              4099\n                                                                               Variável\n1 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n2 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n3 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n4 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n5 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n6 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n\n\nNote, contudo, que temos o resultado somente nas capitais. Apesar do argumento “City” ser o mesmo que usamos na pesquisa de Estimativas da População, aqui o argumento “City” retorna apenas as capitais. Isto acontece porque a PNADC é uma pesquisa que é feita apenas nas capitais brasileiras e não tem uma desagregação a nível municipal para todos os municípios do Brasil. Este importante detalhe não fica evidente e depende do conhecimento prévio do usuário a respeito das pesquisas do IBGE."
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-muitos-dados",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-muitos-dados",
    "title": "Importando dados do SIDRA",
    "section": "Importando muitos dados",
    "text": "Importando muitos dados\nComo comentei acima, a API do SIDRA tem um limite de linhas nas queries. Isto significa que é preciso montar algumas estratégias para pegar as tabelas em partes menores. Na prática, isto vai depender muito da tabela e da informação que se quer.\nO exemplo abaixo é bastante simples mas pode eventualmente ser útil em alguma outra aplicação. O código importa a tabela de estimativas populacionais ano a ano. O mesmo poderia ter sido feito num for-loop, mas prefiro usar o lapply por simplicidade.\nPara acelerar o processo pode-se trocar o lapply por parallel::mclapply que roda a função em paralelo. Contudo, na minha experiência, esta estratégia acaba resultando em erros imprevistos. Não sei se isto é resultado de alguma restrição da API. Na prática, é mais seguro evitar o mclapply.\n\n# Define um vetor com todos os anos da pesquisa\nperiods &lt;- seq(2001:2021)\n# Converte para character\nperiods &lt;- as.character(periods)\n# Aplica a função get_sidra a cada elemento de periods (a cada ano)\nreq &lt;- lapply(periods, function(p) get_sidra(6579, period = p, geo = \"City\"))\n# Junta o resultado numa única tabela\ntab &lt;- dplyr::bind_rows(req)"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#tabelas-do-censo",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#tabelas-do-censo",
    "title": "Importando dados do SIDRA",
    "section": "Tabelas do Censo",
    "text": "Tabelas do Censo\nTabelas que vem do Censo costumam ter maior complexidade. Considere, por exemplo, a tabela 1378, que retorna a população residente, por situação do domicílio (urbano x rural), sexo e idade, segundo a condição no domicílio e compartilhamento da responsabilidade (responsável, cônjuge, filho, etc.)\nO resultado da função abaixo é omitido para poupar espaço pois a saída é muito grande. Em casos como este é muito importante filtrar o resultado para conseguir uma tabela que faça sentido.\n\ninfo_sidra(1378)\n\nVamos nos focar somente na população residente (var 93) de pessoas responsáveis do domicílio (cod 11941) por bairro de Porto Alegre (4314902).\n\nresp &lt;- get_sidra(\n  1378,\n  variable = 93,\n  classific = c(\"c455\"),\n  geo = \"Neighborhood\",\n  geo.filter = list(\"City\" = 4314902)\n  )\n\nApós um pouco de limpeza no nome das colunas podemos ver o resultado abaixo.\n\n\n\n\n\n\n\nbairro\ncond_domi\nsexo\nidade\nsituacao_domi\nvalor\n\n\n\n\n2\nMedianeira\nTotal\nTotal\nTotal\nTotal\n11568\n\n\n3\nMedianeira\nPessoa responsável\nTotal\nTotal\nTotal\n4206\n\n\n4\nMedianeira\nPessoa responsável - com responsabilidade compartilhada\nTotal\nTotal\nTotal\n1054\n\n\n5\nMedianeira\nPessoa responsável - sem responsabilidade compartilhada\nTotal\nTotal\nTotal\n3152\n\n\n6\nMedianeira\nCônjuge ou companheiro(a)\nTotal\nTotal\nTotal\n2101\n\n\n7\nMedianeira\nCônjuge ou companheiro(a) - de sexo diferente\nTotal\nTotal\nTotal\n2094\n\n\n8\nMedianeira\nCônjuge ou companheiro(a) - de mesmo sexo\nTotal\nTotal\nTotal\n7\n\n\n9\nMedianeira\nFilho(a)\nTotal\nTotal\nTotal\n3429\n\n\n10\nMedianeira\nFilho(a) - da pessoa responsável e do cônjuge\nTotal\nTotal\nTotal\n1916\n\n\n11\nMedianeira\nFilho(a) - somente da pessoa responsável\nTotal\nTotal\nTotal\n1513\n\n\n12\nMedianeira\nEnteado(a)\nTotal\nTotal\nTotal\n145\n\n\n13\nMedianeira\nGenro ou nora\nTotal\nTotal\nTotal\n170\n\n\n14\nMedianeira\nPai, mãe, padastro ou madrasta\nTotal\nTotal\nTotal\n183\n\n\n15\nMedianeira\nSogro(a)\nTotal\nTotal\nTotal\n51\n\n\n16\nMedianeira\nNeto(a)\nTotal\nTotal\nTotal\n549\n\n\n\n\n\n\n\n\nNote que temos a coluna da condição do domicílio desagregada (c455), mas todas as outras (sexo, idade, etc.) estão agregadas."
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#api-do-sidra",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#api-do-sidra",
    "title": "Importando dados do SIDRA",
    "section": "API do SIDRA",
    "text": "API do SIDRA\nEntão imagine, por exemplo, que queremos esta mesma informação, por grupos de idade. Esta requisição quebra o limite da query. Além disso, os grupos de idade disponíveis são caóticos e não é possível filtrar dentro deste grupo da mesma maneira como fazemos com o argumento variable.\nA solução é interagir diretamente com a API do SIDRA! O código acaba sendo bastante complexo/confuso. Muito provavelmente há maneiras de melhorar o código abaixo, mas foi como fiz para conseguir esta informação. Por conveniência chamo explicitamente todos os pacotes que uso. A limpeza dos dados foi a parte mais desafiadora do processo visto que não foi possível transformar o input de JSON para um data.frame de maneira direta.\nEm partes, acho que dei o azar de ter escolhido uma tabela especialmente complicada. Se jsonlite::fromJSON(x, simplifyDataFrame = TRUE) tivesse funcionado, o código seria muito mais simples.\n\n\nCode\n# Bibliotecas usadas\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(tidyjson)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(geobr)\n\n# 1. Montar a query\n\n# Montar um vetor com o código de todos os bairros de Porto Alegre\n\n# Importa o shapefile de todos os bairros IBGE (2010)\nbairros &lt;- geobr::read_neighborhood()\n\nbairros_poa &lt;- bairros |&gt; \n  # Pega apenas os códigos dos bairros de POA\n  filter(code_muni == 4314902) |&gt; \n  # O código do bairro tem alguns números sobrando no meio\n  mutate(code_nb = paste0(\n    str_sub(code_neighborhood, 1, 7),\n    str_sub(code_neighborhood, 10, 12))\n    )\n# Extrai o vetor com os códgios dos bairros\ncodenb_poa &lt;- bairros_poa[[\"code_nb\"]]\n# Colapsa o vetor num único string separado por vírgulas\nlocal = paste(codenb_poa, collapse = \",\")\n# Insere o string no formato da API\nlocal = stringr::str_glue(\"localidades=N102[{local}]\")\n\n# Base do url da API\nbase = \"https://servicodados.ibge.gov.br/api/v3/agregados\"\n# Número da tabela\ntable &lt;- 1378\n# Período da extração (ano)\nperiod &lt;- 2010\n# Código da variável (Pessoas residentes)\nvariable &lt;- 93\n# Códigos das classes\nclass &lt;- c(1, 2, 287, 455)\n# Filtro das classes (veja info_sidra(1378))\ncategory &lt;- list(c(1), c(4, 5), c(93087:93100), c(11941))\n# Colapsa os strings\ncategory &lt;- sapply(category, paste, collapse = \",\")\n# Coloca os strings no format da API\ncat_filter &lt;- stringr::str_glue(\"{class}[{category}]\")\n# Colapsa os strings num único string separado pelo operador booleano |\ncat_filter &lt;- paste(cat_filter, collapse = \"|\")\n\n# Monta a query\nquery &lt;- stringr::str_glue(\n  \"{base}/{table}/periodos/{period}/variaveis/{variable}?{local}&classificacao={cat_filter}\"\n)\n\n# 2. Importar dados\n\n# Importar os dados da API\nreq &lt;- httr::GET(url = query)\njson &lt;- httr::content(req, as = \"text\", encoding = \"UTF-8\")\njson &lt;- jsonlite::fromJSON(json, simplifyVector = FALSE)\n\n# 3. Limpar os dados\n\n# Simplificar os dados de JSON para um tibble\nvalores &lt;- json |&gt; \n  spread_all() |&gt; \n  enter_object(resultados) |&gt; \n  gather_array() |&gt; \n  spread_all() |&gt; \n  enter_object(series) |&gt; \n  gather_array() |&gt; \n  spread_all()\n\n# Extrair somente os valores da série\nvalores &lt;- valores |&gt; \n  as_tibble() |&gt; \n  mutate(serie.2010 = as.numeric(serie.2010)) |&gt; \n  pivot_wider(\n    id_cols = \"array.index\",\n    names_from = \"localidade.id\",\n    values_from = \"serie.2010\"\n  )\n\n# Extrair a classificação das variáveis (sexo, idade)\nclassificacao &lt;- json |&gt; \n  spread_all() |&gt; \n  enter_object(resultados) |&gt; \n  gather_array() |&gt; \n  spread_all() |&gt; \n  enter_object(classificacoes) |&gt; \n  gather_array() |&gt; \n  spread_all()\n\n# Extrai somente a coluna com os grupos de idade\ntblage &lt;- classificacao |&gt; \n  select(contains(\"93\")) |&gt;\n  as_tibble() |&gt; \n  unite(\"age_group\", everything(), na.rm = TRUE) |&gt; \n  filter(age_group != \"\") |&gt; \n  distinct()\n\n# Cria um grid com todas as classificações na ordem correta\ntblclass &lt;- expand_grid(\n  situacao_domicilio = \"urban\",\n  resp = \"Responsável pelo Domicilio\",\n  sex = c(\"male\", \"female\"),\n  age_group = tblage$age_group\n)\n\n# Junta as classificações com os valores \ntab &lt;- cbind(valores, tblclass)\n\n# Converte os dados para wide\ntab &lt;- tab |&gt; \n  pivot_longer(starts_with(\"431\"), names_to = \"code_nb\") |&gt; \n  select(code_nb, situacao_domicilio, resp, sex, age_group, value)\n\n# Converte os dados para uma tabela final\nrespnb &lt;- tab |&gt; \n  pivot_wider(\n    id_cols = c(\"code_nb\", \"age_group\"),\n    names_from = \"sex\",\n    values_from = \"value\"\n  ) |&gt; \n  mutate(total = male + female)|&gt; \n  group_by(code_nb) |&gt; \n  mutate(total_nb = sum(total)) |&gt; \n  ungroup() |&gt; \n  mutate(share_nb = total / total_nb * 100)\n\n\nA tabela abaixo mostra o resultado final do código. A tabela apresenta o número total de pessoas responsáveis por domicílios em grupos de idade quinquenais por sexo e por bairro de Porto Alegre. Além disso, a coluna total_nb traz o número total de pessoas responsáveis por domicílio no bairro e share_nb computa a participação relativa de cada faixa de idade no total do bairro.\n\n\n\n\n\n\ncode_nb\nage_group\nmale\nfemale\ntotal\ntotal_nb\nshare_nb\n\n\n\n\n4314902003\n20 a 24 anos\n383\n406\n789\n8757\n9\n\n\n4314902005\n20 a 24 anos\n3\n7\n10\n435\n2\n\n\n4314902007\n20 a 24 anos\n214\n245\n459\n9116\n5\n\n\n4314902069\n20 a 24 anos\n42\n62\n104\n2071\n5\n\n\n4314902009\n20 a 24 anos\n331\n359\n690\n13137\n5\n\n\n4314902072\n20 a 24 anos\n75\n57\n132\n3706\n4\n\n\n4314902068\n20 a 24 anos\n163\n144\n307\n8052\n4\n\n\n4314902013\n20 a 24 anos\n193\n177\n370\n6319\n6\n\n\n4314902012\n20 a 24 anos\n31\n19\n50\n2719\n2\n\n\n4314902004\n20 a 24 anos\n206\n227\n433\n13342\n3\n\n\n4314902032\n20 a 24 anos\n261\n285\n546\n12171\n4\n\n\n4314902022\n20 a 24 anos\n117\n105\n222\n6910\n3\n\n\n4314902071\n20 a 24 anos\n49\n29\n78\nNA\nNA\n\n\n4314902045\n20 a 24 anos\n70\n56\n126\n4190\n3\n\n\n4314902049\n20 a 24 anos\n71\n65\n136\n4365\n3\n\n\n\n\n\n\n\n\nApesar de claramente não ser o foco do post, pareceu um desperdício de esforço não fazer algo com esta informação. O código abaixo agrupa os dados um pouco mais e monta um mapa interativo dos domicílios por grupo de idade da pessoa responsável. Não seria difícil transformar isto num aplicativo para ver esta mesma info nas outras faixas de idade, mas isto fica para outro post.\n\n\nCode\nbairros &lt;- geobr::read_neighborhood(showProgress = FALSE)\n\nbairros_poa &lt;- bairros |&gt; \n  # Pega apenas os códigos dos bairros de POA\n  filter(code_muni == 4314902) |&gt; \n  # O código do bairro tem alguns números sobrando no meio\n  mutate(code_nb = paste0(\n    stringr::str_sub(code_neighborhood, 1, 7),\n    stringr::str_sub(code_neighborhood, 10, 12))\n    )\n\n\nrespnb &lt;- respnb |&gt;\n  separate(age_group, into = c(\"age_min\", \"age_max\"), sep = \" a \") |&gt; \n  mutate(\n    age_generation = case_when(\n      age_min &gt;= 25 & age_min &lt; 40 ~ \"25-39\",\n      age_min &gt;= 40 & age_min &lt; 60 ~ \"40-59\",\n      age_min &gt;= 60 & age_min &lt; 80 ~ \"60-79\",\n      age_min &gt;= 80 ~ \"80+\",\n      TRUE ~ \"&lt;25\"\n    )\n  )\n\nresp_grouped_nb &lt;- respnb |&gt; \n  group_by(code_nb, age_generation) |&gt; \n  summarise(pop = sum(total, na.rm = TRUE)) |&gt; \n  group_by(code_nb) |&gt; \n  mutate(total_nb = sum(pop)) |&gt; \n  ungroup() |&gt; \n  mutate(share_nb = pop / total_nb * 100)\n\nresp_ages &lt;- pivot_wider(\n  resp_grouped_nb,\n  id_cols = \"code_nb\",\n  names_from = \"age_generation\",\n  names_prefix = \"age_\",\n  values_from = \"share_nb\"\n)\n\nresp_ages &lt;- janitor::clean_names(resp_ages)\n\nresp_bairros &lt;- left_join(bairros_poa, resp_ages, by = \"code_nb\")\n\n\nO mapa é gerado pelo código abaixo. O mapa mostra o share de domicílios, em cada bairro, que são chefiados por “adultos-jovens”, isto é, de 25 a 39 anos. Nota-se que os bairros que tem maiores percentuais estão mais na periferia da cidade. Alguns bairros de classe alta como Moinhos de Vento, Três Figueiras e Vila Assunção tem os menores percentuais.\n\n\nCode\nlibrary(tmap)\nlibrary(tmaptools)\ntmap_mode(mode = \"view\")\n\nm &lt;- tm_shape(resp_bairros) +\n  tm_fill(\n    col = \"age_25_39\",\n    palette = \"inferno\",\n    alpha = 0.8,\n    style = \"jenks\",\n    n = 7,\n    id = \"name_neighborhood\",\n    title = \"Percentual de domicílios&lt;br&gt;chefiados por pessoas&lt;br&gt;de 25 a 39 anos (%).\",\n    popup.vars = c(\n      \"Menos de 25 anos\" = \"age_25\",\n      \"25 a 39 anos\" = \"age_25_39\",\n      \"40 a 59 anos\" = \"age_40_59\",\n      \"60 a 79 anos\" = \"age_60_79\",\n      \"80 anos ou mais\" = \"age_80\"),\n    popup.format = list(digits = 1),\n    legend.format = list(digits = 0)) +\n  tm_borders() +\n  tm_basemap(server = \"CartoDB.Positron\") +\n  tm_view(set.view = c(-51.179152, -30.025976, 13))\n\nleaf &lt;- tmap_leaflet(m)\nwidgetframe::frameWidget(leaf, width = \"100%\")"
  },
  {
    "objectID": "posts/general-posts/repost-mqo-teoria-assintotica/index.html",
    "href": "posts/general-posts/repost-mqo-teoria-assintotica/index.html",
    "title": "MQO - teoria assintótica",
    "section": "",
    "text": "Disclaimer\nEste é um repost antigo que fiz ainda na época do mestrado em economia. Apesar de intuitivo o código dos loops abaixo é muito ineficiente. De maneira geral, for-loops são melhores do que loops feitos com repeat; melhor ainda é montar funções e usar parallel::mclapply ou replicate. Além disso, é importante pre-definir o tamanho do objeto antes de um loop, e.g., x &lt;- vector(\"numeric\", length = 10000).\n\n\nMínimos Quadrados\nA maior parte dos resultados assintóticos dos estimadores de mínimos quadrados (MQO) são um misto da LGN, do TCL e de outros resultados de convergência como o método delta e o teorema de Slutsky. Um resultado simples que podemos visualizar através de uma simulação é a propriedade de não-viés dos estimadores de MQO. Em linhas gerais, desde que o termo de erro seja ortogonal às variáveis independentes, os estimadores de MQO não serão viesados, i.e., os estimadores \\(\\hat{\\beta}\\) vão convergir para os verdadeiros \\(\\beta\\). Dizer que eles são ortogonais costuma ser o mesmo que dizer que eles são independentes. Isto é, na prática queremos que a esperança condicional de \\(u_{t}\\), o erro, dado \\(x_{t}\\), as variáveis explicativas, seja nulo:\n\\[\n\\mathbb{E}(u_{t} | x_{t}) = 0\n\\]\nSuponha que o modelo verdadeiro (o processo gerador de dados) seja da forma:\n\\[\\begin{equation}\n    y_{t} = 5 + 2.5x_{1t} -1.75x_{2t} + 5x_{3t} + u_{t}\n\\end{equation}\\] onde \\[\\begin{align}\n    x_{1} & \\sim N(0,1)\\\\\n    x_{2} & \\sim \\text{Beta}(2, 5)\\\\\n    x_{3} & \\sim U(0,1)\\\\\n    u_{t} & \\sim N(0,1)\n\\end{align}\\]\nSe as variáveis \\(x_{1}, x_{2}\\) e \\(x_{3}\\) forem independentes de \\(u_{t}\\) então o modelo: \\[\\begin{equation}\n    y_{t} = \\beta_{0} + \\beta_{1}x_{1t} + \\beta_{2}x_{2t} + \\beta_{3}x_{3t}\n\\end{equation}\\] fornecerá estimativas consistentes para os betas.\n\n# Modelo verdadeiro (DGP)\nx1 &lt;- rnorm(n = 200)\nx2 &lt;- rbeta(n = 200, shape1 = 2, shape2 = 5)\nx3 &lt;- runif(n = 200)\ny &lt;- 5 + 2.5 * x1 - 1.75 * x2 + 5 * x3 + rnorm(200)\n\nsummary(fit &lt;- lm(y ~ x1 + x2 + x3))\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.40891 -0.71728 -0.03212  0.67974  2.85561 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.19288    0.18462  28.128  &lt; 2e-16 ***\nx1           2.55059    0.06534  39.035  &lt; 2e-16 ***\nx2          -1.79698    0.42362  -4.242 3.41e-05 ***\nx3           4.58214    0.24240  18.903  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9608 on 196 degrees of freedom\nMultiple R-squared:  0.9122,    Adjusted R-squared:  0.9109 \nF-statistic: 678.7 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nNote que os valores estimados estão bastante próximos dos valores verdadeiros. Podemos ver o impacto que o tamanho da amostra tem sobre as estimativas fazendo um loop para diferentes tamanhos. Para conseguir resultados mais consistentes podemos gerar os valores do DGP várias vezes. Abaixo os dados são gerados e estimados 100 vezes para diferentes amostras com \\(n = 10, 50, 10000\\).\n\npar(mfrow = c(2, 2))\ntabela &lt;- matrix(ncol = 4)\nfor (n in c(10, 50, 10000)) {\n    i &lt;- 0\n    X &lt;- matrix(ncol = 4, nrow = 100)\n    repeat{\n\n        x1 &lt;- rnorm(n) \n        x2 &lt;- rbeta(n, 2, 5)\n        x3 &lt;- runif(n)\n        y &lt;- 5 + 2.5 * x1 - 1.75 * x2 + 5 * x3 + rnorm(n)\n\n        fit &lt;- lm(y ~ x1 + x2 + x3)\n\n        coeficientes &lt;- coef(fit)\n\n        X[i + 1, ] &lt;- coeficientes\n\n        i &lt;- i + 1\n        if (i == 100) {break}\n    }\n\n    tabela &lt;- rbind(tabela, colMeans(X, na.rm = TRUE))\n\n    hist(X[, 1], freq = FALSE,\n         main = bquote(beta[0]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = 5, col = \"red\")\n    hist(X[, 2], freq = FALSE,\n         main = bquote(beta[1]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = 2.5, col = \"red\")\n    hist(X[, 3], freq = FALSE,\n         main = bquote(beta[2]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = -1.75, col = \"red\")\n    hist(X[, 4], freq = FALSE,\n         main = bquote(beta[3]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = 5, col = \"red\")\n}\n\n\n\n\n\n\n\n\n\n\nA tabela abaixo mostra a média dos valores estimados para cada coeficiente\n\n\n\n\n\n\nb0 = 5\nb1 = 2,5\nb2 = -1,75\nb3 = 5\n\n\n\n\nn = 10\n5.092687\n2.563573\n-1.852012\n4.862016\n\n\nn = 50\n5.011683\n2.510650\n-1.792762\n5.019113\n\n\nn = 10000\n4.995774\n2.499198\n-1.736876\n5.003811\n\n\n\n\n\n\n\n\n\nMQO viesado\n\nProcesso não-estacionário\nUm dos casos em que os estimadores de MQO se tornam viesados acontece quando o processo é auto-regressivo e não-estacionário, isto é, quando o DGP é da forma\n\\[\\begin{equation}\n    y_{t} = \\phi y_{t-1} + u_{t}\n\\end{equation}\\]\nem que \\(|\\phi| \\geq 1\\). O código abaixo simula o modelo acima mil vezes para diferentes valores de \\(\\phi\\). Quando o processo é estacionário temos estimativas boas para o parâmetro, mas quando \\(\\phi = 1.1\\) as estimativas tornam-se muito ruins. Quando \\(\\phi = 1\\) temos um caso de raiz unitária que tem propriedades bastante específicas\n\nlibrary(dynlm)\npar(mfrow = c(2, 2))\nfor(phi in c(0.5, 0.9, 1, 1.1)){\n    j = 0\n    x &lt;- c()\n    \n    repeat{\n\n    y &lt;- 0\n    for(i in 1:99) { \n        y[i + 1] &lt;- phi * y[i] + rnorm(1)\n    }\n    \n    y &lt;- ts(y)\n    fit &lt;- dynlm(y ~ lag(y))\n    phi_hat &lt;- coef(fit)[2]\n    x &lt;- c(x, phi_hat)\n\n    j = j + 1\n    if (j == 1000) {break}\n    }\n\n    hist(x, freq = FALSE,\n         main = bquote(phi==.(phi)),\n         xlim = c(phi - 0.2, 1),\n         xlab = \"\")\n    abline(v = phi, col = \"red\")\n}\n\n\n\n\n\n\nErros ARMA\nQuando os erros do modelo são autocorrelacionados os estimadores de MQO perdem algumas de suas boas propriedades. Ainda assim, desde que o erro seja ortogonal aos regressores os estimadores continuaram sendo não-viesados. Há, contudo, um caso em que os estimadores de MQO são viesados: quando algum dos regressores independentes é uma defasagem da variável depedente.\nIsto acontece porque este tipo de autocorrelação implica que os regressores não são ortogonais aos erros. Tome o caso simples em que o erro \\(u_{t}\\) segue um processo AR(1) da forma\n\\[\\begin{equation}\n    u_{t} = \\phi u_{t-1} + \\eta_{t}\n\\end{equation}\\] em que \\(\\eta_{t}\\) é ruído branco. E o modelo é \\[\\begin{equation}\n    y_{t} = \\beta_{0} + \\beta_{1}y_{t-1} + u_{t}\n\\end{equation}\\] Agora note que \\[y_{t - 1} = \\beta_{0} + \\beta_{1}y_{t - 2} + u_{t - 1}\\] Reescrevendo a expressão do erro e substituindo na equação acima temos que: \\[y_{t - 1} = \\beta_{0} + \\beta_{1}y_{t - 2} + \\frac{u_{t} - \\eta_{t}}{\\phi}\\] logo \\(y_{t - 1} \\propto u_{t}\\), isto é, \\(y_{t - 1}\\) é correlacionado com o termo de erro \\(u_{t}\\).\nPara exemplificar este resultado, o código abaixo simula a seguinte série 1000 vezes para diferentes valores de \\(\\phi\\). O termo de erro segue um processo AR(1). Note como as estimativas \\(\\hat{\\phi}\\) são bastante ruins. Para remediar este problema teríamos que levar em conta a estrutura autoregressiva do erro no modelo.\n\\[\\begin{align}\n    y_{t} & = \\phi y_{t-1} + u_{t}\\\\\n    u_{t} & = 0.7u_{t-1} + \\eta_{t}\\\\\n\\end{align}\\]\n\npar(mfrow = c(2, 2))\n\nfor(phi in c(0.25, 0.5, 0.9, 1)) {\n    j &lt;- 0\n    x &lt;- c()\n    \n    repeat{\n\n    y &lt;- 0\n    erro &lt;- arima.sim(n = 100, model = list(ar = .7))\n\n    for(i in 1:99) { y[i + 1] &lt;- phi*y[i] + erro[i] }\n    \n    y &lt;- ts(y)\n\n    fit &lt;- dynlm(y ~ lag(y))\n    \n    phi_hat &lt;- coef(fit)[2]\n\n    x &lt;- c(x, phi_hat)\n\n    j = j + 1\n    if (j == 1000) {break}\n    }\n\n    hist(x, main = bquote(phi==.(phi)), xlim = c(phi - 0.2, 1), xlab = \"\")\n    abline(v = phi, col = \"red\")\n\n}\n\n\n\n\nEste resultado não muda assintoticamente, isto é, não importa qual o tamanho da amostra: as estimativas de \\(\\hat{\\phi}\\) serão viesadas. Na verdade, os resultados vão piorando à medida que cresce o tamanho da amostra. Abaixo mostro isto para o caso de \\(\\phi = 0.5\\) e n = \\(100, 200, 500, 10000\\).\n\npar(mfrow = c(2, 2))\nphi &lt;- 0.5\nfor(n in c(100, 200, 500, 10000)){\n    j = 0\n    x &lt;- c()\n    \n    repeat{\n\n    y &lt;- 0\n    erro &lt;- arima.sim(n = n, model = list(ar = .7))\n\n    for(i in 1:(n-1)) { y[i + 1] &lt;- phi*y[i] + erro[i] }\n    \n    y &lt;- ts(y)\n\n    fit &lt;- dynlm(y ~ lag(y))\n    \n    phi_hat &lt;- coef(fit)[2]\n\n    x &lt;- c(x, phi_hat)\n\n    j = j + 1\n    if (j == 1000) {break}\n    }\n\n    hist(x,\n         freq = FALSE,\n         main = bquote(phi==.(phi)~\", n = \"~.(n)),\n         xlim = c(0, 1),\n         xlab = \"\")\n    abline(v = phi, col = \"red\")\n\n}\n\n\n\n\nConsiderando a estrutura autoregressiva do modelo chegamos em estimativas melhores para os parâmetros. O código abaixo faz um ARMAX, isto é, uma estimativa de mínimos quadrados com erros ARMA. O modelo segue o processo \\[\\begin{align}\n    y_{t} & = \\beta_{0} + \\beta_{1}y_{t-1} + u_{t}\\\\\n    u_{t} & = \\beta_{2} u_{t-1} + \\eta_{t}\\\\\n\\end{align}\\]\nonde os valores foram definidos como \\(\\beta_{0} = 0\\), \\(\\beta_{1} = 0.5\\) e \\(\\beta_{2} = 0.7\\).\n\npar(mfrow = c(2, 2))\nn &lt;- 1000\nparam &lt;- matrix(nrow = n, ncol = 3)\nj &lt;- 0\nrepeat {\n\n    y &lt;- 0\n    erro &lt;- arima.sim(n = n, model = list(ar = .7))\n\n    for(i in 1:(n-1)) {\n        y[i + 1] &lt;- 0.5 * y[i] + erro[i + 1]\n    }\n    \n    y &lt;- ts(y)\n    ly &lt;- lag(y, -1)\n    x &lt;- ts.intersect(y, ly, dframe = TRUE)\n    fit &lt;- arima(x$y, order = c(1, 0, 0), xreg = x$ly)\n    param[j + 1, ] &lt;- coef(fit)\n\n    j = j + 1\n    if (j == 1000) {break}\n}\n\nhist(param[, 2], main = bquote(beta[0]==0~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0, col = \"red\")\nhist(param[, 1], main = bquote(beta[1]==0.5~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0.5, col = \"red\")\nhist(param[, 3], main = bquote(beta[2]==0.7~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0.7, col = \"red\")"
  },
  {
    "objectID": "posts/general-posts/precos-imoveis-demografia/index.html",
    "href": "posts/general-posts/precos-imoveis-demografia/index.html",
    "title": "Preços de Imóveis e Demografia",
    "section": "",
    "text": "Já vi em alguns lugares uma suposta ligação entre fatores demográficos e tendências de longo prazo no mercado imobiliário. Intuitivamente, alguns dos principais motivadores para comprar ou vender um imóvel estão ligados a fatores demográficos: nascimentos, casamentos, divórcios ou óbitos.\nEste tipo de análise omite fatores importantes como renda, condições de financiamento e o contexto geral da economia. Ainda assim, fiquei curioso para ver se havia algum padrão entre tendências demográficas mais simples e o comportamento dos preços."
  },
  {
    "objectID": "posts/general-posts/precos-imoveis-demografia/index.html#todos-os-países",
    "href": "posts/general-posts/precos-imoveis-demografia/index.html#todos-os-países",
    "title": "Preços de Imóveis e Demografia",
    "section": "Todos os países",
    "text": "Todos os países\nEsta análise é ainda bastante preliminar. Ainda que a demografia seja um motor para a demanda imobiliária, outros fatores como oferta de moradia e condições de crédito são importantes demais parecem serem omitidos.\nO gráfico abaixo compara a população em 2010/2020 com os preços em 2010/2020. Os países ao lado direito do gráfico, são os países onde houve crescimento populacional. Os países na parte de cima do gráfico são os países onde houve crescimento real do preço dos imóveis. Na média da amostra, destacada como WLD, houve crescimento de ambos.\nPaíses que estão muito para cima como Chile (CHL), Índia (IND) e Estônia (EDT) estão com imóveis muito “caros”. Já em países com França e Finlândia tanto a população como o nível de preço dos imóveis cresceram muito pouco. No caso da Grécia, tanto a população como o preço dos imóveis diminuiu nos últimos dez anos.\nO gráfico não me surpreendeu muito, mas esperava que os EUA estivessem mais para cima no gráfico e que o Brasil estivesse ao menos do lado positivo do eixo-x. Isso me sugere que a impressão de que os imóveis no Brasil são ou estão caros tem muito mais a ver com a baixa renda da população.\n\n\nCode\np5 &lt;- \n  ggplot(\n    data = na.omit(tbl_wide),\n    aes(x = index_pop, y = index_house)\n    ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, color = colors[1]) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(aes(label = iso3c, color = highlight)) +\n  scale_x_continuous(breaks = seq(-10, 30, 5)) +\n  scale_y_continuous(breaks = seq(-30, 90, 15)) +\n  scale_color_manual(values = c(\"black\", colors[1])) +\n  guides(color = \"none\") +\n  labs(\n    title = \"Real House Prices x Population (2010/20)\",\n    x = \"Population (2010/2020)\",\n    y = \"RPPI (2010/2020)\",\n    caption = \"Source: Real House Price Indexes (BIS), Population (UN).\") +\n  theme_vini\n\np5"
  },
  {
    "objectID": "posts/general-posts/precos-imoveis-demografia/index.html#footnotes",
    "href": "posts/general-posts/precos-imoveis-demografia/index.html#footnotes",
    "title": "Preços de Imóveis e Demografia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHong Kong: https://exame.com/economia/as-raizes-economicas-dos-protestos-de-hong-kong/↩︎"
  },
  {
    "objectID": "posts/general-posts/ols-com-matrizes/index.html",
    "href": "posts/general-posts/ols-com-matrizes/index.html",
    "title": "OLS com matrizes",
    "section": "",
    "text": "Uma forma instrutiva de entender o modelo de regressão linear é expressando ele em forma matricial. Os cursos introdutórios de econometria costumam omitir esta abordagem e expressam todas as derivações usando somatórios, deixando a abordagem matricial para cursos mais avançados. É bastante simples computar uma regressão usando apenas matrizes no R.\nDe fato, um dos objetos fundamentais do R é a e muitas das operações matriciais (decomposições, inversa, transposta, etc.) já estão implementadas em funções base. Uma var \\(k\\) .\nNeste post vou mostrar como fazer uma regressão linear usando somente matrizes no R. Além disso, vou computar algumas estatísticas típicas (t, F)\nO modelo linear é da forma\n\\[\ny_{t} = x^\\intercal_{t} \\beta + e_{t}\n\\]\nonde \\(x^\\intercal\\) é o vetor transposto de \\(x\\) . É importante sempre ter em mente a dimensão destes vetores. O vetor \\(y_{t}\\) é \\(n\\times1\\) onde \\(n\\) representa o número de observações na amostra. O vetor \\(\\beta\\) é \\(k\\times1\\) onde \\(k\\) é o número de regressores (ou variáveis explicativas). Como há \\(n\\) observações para cada uma dos \\(k\\) regressores, \\(x\\) é \\(k\\times1\\) ; o detalhe é que \\(x = (1 \\, \\,x_{1} \\, \\dots \\,x_{k-1})\\) , onde cada \\(x_{i}\\) é \\(n\\times 1\\) e \\(1\\) é um vetor de uns \\(n\\times1\\) . Finalmente, \\(e_{t}\\) é \\(n\\times1\\) . Temos então que:\n\\[\n\\begin{pmatrix}\ny_{1} \\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}=\n\\begin{pmatrix}\n1\\\\\nx_{1}\\\\\n\\vdots\\\\\nx_{k-1}\n\\end{pmatrix}^\\intercal\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k-1}\n\\end{pmatrix}+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n\\]\nonde:\n\\[\n\\begin{bmatrix}\nx_{1} & = (x_{11} & x_{12} & x_{13} & \\dots & x_{1n})\\\\\nx_{2} & = (x_{21} & x_{22} & x_{23} & \\dots & x_{2n})\\\\\nx_{3} & = (x_{31} & x_{32} & x_{33} & \\dots & x_{3n})\\\\\n\\vdots\\\\\nx_{k-1} & = (x_{(k-1)1} & x_{(k-1)2} & x_{(k-1)3} & \\dots & x_{(k-1)n})\n\\end{bmatrix}\n\\]\nQueremos encontrar o vetor \\(\\hat{\\beta}\\) que minimiza o a soma do quadrado dos erros, isto é, que minimiza\n\\[\nS(\\beta) = \\sum_{t = 1}^{T}(y_{t} - x^\\intercal_{t}\\beta)^{2}\n\\]\nEncontramos o ponto crítico derivando a expressão acima e igualando-a a zero. O resultado é o conhecido estimador de mínimos quadrados:\n\\[\n\\hat{\\beta} = \\left ( \\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} \\right )^{-1} \\sum_{t = 1}^{T}x_{t}y_{t}\n\\]\nPara reescrever as equações acima usando matrizes usamos os seguintes fatos:\n\\[\n\\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} = X^\\intercal X\n\\]\nonde \\(X\\) é uma matriz \\(n \\times k\\)\n\\[\n\\sum_{t = 1}^{T}x_{t}y_{t} = X^\\intercal y\n\\]\nonde \\(X^\\intercal y\\) é \\(k \\times 1\\) . Lembre-se que uma hipótese do modelo linear é de que \\(X\\) é uma matriz de posto completo, logo \\(X^\\intercal X\\) possui inversa e podemos escrever:\n\\[\n\\hat{\\beta} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]"
  },
  {
    "objectID": "posts/general-posts/ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "href": "posts/general-posts/ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "title": "OLS com matrizes",
    "section": "Exemplo: salário (Wooldridge)",
    "text": "Exemplo: salário (Wooldridge)\nComo exemplo vou usar um exemplo clássico de livro texto de econometria: uma regressão de salário (rendimento) contra algumas variáveis explicativas convencionais: anos de educação, sexo, anos de experiência, etc. As bases de dados do livro Introductory Econometrics estão disponíveis no pacote wooldridge. O código abaixo carrega a base de dados .\n\nlibrary(wooldridge)\n# Carrega a base\ndata(wage2)\n# Remove os valores ausentes (NAs)\nsal &lt;- na.omit(wage2)\n\n\n\n\n\n\n\n\nwage\nhours\nIQ\nKWW\neduc\nexper\ntenure\nage\nmarried\nblack\nsouth\nurban\nsibs\nbrthord\nmeduc\nfeduc\nlwage\n\n\n\n\n1\n769\n40\n93\n35\n12\n11\n2\n31\n1\n0\n0\n1\n1\n2\n8\n8\n6.645091\n\n\n3\n825\n40\n108\n46\n14\n11\n9\n33\n1\n0\n0\n1\n1\n2\n14\n14\n6.715383\n\n\n4\n650\n40\n96\n32\n12\n13\n7\n32\n1\n0\n0\n1\n4\n3\n12\n12\n6.476973\n\n\n5\n562\n40\n74\n27\n11\n14\n5\n34\n1\n0\n0\n1\n10\n6\n6\n11\n6.331502\n\n\n7\n600\n40\n91\n24\n10\n13\n0\n30\n0\n0\n0\n1\n1\n2\n8\n8\n6.396930\n\n\n9\n1154\n45\n111\n37\n15\n13\n1\n36\n1\n0\n0\n0\n2\n3\n14\n5\n7.050990\n\n\n10\n1000\n40\n95\n44\n12\n16\n16\n36\n1\n0\n0\n1\n1\n1\n12\n11\n6.907755\n\n\n11\n930\n43\n132\n44\n18\n8\n13\n38\n1\n0\n0\n0\n1\n1\n13\n14\n6.835185\n\n\n14\n1318\n38\n119\n24\n16\n7\n2\n28\n1\n0\n0\n1\n3\n1\n10\n10\n7.183871\n\n\n15\n1792\n40\n118\n47\n16\n9\n9\n34\n1\n0\n0\n1\n1\n1\n12\n12\n7.491087\n\n\n\n\n\n\n\nA base traz 663 observações de 17 variáveis. A função str é útil para entender a estrutura dos dados.\n\n# Dimensão da base (# linhas  # colunas)\ndim(sal)\n\n[1] 663  17\n\n# Descrição da base\nstr(sal)\n\n'data.frame':   663 obs. of  17 variables:\n $ wage   : int  769 825 650 562 600 1154 1000 930 1318 1792 ...\n $ hours  : int  40 40 40 40 40 45 40 43 38 40 ...\n $ IQ     : int  93 108 96 74 91 111 95 132 119 118 ...\n $ KWW    : int  35 46 32 27 24 37 44 44 24 47 ...\n $ educ   : int  12 14 12 11 10 15 12 18 16 16 ...\n $ exper  : int  11 11 13 14 13 13 16 8 7 9 ...\n $ tenure : int  2 9 7 5 0 1 16 13 2 9 ...\n $ age    : int  31 33 32 34 30 36 36 38 28 34 ...\n $ married: int  1 1 1 1 0 1 1 1 1 1 ...\n $ black  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ south  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urban  : int  1 1 1 1 1 0 1 0 1 1 ...\n $ sibs   : int  1 1 4 10 1 2 1 1 3 1 ...\n $ brthord: int  2 2 3 6 2 3 1 1 1 1 ...\n $ meduc  : int  8 14 12 6 8 14 12 13 10 12 ...\n $ feduc  : int  8 14 12 11 8 5 11 14 10 12 ...\n $ lwage  : num  6.65 6.72 6.48 6.33 6.4 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n - attr(*, \"na.action\")= 'omit' Named int [1:272] 2 6 8 12 13 19 20 21 31 36 ...\n  ..- attr(*, \"names\")= chr [1:272] \"2\" \"6\" \"8\" \"12\" ...\n\n\nO modelo proposto é o abaixo:\n\\[\n\\text{lwage}_{t} = \\beta_{0} + \\beta_{1}\\text{educ}_{t} + \\beta_{2}\\text{exper}_{t} + \\beta_{3}\\text{exper}^{2}_{t} + \\beta_{4}\\text{tenure}_{t} + \\beta{5}\\text{married}_{t} + u_{t}\n\\]\nonde:\n\nlwage = logaritmo natural do salário\neduc = anos de educação\nexper = anos de experiência (trabalhando)\ntenure = anos trabalhando com o empregador atual\nmarried = dummy (1 = casado, 0 = não-casado)\n\nHá 6 coeficientes para estimar logo \\(k = 6\\) . Além disso, como há \\(663\\) observações temos que \\(n = 663\\) . A matriz de “dados” é da forma:\n\\[\nX = \\begin{bmatrix}\n1 & 12 & 11 & 121 & 2 & 1\\\\\\\\\n1 & 14 & 11 & 121 & 9 & 1\\\\\\\\\n1 & 12 & 13 & 169 & 7 & 1\\\\\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\n1 & 13 & 10 & 100 & 3 & 1\n\\end{bmatrix}_{663\\times 6}\n\\]\nO código abaixo monta a matriz \\(X\\) acima. As funções head e tail podem ser usadas para verificar as primeiras e últimas linhas da matriz. Vale esclarecer dois pontos sobre o código a ser usado neste post. Primeiro, quando se cria um objeto usando o operador &lt;- pode-se forçar o R a imprimir o seu valor colocando a expressão entre parêntesis. Por exemplo, teste (x &lt;- 3). O segundo ponto é que o operador de multiplicação matricial é %*%.\n\n# Define alguns valores úteis: \n## N = número de observações\n## k = número de regressores\n## const = vetor com 1^\\intercals (uns)\nN &lt;- 663; k &lt;- 6; const &lt;- rep(1, 663)\n# Monta a matriz de observações da regressão\nX &lt;- cbind(const, sal$educ, sal$exper, sal$exper^2, sal$tenure, sal$married)\nX &lt;- as.matrix(X)\n# Define o nome das colunas da matriz de observações\ncolnames(X) &lt;- c(\"const\", \"educ\", \"exper\", \"exper2\", \"tenure\", \"married\")\n# Função para verificar as primeiras linhas da matriz X\nhead(X)\n\n     const educ exper exper2 tenure married\n[1,]     1   12    11    121      2       1\n[2,]     1   14    11    121      9       1\n[3,]     1   12    13    169      7       1\n[4,]     1   11    14    196      5       1\n[5,]     1   10    13    169      0       0\n[6,]     1   15    13    169      1       1\n\n# Função para verificar as últimas linhas da matriz X\ntail(X)\n\n       const educ exper exper2 tenure married\n[658,]     1   12     9     81      2       1\n[659,]     1   16     8     64     10       1\n[660,]     1   12    11    121      3       1\n[661,]     1   12     9     81      3       1\n[662,]     1   16    10    100      9       1\n[663,]     1   13    10    100      3       1\n\n\nLembrando que o problema de mínimos quadrados é de encontrar os valores de \\(\\beta\\) que minimizam a soma dos erros ao quadrado.\n\\[\n\\underset{\\beta}{\\text{Min }} e^\\intercal e\n\\]\nAbrindo mais a expressão acima:\n\\[\n\\begin{align}\n  e^\\intercal e & = (y - X\\beta )^\\intercal(y - X\\beta ) \\\\\\\\\n      & = y^\\intercal y - y^\\intercal X\\beta - \\beta ^\\intercal X^\\intercal y + \\beta ^\\intercal X^\\intercal X \\beta \\\\\\\\\n      & = y^\\intercal y - 2 y^\\intercal X \\beta + \\beta^\\intercal X^\\intercal X \\beta\n\\end{align}\n\\]\nDerivando em relação a \\(\\beta\\) e igualando a zero chega-se no estimador de MQO\n\\[\n\\beta_{\\text{MQO}} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]\nO código abaixo computa \\(\\beta_{\\text{MQO}}\\) . Note que os parêntesis por fora da expressão forçam o R a imprimir o valor do objeto. Além disso, como estamos multiplicando matrizes/vetores usamos %*%.\n\n# Define o vetor y (log do salário)\ny &lt;- sal$lwage\n# Computa a estimativa para os betas\n(beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y)\n\n                [,1]\nconst   5.3563606713\neduc    0.0767258959\nexper   0.0104985672\nexper2  0.0002881339\ntenure  0.0091039254\nmarried 0.2002468574\n\n\nOs valores estimados dos betas são reportados na tabela abaixo.\n\ntabela &lt;- as.data.frame(round(beta, 4))\ncolnames(tabela) &lt;- c(\"Coeficiente estimado\")\nround(beta, 4) %&gt;%\n  kable(align = \"c\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\nconst\n5.3564\n\n\neduc\n0.0767\n\n\nexper\n0.0105\n\n\nexper2\n0.0003\n\n\ntenure\n0.0091\n\n\nmarried\n0.2002"
  },
  {
    "objectID": "posts/general-posts/ols-com-matrizes/index.html#resíduo-e-variância",
    "href": "posts/general-posts/ols-com-matrizes/index.html#resíduo-e-variância",
    "title": "OLS com matrizes",
    "section": "Resíduo e variância",
    "text": "Resíduo e variância\nO resíduo do modelo é simplesmente a diferença entre o observado \\(y_{t}\\) e o estimado \\(\\hat{y_{t}}\\) . Isto é,\n\\[\n\\hat{e}_{t} = y_{t} - \\hat{y}_{t} = y_{t} - x_{t}^\\intercal\\hat{\\beta}\n\\]\nou, de forma equivalente,\n\\[\n\\hat{e} = y - X\\hat{\\beta}\n\\]\n\n# Computa o resíduo da regressão\nu_hat &lt;- y - X %*% beta\n\nUsando o histograma pode-se visualizar a distribuição dos resíduos.\n\nhist(u_hat, breaks = 30, freq = FALSE, main = \"Histograma dos resíduos\")\n\n\n\n\nO estimador da variância é dado por:\n\\[\n\\hat{\\sigma}^{2} = \\frac{1}{N-k}\\sum_{t = 1}^{N}\\hat{e}_{t}^{2}\n\\]\nSubstituindo os valores calculados acima chegamos em:\n\\[\n\\hat{\\sigma}^{2} = \\frac{\\hat{e}^\\intercal\\hat{e}}{N-k} = 0.1403927\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html",
    "title": "ARMA: um exemplo simples",
    "section": "",
    "text": "Neste post apresento como estimar e diagnosticar um modelo ARMA simples no R. O modelo ARMA decompõe uma série em função de suas observações passadas e de “passados” (às vezes chamados de inovações). O ARMA(1, 1) com constante tem a seguinte forma.\n\\[\ny_{t} = \\phi_{0} +\\phi_{1}y_{t-1} + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t - 1}\n\\]\nonde ambas as condições de estacionaridade como de invertibilidade devem ser atendidas. Assume-se que o termo \\(\\varepsilon_{t}\\) é um ruído branco. De maneira mais geral, um modelo ARMA(p, q) tem a forma:\n\\[\ny_{t} = \\phi_{0} +\\phi_{1}y_{t-1} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t - 1} + \\dots + \\theta_{q}\\varepsilon_{t- q}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-1",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-1",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 1",
    "text": "Modelo 1\nVou primeiro estimar o ARMA(1, 2) usando a função arima() do R.\n\n(m1 &lt;- arima(train, order = c(1, 0, 2)))\n#&gt; \n#&gt; Call:\n#&gt; arima(x = train, order = c(1, 0, 2))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1     ma1     ma2  intercept\n#&gt;       0.2324  0.0971  0.1623     0.0084\n#&gt; s.e.  0.2294  0.2247  0.0958     0.0012\n#&gt; \n#&gt; sigma^2 estimated as 0.0001038:  log likelihood = 566.91,  aic = -1123.82\n\nA estimativa tem a forma: \\[\n  y_{t} = \\underset{(0.0012)}{0.0084} + \\underset{(0.2324)}{0.2294}y_{t - 1} + \\underset{(0.2247)}{0.0971}\\epsilon_{t-1} + \\underset{(0.0958)}{0.1623}\\epsilon_{t-2}\n\\] Note que os erros-padrão de \\(\\hat{\\phi_{1}}\\) e \\(\\hat{\\theta_{1}}\\) são bastante elevados. De fato, um teste-t revela que estes coeficientes não são significantes.\n\nDiagnóstico de resíduos\nPode-se verificar os resíduos do modelo de muitas formas. Idealmente, quer-se que os resíduos não apresentem autocorrelação alguma. Uma forma gráfica de ver isto é usando a função lag1.plot que apresenta gráficos de dispersão entre o resíduo \\(u_{t}\\) contra suas defasagens \\(u_{t-1}, u_{t-2}, \\dots\\). Abaixo faço isto para as primieras quatro defasagens.\n\nresiduos &lt;- resid(m1)\nlag1.plot(residuos, max.lag = 4)\n\n\n\n\n\n\n\n\nNa prática, é mais conveniente analisar diretamente os gráficos da FAC e da FACP dos resíduos.\n\nacf2(residuos)\n\n\n\n\n\n\n\n#&gt;      [,1]  [,2] [,3]  [,4]  [,5]  [,6]  [,7]  [,8] [,9] [,10] [,11] [,12] [,13]\n#&gt; ACF  0.01 -0.01    0 -0.09 -0.12 -0.03 -0.04 -0.05 0.04  0.06  0.07 -0.12 -0.07\n#&gt; PACF 0.01 -0.01    0 -0.09 -0.12 -0.03 -0.05 -0.06 0.02  0.04  0.06 -0.15 -0.09\n#&gt;      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]\n#&gt; ACF  -0.03 -0.12  0.07  0.06  0.06  0.02  0.09  -0.1  0.00 -0.07 -0.04\n#&gt; PACF -0.02 -0.11  0.06  0.02  0.04 -0.02  0.05  -0.1  0.01 -0.05 -0.01\n\nUm teste basatante usual para verificar a presença de autocorrelação nos resíduos é o Ljung-Box. Para computá-lo uso a função Box.test() do pacote tseries. Note que é preciso suplementar o argumento fitdf com o número de parâmetros estimados do modelo. Isto serve para corrigir a estatística do teste. A escolha da ordem do teste é um tanto arbitrária e, na prática, seria recomendado fazer o teste para várias ordens diferentes. O código abaixo computa a estatística do teste para uma defasagem igual a 8.\nNote o uso do argumento fitdf que leva em conta o número de parâmetros estimados no modelo. Segundo o p-valor, não temos evidência para rejeitar a hipótese nula de que os resíduos não são conjuntamente autocorrelacionados. Isto é, temos evidência de que o modelo está bem ajustado pois os resíduos parecem se comportar como ruído branco.\n\nlibrary(tseries)\nBox.test(residuos, type = \"Ljung-Box\", lag = 8, fitdf = 4)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  residuos\n#&gt; X-squared = 4.9486, df = 4, p-value = 0.2926\n\nNa prática, é bom repetir o teste para várias defasagens diferentes. A tabela abaixo resume os valores do teste para várias ordens de defasagem. Vale lembrar que o teste Ljung-Box tende a não-rejeitar H0 para defasagens muito elevadas.\n\n\n\n\n\nDefasagem\nEstatística de teste\nP-valor\n\n\n\n\n8\n4.9486\n0.2926\n\n\n9\n5.2251\n0.3890\n\n\n10\n5.8669\n0.4383\n\n\n11\n6.7991\n0.4501\n\n\n12\n9.7788\n0.2809\n\n\n13\n10.8054\n0.2893\n\n\n14\n10.9313\n0.3629\n\n\n15\n13.6377\n0.2537\n\n\n16\n14.5903\n0.2646\n\n\n17\n15.2158\n0.2941\n\n\n18\n15.8735\n0.3212\n\n\n19\n15.9294\n0.3868\n\n\n20\n17.4548\n0.3568\n\n\n\n\n\n\n\n\n\nUsando o pacote astsa\nUma forma bastante conveniente de trabalhar com modelos ARMA é com a função sarima do pacote astsa. Esta função apresenta automaticamente algumas valiosas informações para o diagnóstico dos resíduos: o gráfico do ACF, o gráfico qq-plot (para verificar a normalidade dos resíduos) e os p-valores do teste Ljung-Box para várias ordens de defasagem.\nA saída abaixo reúne quatro gráficos. O primeiro deles apresenta o resíduo normalizado (ou escalado). Este gráfico não deve apresentar um padrão claro. O segundo gráfico é a FAC do resíduo: idealmente, nenhuma defasagem deve ser significativa neste gráfico. Ao lado da FAC temos o QQ-plot: se todos os pontos caem sobre a linha azul temos evidência de que os resíduos são normalmente distribuídos.\nPor fim, o último gráfico mostra o p-valor do teste Ljung-Box (já ajustado pelo número de parâmetros do modelo estimado) para diferentes defasagens. A linha tracejada em azul indica o valor 0.05. Idealmente, todos os pontos devem estar acima desta linha.\n\nsarima(train, p = 1, d = 0, q = 2)\n#&gt; initial  value -4.507231 \n#&gt; iter   2 value -4.512000\n#&gt; iter   3 value -4.582718\n#&gt; iter   4 value -4.583615\n#&gt; iter   5 value -4.583727\n#&gt; iter   6 value -4.583754\n#&gt; iter   7 value -4.583896\n#&gt; iter   8 value -4.583941\n#&gt; iter   9 value -4.583957\n#&gt; iter  10 value -4.583958\n#&gt; iter  10 value -4.583958\n#&gt; final  value -4.583958 \n#&gt; converged\n#&gt; initial  value -4.586016 \n#&gt; iter   2 value -4.586020\n#&gt; iter   3 value -4.586022\n#&gt; iter   4 value -4.586023\n#&gt; iter   5 value -4.586025\n#&gt; iter   6 value -4.586026\n#&gt; iter   7 value -4.586027\n#&gt; iter   8 value -4.586027\n#&gt; iter   9 value -4.586028\n#&gt; iter   9 value -4.586028\n#&gt; iter   9 value -4.586028\n#&gt; final  value -4.586028 \n#&gt; converged\n\n\n\n\n\n\n\n#&gt; $fit\n#&gt; \n#&gt; Call:\n#&gt; arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n#&gt;     xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n#&gt;     optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1     ma1     ma2   xmean\n#&gt;       0.2324  0.0971  0.1623  0.0084\n#&gt; s.e.  0.2294  0.2247  0.0958  0.0012\n#&gt; \n#&gt; sigma^2 estimated as 0.0001038:  log likelihood = 566.91,  aic = -1123.82\n#&gt; \n#&gt; $degrees_of_freedom\n#&gt; [1] 175\n#&gt; \n#&gt; $ttable\n#&gt;       Estimate     SE t.value p.value\n#&gt; ar1     0.2324 0.2294  1.0130  0.3125\n#&gt; ma1     0.0971 0.2247  0.4319  0.6664\n#&gt; ma2     0.1623 0.0958  1.6946  0.0919\n#&gt; xmean   0.0084 0.0012  6.7491  0.0000\n#&gt; \n#&gt; $AIC\n#&gt; [1] -6.278312\n#&gt; \n#&gt; $AICc\n#&gt; [1] -6.277028\n#&gt; \n#&gt; $BIC\n#&gt; [1] -6.189279"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-2",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-2",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 2",
    "text": "Modelo 2\nEstimo também o modelo ARMA(1, 1). A análise de resíduos é análoga à apresentada acima.\n\nm2 &lt;- arima(train, order = c(1, 0, 1))"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-3",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-3",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 3",
    "text": "Modelo 3\nPara o terceiro modelo uso o método automático do auto.arima(). A função escolhe o AR(1) com constante como melhor modelo para representar os dados. Note que a na inspeação visual também tínhamos verificado que o AR(1) seria um possível candidato.\n\n(m3 &lt;- auto.arima(train))\n#&gt; Series: train \n#&gt; ARIMA(1,0,0) with non-zero mean \n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1    mean\n#&gt;       0.3568  0.0084\n#&gt; s.e.  0.0695  0.0012\n#&gt; \n#&gt; sigma^2 = 0.0001066:  log likelihood = 565.52\n#&gt; AIC=-1125.04   AICc=-1124.91   BIC=-1115.48\n\n\nSeleção\nPara escolher o melhor modelo pode-se usar algum critério de informação. Abaixo comparo os modelos segundo os critérios AIC, AICc (AIC corrigido) e BIC (Bayesian Information Criterion). Na prática, não é comum que os três critérios escolham o mesmo modelo; em particular, o BIC penaliza o número de parâmetros mais fortemente que o AIC. Neste caso específico, os três critérios escolhem o AR(1).\n\n\n\n\n\n\nAIC\nAICc\nBIC\n\n\n\n\nARMA(1, 2)\n-1123.818\n-1124.908\n-1115.483\n\n\nARMA(1, 1)\n-1123.563\n-1124.908\n-1115.483\n\n\nAR(1)\n-1125.045\n-1124.908\n-1115.483\n\n\n\n\n\n\n\n\n\nPrevisão\nPara gerar previsões usamos a função forecast() (outra opção é usar a função base predict()). Abaixo computo previsões para os três modelos acima mais um modelo ingênuo que será usado como bench-mark. O modelo ingênuo é simplesmente um random-walk que prevê sempre o valor anterior, isto é, \\(\\hat{y_{T+1}} = y_{T}\\).\n\nprev1 &lt;- forecast(m1, h = length(teste))\nprev2 &lt;- forecast(m2, h = length(teste))\nprev3 &lt;- forecast(m3, h = length(teste))\nprev4 &lt;- forecast(naive(train, h = length(teste)), h = length(teste))\n\nerros &lt;- t(matrix(c(teste - prev1$mean,\n                    teste - prev2$mean,\n                    teste - prev3$mean,\n                    teste - prev4$mean),\n                    ncol = 4))\nerros &lt;- as.data.frame(erros)\ncolnames(erros) &lt;- paste(\"t =\", as.numeric(time(teste)))\nrow.names(erros) &lt;- c(\"ARMA(1, 2)\",\n                      \"ARMA(1, 1)\",\n                      \"AR(1)\",\n                      \"Y[t+1] = Y[t]\")\n\nOs erros de previsão são apresentados na tabela abaixo.\n\n\n\n\n\n\n\nt = 1992\nt = 1992.25\nt = 1992.5\nt = 1992.75\nt = 1993\nt = 1993.25\nt = 1993.5\nt = 1993.75\nt = 1994\nt = 1994.25\nt = 1994.5\nt = 1994.75\nt = 1995\nt = 1995.25\nt = 1995.5\nt = 1995.75\nt = 1996\nt = 1996.25\nt = 1996.5\nt = 1996.75\nt = 1997\nt = 1997.25\nt = 1997.5\nt = 1997.75\nt = 1998\nt = 1998.25\nt = 1998.5\nt = 1998.75\nt = 1999\nt = 1999.25\nt = 1999.5\nt = 1999.75\nt = 2000\nt = 2000.25\nt = 2000.5\nt = 2000.75\nt = 2001\nt = 2001.25\nt = 2001.5\nt = 2001.75\nt = 2002\nt = 2002.25\nt = 2002.5\n\n\n\n\nARMA(1, 2)\n0.0017385\n0.0009996\n-0.0011832\n0.0047685\n-0.0075880\n-0.0034300\n-0.0032720\n0.0041505\n0.0009645\n0.0048053\n-0.0031499\n0.0038158\n-0.0038534\n-0.0059706\n-0.0026298\n0.0011636\n-0.0008728\n0.0066953\n-0.0041940\n0.0035309\n0.0012488\n0.0064862\n0.0012950\n-0.0021395\n0.0069883\n-0.0032794\n-0.0002956\n0.0083934\n0.0023188\n-0.0031434\n0.0033963\n0.0099058\n-0.0030742\n0.0039295\n-0.0078546\n-0.0045499\n-0.0119065\n-0.0103021\n-0.0110772\n0.0006393\n0.0005900\n-0.0074360\n0.0023173\n\n\nARMA(1, 1)\n0.0015678\n0.0011472\n-0.0010609\n0.0048359\n-0.0075556\n-0.0034158\n-0.0032667\n0.0041516\n0.0009637\n0.0048036\n-0.0031520\n0.0038136\n-0.0038557\n-0.0059730\n-0.0026323\n0.0011611\n-0.0008752\n0.0066928\n-0.0041964\n0.0035284\n0.0012464\n0.0064838\n0.0012925\n-0.0021420\n0.0069858\n-0.0032818\n-0.0002980\n0.0083910\n0.0023164\n-0.0031458\n0.0033939\n0.0099034\n-0.0030766\n0.0039271\n-0.0078570\n-0.0045523\n-0.0119090\n-0.0103046\n-0.0110796\n0.0006369\n0.0005876\n-0.0074384\n0.0023149\n\n\nAR(1)\n0.0013079\n0.0009620\n-0.0011721\n0.0047728\n-0.0075918\n-0.0034379\n-0.0032817\n0.0041400\n0.0009537\n0.0047944\n-0.0031608\n0.0038049\n-0.0038643\n-0.0059815\n-0.0026408\n0.0011526\n-0.0008837\n0.0066843\n-0.0042049\n0.0035199\n0.0012379\n0.0064753\n0.0012840\n-0.0021505\n0.0069773\n-0.0032903\n-0.0003065\n0.0083825\n0.0023079\n-0.0031543\n0.0033854\n0.0098949\n-0.0030851\n0.0039186\n-0.0078655\n-0.0045608\n-0.0119175\n-0.0103131\n-0.0110881\n0.0006284\n0.0005791\n-0.0074469\n0.0023064\n\n\nY[t+1] = Y[t]\n0.0024552\n0.0025186\n0.0005306\n0.0065276\n-0.0058184\n-0.0016579\n-0.0014994\n0.0059232\n0.0027372\n0.0065781\n-0.0013772\n0.0055886\n-0.0020806\n-0.0041978\n-0.0008571\n0.0029363\n0.0009000\n0.0084680\n-0.0024213\n0.0053036\n0.0030215\n0.0082589\n0.0030677\n-0.0003668\n0.0087610\n-0.0015066\n0.0014772\n0.0101662\n0.0040916\n-0.0013707\n0.0051690\n0.0116786\n-0.0013015\n0.0057023\n-0.0060818\n-0.0027771\n-0.0101338\n-0.0085294\n-0.0093044\n0.0024120\n0.0023628\n-0.0056632\n0.0040900\n\n\n\n\n\n\n\nPode-se melhor comparar a performance das previsões usando alguma medida agregada de erro. Duas medidas bastante comuns são o Erro Absoluto Médio (EAM) e o Erro Quadrático Médio. A primeira toma o módulo da diferença entre o previsto (\\(\\hat{y}\\)) e o observado (\\(y\\)) e tira uma média, enquanto a última toma a diferença quadrática. Formalmente, para um horizonte de previsão \\(h\\) começando na última observação \\(T\\):\n\\[\\begin{align}\n\\text{EAM} & = \\frac{1}{h}\\sum_{i = T + 1}^{T + h} |y_{i} - \\hat{y}_{i}| \\\\\n\\text{EQM} & = \\frac{1}{h}\\sum_{i = T + 1}^{T + h} (y_{i} - \\hat{y}_{i})^2\n\\end{align}\\]\nA tabela abaixo compara os modelos segundo estas medidas de erro.\n\n\n\n\n\n\n\nEAM\nEQM\n\n\n\n\nARMA(1, 2)\n0.0042173\n0.0042062\n\n\nARMA(1, 1)\n0.0000268\n0.0000268\n\n\nAR(1)\n0.0042143\n0.0043644\n\n\nY[t+1] = Y[t]\n0.0000268\n0.0000280\n\n\n\n\n\n\n\nNem sempre é fácil comparar estas medidas, i.e., verificar se elas são estatisticamente significantes. Pode-se usar o teste Diebold-Mariano para comparar estas medidas de erro, mas é importante frisar que ele contém uma série de hipóteses sobre a distribuição dos erros. A função dm.test do pacote forecast faz este teste e mais informações sobre ele podem ser encontradas usando ?dm.test.\nPode-se visualizar as previsões usando as funções autoplot() e autolayer() do pacote forecast. Abaixo mostro os resultados para o modelo AR(1) e também para o modelo ingênuo. note como o erros-padrão deste último cresce muito rapidamente (pois a variância de um processo random-walk cresce sem limite).\n\nlibrary(ggplot2)\n\nautoplot(prev2, include = 50) +\n  autolayer(teste) +\n  theme_bw()\n\n\n\n\n\n\n\n\nautoplot(prev3, include = 50) +\n  autolayer(teste) +\n  labs(x = \"\", y = \"(%)\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nautoplot(prev4, include = 50) +\n  autolayer(teste) +\n  labs(x = \"\", y = \"(%)\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html",
    "href": "posts/general-posts/repost-gapminder/index.html",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Muitos já devem estar familiarizados com a apresentação do historiador Hans Rosling sobre a evolução da expectativa de vida e do PIB per capita dos países em torno do mundo. Este post vai mostrar como usar o ggplot2 e o tidyverse para explorar estes dados. Pode-se acessar uma versão simplificada da base de dados pelo pacote gapminder.\n\n# Tutorial Gapminder\n\n# Pacotes\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(kableExtra)\nlibrary(ggrepel)\nlibrary(showtext)\n\n# Carregar fontes\nsysfonts::font_add_google(\"Jost\", \"Jost\")\nshowtext::showtext_auto()\n# Carregar os dados\ndata(\"gapminder\")\ndata(\"continent_colors\")\n\n\n\n\nDe início é sempre importante verificar se há problemas com os dados. Como estamos usando uma base que já foi tratada é de se esperar que tudo esteja em ordem. Tipicamente, queremos verificar quantas observações ausentes (NAs) existem; se as variáveis estão no formato correto (ex: variáveis de texto como factor, números como numeric, etc.). Neste caso, além destas checagens também vamos criar uma nova variável que é o log do PIB per capita.\n\n# Checagens iniciais #\n# Verifica se há valores ausentes\ngapminder %&gt;%\n  summarise(across(everything(), ~sum(is.na(.x))))\n\n# A tibble: 1 × 6\n  country continent  year lifeExp   pop gdpPercap\n    &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1       0         0     0       0     0         0\n\n# Informações gerais sobre os dados\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n# Nome das variáveis minúsculas\nnames(gapminder) &lt;- tolower(names(gapminder))\n# Tranformações #\n# Computa o log do pib per capita\ngapminder &lt;- mutate(gapminder, lgdppc = log10(gdppercap))\n\n\n\n\nPara manter o padrão das visualizações pode-se criar um tema personalizado. A maneira mais simples de fazer isto é a partir de um tema padrão do ggplot2 mas é possível começar do zero. Aqui, por simplicidade, começo com o tema bw e apenas mudo a posição da legenda e o tamanho e a fonte do texto que será plotado nos eixos e no título do gráfico. Além disso, como o nome dos eixos vai ser repetido muitas vezes defino uma lista com o nome mais comum deles.\n\n# Tema customizado #\ntheme_vini &lt;- theme_bw() +\n  theme(\n    text = element_text(family = \"Jost\", size = 12, colour = \"gray20\"),\n    plot.title = element_text(size = 16),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n    )\n\n# Nomes que serão usados várias vezes para os eixos\nnomes &lt;- list(\n  title = \"Expectativa de vida e PIB per capita\",\n    x = \"Log do PIB per capita (US$ 2010)\",\n    y = \"Expectativa de vida ao nascer\",\n    fonte = \"Fonte: Gapmineder (www.gapminder.org) e World Bank Open Data.\"\n  )\n\n\n\n\n\n\n\nNote que não temos dados para todos os anos do período. Os dados estão disponíveis de cinco em cinco anos começando em 1952 e terminando em 2007. Podemos começar com um gráfico de dispersão para ver a relação entre a “economia” (PIB per capita) e a “qualidade da saúde/vida” (expectativa de vida ao nascer) de um país. Apenas como exemplo incluo também uma linha de regressão quadrática no gráfico.\n\nunique(gapminder$year)\n\n [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\n\n\n\ngap2007 &lt;- filter(gapminder, year == 2007)\n\nggplot(gap2007, aes(lgdppc, lifeexp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    subtitle = \"Relação entre expectativa de vida ao nascer e o logaritmo do PIB per capita (fit linear)\",\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nPodemos nos valer de outras informações disponíveis na base para alterar atributos estéticos do gráfico. O tamanho de cada círculo pode refletir o tamanho da população daquele país; a cor do círculo, por sua vez, pode representar o continente daquele país. Por conveniência uso as cores pré-definidas do pacote gapminder. Além disso também podemos destacar o nome de alguns países usando o pacote ggrepel.\n\ndestaque &lt;- c(\n  \"Australia\", \"Argentina\", \"Brazil\", \"Chile\", \"India\", \"Nigeria\", \n  \"Sudan\", \"Taiwan\", \"Mozambique\", \"Angola\", \"Vietnam\"\n  )\n\ngap_highlight &lt;- gap2007 %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  mutate(sel = ifelse(country %in% destaque, country, \"\"))\n\nggplot(gap_highlight, aes(lgdppc, lifeexp)) +\n  geom_point(aes(size = pop, colour = continent), alpha = .75) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_text_repel(\n    aes(label = sel),\n    family = \"Jost\",\n    force = 20,\n    max.overlaps = 30,\n    size = 5\n    ) + \n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  scale_color_manual(values = continent_colors, name = \"\") +\n  scale_size_continuous(range = c(1, 20)) +\n  guides(size = FALSE) + \n  theme_vini\n\n\n\n\n\n\n\n\nO gráfico acima é um retrato do momento, mas pode ser interessante entender como estas variáveis se comportaram ao longo do tempo.\n\n\n\n\n# Calcula a expectativa de vida média\ngap_life &lt;- gapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(media = mean(lifeexp))\n\nggplot(gap_life, aes(year, media)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = paste(nomes$title, \"(média mundial)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos desagregar a análise acima por país. É claro que fica difícil discernir um país específico, mas pode-se ver uma tendência geral de crescimento, ainda que haja alguns outliers. A maior parte das quedas significativas pode ser relacionada com alguma guerra. O país cuja expectativa de vida cai bruscamente no começo dos anos 90, por exemplo, é a Ruanda, que vivia uma guerra civil nesta época.\n\nggplot(gapminder, aes(year, lifeexp, group = country, colour = continent)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 30, to = 80, by = 10)) +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nSe nos atermos somente ao nível de continente a visualização fica mais simples.\n\ngap_life_continent &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(expec_media = mean(lifeexp))\n\nggplot(gap_life_continent, aes(year, expec_media, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida média (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para o log do PIB per capita. Aqui o gráfico é feito usando o log do PIB per capita, mas no eixo indico o valor equivalente em dólares para facilitar a interpretação.\n\n# Calcula o PIB per capita médio por continente a cada ano\ngap_gdp &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(avg_gdp = mean(gdppercap))\n\nggplot(gap_gdp, aes(year, avg_gdp, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  # Escala log\n  scale_y_log10() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"PIB per capita médio (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$x\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nAntes de começar a análise tabular dos dados vale definir uma função simples para apresentar as tabelas.\n\nprint_table &lt;- function(df, ...) {\n  \n  str_title &lt;- function(string) {\n    \n    x &lt;- stringr::str_replace_all(string, \"_\", \" \")\n    x &lt;- stringr::str_to_title(x)\n    return(x)\n    \n  }\n  \n  knitr::kable(df, col.names = str_title(colnames(df)), ...) %&gt;%\n    kableExtra::kable_styling(\n      full_width = FALSE,\n      bootstrap_options = c(\"hover\", \"condensed\")\n    )\n  \n}\n\nPodemos encontrar fatos interessantes simplesmente agregando e reorganizando os dados. No gráfico anterior vimos que o continente com maior PIB per capita médio é a Oceania. Curiosamente, a base inclui somente dois países na Oceania: Austrália e Nove Zelândia.\n\n# Note que na Oceania os dados só incluem Autrália e Nova Zelândia\ngapminder %&gt;%\n  filter(continent == \"Oceania\") %&gt;%\n  count(country) %&gt;%\n  print_table()\n\n\n\n\nCountry\nN\n\n\n\n\nAustralia\n12\n\n\nNew Zealand\n12\n\n\n\n\n\n\n\nPodemos encontrar os países que mais cresceram (em termos absolutos e relativos) durante o período observado. Note como há vários países asiáticos listados qual seja a métrica escolhida.\n\n# Países que mais cresceram no período da amostra em termos absolutos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = last(gdppercap) - first(gdppercap)) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(caption = \"Países que mais cresceram de 1952 a 2007\")\n\n\nPaíses que mais cresceram de 1952 a 2007\n\n\nCountry\nGrowth\n\n\n\n\nSingapore\n44828.04\n\n\nNorway\n39261.77\n\n\nHong Kong, China\n36670.56\n\n\nIreland\n35465.72\n\n\nAustria\n29989.42\n\n\nUnited States\n28961.17\n\n\nIceland\n28913.10\n\n\nJapan\n28439.11\n\n\nNetherlands\n27856.36\n\n\nTaiwan\n27511.33\n\n\n\n\n\n\n\n\n# Países que mais cresceram no período da amostra em termos relativos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = (last(gdppercap) / first(gdppercap) - 1) * 100) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(\n    caption = \"Países que mais cresceram de 1952 a 2007 (%)\",\n    digits = 2\n    )\n\n\nPaíses que mais cresceram de 1952 a 2007 (%)\n\n\nCountry\nGrowth\n\n\n\n\nEquatorial Guinea\n3135.54\n\n\nTaiwan\n2279.41\n\n\nKorea, Rep.\n2165.51\n\n\nSingapore\n1936.30\n\n\nBotswana\n1376.65\n\n\nHong Kong, China\n1200.57\n\n\nChina\n1138.39\n\n\nOman\n1120.64\n\n\nThailand\n884.22\n\n\nJapan\n884.04\n\n\n\n\n\n\n\nA mesma análise também pode ser feita para a expectativa de vida. Adicionalmente também podemos encontrar qual foi a maior variação (negativa) entre um ponto observado e outro dentro da amostra.\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = last(lifeexp) - first(lifeexp)) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 0)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n38\n\n\nVietnam\n34\n\n\nIndonesia\n33\n\n\nSaudi Arabia\n33\n\n\nLibya\n31\n\n\nKorea, Rep.\n31\n\n\nNicaragua\n31\n\n\nWest Bank and Gaza\n30\n\n\nYemen, Rep.\n30\n\n\nGambia\n29\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = (last(lifeexp) / first(lifeexp) - 1) * 100) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 2)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n101.29\n\n\nGambia\n98.16\n\n\nYemen, Rep.\n92.63\n\n\nIndonesia\n88.56\n\n\nVietnam\n83.73\n\n\nSaudi Arabia\n82.51\n\n\nNepal\n76.41\n\n\nIndia\n73.11\n\n\nLibya\n73.10\n\n\nNicaragua\n72.28\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  mutate(\n    life_diff = lifeexp - lag(lifeexp),\n    life_abs  = abs(lifeexp - lag(lifeexp))) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(life_abs)) %&gt;%\n  select(country, year, life_diff) %&gt;%\n  slice(1:10) %&gt;%\n  print_table(digits = 1)\n\n\n\n\nCountry\nYear\nLife Diff\n\n\n\n\nRwanda\n1992\n-20.4\n\n\nCambodia\n1982\n19.7\n\n\nChina\n1967\n13.9\n\n\nZimbabwe\n1997\n-13.6\n\n\nRwanda\n1997\n12.5\n\n\nLesotho\n2002\n-11.0\n\n\nSwaziland\n2002\n-10.4\n\n\nBotswana\n1997\n-10.2\n\n\nCambodia\n1977\n-9.1\n\n\nNamibia\n2002\n-7.4"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#preparativos",
    "href": "posts/general-posts/repost-gapminder/index.html#preparativos",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Muitos já devem estar familiarizados com a apresentação do historiador Hans Rosling sobre a evolução da expectativa de vida e do PIB per capita dos países em torno do mundo. Este post vai mostrar como usar o ggplot2 e o tidyverse para explorar estes dados. Pode-se acessar uma versão simplificada da base de dados pelo pacote gapminder.\n\n# Tutorial Gapminder\n\n# Pacotes\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(kableExtra)\nlibrary(ggrepel)\nlibrary(showtext)\n\n# Carregar fontes\nsysfonts::font_add_google(\"Jost\", \"Jost\")\nshowtext::showtext_auto()\n# Carregar os dados\ndata(\"gapminder\")\ndata(\"continent_colors\")\n\n\n\n\nDe início é sempre importante verificar se há problemas com os dados. Como estamos usando uma base que já foi tratada é de se esperar que tudo esteja em ordem. Tipicamente, queremos verificar quantas observações ausentes (NAs) existem; se as variáveis estão no formato correto (ex: variáveis de texto como factor, números como numeric, etc.). Neste caso, além destas checagens também vamos criar uma nova variável que é o log do PIB per capita.\n\n# Checagens iniciais #\n# Verifica se há valores ausentes\ngapminder %&gt;%\n  summarise(across(everything(), ~sum(is.na(.x))))\n\n# A tibble: 1 × 6\n  country continent  year lifeExp   pop gdpPercap\n    &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1       0         0     0       0     0         0\n\n# Informações gerais sobre os dados\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n# Nome das variáveis minúsculas\nnames(gapminder) &lt;- tolower(names(gapminder))\n# Tranformações #\n# Computa o log do pib per capita\ngapminder &lt;- mutate(gapminder, lgdppc = log10(gdppercap))\n\n\n\n\nPara manter o padrão das visualizações pode-se criar um tema personalizado. A maneira mais simples de fazer isto é a partir de um tema padrão do ggplot2 mas é possível começar do zero. Aqui, por simplicidade, começo com o tema bw e apenas mudo a posição da legenda e o tamanho e a fonte do texto que será plotado nos eixos e no título do gráfico. Além disso, como o nome dos eixos vai ser repetido muitas vezes defino uma lista com o nome mais comum deles.\n\n# Tema customizado #\ntheme_vini &lt;- theme_bw() +\n  theme(\n    text = element_text(family = \"Jost\", size = 12, colour = \"gray20\"),\n    plot.title = element_text(size = 16),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n    )\n\n# Nomes que serão usados várias vezes para os eixos\nnomes &lt;- list(\n  title = \"Expectativa de vida e PIB per capita\",\n    x = \"Log do PIB per capita (US$ 2010)\",\n    y = \"Expectativa de vida ao nascer\",\n    fonte = \"Fonte: Gapmineder (www.gapminder.org) e World Bank Open Data.\"\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#visualizando",
    "href": "posts/general-posts/repost-gapminder/index.html#visualizando",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Note que não temos dados para todos os anos do período. Os dados estão disponíveis de cinco em cinco anos começando em 1952 e terminando em 2007. Podemos começar com um gráfico de dispersão para ver a relação entre a “economia” (PIB per capita) e a “qualidade da saúde/vida” (expectativa de vida ao nascer) de um país. Apenas como exemplo incluo também uma linha de regressão quadrática no gráfico.\n\nunique(gapminder$year)\n\n [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\n\n\n\ngap2007 &lt;- filter(gapminder, year == 2007)\n\nggplot(gap2007, aes(lgdppc, lifeexp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    subtitle = \"Relação entre expectativa de vida ao nascer e o logaritmo do PIB per capita (fit linear)\",\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nPodemos nos valer de outras informações disponíveis na base para alterar atributos estéticos do gráfico. O tamanho de cada círculo pode refletir o tamanho da população daquele país; a cor do círculo, por sua vez, pode representar o continente daquele país. Por conveniência uso as cores pré-definidas do pacote gapminder. Além disso também podemos destacar o nome de alguns países usando o pacote ggrepel.\n\ndestaque &lt;- c(\n  \"Australia\", \"Argentina\", \"Brazil\", \"Chile\", \"India\", \"Nigeria\", \n  \"Sudan\", \"Taiwan\", \"Mozambique\", \"Angola\", \"Vietnam\"\n  )\n\ngap_highlight &lt;- gap2007 %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  mutate(sel = ifelse(country %in% destaque, country, \"\"))\n\nggplot(gap_highlight, aes(lgdppc, lifeexp)) +\n  geom_point(aes(size = pop, colour = continent), alpha = .75) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_text_repel(\n    aes(label = sel),\n    family = \"Jost\",\n    force = 20,\n    max.overlaps = 30,\n    size = 5\n    ) + \n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  scale_color_manual(values = continent_colors, name = \"\") +\n  scale_size_continuous(range = c(1, 20)) +\n  guides(size = FALSE) + \n  theme_vini\n\n\n\n\n\n\n\n\nO gráfico acima é um retrato do momento, mas pode ser interessante entender como estas variáveis se comportaram ao longo do tempo.\n\n\n\n\n# Calcula a expectativa de vida média\ngap_life &lt;- gapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(media = mean(lifeexp))\n\nggplot(gap_life, aes(year, media)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = paste(nomes$title, \"(média mundial)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos desagregar a análise acima por país. É claro que fica difícil discernir um país específico, mas pode-se ver uma tendência geral de crescimento, ainda que haja alguns outliers. A maior parte das quedas significativas pode ser relacionada com alguma guerra. O país cuja expectativa de vida cai bruscamente no começo dos anos 90, por exemplo, é a Ruanda, que vivia uma guerra civil nesta época.\n\nggplot(gapminder, aes(year, lifeexp, group = country, colour = continent)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 30, to = 80, by = 10)) +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nSe nos atermos somente ao nível de continente a visualização fica mais simples.\n\ngap_life_continent &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(expec_media = mean(lifeexp))\n\nggplot(gap_life_continent, aes(year, expec_media, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida média (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para o log do PIB per capita. Aqui o gráfico é feito usando o log do PIB per capita, mas no eixo indico o valor equivalente em dólares para facilitar a interpretação.\n\n# Calcula o PIB per capita médio por continente a cada ano\ngap_gdp &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(avg_gdp = mean(gdppercap))\n\nggplot(gap_gdp, aes(year, avg_gdp, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  # Escala log\n  scale_y_log10() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"PIB per capita médio (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$x\n    ) +\n  theme_vini"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#analisando-os-dados",
    "href": "posts/general-posts/repost-gapminder/index.html#analisando-os-dados",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Antes de começar a análise tabular dos dados vale definir uma função simples para apresentar as tabelas.\n\nprint_table &lt;- function(df, ...) {\n  \n  str_title &lt;- function(string) {\n    \n    x &lt;- stringr::str_replace_all(string, \"_\", \" \")\n    x &lt;- stringr::str_to_title(x)\n    return(x)\n    \n  }\n  \n  knitr::kable(df, col.names = str_title(colnames(df)), ...) %&gt;%\n    kableExtra::kable_styling(\n      full_width = FALSE,\n      bootstrap_options = c(\"hover\", \"condensed\")\n    )\n  \n}\n\nPodemos encontrar fatos interessantes simplesmente agregando e reorganizando os dados. No gráfico anterior vimos que o continente com maior PIB per capita médio é a Oceania. Curiosamente, a base inclui somente dois países na Oceania: Austrália e Nove Zelândia.\n\n# Note que na Oceania os dados só incluem Autrália e Nova Zelândia\ngapminder %&gt;%\n  filter(continent == \"Oceania\") %&gt;%\n  count(country) %&gt;%\n  print_table()\n\n\n\n\nCountry\nN\n\n\n\n\nAustralia\n12\n\n\nNew Zealand\n12\n\n\n\n\n\n\n\nPodemos encontrar os países que mais cresceram (em termos absolutos e relativos) durante o período observado. Note como há vários países asiáticos listados qual seja a métrica escolhida.\n\n# Países que mais cresceram no período da amostra em termos absolutos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = last(gdppercap) - first(gdppercap)) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(caption = \"Países que mais cresceram de 1952 a 2007\")\n\n\nPaíses que mais cresceram de 1952 a 2007\n\n\nCountry\nGrowth\n\n\n\n\nSingapore\n44828.04\n\n\nNorway\n39261.77\n\n\nHong Kong, China\n36670.56\n\n\nIreland\n35465.72\n\n\nAustria\n29989.42\n\n\nUnited States\n28961.17\n\n\nIceland\n28913.10\n\n\nJapan\n28439.11\n\n\nNetherlands\n27856.36\n\n\nTaiwan\n27511.33\n\n\n\n\n\n\n\n\n# Países que mais cresceram no período da amostra em termos relativos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = (last(gdppercap) / first(gdppercap) - 1) * 100) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(\n    caption = \"Países que mais cresceram de 1952 a 2007 (%)\",\n    digits = 2\n    )\n\n\nPaíses que mais cresceram de 1952 a 2007 (%)\n\n\nCountry\nGrowth\n\n\n\n\nEquatorial Guinea\n3135.54\n\n\nTaiwan\n2279.41\n\n\nKorea, Rep.\n2165.51\n\n\nSingapore\n1936.30\n\n\nBotswana\n1376.65\n\n\nHong Kong, China\n1200.57\n\n\nChina\n1138.39\n\n\nOman\n1120.64\n\n\nThailand\n884.22\n\n\nJapan\n884.04\n\n\n\n\n\n\n\nA mesma análise também pode ser feita para a expectativa de vida. Adicionalmente também podemos encontrar qual foi a maior variação (negativa) entre um ponto observado e outro dentro da amostra.\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = last(lifeexp) - first(lifeexp)) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 0)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n38\n\n\nVietnam\n34\n\n\nIndonesia\n33\n\n\nSaudi Arabia\n33\n\n\nLibya\n31\n\n\nKorea, Rep.\n31\n\n\nNicaragua\n31\n\n\nWest Bank and Gaza\n30\n\n\nYemen, Rep.\n30\n\n\nGambia\n29\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = (last(lifeexp) / first(lifeexp) - 1) * 100) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 2)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n101.29\n\n\nGambia\n98.16\n\n\nYemen, Rep.\n92.63\n\n\nIndonesia\n88.56\n\n\nVietnam\n83.73\n\n\nSaudi Arabia\n82.51\n\n\nNepal\n76.41\n\n\nIndia\n73.11\n\n\nLibya\n73.10\n\n\nNicaragua\n72.28\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  mutate(\n    life_diff = lifeexp - lag(lifeexp),\n    life_abs  = abs(lifeexp - lag(lifeexp))) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(life_abs)) %&gt;%\n  select(country, year, life_diff) %&gt;%\n  slice(1:10) %&gt;%\n  print_table(digits = 1)\n\n\n\n\nCountry\nYear\nLife Diff\n\n\n\n\nRwanda\n1992\n-20.4\n\n\nCambodia\n1982\n19.7\n\n\nChina\n1967\n13.9\n\n\nZimbabwe\n1997\n-13.6\n\n\nRwanda\n1997\n12.5\n\n\nLesotho\n2002\n-11.0\n\n\nSwaziland\n2002\n-10.4\n\n\nBotswana\n1997\n-10.2\n\n\nCambodia\n1977\n-9.1\n\n\nNamibia\n2002\n-7.4"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html",
    "href": "posts/general-posts/emv-no-r/index.html",
    "title": "EMV no R",
    "section": "",
    "text": "A estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é consistente (converge para o valor verdadeiro), normalmente assintótico (distribuição assintórica segue uma normal padrão) e eficiente (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.\nA intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros \\(\\mu\\) e \\(\\sigma\\) de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido “gerada” por uma normal).\nHá vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no R. Neste post vou me ater apenas a dois pacotes: o optimx e o maxLik. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.\nVale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função optim do R. Os dois pacotes simplesmente trazem algumas comodidades.\n\nlibrary(maxLik)\nlibrary(optimx)\n# Para reproduzir os resultados\nset.seed(33)"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-optimx",
    "href": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-optimx",
    "title": "EMV no R",
    "section": "Usando o pacote optimx",
    "text": "Usando o pacote optimx\nA função optim já é bastante antiga e um novo pacote, chamado optimx, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são optimx e optimr. Mais informações sobre o pacote podem ser encontradas aqui.\nA sintaxe das funções é muito similar à sintaxe original do optim. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead\n\nsummary(fit &lt;- optimx(par = 1, fn = ll_pois, x = amostra))\n\n                p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nNelder-Mead 4.9375 2202.837     32     NA    NA        0   NA   NA 0.001\nBFGS        4.9380 2202.837     34      9    NA        0   NA   NA 0.003\n\n\nUma das principais vantagens do optimx é a possibilidade de usar vários métodos de otimização numérica numa mesma função.\n\nfit &lt;- optimx(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n  method = c(\"nlm\", \"BFGS\", \"Rcgmin\", \"nlminb\")\n  )\n\nfit\n\n             p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nnlm    4.937998 2202.837     NA     NA     8        0   NA   NA 0.001\nBFGS   4.938000 2202.837     34      9    NA        0   NA   NA 0.002\nRcgmin 4.938000 2202.837     15     10    NA        0   NA   NA 0.002\nnlminb 4.938000 2202.837     10     12     9        0   NA   NA 0.002\n\n\nComo este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos."
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-maxlik",
    "href": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-maxlik",
    "title": "EMV no R",
    "section": "Usando o pacote maxLik",
    "text": "Usando o pacote maxLik\nA função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, não é preciso montar a função com sinal de menos como fizemos acima; segundo, ela já calcula erros-padrão e estatísticas-t dos coeficientes estimados. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas aqui.\nPara usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o dpois que implementa a função densidade da Poisson. O argumento log = TRUE retorna as probabilidades \\(p\\) como \\(log(p)\\).\n\nll_pois &lt;- function(x, theta) {\n    ll &lt;- dpois(x, theta, log = TRUE)\n    return(sum(ll))\n}\n\nO comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.\n\nsummary(fit &lt;- maxLik(ll_pois, start = 1, x = amostra))\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 8 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07617   64.83  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função optimx. Derivando a função de log-verossimilhança:\n\\[\n\\begin{align}\n  \\frac{\\partial \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda} & = \\frac{1}{\\lambda}\\sum_{k = 1}^{n}x_{k} - n \\\\\n  \\frac{\\partial^2 \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda^2} & = -\\frac{1}{\\lambda^2}\\sum_{k = 1}^{n}x_{k}\n\\end{align}\n\\]\nO código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.\n\ngrad_pois &lt;- function(x, theta) {\n  (1 / theta) * sum(x) - length(x)\n  }\n\nhess_pois &lt;- function(x, theta) {\n    -(1 / theta^2) * sum(x)\n}\n\nfit2 &lt;- maxLik(\n  ll_pois,\n  grad = grad_pois,\n  hess = hess_pois,\n  start = 1,\n  x = amostra\n  )\n\nsummary(fit2)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 7 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07027   70.27  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#consistência",
    "href": "posts/general-posts/emv-no-r/index.html#consistência",
    "title": "EMV no R",
    "section": "Consistência",
    "text": "Consistência\nVamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição \\(N(2, 3)\\); computamos as estimativas para \\(\\mu\\) e \\(\\sigma\\) e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.\n\nSimular uma amostra segundo uma distribuição.\nEstimata os parâmetros da distribuição.\nCalcula a variância assintótica dos estimadores.\nRepete 5000 vezes os passos 1-3.\n\nO código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.\n\nr &lt;- 5000\nn &lt;- 1000\n\nestimativas &lt;- matrix(ncol = 4, nrow = r)\n\nfor(i in 1:r) {\n    x &lt;- rnorm(n = n, mean = 2, sd = 3)\n    \n    fit &lt;- optimr(\n      par = c(1, 1),\n      fn = ll_norm,\n      method = \"BFGS\",\n      hessian = TRUE\n      )\n    # Guarda o valor estimado do parâmetro\n    estimativas[i, 1:2] &lt;- fit$par\n    estimativas[i, 3:4] &lt;- diag(n * solve(fit$hess))\n}\n\nA consistência dos estimadores \\(\\hat{\\theta}_{MV}\\) significa que eles aproximam os valores verdadeiros do parâmetros \\(\\theta_{0}\\) à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande \\(\\mathbb{N} \\to \\infty\\) então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros \\(\\hat{\\theta}_{\\text{MV}} \\to \\theta_{0}\\)\nO código abaixo calcula a média das estimativas para cada parâmetro - lembrando que \\(\\mu_{0} = 2\\) e que \\(\\sigma_{0} = 3\\). Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).\n\n# | fig-width: 10\npar(mfrow = c(1, 2))\n# Consistência dos estimadores de MV\nmu &lt;- estimativas[, 1]; sigma &lt;- estimativas[, 2]\nmean(mu)\n\n[1] 2.000883\n\nmean(sigma)\n\n[1] 2.997335\n\nhist(mu, main = bquote(\"Estimativas para \"~~mu), freq = FALSE, xlim = c(1.5, 2.5))\nabline(v = 2, col = \"indianred\")\nhist(sigma, main = bquote(\"Estimativas para \"~~sigma), freq = FALSE, xlim = c(2.7, 3.3))\nabline(v = 3, col = \"indianred\")"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#normalmente-assintótico",
    "href": "posts/general-posts/emv-no-r/index.html#normalmente-assintótico",
    "title": "EMV no R",
    "section": "Normalmente assintótico",
    "text": "Normalmente assintótico\nDizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:\n\\[\nz_{\\theta} = \\sqrt{N}\\frac{\\hat{\\theta}_{MV} - \\theta}{\\sqrt{\\text{V}_{ast}}} \\to \\mathbb{N}(0, 1)\n\\]\nonde \\(\\text{V}_{ast}\\) é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\nNo loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\n# Normalidade assintótica\n\n# Define objetos para facilitar a compreensão\nmu_hat &lt;- estimativas[, 1]\nsigma_hat &lt;- estimativas[, 2]\nvar_mu_hat &lt;- estimativas[, 3]\nvar_sg_hat &lt;- estimativas[, 4]\n\n# Centra a estimativa\nmu_centrado &lt;- mu_hat - 2 \nsigma_centrado &lt;- sigma_hat - 3\n# Computa z_mu z_sigma\nmu_normalizado &lt;- sqrt(n) * mu_centrado / sqrt(var_mu_hat)\nsigma_normalizado &lt;- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)\n\n\n# Monta o gráfico para mu\n\n# Eixo x\ngrid_x &lt;- seq(-3, 3, 0.01)\n\nhist(\n  mu_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[mu]),\n  freq = FALSE,\n  xlim = c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[mu]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n\n\n\n\n\n\n\n\n\n# Monta o gráfico para sigma2\nhist(\n  sigma_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[sigma]),\n  freq = FALSE,\n  xlim =c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[sigma]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#escolha-de-valores-inicias",
    "href": "posts/general-posts/emv-no-r/index.html#escolha-de-valores-inicias",
    "title": "EMV no R",
    "section": "Escolha de valores inicias",
    "text": "Escolha de valores inicias\nComo comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.\nO exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.\n\n# sensível a escolha de valores inicias\nx &lt;- rnorm(n = 1000, mean = 15, sd = 4)\nfit &lt;- optim(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n\n$par\n[1] 618.6792 962.0739\n\n$value\n[1] 7984.993\n\n$counts\nfunction gradient \n     107      100 \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]\n[1,]  0.001070703 -0.0013531007\n[2,] -0.001353101  0.0001884928\n\n\nNote que as estimativas estão muito distantes dos valores corretos \\(\\mu = 15\\) e \\(\\sigma = 4\\). Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.\nO código abaixo usa os momentos empíricos como valores inicias para \\(\\mu\\) e \\(\\sigma\\):\n\\[\n\\begin{align}\n  \\mu_{inicial} & = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\\\\n  \\sigma_{inicial} & = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} (x_{i} - \\mu_{inicial})^2}\n\\end{align}\n\\]\n\n(chute_inicial &lt;- c(mean(x), sqrt(var(x))))\n\n[1] 14.859702  3.930849\n\n(est &lt;- optimx(par = chute_inicial, fn = ll_norm))\n\n                  p1       p2    value fevals gevals niter convcode kkt1 kkt2\nNelder-Mead 14.85997 3.929097 2787.294     47     NA    NA        0 TRUE TRUE\nBFGS        14.85970 3.928884 2787.294     15      2    NA        0 TRUE TRUE\n            xtime\nNelder-Mead 0.001\nBFGS        0.001\n\n\nAgora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função optimx se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.\nNote como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método bobyqa retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos nlminb e Nelder-Mead ambos retornam os valores corretos.\n\n# Usando outros métodos numéricos\noptimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = c(\"BFGS\", \"Nelder-Mead\", \"CG\", \"nlminb\", \"bobyqa\")\n  )\n\n                   p1         p2    value fevals gevals niter convcode  kkt1\nBFGS        618.67917 962.073907 7984.993    107    100    NA        1  TRUE\nNelder-Mead  14.85571   3.929621 2787.294     83     NA    NA        0  TRUE\nCG           46.43586 628.570987 7358.601    204    101    NA        1  TRUE\nnlminb       14.85970   3.928883 2787.294     23     47    19        0  TRUE\nbobyqa       15.20011   8.993240 3211.556    109     NA    NA        0 FALSE\n             kkt2 xtime\nBFGS        FALSE 0.006\nNelder-Mead  TRUE 0.001\nCG          FALSE 0.008\nnlminb       TRUE 0.001\nbobyqa      FALSE 0.036\n\n\nVale notar também alguns detalhes técnicos da saída. Em particular, convcode == 0 significa que o otimizador conseguiu convergir com sucesso, enquanto convcode == 1 indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.\nJá o kkt1 e kkt2 verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte necessária do teorema enquanto a segunda condição verifica a parte suficiente. Note que o bobyqa falha em ambas as condições (pois ele não é feito para este tipo de problema).\nOs métodos que retornam os melhores valores, o Nelder-Mead e nlminb são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.\nMais detalhes sobre os métodos podem ser encontrados na página de ajuda da função ?optimx."
  },
  {
    "objectID": "posts/shiny-apps/ifdm.html",
    "href": "posts/shiny-apps/ifdm.html",
    "title": "IDH dos municípios do Brasil",
    "section": "",
    "text": "Sobre o aplicativo\nLink para o aplicativo\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\nEste aplicativo foi construído no R usando {shiny} e {shinydashboardplus}. Para acessar o código do aplicativo confira o repositório no GitHub.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. Os quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\n\n\n\n\n\nA lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque. O dashboard também mostra a evolução dos indicadores de desenvolvimento humano ao longo do tempo. Nesta comparação fica evidente o impacto da Crise de 2014-16.\n\n\n\n\n\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "ggplot2-tutorial.html",
    "href": "ggplot2-tutorial.html",
    "title": "ggplot2: Do básico ao intermediário",
    "section": "",
    "text": "Introdução\n\n\n\n\n\nComeçe por aqui\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nApêndice: manipular para enxergar\n\n\n\n\n\nEste post revisa as principais funções para manipulação de dados do Tidyverse. O material serve mais como referência e apoio ao tutorial de ggplot2 do que como introdução ao assunto.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: gráfico de dispersão\n\n\n\n\n\nO gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente a correlação entre as duas variáveis escolhidas.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: gráfico de coluna\n\n\n\n\n\nUm gráfico do colunas é uma ferramenta de visualização poderosa e versátil para visualizar a diferença de valores entre classes e também a evolução de valores ao longo do tempo.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: histograma\n\n\n\n\n\nUm histograma serve para visualizar a distribuição de um conjunto de dados. É uma visualização estatística poderosa para entender o comportamento dos dados\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: gráfico de linha\n\n\n\n\n\nGráficos de linha são frequentemente usados para representar séries de tempo, isto é, valores que mudam ao longo do tempo. Estes gráficos revelam a evolução de uma variável ao longo do tempo.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEstético: Destacando informação\n\n\n\n\n\nUm gráfico deve ser autoexplicativo. Neste post, discutiremos três estratégias simples para realçar informações em um gráfico: usar linhas com geom_vline(), geom_hline() e geom_abline() para destacar eixos ou informações numéricas; realçar seções do gráfico com geom_rect(); e destacar informações numéricas e texto usando geom_text() e annotate().\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEstético: Escalas e Cores\n\n\n\n\n\nEscalas, legendas e cores são elementos essenciais numa boa visualização. Este post apresenta a lógica das funções que controlam as escalas do gráfico e as suas cores com diversos exemplos.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEstético: Tipografia e temas\n\n\n\n\n\nEste post encerra a discussão de elementos ‘estéticos’ de gráficos. Primeiro apresento, brevemente, uma discussão sobre tipografias e como utilizar fontes em gráficos de ggplot2. Depois, entro numa discussão mais detalhada sobre a função theme, que controla todos os aspectos ‘temáticos’ do gráfico\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIndo Além: Lollipops\n\n\n\n\n\nPost intermediário que ensina a fazer gráficos de lollipop no R usando o pacote ggplot2. Os gráficos de lollipop consistem de barras com círculos no topo, que representam os valores das observações. Eles são utilizados tanto para substituir gráficos de coluna convencionais, como para destacar e comparar valores entre diferentes categorias ou momentos no tempo.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIndo além: facets\n\n\n\n\n\nFacets são pequenos gráficos que, lado a lado, ajudam a comparar várias informações ao mesmo tempo. Este post intermediário ensina a fazer gráficos de facets no R usando o ggplot2.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIndo além: empilhando áreas\n\n\n\n\n\nPost intermediário que ensina a fazer gráficos de área no R usando o pacote ggplot2.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIndo além: mapas de clusters\n\n\n\n\n\nMapas de calor ou de clusters apresentam a variação de uma variável num plano bidemensional para sugerir padrões, tendências, ou mesmo para visualizar a evolução de uma variável num grupo de classes. Este post intermediário ensina a fazer este gráfico no R.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nYou need a map - Parte 1\n\n\n\n\n\nO ggplot2 tem uma ótima interface para a produção de mapas de alta qualidade. Neste primeiro post, mostro como fazer mapas usando os conhecimentos adquiridos até aqui sem a necessidade de se preocupar com objetos geométricos.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nYou need a map - Parte 2\n\n\n\n\n\nNeste segundo post mostro o básico de como trabalhar com objetos espaciais e como montar mapas coropléticos. Estes mapas apresentam a distribuição espacial de uma variável numérica e são extremamente úteis. Apresento também alguns mapas temáticas como o mapa de quebras naturais e o mapa de desvios-padrão.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "",
    "text": "A taxa de juros é talvez a variável macroeconômica mais importante para se observar quando se pensa em financiamento imobiliário. Quanto maior for a taxa de juros, mais “caro” fica o financiamento habitacional. Ou seja, mais difícil fica de comprar um imóvel.\nO financiamento imobiliário nada mais é do que um empréstimo que uma família contrai com o sistema financeiro (um banco); este empréstimo é uma dívida que a família deve repagar em parcelas mensais e sobre cada pagamento incide um valor de juros.\n\n\nA taxa de juros tem um efeito geral sobre a economia, mas o seu efeito é mais notável para o consumidor na hora de fazer compras grandes, de longo prazo: caso de um automóvel ou de um imóvel. Uma taxa menor significa que fica mais “barato” tomar crédito, enquanto uma taxa maior significa o contrário.\nQuem define a taxa de juros “geral” da economia, a taxa SELIC, é o Cômite de Política Monetária (COPOM). A SELIC é, na verdade, uma meta de taxa de juros, que o Banco Central do Brasil (BCB) deve perseguir. O COPOM se reúne periodicamente para definir a taxa SELIC; na ocasião mais recente, no início de agosto, decidiu-se reduzir a taxa de 13,75% a.a. para 13,25% a.a. - a primeira queda depois de um ciclo de alta de 3 anos.\nTipicamente, a taxa de juros é a ferramenta de política monetária que se usa para controlar a inflação: quando a taxa de inflação aumenta muito, o COPOM decide aumentar a taxa de juros. Foi isto o que aconteceu em 2014-16 e também em 2021-23.\n\n\n\n\n\n\n\n\n\nO gráfico acima mostra as oscilações recentes da SELIC. No final de 2016 encerrou-se um longo ciclo de alta que começou em reação à escalada da inflação em 2014. A taxa, então, chegou a cair até 2% a.a. no final de 2020, acompanhando a tendência internacional de taxas de juros baixíssimas, muito próximas de 0% a.a. A volta da inflação com a pandemia levou o COPOM a agressivamente aumentar a taxa SELIC nos meses seguintes. A taxa manteve-se estável em 13,75% a.a. desde agosto do ano passado.\nA taxa SELIC influencia as demais taxas de juros da economia indiretamente. Como será mostrado mais adiante, há várias taxas de financiamento imobiliário disponíveis."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#um-pouco-sobre-juros",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#um-pouco-sobre-juros",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "",
    "text": "A taxa de juros tem um efeito geral sobre a economia, mas o seu efeito é mais notável para o consumidor na hora de fazer compras grandes, de longo prazo: caso de um automóvel ou de um imóvel. Uma taxa menor significa que fica mais “barato” tomar crédito, enquanto uma taxa maior significa o contrário.\nQuem define a taxa de juros “geral” da economia, a taxa SELIC, é o Cômite de Política Monetária (COPOM). A SELIC é, na verdade, uma meta de taxa de juros, que o Banco Central do Brasil (BCB) deve perseguir. O COPOM se reúne periodicamente para definir a taxa SELIC; na ocasião mais recente, no início de agosto, decidiu-se reduzir a taxa de 13,75% a.a. para 13,25% a.a. - a primeira queda depois de um ciclo de alta de 3 anos.\nTipicamente, a taxa de juros é a ferramenta de política monetária que se usa para controlar a inflação: quando a taxa de inflação aumenta muito, o COPOM decide aumentar a taxa de juros. Foi isto o que aconteceu em 2014-16 e também em 2021-23.\n\n\n\n\n\n\n\n\n\nO gráfico acima mostra as oscilações recentes da SELIC. No final de 2016 encerrou-se um longo ciclo de alta que começou em reação à escalada da inflação em 2014. A taxa, então, chegou a cair até 2% a.a. no final de 2020, acompanhando a tendência internacional de taxas de juros baixíssimas, muito próximas de 0% a.a. A volta da inflação com a pandemia levou o COPOM a agressivamente aumentar a taxa SELIC nos meses seguintes. A taxa manteve-se estável em 13,75% a.a. desde agosto do ano passado.\nA taxa SELIC influencia as demais taxas de juros da economia indiretamente. Como será mostrado mais adiante, há várias taxas de financiamento imobiliário disponíveis."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#exemplo-guiado",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#exemplo-guiado",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "Exemplo Guiado",
    "text": "Exemplo Guiado\n\nO empréstimo\nVamos simular o financiamento de um imóvel de R$450.000. Supondo um LTV de 80%, o valor da entrada é de R$90.000 e o valor a ser financiado, portanto, é de R$360.000. Num contrato de 30 anos, o valor da amortização é de:\n\\[\nA = \\frac{R\\$360.000}{R\\$360} = R\\$1.000\n\\]\nVamos começar assumindo que a taxa de juros seja de 10% a.a. A tabela abaixo mostra o fluxo de pagamentos do primeiro ano do empréstimo. Note como o valor da amortização é sempre o mesmo. À medida que a dívida vai sendo paga, o valor cobrado de juros também diminui e, por conseguinte, diminui também o valor da parcela mensal.\n\n\n\n\n\n\n  Fluxo de pagamento do financiamento de um imóvel de 450.000 (SAC).\n  \n    \n    \n      Período (mês)\n      Amortização\n      Juros\n      Parcela\n      Dívida\n    \n  \n  \n    1\nR$1.000\nR$2.870,69\nR$3.870,69\nR$360.000\n    2\nR$1.000\nR$2.862,72\nR$3.862,72\nR$359.000\n    3\nR$1.000\nR$2.854,74\nR$3.854,74\nR$358.000\n    4\nR$1.000\nR$2.846,77\nR$3.846,77\nR$357.000\n    5\nR$1.000\nR$2.838,79\nR$3.838,79\nR$356.000\n    6\nR$1.000\nR$2.830,82\nR$3.830,82\nR$355.000\n    7\nR$1.000\nR$2.822,85\nR$3.822,85\nR$354.000\n    8\nR$1.000\nR$2.814,87\nR$3.814,87\nR$353.000\n    9\nR$1.000\nR$2.806,90\nR$3.806,90\nR$352.000\n    10\nR$1.000\nR$2.798,92\nR$3.798,92\nR$351.000\n    11\nR$1.000\nR$2.790,95\nR$3.790,95\nR$350.000\n    12\nR$1.000\nR$2.782,98\nR$3.782,98\nR$349.000\n  \n  \n  \n\n\n\n\nAo longo dos 360 meses do financiamento, o valor dos juros e da parcela vão diminuindo até que a dívida tenha sido totalmente paga. Note como no início do financiamento, a parcela mensal está na faixa de R$3800, mas já no final está próxima de R$1000.\n\n\n\n\n\n\n\n\n\n\n\nRenda necessária\nAgora podemos responder uma dúvida importante: qual a renda necessária para financiar este imóvel? Cada banco ou instituição financeira usa regras próprias para decidir se libera ou não o valor do financiamento imobiliário. Uma regra de bolso comum é de que o valor da parcela inicial não pode ser maior do que 30% da renda do requerente.\nNo exemplo acima, o valor da primeira parcela é de R$3870. A renda mínima necessária (RMN) para estar elegível a este financiamento é:\n\\[\nRMN = \\frac{R\\$ 3.870,69}{0,3} = R\\$ 12.902,3\n\\]\nComo que este resultado final depende da taxa de juros? Podemos simular o mesmo financiamento para diferentes taxas juros e calcular novamente a renda mínima necessária. A tabela abaixo mostra como a renda varia para valores de taxa de juros de 7% a 12%. É notável como a taxa de juros tem impacto direto no poder de compra e capacidade de pagamento das famílias. A uma taxa favorável de 7%, é necessário ter uma renda de R$10 mil para ser aprovado no financimento; já a uma taxa de 12% é necessário ter quase R$15 mil.\n\n\n\n\n\n\n  Renda mínima necessária para financiar um imóvel de R$450.000 a diferentes taxas de juros.\n  \n    \n    \n      Juros (% a.a)\n      Renda Mínima\n    \n  \n  \n    7,00%\nR$10.118,31\n    7,50%\nR$10.587,24\n    8,00%\nR$11.054,17\n    8,50%\nR$11.519,13\n    9,00%\nR$11.982,12\n    9,50%\nR$12.443,17\n    10,00%\nR$12.902,30\n    10,50%\nR$13.359,52\n    11,00%\nR$13.814,85\n    11,50%\nR$14.268,30\n    12,00%\nR$14.719,88"
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#o-impacto-do-aumento-dos-juros",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#o-impacto-do-aumento-dos-juros",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "O impacto do aumento dos juros",
    "text": "O impacto do aumento dos juros\nNos últimos anos vimos uma alta significativa das taxas de juros. A taxa média de financiamento habitacional era próxima de 7% em 2020 e subiu para 11,6%. Considerando o imóvel do exemplo acima, seria necessário um aumento de mais de R$4000 na renda para conseguir comprar o mesmo imóvel - sem levar em conta o aumento de preço do imóvel.\nVamos retomar o exemplo do imóvel de R$450.000 acima. Mantendo este preço constante, podemos calcular qual a renda necessária para financiar este mesmo imóvel à medida que a taxa de juros foi aumentando. Por simplicidade, uso a taxa de juros média mensal em cada período3.\nO gráfico abaixo apresenta, a cada mês, a renda necessária para financiar um imóvel de R$450.0004. No ponto mais baixo da taxa, seria necessário R$8.550 (a uma taxa de 6,63%) para ser aprovado num financiamento; já no ponto mais recente, seria necessário R$12.600 (a uma taxa de 11,6%).\n\n\n\n\n\n\n\n\n\nA análise acima olha somente o impacto do aumento dos juros e não leva em consideração o aumento médio do preços dos imóveis durante este período. Segundo o IGMI-R (Abrainc/FGV), de janeiro de 2018 a abril 2023, houve um aumento médio de 58% no preços dos imóveis. A tabela abaixo mostra a variação acumulada do IGMI-R em cada ano. Nota-se como os preços aumentam significativamente a partir de 2020.\n\n\n\n\n\n\n  Variação acumulada do IGMI-R por ano.\n  \n    \n    \n      Ano\n      Var. Acum. (%)\n    \n  \n  \n    2018\n0.64%\n    2019\n4.11%\n    2020\n10.28%\n    2021\n16.25%\n    2022\n15.06%\n    2023\n2.52%\n  \n  \n  \n\n\n\n\nO gráfico abaixo refaz o experimento acima, mas leva em conta também o aumento médio do preços dos imóveis. Em cada mês vê-se a renda necessária para financiar um imóvel médio, que em janeiro de 2018 valia R\\$450.000.\nFica evidente como a combinação simultânea de aumento de juros e de preços tornou os imóveis menos acessíveis. Em abril de 2023, seria necessário uma renda em torno de R\\$20.000 para financiar o mesmo imóvel5.\n\n\n\n\n\n\n\n\n\nA análise omite ainda um fator: o crescimento médio da renda ao longo do tempo. No Brasil, o salário mínimo é indexado à variação da inflação e, de maneira geral, quando a economia vai bem a renda média costuma crescer. Deixo esta última etapa da análise de acessibilidade financeira para outro post.\nAlém disso, o programa habitacional do Brasil, o Minha Casa Minha Vida (MCMV) oferece empréstimos com taxas mais atrativas do que as taxas médias de mercado, conforme a renda da família e o preço do imóvel6. Como o programa foi revisto recentemente, vou dedicar um post somente ao MCMV e como ele deve impactar a acessibilidade à moradia no Brasil."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#o-caminho-futuro",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#o-caminho-futuro",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "O caminho futuro",
    "text": "O caminho futuro\nNa última semana o COPOM decidiu reduzir a taxa SELIC em 0.5 p.p., diminuindo a taxa de 13,75% para 13,25%. Esta foi a primeira queda desde que se iniciou o ciclo de altas no início de 2021. Espera-se que o Banco Central agora entre num ciclo de queda de taxa de juros que devem se estabilizar em torno de 8,5% no longo prazo.\nComo se viu na análise acima, as oscilações de SELIC eventualmente traduzem-se em mudanças nas taxas do financiamento imobiliário. Além da queda na taxa de juros, os índices de preços imobiliários, como o IGMI-R e o IVGR, começam a apontar para uma relativa estabilidade nos preços dos imóveis. Esta combinação deve aumentar o poder de compras das famílias e melhorar a acessibilidade financeira à moradia.\nHá um último componente, da equação da acessibilidade à moradia, que ficou inexplorado neste post: a renda das famílias. Evidentemente, um aumento da renda média das famílias permite que elas tenham acesso a imóveis melhores e mais caros. Além disso, para imóveis com ticket menores, o MCMV oferece condições mais favoráveis de financiamento. Deixo esta discussão, contudo, para um outro momento."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#footnotes",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#footnotes",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEm abril de 2023, cerca de 95% do estoque de contratos de financiamentos imobiliários para pessoas físicas era indexado pela TR. O IPCA era utilizado em cerca de 2% e outros indexadores eram utilizados em 2,2% dos contratos. Apenas 0,8% dos contratos eram pré-fixados.↩︎\nAlguns exemplos incluem: cadastro positivo, regulamentação das fintechs de crédito, regulamentação das Letras Imobiliárias Garantidas (LIGs), portabilidade de crédito. Para mais informações veja a Agenda BC#.↩︎\nA taxa de juros média do financiamento habitacional para pessoas físicas toma as cinco taxas apresentadas e pondera elas pelo volume de crédito. Assim, as principais linhas (FGTS e SFH) têm maior peso.↩︎\nPor simplicidade, suponho um financiamento estilo SAC com LTV de 70% e prazo de 360 meses. Para ser aprovado no financiamento, suponho que o valor da primeira parcela não possa ser maior do que 30% da renda familiar bruta.↩︎\nVale notar que o IGMI-R é um índice de preços hedônico então ele provê um índice de preços “ajustado pela qualidade”, isto é, um quality adjusted price index. Assim, este aumento de preços não reflete meramente uma mudança no mix de imóveis disponíveis no mercado.↩︎\nO preço de R\\$450.000 não foi escolhido ao acaso já que ele supera o teto atual do Minha Casa Minha Vida e não estaria elegível ao programa.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html",
    "title": "Fundamentos: gráfico de linha",
    "section": "",
    "text": "Gráficos de linha são frequentemente usados para representar séries de tempo, isto é, valores que mudam ao longo do tempo. Estes gráficos revelam a evolução de uma variável ao longo do tempo. O ggplot oferece alguma variedade de funções para este fim, mas a mais comum é a geom_line(). Este geom exige argumentos tanto para o eixo-x como para o eixo-y. Em geral, o eixo-x representa o tempo e o eixo-y o valor da variável de interesse.\n\n\n\n\n\n\n\n\n\nNeste post vamos aprender a montar gráficos de linha usando o ggplot2 no R. Primeiro vamos fazer um exemplo simples para enxergar a dinâmica da taxa de poupança nos EUA. Depois, vamos entender como customizar o gráfico.\nAlém disso, vamos importar séries do Banco Central para fazer alguns exemplos aplicados usando o pacote GetBCBData."
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#ggplot2",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#ggplot2",
    "title": "Fundamentos: gráfico de linha",
    "section": "ggplot2",
    "text": "ggplot2\nO pacote ggplot2 segue uma sintaxe bastante consistente, que permite “somar” elementos visuais sobre um mesmo gráfico. Isto permite que se crie uma infinidade de gráficos complexos a partir de elementos simples. Os elementos visuais são todos chamados por funções geom_*. Neste primeiro exemplo vamos focar na função geom_line() que desenha linhas. Estes elementos são todos somados de maneira intuitiva usando o sinal de soma +.\nEssencialmente, temos os seguintes elementos principais:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEstes elementos são combinados numa sintaxe recorrente. A função ggplot tipicamente tem apenas dois elementos: data e aes. O argumento data indica o nome da base de dados. Já a função aes é a que indica como transformar os dados (as colunas da base de dados) em elementos visuais. Nos casos mais simples, esta função serve para indicar qual é o nome da variável x e qual é o nome da variável y.\nAdicionamos uma função geom nesta chamada inicial como no exemplo abaixo.\n\nggplot(data = dados, aes(x = varivel_x, y = variavel_y)) +\n  geom_line()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#gráfico-de-linha-1",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#gráfico-de-linha-1",
    "title": "Fundamentos: gráfico de linha",
    "section": "Gráfico de linha",
    "text": "Gráfico de linha\nPara construir um gráfico de linha precisamos, em geral, de apenas dois arguementos: um argumento x que é o nome da variável no eixo-x (comumemente, o tempo) e um argumento y que é o nome da variável no eixo-y (comumemente, a variável numérica).\nVamos utilizar a base economics que é carregada conjuntamente com o pacote ggplot2. Esta base traz a evolução de algumas variáveis econômicas ao longo do tempo.\n\n# Carrega a base de dados (caso ainda não tenha feito)\ndata(\"economics\")\n# Visualiza as primeiras linhas da base de dados\nhead(economics)\n\n# A tibble: 6 × 6\n  date         pce    pop psavert uempmed unemploy\n  &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 1967-07-01  507. 198712    12.6     4.5     2944\n2 1967-08-01  510. 198911    12.6     4.7     2945\n3 1967-09-01  516. 199113    11.9     4.6     2958\n4 1967-10-01  512. 199311    12.9     4.9     3143\n5 1967-11-01  517. 199498    12.8     4.7     3066\n6 1967-12-01  525. 199657    11.8     4.8     3018\n\n\nNote que a primeira coluna à esquerda (date) traz as datas. A coluna psavert é a taxa de poupança individual, mensurada em proporção à renda disponível. Podemos visualizar como se altera a taxa de poupança ao longo do tempo.\n\nggplot(data = economics, aes(x = date, y = psavert)) +\n  geom_line()\n\n\n\n\n\n\n\n\nVamos quebrar o código acima em detalhes. Primeiro, usamos a função ggplot() para declarar que queremos fazer um gráfico. Colocamos os argumentos data e aes dentro desta função. O argumento data deve ser o nome da nossa base de dados: neste caso, data = economics.\nO argumento aes é o que transforma as variáveis (as colunas da base de dados) em elementos visuais. Neste caso ele vai transformar o tempo e a taxa de poupança em algum elemento visual. Escolhemos aes(x = date, y = psavert).\nEspecificamos qual deve ser o elemento visual somando a função geom_line() no código inicial. Esta função indica que queremos um gráfico de linhas.\nUnindo todas estes elementos temos um código enxuto que plota o gráfico. Vemos que a taxa de poupança nos EUA apresenta uma tendência de queda a partir da segunda metade dos anos 1970 que é revertida somente após a primeira metade dos anos 2000.\n\nggplot(\n  # Especifica o nome da base de dados\n  data = economics,\n  # Indica como devem ser mapeadas as variáveis nos eixos x e y\n  aes(x = date, y = psavert)) +\n  # Especifica que queremos um gráfico de linha\n  geom_line()\n\n\n\n\n\n\n\n\n\nOpções estéticas\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nHá quatro opções estéticas básicas para gráficos de linha: color, alpha, linewidth e linetype.\n\ncolor - Define a cor da linha\nalpha - Define o nível de transparência da linha\nlinewidth - Define a espessura da linha\nlinetype - Define o tracejado da linha\n\n\n\nCores\nUtilizamos o argumento color dentro da função geom_line para variar a cor da linha no gráfico. Pode-se escolher a cor da linha tanto por nome como por código hexadecimal. Por padrão, a função geom_line utiliza color = \"black\". No exemplo abaixo utilizo um tom de azul chamado \"steelblue\". Uma lista completa de cores (por nome) está disponível aqui.\n\n# Gráfico de linha\nggplot(data = economics, aes(x = date, y = psavert)) + \n  # Altera a cor da linha\n  geom_line(color = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém é possível chamar as cores via código hexadecimal como no exemplo abaixo.\n\n# Gráfico de linha\nggplot(data = economics, aes(x = date, y = psavert)) +\n  # Altera a cor da linha\n  geom_line(color = \"#e76f51\")\n\n\n\n\n\n\n\n\n\n\nAlpha\nO argumento alpha controla o nível de transparência da cor e varia de 0 a 1, em que alpha = 0 é perfeitamente transparente e alpha = 1 é nada transparente. Por padrão, a função geom_line define que alpha = 1. Na prática, são raros os casos em que vale alterar este argumento. No exemplo abaixo definimos alpha = 0.5.\n\nggplot(data = economics, aes(x = date, y = psavert)) +\n  geom_line(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nEspessura da linha\nO argumento linewidth controla a espessura da linha. No exemplo abaixo defino linewidth = 1 dentro da função geom_line(). Na prática, não recomendo escolher um valor de linewidth acima de 1 pois a linha se torna muito espessa.\n\nggplot(data = economics, aes(x = date, y = psavert)) + \n  geom_line(linewidth = 1)\n\n\n\n\n\n\n\n\nO gráfico abaixo compara alguns tamanhos diferentes de espessura de linha. Note que é possível especificar valores não-inteiros como 0,5. Para evitar de repetir o mesmo código várias vezes, guardo a chamada inicial do ggplot em um objeto chamado p (de plot).\n\np &lt;- ggplot(data = economics, aes(x = date, y = psavert))\n\n# Espessura = 0.5\np + geom_line(linewidth = 0.5)\n# Espessura = 1\np + geom_line(linewidth = 1)\n# Espessura = 2\np + geom_line(linewith = 2)\n# Espessura = 5\np + geom_line(linewidth = 5)\n\n\n\n\n\n\n\n\n\n\n\n\nTracejado da linha\nO argumento linetype controla o tracejado da linha. Por padrão, a função geom_line() utiliza linetype = 1, mas podemos escolher valores distintos. No exemplo abaixo usamos linetype = 2.\n\nggplot(data = economics, aes(x = date, y = psavert)) + \n  geom_line(linetype = 2)\n\n\n\n\n\n\n\n\nAbaixo pode-se ver a diferença entre cada uma delas."
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#combinando-elementos",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#combinando-elementos",
    "title": "Fundamentos: gráfico de linha",
    "section": "Combinando elementos",
    "text": "Combinando elementos\nComo foi aludido no início do post, parte da mágica do ggplot é de poder somar elementos ao mesmo gráfico. Isto permite uma grande flexibilidade na hora de montar nossas visualizações. Uma combinação bastante efetiva é juntar um gráfico de linha com um gráfico de pontos.\nPrimeiro, vamos importar a série do consumo de energia elétrica residencial no Brasil. Podemos importar séries de tempo do site do Banco Central do Brasil usando a função gbcbd_get_series do pacote GetBCBData.\nPara importar uma série de tempo precisamos informar apenas: (1) o código da série pelo argumento id; e (2) a data de início da extração pelo argumento first.date (no formato YYYY-MM-DD).\n\n# Import a série de consumo de energia elétrica residencial\nconsumo &lt;- gbcbd_get_series(id = 1403, first.date = as.Date(\"2014-01-01\"))\n\nO gráfico abaixo combina um geom_line() com um geom_point() para representar tanto os pontos de observação como a linha que une os pontos.\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha da série\n  geom_line() +\n  # Pontos sinalizando as observações\n  geom_point()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#começando-o-gráfico-do-zero",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#começando-o-gráfico-do-zero",
    "title": "Fundamentos: gráfico de linha",
    "section": "Começando o gráfico do zero",
    "text": "Começando o gráfico do zero\nOs gráficos de linha gerados pelo ggplot2 não começam do zero no eixo-y. Isto pode ser um problema e, visualmente, criar a ilusão de que há mais volatilidade nos dados do que realmente existe.\nHá muitas formas de forçar o gráfico a iniciar no zero. Uma das mais efetivas é incluir uma linha horizontal no eixo (\\(y = 0\\)). Pode-se desenhar esta linha horizontal com a função geom_hline() (onde o “hline” sinaliza “horizontal line”, linha horizontal).\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha horizontal y = 0\n  geom_hline(yintercept = 0) +\n  # Linha da série\n  geom_line()\n\n\n\n\n\n\n\n\nOutra forma de chegar no mesmo resultado é manipular o eixo-y para forçá-lo a iniciar no zero. No exemplo abaixo utilizamos a função scale_y_continuous(). Esta função controla todos os aspectos visuais da escala no eixo-y. O sufixo _continuous indica que a variável no eixo-y é contínua.\nPara redefinir os limites do eixo-y variamos o argumento limits que aceita um par de valores. O primeiro valor é o limite inferior e o segundo valor é o limite superior. Por padrão ambos são definidos como NA.\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha da série\n  geom_line() +\n  # Manipular o eixo-y para inicar no zero\n  scale_y_continuous(limits = c(0, NA))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#séries-de-tempo-em-degraus",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#séries-de-tempo-em-degraus",
    "title": "Fundamentos: gráfico de linha",
    "section": "Séries de tempo em degraus",
    "text": "Séries de tempo em degraus\nAlgumas séries de tempo variam de maneira “descontínua” no tempo. É o caso, por exemplo, da taxa SELIC, que é periodicamente definida como uma meta a ser perseguida pelo Banco Central. Neste caso, faz sentido ressaltar isto fazendo um gráfico que varia em “degraus”.\nA função geom_step() faz este tipo de gráfico e possui os mesmos argumentos que a função geom_line(). O gráfico abaixo mostra a variação da taxa SELIC (meta) nos últimos anos.\n\n# Importa a série diária da SELIC (meta) anualizada\nselic &lt;- gbcbd_get_series(id = 1178, first.date = as.Date(\"2016-01-01\"))\n\nggplot(data = selic, aes(x = ref.date, y = value)) +\n  geom_step()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: gráfico de linha",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nComo de costume, as características estéticas do gráfico podem refletir grupos de variáveis. No caso de gráficos de linha é comum querer representar séries de tempo distintas com cores diferentes.\nVamos importar três séries de tempo: o consumo de energia elétrica comercial (1402), residencial (1403) e industrial (1404). Todas as séries são mensais e são importadas a partir de janeiro de 2014.\n\n# Importa as séries mensais do consumo de energia elétrica\nseries &lt;- gbcbd_get_series(\n  id = c(1402, 1403, 1404),\n  first.date = as.Date(\"2014-01-01\")\n  )\n\nOs dados são automaticamente formatados no padrão longitudinal, em que cada linha representa uma observação no tempo de uma série em particular. Este é o formato ideal para trabalhar com visualizações em ggplot, onde os dados estão “empilhados”.\nA coluna ref.date indica a data, a coluna series.name identifica o id da série e a coluna value representa o valor desta observação.\n\n\n\n\n\nref.date\nvalue\nid.num\nseries.name\n\n\n\n\n2014-01-01\n7745\n1402\nid = 1402\n\n\n2014-01-01\n11798\n1403\nid = 1403\n\n\n2014-01-01\n14537\n1404\nid = 1404\n\n\n2014-02-01\n8204\n1402\nid = 1402\n\n\n2014-02-01\n11879\n1403\nid = 1403\n\n\n2014-02-01\n15107\n1404\nid = 1404\n\n\n\n\n\n\n\nVamos mapear a coluna series.name nas cores das linhas. Para especificar isto utilizamos a função aes, que serve para transformar dados em elementos visuais. Da mesma forma como mapeamos a coluna ref.date e value numa linha, agora mapeamos series.name nas cores do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Indica que a variável series.name deve ser mapeada nas cores das linhas\n    aes(color = series.name))\n\n\n\n\n\n\n\n\nAs escolhas de cores padrão nem sempre são satisfatórias. Para modificar as cores utilizamos a função scale_color_manual() que também nos dá controle sobre a legenda do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_manual(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Cores das linhas\n    values = c(\"#264653\", \"#e9c46a\", \"#e76f51\"),\n  )\n\n\n\n\n\n\n\n\nVale notar que existem vários pacotes e funções com cores pré-definidas que simplificam o processo manual de escolher as cores. O exemplo mais simples é o scale_color_brewer() que utiliza as paletas de cores do Color Brewer.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_brewer(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Tipo (\"qual\" - qualitativo, \"div\" - divergente ou \"seq\" - sequencial)\n    type = \"qual\",\n    # Escolha da paleta\n    palette = 6\n  )\n\n\n\n\n\n\n\n\nPor padrão, a legenda das cores é colocada no lado direito do gráfico. Para modificar a sua posição usamos a função theme().\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  scale_color_brewer(\n    name = \"Consumo de energia\",\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    type = \"qual\",\n    palette = 6) +\n  theme(\n    # Define a posição da legenda (top, bottom, right ou left)\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nDa mesma forma que modificamos a cor das linhas, podemos variar outros aspectos estéticos como, por exemplo, o tracejado da linha de cada série. Este tipo de modificação pode ser útil no caso de uma publicação que vai ser visualizada ou impressa em escala de cinza.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Cada série é desenhada com um tracejado diferente\n    aes(linetype = series.name)\n    ) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )\n\n\n\n\n\n\n\n\nPor fim, podemos modificar elementos estéticos de dois geoms no mesmo gráfico. No exemplo abaixo tanto o tracejado da linha como o formato do ponto variam segundo a série.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(linetype = series.name)) +\n  geom_point(aes(shape = series.name)) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  ) +\n  scale_shape_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: gráfico de linha",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNo caso de gráficos de linha é muito comum omitir o nome do eixo-x, pois ele geralmente representa o tempo. Para omitir o nome de qualquer eixo basta defini-lo como NULL.\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha da série\n  geom_line() +\n  # Define os elementos textuais do gráfico\n  labs(\n    title = \"Aumento gradual na demanda por energia elétrica\",\n    subtitle = \"Consumo residencial de energia elétrica no Brasil.\",\n    # Omite o título do eixo-x\n    x = NULL,\n    y = \"GWh\",\n    caption = \"Fonte: Eletrobras.\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#resumo",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#resumo",
    "title": "Fundamentos: gráfico de linha",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar gráficos de linha sofisticados usando poucas linhas de código. Em qualquer gráfico temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos de destaque:\n\nUtilize cores diferentes para séries distintas usando aes(color = ...).\nCombine elementos como linhas e pontos somando os “geoms”.\nComece o gráfico no zero utilizando ou geom_hline(yintercept = 0) ou scale_y_continuous(limits=c(0,NA)).\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos.\n\n# Importa as séries mensais do consumo de energia elétrica\nseries &lt;- gbcbd_get_series(\n  id = c(1402, 1403, 1404),\n  first.date = as.Date(\"2019-01-01\")\n  )\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_hline(yintercept = 0) +\n  geom_line(\n    aes(color = series.name),\n    linewidth = 1) +\n  geom_point(\n    aes(color = series.name),\n    size = 2) +\n  scale_color_manual(\n    name = \"Consumo de Energia\",\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    values = c(\"#264653\", \"#e9c46a\", \"#e76f51\")) +\n  labs(\n    title = \"Recuperação do consumo de energia pós-pandemia\",\n    subtitle = \"Consumo mensal de energia elétrica segundo a finalidade.\",\n    x = NULL,\n    y = \"(GWh)\",\n    caption = \"Fonte: Eletrobras.\") +\n  theme(\n    legend.position = \"bottom\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#mapeando-elementos-estéticos",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#mapeando-elementos-estéticos",
    "title": "Fundamentos: gráfico de linha",
    "section": "Mapeando elementos estéticos",
    "text": "Mapeando elementos estéticos\n\nCores\nComo de costume, as características estéticas do gráfico podem refletir grupos de variáveis. No caso de gráficos de linha é comum querer representar séries de tempo distintas com cores diferentes.\nVamos importar três séries de tempo: o consumo de energia elétrica comercial (1402), residencial (1403) e industrial (1404). Todas as séries são mensais e são importadas a partir de janeiro de 2014.\n\n# Importa as séries mensais do consumo de energia elétrica\nseries &lt;- gbcbd_get_series(\n  id = c(1402, 1403, 1404),\n  first.date = as.Date(\"2014-01-01\")\n  )\n\nOs dados são automaticamente formatados no padrão longitudinal, em que cada linha representa uma observação no tempo de uma série em particular. Este é o formato ideal para trabalhar com visualizações em ggplot, onde os dados estão “empilhados”.\nA coluna ref.date indica a data, a coluna series.name identifica o id da série e a coluna value representa o valor desta observação.\n\n\n\n\n\nref.date\nvalue\nid.num\nseries.name\n\n\n\n\n2014-01-01\n7745\n1402\nid = 1402\n\n\n2014-01-01\n11798\n1403\nid = 1403\n\n\n2014-01-01\n14537\n1404\nid = 1404\n\n\n2014-02-01\n8204\n1402\nid = 1402\n\n\n2014-02-01\n11879\n1403\nid = 1403\n\n\n2014-02-01\n15107\n1404\nid = 1404\n\n\n\n\n\n\n\nVamos mapear a coluna series.name nas cores das linhas. Para especificar isto utilizamos a função aes, que serve para transformar dados em elementos visuais. Da mesma forma como mapeamos a coluna ref.date e value numa linha, agora mapeamos series.name nas cores do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Indica que a variável series.name deve ser mapeada nas cores das linhas\n    aes(color = series.name))\n\n\n\n\n\n\n\n\nAs escolhas de cores padrão nem sempre são satisfatórias. Para modificar as cores utilizamos a função scale_color_manual() que também nos dá controle sobre a legenda do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_manual(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Cores das linhas\n    values = c(\"#264653\", \"#e9c46a\", \"#e76f51\"),\n  )\n\n\n\n\n\n\n\n\nVale notar que existem vários pacotes e funções com cores pré-definidas que simplificam o processo manual de escolher as cores. O exemplo mais simples é o scale_color_brewer() que utiliza as paletas de cores do Color Brewer.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_brewer(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Tipo (\"qual\" - qualitativo, \"div\" - divergente ou \"seq\" - sequencial)\n    type = \"qual\",\n    # Escolha da paleta\n    palette = 6\n  )\n\n\n\n\n\n\n\n\nPor padrão, a legenda das cores é colocada no lado direito do gráfico. Para modificar a sua posição usamos a função theme().\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  scale_color_brewer(\n    name = \"Consumo de energia\",\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    type = \"qual\",\n    palette = 6) +\n  theme(\n    # Define a posição da legenda (top, bottom, right ou left)\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nTipo de linha\nDa mesma forma que modificamos a cor das linhas, podemos variar outros aspectos estéticos como, por exemplo, o tracejado da linha de cada série. Este tipo de modificação pode ser útil no caso de uma publicação que vai ser visualizada ou impressa em escala de cinza.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Cada série é desenhada com um tracejado diferente\n    aes(linetype = series.name)\n    ) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )\n\n\n\n\n\n\n\n\nPor fim, podemos modificar elementos estéticos de dois geoms no mesmo gráfico. No exemplo abaixo tanto o tracejado da linha como o formato do ponto variam segundo a série.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(linetype = series.name)) +\n  geom_point(aes(shape = series.name)) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  ) +\n  scale_shape_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2022-08-recife/index.html",
    "href": "posts/general-posts/2022-08-recife/index.html",
    "title": "Weekly Viz: Recife em mapas",
    "section": "",
    "text": "Semana que vem vou estar no Recife a trabalho e para conhecer um pouco da cidade resolvi tirar um tempo para fazer alguns mapas da cidade.\n\n\n\n\n\n\n\nOs dados dos dois primeiros mapas, das principais vias e dos corpos d’água, são do OpenStreetMap. Pode-se ver, por exemplo, a Rodovia Mario Covas, que corta a cidade de norte a sul e o famoso Rio Capibaribe que atravessa a cidade.\nPara o terceiro mapa puxei os dados do Ciclomapa. Apesar do título, eu incluí tanto ciclovias, ciclofaixas e ciclorrotas.\nOs mapas da segunda linha foram todos feitos com dados do projeto de Acesso a Oportunidades do IPEA. Os dados de população e renda são do Censo de 2010 e espacialmente interpolados com os hexágonos H3, da Uber. Para facilitar a visualização usei o algoritmo de Jenks para agrupar os dados em 7 grupos. Além disso, usei uma transformação log tanto na renda como na população para reduzir um pouco da variância nos dados.\nA renda da cidade é visivelmente concentrada na região sul-sudeste, próxima do litoral e na região centro-leste, no que me parece ser a região dos bairros Madalena e Boa Vista. Já a densidade populacional não segue um padrão simples; a população está espalhada por toda a cidade.\nComo medida de acesso a empregos usei o percentual de oportunidades de emprego acessíveis a 15 minutos de carro em horário de pico. Segundo os dados do Censo, a maior parte dos deslocamentos casa-trabalho na RM de Recife são de menos de uma hora (83,4%). Assim, 15 minutos de carro (em horário de pico) me parece ser um “luxo” que as pessoas estão dispostas a pagar e que deve se refletir, em algum nível, no preço dos imóveis.\nO mapa sugere que Recife é uma cidade monocêntrica, com a maior parte dos empregos concentrada na região central. Interessante notar que, apesar da distância geográfica, a região sul, de Boa Viagem, Pina, etc. continua com indicadores de acessibilidade relativamente altos.\nPara tentar mensurar a verticalização calculei a proporção de apartamentos em relação ao total de domicílios em cada região. Novamente os dados vem do Censo e são agrupados a nível de setor censitário. Eu faço uma interpolação espacial simples com os hexágonos H3, na mesma resolução 9 do projeto do IPEA, para manter o padrão. Grosso modo, parece que as regiões de rendas mais altas coincidem com as regiões mais verticalizadas.\nPor fim, os dados de anúncios e preço provém de anúncios online de venda de imóveis ativos entre janeiro e junho de 2023. Por simplicidade, eu escolhi trabalhar apenas com anúncios de apartamentos e removi algumas observações discrepantes para conseguir um valor médio mais razoável. O preço médio observado na cidade neste período foi de R$7.900, com a maior parte das observações caindo dentro do intervalo R$3.800-R$12.700."
  },
  {
    "objectID": "posts/general-posts/2022-08-recife/index.html#detalhes-do-mapa",
    "href": "posts/general-posts/2022-08-recife/index.html#detalhes-do-mapa",
    "title": "Weekly Viz: Recife em mapas",
    "section": "",
    "text": "Os dados dos dois primeiros mapas, das principais vias e dos corpos d’água, são do OpenStreetMap. Pode-se ver, por exemplo, a Rodovia Mario Covas, que corta a cidade de norte a sul e o famoso Rio Capibaribe que atravessa a cidade.\nPara o terceiro mapa puxei os dados do Ciclomapa. Apesar do título, eu incluí tanto ciclovias, ciclofaixas e ciclorrotas.\nOs mapas da segunda linha foram todos feitos com dados do projeto de Acesso a Oportunidades do IPEA. Os dados de população e renda são do Censo de 2010 e espacialmente interpolados com os hexágonos H3, da Uber. Para facilitar a visualização usei o algoritmo de Jenks para agrupar os dados em 7 grupos. Além disso, usei uma transformação log tanto na renda como na população para reduzir um pouco da variância nos dados.\nA renda da cidade é visivelmente concentrada na região sul-sudeste, próxima do litoral e na região centro-leste, no que me parece ser a região dos bairros Madalena e Boa Vista. Já a densidade populacional não segue um padrão simples; a população está espalhada por toda a cidade.\nComo medida de acesso a empregos usei o percentual de oportunidades de emprego acessíveis a 15 minutos de carro em horário de pico. Segundo os dados do Censo, a maior parte dos deslocamentos casa-trabalho na RM de Recife são de menos de uma hora (83,4%). Assim, 15 minutos de carro (em horário de pico) me parece ser um “luxo” que as pessoas estão dispostas a pagar e que deve se refletir, em algum nível, no preço dos imóveis.\nO mapa sugere que Recife é uma cidade monocêntrica, com a maior parte dos empregos concentrada na região central. Interessante notar que, apesar da distância geográfica, a região sul, de Boa Viagem, Pina, etc. continua com indicadores de acessibilidade relativamente altos.\nPara tentar mensurar a verticalização calculei a proporção de apartamentos em relação ao total de domicílios em cada região. Novamente os dados vem do Censo e são agrupados a nível de setor censitário. Eu faço uma interpolação espacial simples com os hexágonos H3, na mesma resolução 9 do projeto do IPEA, para manter o padrão. Grosso modo, parece que as regiões de rendas mais altas coincidem com as regiões mais verticalizadas.\nPor fim, os dados de anúncios e preço provém de anúncios online de venda de imóveis ativos entre janeiro e junho de 2023. Por simplicidade, eu escolhi trabalhar apenas com anúncios de apartamentos e removi algumas observações discrepantes para conseguir um valor médio mais razoável. O preço médio observado na cidade neste período foi de R$7.900, com a maior parte das observações caindo dentro do intervalo R$3.800-R$12.700."
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html",
    "href": "posts/general-posts/2022-08-firjan-app/index.html",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório\n\n\n\n\nO app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16.\n\n\n\n\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html#introdução",
    "href": "posts/general-posts/2022-08-firjan-app/index.html#introdução",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório"
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html#sobre-o-app",
    "href": "posts/general-posts/2022-08-firjan-app/index.html#sobre-o-app",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "O app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16."
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "href": "posts/general-posts/2022-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Um caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html",
    "title": "Estético: Destacando informação",
    "section": "",
    "text": "Um gráfico deve ser o mais autosuficiente possível; a imagem deve explicar-se por si mesma. Existem algumas ferramentas adicionais que pode-se usar para atingir este objetivo. Neste post vou discutir três estratégias simples para destacar uma informação no gráfico.\nEspecificamente, vamos ver como:\n\nUsar linhas para destacar os eixos ou informações numéricas com geom_vline(), geom_hline() e geom_abline()\nDestacar partes do gráfico com geom_rect()\nDestacar informações numéricas e texto usando geom_text() e annotate()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#básico",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#básico",
    "title": "Estético: Destacando informação",
    "section": "Básico",
    "text": "Básico\nJá sabemos como desenhar linhas usando tanto o geom_line() como o geom_path(). Estas funções exigem dois a três arguemntos que especficam as coordenadas da linha e de que forma a linha deve unir estas coordenadas.\nSuponha, por exemplo, que se queira desenhar uma linha reta no eixo-x. Para isto precisamos estruturar um data.frame, informar as coordenadas e então chamar geom_line().\nNote como no código abaixo, não preciso repetir o valor y = 0 já que o tibble “recicla” este número para que ele tenha o mesmo comprimento (length) que o vetor x.\n\ndf &lt;- tibble(\n  x = c(1, 10),\n  y = 0\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line()\n\n\n\n\n\n\n\n\nO procedimento acima é nada prático, exige a criação de um data.frame apartado dos dados e as linhas ainda tem comprimento fixo. Para desenhar linhas retas arbitrárias com maior facilidade há três funções:\n\ngeom_vline - Desenha linhas verticais\ngeom_hline - Desenha linhas horizontais\ngeom_abline - Desenha linhas retas especificando o intercepto e a inclinação.\n\n\nggplot() +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\nA função geom_abline() segue a equação da reta \\(y = ax + b\\) onde \\(a\\) é a inclinação da reta (slope) e \\(b\\) é o intercepto (intercept) o ponto onde a linha cruza o eixo vertical.\n\nggplot() +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_abline(slope = 1, intercept = 0)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#desenhando-eixos-num-gráfico",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#desenhando-eixos-num-gráfico",
    "title": "Estético: Destacando informação",
    "section": "Desenhando eixos num gráfico",
    "text": "Desenhando eixos num gráfico\nO uso mais imediato destas funções é de desenhar eixos num gráfico. O gráfico de dispersão abaixo mostra a relação entre a taxa de desmprego e a taxa de poupança, usando a base economics. A inclusão do eixo-x e eixo-y ajudam a melhor contextualizar os dados.\n\nggplot(economics, aes(uempmed, psavert)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#destacando-informações-com-linhas",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#destacando-informações-com-linhas",
    "title": "Estético: Destacando informação",
    "section": "Destacando informações com linhas",
    "text": "Destacando informações com linhas\nTambém pode-se usar linhas para destacar algum tipo de informação no gráfico. Nos exemplos abaixo mostro como destacar o valor médio e mediano ou os quintis.\n\nMédia e mediana num gráfico\nNo gráfico abaixo, mostro a distribuição mensal de listings de imóveis (anúncios de imóveis) em Houston, TX. No código eu uso geom_hline() para exibir uma linha reta no eixo-x e geom_vline() para destacar os valores médio e mediano. A linha tracejada em vermelho indica o valor médio, enquanto a linha tracejada em amarelo indica o valor mediano.\n\nsubhousing &lt;- filter(txhousing, city == \"Houston\")\n\nggplot(subhousing, aes(x = listings)) +\n  geom_histogram(bins = 6, color = \"white\", fill = \"#1B6F88\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(\n    xintercept = mean(subhousing$listings, na.rm = TRUE),\n    color = \"#9e2a2b\",\n    linewidth = 1,\n    linetype = 2) +\n  geom_vline(\n    xintercept = median(subhousing$listings, na.rm = TRUE),\n    color = \"#e09f3e\",\n    linewidth = 1,\n    linetype = 2\n    )\n\n\n\n\n\n\n\n\n\n\nQuintis ou grupos\nPara desenhar múltiplas linhas basta passar o argumento como um vetor. No caso abaixo eu calculo os quintis do taxa de poupança da base economics e sobreponho estas linhas ao histograma.\n\nquintil &lt;- quantile(economics$psavert, probs = c(0.2, 0.4, 0.6, 0.8))\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 15, color = \"white\") +\n  geom_vline(xintercept = quintil)\n\n\n\n\n\n\n\n\nPor fim, pode-se também passar as linhas como um argumento estético em color. No código abaixo eu calculo o número total de listings por ano; no gráfico, cada ano é exibido numa linha com cor diferente.\n\nlisting &lt;- subhousing %&gt;%\n  filter(year &lt; 2015) %&gt;%\n  summarise(total_listing = sum(listings, na.rm = TRUE), .by = \"year\")\n\nggplot(listing) +\n  geom_vline(aes(color = year, xintercept = total_listing)) +\n  scale_color_viridis_c()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#pib-e-ciclos-de-recessão",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#pib-e-ciclos-de-recessão",
    "title": "Estético: Destacando informação",
    "section": "PIB e ciclos de recessão",
    "text": "PIB e ciclos de recessão\nVamos aplicar esta função num exemplo mais interessante. Primeiro vamos baixar a série do PIB brasileiro1 do site do Banco Central do Brasil usando a função gbcbd_get_series(). O código abaixo, além de baixar a série, também faz um gráfico de linha simples.\n\n# Importa a série do PIB\npib &lt;- gbcbd_get_series(22109, first.date = as.Date(\"1995-01-01\"))\n\nggplot(pib, aes(ref.date, value)) +\n  geom_line()\n\n\n\n\n\n\n\n\nPara mapearmos os ciclos de recessão uso as definições da CODACE2. A tabela indica o início e o final de cada recessão, junto com uma sigla que identifica cada recessão. Eu crio esta tabela usando a função tribble(). Esta função é, essencialmente, equivalente a um data.frame() convencional, mas permite colocar os dados num formato que fica mais legível3.\n\ncodace &lt;- tribble(\n            ~rec_start,               ~rec_end, ~label,\n  #-------------------#----------------------#----------#\n  as.Date(\"1997-10-01\"), as.Date(\"1999-01-01\"), \"FHC-1\",\n  as.Date(\"2001-01-01\"), as.Date(\"2001-10-01\"), \"FHC-2\",\n  as.Date(\"2002-10-01\"), as.Date(\"2003-04-01\"), \"LULA\",\n  as.Date(\"2008-07-01\"), as.Date(\"2009-01-01\"), \"GFR\",\n  as.Date(\"2014-01-01\"), as.Date(\"2016-10-01\"), \"DILMA\",\n  as.Date(\"2019-10-01\"), as.Date(\"2020-04-01\"), \"COVID\"\n)\n\nO código abaixo gera o mesmo gráfico do PIB, mas agora temos retângulos sombreados destacando cada uma das recessões da história econômica recente do Brasil.\nComo estou mapeando dados de dois objetos distintos, eu coloco o argumento data dentro da respectiva função geom(). Isto não é obrigatório, mas acredito que o código fica melhor estruturado desta maneira.\nNote o uso do Inf para garantir que o retângulo se estenda verticalmente no gráfico de maneira indefinida. Além disso, uso o argumento alpha para deixar a região mais transparente.\n\nggplot() +\n  geom_line(\n    data = pib,\n    aes(x = ref.date, y = value)\n  ) +\n  geom_rect(\n    data = codace,\n    aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = Inf, group = label),\n    alpha = 0.4\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#usando-geom_text",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#usando-geom_text",
    "title": "Estético: Destacando informação",
    "section": "Usando geom_text",
    "text": "Usando geom_text\nQuando queremos trabalhar com texto como uma variável num data.frame é mais adequado usar a função geom_text(). Esta função tem os memos três argumentos obrigatórios da função annotate():\n\nx - Especifica a posição do texto no eixo-x\ny - Especifica a posição do texto no eixo-y\nlabel - Texto a ser plotado\n\nAlém do básico, também pode-se alterar diversos elementos estéticos usando:\n\ncolor - Para alterar a cor do texto.\nfamily e fontface - Para alterar a fonte do texto ou deixar ele negrito, itálico, etc.\nsize - Para alterar o tamnho do texto.\nalpha - Para alterar a transparência do texto.\nangle, vjust, hjust - Para ajustar o alinhamento horizontal e vertical do texto.\n\nO código abaixo refaz o gráfico acima, mas ao invés de pontos, agora vê-se o nome do modelo de cada automóvel.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_text(aes(label = rownames(mtcars)), size = 3)\n\n\n\n\n\n\n\n\nNo gráfico acima, o eixo-y (mpg) é “milhas por galão”, isto é, quantas milhas o carro consegue percorrer por galão de combustível enquanto o eixo-x (wt) é o peso do veículo em milhares de libras. Assim, o Toyota Corolla é um carro leve e eficiente, enquanto o Chrysler Imperial é um carro pesado e ineficiente.\nUm problema frequente deste tipo de gráfico é a sobreposição dos nomes. De fato, vale a pena considerar encurtar os nomes ou usar abreviações para atenuar este tipo de problema. Outra opção é usar a extensão ggrepel5.\n\nExemplo: Estados e Aluguel\nA tabela abaixo reúne alguns fatos da PNADC/A mais recente sobre o percentual de apartamentos, em relação ao total de domicílios, e o percentual de imóveis alugados, também em relação ao total de domicílios.\n\nrented &lt;- tibble::tribble(\n  ~abbrev_state, ~share_apto, ~share_rented, ~name_region, \n  #------------#------------#--------------#------------#      \n           \"RO\",           8,          11.1, \"Norte\", \n           \"AC\",        7.47,          6.93, \"Norte\",\n           \"AM\",        15.8,          7.26, \"Norte\",\n           \"RR\",        14.3,          11.7, \"Norte\", \n           \"PA\",        4.53,          6.39, \"Norte\",\n           \"AP\",        10.7,          6.34, \"Norte\",\n           \"TO\",        3.17,          11.7, \"Norte\", \n           \"MA\",        4.06,          5.74, \"Nordeste\",\n           \"PI\",        4.08,          5.15, \"Nordeste\",\n           \"CE\",        9.85,          9.68, \"Nordeste\",\n           \"RN\",        9.27,          10.8, \"Nordeste\", \n           \"PB\",        11.9,          9.18, \"Nordeste\",\n           \"PE\",        10.4,          9.98, \"Nordeste\",\n           \"AL\",        6.32,          10.0, \"Nordeste\", \n           \"SE\",        12.0,            11, \"Nordeste\",   \n           \"BA\",        12.0,          7.61, \"Nordeste\",\n           \"MG\",        14.0,          10.5, \"Sudeste\", \n           \"ES\",        21.7,          10.7, \"Sudeste\", \n           \"RJ\",        26.8,          10.7, \"Sudeste\", \n           \"SP\",        19.4,          12.7, \"Sudeste\", \n           \"PR\",        11.9,          11.7, \"Sul\", \n           \"SC\",          17,          11.8, \"Sul\", \n           \"RS\",        16.5,          8.46, \"Sul\",\n           \"MS\",        3.67,          11.9, \"Centro Oeste\", \n           \"MT\",        3.29,          12.7, \"Centro Oeste\", \n           \"GO\",        9.88,          13.6, \"Centro Oeste\", \n           \"DF\",        35.4,          17.8, \"Centro Oeste\"\n)\n\nPode-se visualizar este gráfico usando a sigla de cada estado e os números na tabela como a posição do texto. Assim, fica evidente como o DF tem uma proporção grande de apartamentos e de domicílios alugados. Maranhão e Piauí, por outro lado, tem poucos apartamentos e poucos domicílios alugados. Já Goiás tem um percentual relativamente grande de unidades alugadas, apesar de ter quase 90% de casas.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state),\n    size = 4\n    )\n\n\n\n\n\n\n\n\nPode-se, naturalmente, mapear uma das variáveis categóricas num elemento estético. No gráfico abaixo, cada região é destaca com uma cor distinta.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state, color = as.factor(name_region)),\n    size = 4\n    ) +\n  scale_color_brewer(name = \"Região\", type = \"qual\", palette = 6) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nPor fim, vale notar que existe uma função equivalente chamada geom_label. A única diferença desta função é que ela coloca o texto dentro de um pequeno quadrado com fundo branco.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_label(\n    aes(label = abbrev_state, color = as.factor(name_region)),\n    size = 4\n    ) +\n  scale_color_brewer(name = \"Região\", type = \"qual\", palette = 6) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nTrocando a fonte\nComo citado acima, é possível modificar a fonte do texto usando o argumento family. As fontes disponíveis no R dependem do sistema operacional em que ele está rodando. Para verificar as fontes disponíveis no seu computador você pode tentar uma das duas opções abaixo\n\n#&gt; Instale o pacote sysfonts se necessário\ninstall.packages(\"sysfonts\")\nlibrary(sysfonts)\n\nsysfonts::font_families()\n#&gt; [1] \"sans\"  \"serif\" \"mono\"\n#&gt; \n\n#&gt; Ou use a função system\navailable_fonts &lt;- system(\"fc-list : family\", intern = TRUE)\navailable_fonts &lt;- available_fonts[order(available_fonts)]\n#&gt; Mostra as primeiras dez fontes encontradas no sistema\navailable_fonts[1:10]\n\n#&gt;  [1] \".Al Bayan PUA\"                                            \n#&gt;  [2] \".Al Nile PUA\"                                             \n#&gt;  [3] \".Al Tarikh PUA\"                                           \n#&gt;  [4] \".Apple Color Emoji UI\"                                    \n#&gt;  [5] \".Apple SD Gothic NeoI,Apple SD 산돌고딕 Neo\"              \n#&gt;  [6] \".Aqua Kana,.Aqua かな\"                                    \n#&gt;  [7] \".Aqua Kana,.Aqua かな,.Aqua Kana Bold,.Aqua かな ボールド\"\n#&gt;  [8] \".Arial Hebrew Desk Interface\"                             \n#&gt;  [9] \".Baghdad PUA\"                                             \n#&gt; [10] \".Beirut PUA\"  \n\nO gráfico abaixo mostra um exemplo de como modificar a fonte do texto. A depender do seu sistema operacional o código pode não funcionar.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state),\n    family = \"mono\"\n    )\n\n\n\n\n\n\n\n\nUma solução mais consistente é de usar o pacote showtext. Tenho um post explicando como este pacote funciona. O código abaixo importa a fonte Montserrat do Google Fonts e aplica no gráfico.\n\nlibrary(showtext)\nsysfonts::font_add_google(\"Montserrat\", \"Montserrat\")\nshowtext::showtext_auto()\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state),\n    family = \"Montserrat\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#combinando-texto-com-outros-gráficos",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#combinando-texto-com-outros-gráficos",
    "title": "Estético: Destacando informação",
    "section": "Combinando texto com outros gráficos",
    "text": "Combinando texto com outros gráficos\nComo de costume, um dos grandes diferenciais do ggplot2 é a possibilidade de adicionar elementos aos gráficos sequencialmente. Podemos resgatar nosso exemplo do PIB com os ciclos de recessão.\nNo código abaixo, uso um pouco de álgebra para plotar o texto no meio da região sombreada.\n\nggplot() +\n  geom_hline(yintercept = 100) +\n  geom_line(\n    data = pib,\n    aes(x = ref.date, y = value)\n  ) +\n  geom_rect(\n    data = codace,\n    aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = Inf, group = label),\n    alpha = 0.4\n  ) +\n  geom_text(\n    data = codace,\n    aes(x = (rec_end - rec_start) / 2 + rec_start, y = 180, label = label),\n    size = 3\n  )\n\n\n\n\n\n\n\n\n\nExemplo: IPCA\nOutra combinação popular é de texto com colunas. No gráfico abaixo mostro a variação anual do IPCA com o valor percentual indicado em cima de cada coluna. Note como a função geom_text() recebe y + 0.5 como posição vertical.\n\nipca &lt;- tibble::tribble(\n  ~ano, ~ipca,\n  2013, 5.91,\n  2014, 6.41,\n  2015, 10.67,\n  2016, 6.29,\n  2017, 2.95,\n  2018, 3.75,\n  2019, 4.31,\n  2020, 4.52,\n  2021, 10.06,\n  2022, 5.79\n)\n\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  #&gt; Desenha a coluna com o valor do IPCA\n  geom_col() + \n  #&gt; Linha horizontal no eixo-e\n  geom_hline(yintercept = 0) +\n  #&gt; Número em cima de cada coluna\n  geom_text(aes(y = ipca + 0.5, label = ipca)) +\n  #&gt; Força o eixo-x a imprimir todos os valores\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nUma maneira alternativa de chegar no mesmo resultado é usando o argumento nudge_y, que cria um pequeno deslocamento no eixo-y.\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  #&gt; Usa nudge_y ao invés de ipca + 0.5\n  geom_text(aes(y = ipca, label = ipca), nudge_y = 0.5) +\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nAs funções round, paste, paste0 e format podem ser muito úteis para melhor apresentar resultados numéricos6. Esta funções servem para arredondar números, concatenar strings e formatar números (usando ponto para separar milhar, vírgula para separar decimal, etc.). No código abaixo mostro um passo-a-passo de como usar estas funções para formatar o número em cima de cada coluna.\n\nipca &lt;- ipca %&gt;%\n  mutate(\n    # Arredonda até a primeira casa decimal\n    ipca_label = round(ipca, 1),\n    # Usa vírgula como separador de decimal\n    ipca_label = format(ipca_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    ipca_label = paste0(ipca_label, \"%\")\n  )\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  geom_text(aes(y = ipca + 0.5, label = ipca_label)) +\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\n\n\nExemplo: Demografia\nA tabela abaixo mostra a representatividade de cada grande grupo demográfico na população brasileira aos longo dos anos. Aqui, usam-se as seguintes definições: população jovem (14 anos ou menos); população adulta (de 15 a 64 anos); população idosa (65 anos ou mais). Os valores estão em percentuais.\n\ntbl_demographics &lt;- tribble(\n  ~year, ~young, ~adult, ~elder,\n  #----#-------#-------#-------#\n   1950, 0.4246, 0.5515, 0.0239,\n   1960, 0.4400, 0.5338, 0.0261,\n   1970, 0.4284, 0.5400, 0.0315,\n   1980, 0.3859, 0.5763, 0.0377,\n   1990, 0.3545, 0.6012, 0.0442,\n   2000, 0.3011, 0.6442, 0.0545,\n   2010, 0.2499, 0.6817, 0.0683,\n   2020, 0.2099, 0.6986, 0.0914\n)\n\nAntes de visualizar os dados é preciso converter esta tabela para um formato longitudinal, onde cada linha representa um ano-grupo, isto é, cada linha indica o share da população de determinado grupo da população num determinado ano. Para isto uso a função pivot_longer(). Além disso eu transformo a coluna age_group num factor ordenado para melhorar a visualização.\nNo exemplo abaixo eu também ajusto o dado da coluna share: eu multiplico o valor por 100, arredondo até a primeira casa decimal, uso a vírgula como separador de decimal e coloco um sinal de “%” após o número.\n\ndemo &lt;- tbl_demographics %&gt;%\n  pivot_longer(\n    cols = young:elder,\n    names_to = \"age_group\",\n    values_to = \"share\"\n  ) %&gt;%\n  mutate(\n    age_group = factor(age_group, levels = c(\"elder\", \"adult\", \"young\")),\n    # Arredonda na primeira casa decimal\n    share_label = round(share * 100, 1),\n    # Usa vírgula como separador de decimal\n    share_label = format(share_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    share_label = paste0(share_label, \"%\")\n    )\n\nPara montar o gráfico eu utilizo o argumento position_stack(), já apresentado no post introdutório sobre gráficos de coluna. A função geom_text() aceita o mesmo argumento; note que coloco o valor vjust = 0.5. Este valor indica que desejo que o texto fica exatamente no meio da coluna, no seu respectivo grupo.\nOutras opções seriam: vjust = 0 para colocar o valor na base da coluna ou vjust = 1 para colocar o valor no topo da coluna.\n\nggplot(demo, aes(x = year, y = share, fill = age_group)) +\n  geom_col(position = position_stack()) +\n  geom_text(aes(label = share_label), position = position_stack(vjust = 0.5)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nExemplo: carros (de novo…)\nPor fim, monto mais um exemplo de como combinar gráficos de coluna com texto. No gráfico abaixo eu mostro a eficiência (milhas por galão) de um subconjunto de carros (os que tem 4 cilindradas). Além disso, eu ordeno o nome dos carros pela variável mpg.\n\ntbl_cars &lt;- mtcars %&gt;%\n  filter(cyl == 4) %&gt;%\n  mutate(\n    name_car = rownames(.),\n    name_car = factor(name_car),\n    name_car = forcats::fct_reorder(name_car, mpg)\n    ) %&gt;%\n  select(name_car, mpg)\n\nUsando o geom_text() eu consigo plotar o nome do modelo dentro da coluna assim como o respectivo valor de mpg. Note como uso fontface = 'bold' para deixar o valor numérico em negrito.\n\nggplot(tbl_cars, aes(x = name_car, y = mpg)) +\n  geom_col(fill = \"#186177\") +\n  geom_text(\n    aes(y = 1, label = name_car),\n    color = \"#F9F7F3\",\n    hjust = 0) +\n  geom_text(\n    aes(y = mpg - 2, label = mpg),\n    color = \"#F9F7F3\",\n    fontface = \"bold\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nInfelizmente, o gráfico acaba sobrecarregado com informação redundante, mas veremos como controlar os aspectos estéticos do gráfico em si (e.g. cor do fundo, texto nos eixos, etc.) num post futuro."
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#footnotes",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#footnotes",
    "title": "Estético: Destacando informação",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMais especificamente, a série trimestral do PIB dessazonalizado a preços de mercado com valores encadeados com base no primeiro trimestre de 1995.↩︎\nO Comitê de Datação de Ciclos Econômicos (CODACE) organizado pela Fundação Getúlio Vargas (FGV) se reúne periodicamente para datar os ciclos econômicos brasileiros.↩︎\nEste esforço adicional é feito por motivos didáticos. Na prática não é necessário usar a função tribble.↩︎\nPara mais informações sobre esta base consulte help(\"mtcars\").↩︎\nO pacote {ggrepel} cria pequenos deslocamentos na posição do texto e setas para melhor dispor os dados. Para mais informações veja o site.↩︎\nOutra opção interessante é função glue::glue().↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-pib-e-ciclos",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-pib-e-ciclos",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: PIB e ciclos",
    "text": "Exemplo: PIB e ciclos\nPodemos resgatar nosso exemplo do PIB com os ciclos de recessão.\nNo código abaixo, uso um pouco de álgebra para plotar o texto no meio da região sombreada.\n\nggplot() +\n  geom_hline(yintercept = 100) +\n  geom_line(\n    data = pib,\n    aes(x = ref.date, y = value)\n  ) +\n  geom_rect(\n    data = codace,\n    aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = Inf, group = label),\n    alpha = 0.4\n  ) +\n  geom_text(\n    data = codace,\n    aes(x = (rec_end - rec_start) / 2 + rec_start, y = 180, label = label),\n    size = 3\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-ipca",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-ipca",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: IPCA",
    "text": "Exemplo: IPCA\nPodemos também mesclar um gráfico de colunas com texto. A tabela traz os valores do IPCA acumulados ano a ano desde 2013.\n\nipca &lt;- tibble::tribble(\n  ~ano, ~ipca,\n  2013, 5.91,\n  2014, 6.41,\n  2015, 10.67,\n  2016, 6.29,\n  2017, 2.95,\n  2018, 3.75,\n  2019, 4.31,\n  2020, 4.52,\n  2021, 10.06,\n  2022, 5.79\n)\n\nNo gráfico abaixo mostro a variação anual do IPCA com o valor percentual indicado em cima de cada coluna. Note como a função geom_text() recebe y + 0.5 como posição vertical.\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  #&gt; Desenha a coluna com o valor do IPCA\n  geom_col() + \n  #&gt; Linha horizontal no eixo-e\n  geom_hline(yintercept = 0) +\n  #&gt; Número em cima de cada coluna\n  geom_text(aes(y = ipca + 0.5, label = ipca)) +\n  #&gt; Força o eixo-x a imprimir todos os valores\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nUma maneira alternativa de chegar no mesmo resultado é usando o argumento nudge_y, que cria um pequeno deslocamento no eixo-y.\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  #&gt; Usa nudge_y ao invés de ipca + 0.5\n  geom_text(aes(y = ipca, label = ipca), nudge_y = 0.5) +\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nAs funções round, paste, paste0 e format podem ser muito úteis para melhor apresentar resultados numéricos6. Esta funções servem para arredondar números, concatenar strings e formatar números (usando ponto para separar milhar, vírgula para separar decimal, etc.). No código abaixo mostro um passo-a-passo de como usar estas funções para formatar o número em cima de cada coluna.\n\nipca &lt;- ipca %&gt;%\n  mutate(\n    # Arredonda até a primeira casa decimal\n    ipca_label = round(ipca, 1),\n    # Usa vírgula como separador de decimal\n    ipca_label = format(ipca_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    ipca_label = paste0(ipca_label, \"%\")\n  )\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  geom_text(aes(y = ipca + 0.5, label = ipca_label)) +\n  scale_x_continuous(breaks = 2013:2022)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-demografia",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-demografia",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: Demografia",
    "text": "Exemplo: Demografia\nA tabela abaixo mostra a representatividade de cada grande grupo demográfico na população brasileira aos longo dos anos. Aqui, usam-se as seguintes definições: população jovem (14 anos ou menos); população adulta (de 15 a 64 anos); população idosa (65 anos ou mais). Os valores estão em percentuais.\n\ntbl_demographics &lt;- tribble(\n  ~year, ~young, ~adult, ~elder,\n  #----#-------#-------#-------#\n   1950, 0.4246, 0.5515, 0.0239,\n   1960, 0.4400, 0.5338, 0.0261,\n   1970, 0.4284, 0.5400, 0.0315,\n   1980, 0.3859, 0.5763, 0.0377,\n   1990, 0.3545, 0.6012, 0.0442,\n   2000, 0.3011, 0.6442, 0.0545,\n   2010, 0.2499, 0.6817, 0.0683,\n   2020, 0.2099, 0.6986, 0.0914\n)\n\nAntes de visualizar os dados é preciso converter esta tabela para um formato longitudinal, onde cada linha representa um ano-grupo, isto é, cada linha indica o share da população de determinado grupo da população num determinado ano. Para isto uso a função pivot_longer(). Além disso eu transformo a coluna age_group num factor ordenado para melhorar a visualização.\nNo exemplo abaixo eu também ajusto o dado da coluna share: eu multiplico o valor por 100, arredondo até a primeira casa decimal, uso a vírgula como separador de decimal e coloco um sinal de “%” após o número.\n\ndemo &lt;- tbl_demographics %&gt;%\n  pivot_longer(\n    cols = young:elder,\n    names_to = \"age_group\",\n    values_to = \"share\"\n  ) %&gt;%\n  mutate(\n    age_group = factor(age_group, levels = c(\"elder\", \"adult\", \"young\")),\n    # Arredonda na primeira casa decimal\n    share_label = round(share * 100, 1),\n    # Usa vírgula como separador de decimal\n    share_label = format(share_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    share_label = paste0(share_label, \"%\")\n    )\n\nPara montar o gráfico eu utilizo o argumento position_stack(), já apresentado no post introdutório sobre gráficos de coluna. A função geom_text() aceita o mesmo argumento; note que coloco o valor vjust = 0.5. Este valor indica que desejo que o texto fica exatamente no meio da coluna, no seu respectivo grupo.\nOutras opções seriam: vjust = 0 para colocar o valor na base da coluna ou vjust = 1 para colocar o valor no topo da coluna.\n\nggplot(demo, aes(x = year, y = share, fill = age_group)) +\n  geom_col(position = position_stack()) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = share_label), position = position_stack(vjust = 0.5)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-carros-de-novo",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-carros-de-novo",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: carros (de novo…)",
    "text": "Exemplo: carros (de novo…)\nPor fim, monto mais um exemplo de como combinar gráficos de coluna com texto. No gráfico abaixo eu mostro a eficiência (milhas por galão) de um subconjunto de carros (os que tem 4 cilindradas). Além disso, eu ordeno o nome dos carros pela variável mpg.\n\ntbl_cars &lt;- mtcars %&gt;%\n  filter(cyl == 4) %&gt;%\n  mutate(\n    name_car = rownames(.),\n    name_car = factor(name_car),\n    name_car = forcats::fct_reorder(name_car, mpg)\n    ) %&gt;%\n  select(name_car, mpg)\n\nUsando o geom_text() eu consigo plotar o nome do modelo dentro da coluna assim como o respectivo valor de mpg. Note como uso fontface = 'bold' para deixar o valor numérico em negrito.\n\nggplot(tbl_cars, aes(x = name_car, y = mpg)) +\n  geom_col(fill = \"#186177\") +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    aes(y = 1, label = name_car),\n    color = \"#F9F7F3\",\n    hjust = 0) +\n  geom_text(\n    aes(y = mpg - 2, label = mpg),\n    color = \"#F9F7F3\",\n    fontface = \"bold\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nInfelizmente, o gráfico acaba sobrecarregado com informação redundante, mas veremos como controlar os aspectos estéticos do gráfico em si (e.g. cor do fundo, texto nos eixos, etc.) num post futuro."
  },
  {
    "objectID": "posts/general-posts/2022-09-brasilia/index.html",
    "href": "posts/general-posts/2022-09-brasilia/index.html",
    "title": "Mapa de altitude de ruas de Brasília",
    "section": "",
    "text": "Inspirado num antigo post de BlakeRMills, criador do pacote {MetBrewer}, criei um mapa com a altitude das ruas em Brasília.\nEm breve farei um post com tutorial detalhado e também pretendo replicar este tipo de mapa para outras cidades interessantes. O código para replicar o gráfico está abaixo.\n\n\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(osmdata)\nlibrary(purrr)\nsf::sf_use_s2(FALSE)\nsysfonts::font_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nshowtext::showtext_auto()\n\nurl = \"https://pt.wikipedia.org/wiki/Lista_de_municípios_do_Brasil_por_população\"\n\ntab = xml2::read_html(url) |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(2)\n\nas_numeric_char = Vectorize(function(x) {\n  ls = stringr::str_extract_all(x, \"[[:digit:]]\")\n  y = paste(ls[[1]], collapse = \"\")\n  as.numeric(y)\n})\n\nclean_tab = tab |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    code_muni = codigo_ibge,\n    name_muni = municipio,\n    rank = posicao,\n    name_state = unidade_federativa,\n    pop = populacao\n  ) |&gt; \n  filter(name_muni != \"Brasil\") |&gt; \n  mutate(\n    code_muni = as.numeric(code_muni),\n    pop = as_numeric_char(pop),\n    rank = rank(-pop)\n  )\n\ntop20 = slice_max(clean_tab, pop, n = 20)\n\nget_border = function(city) {\n  \n  #&gt; Encontra o código do município\n  code_muni = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(code_muni) |&gt; \n    unique()\n  \n  stopifnot(length(code_muni) == 1)\n  \n  #&gt; Baixa o shapefile do município\n  border = geobr::read_municipality(code_muni, showProgress = FALSE)\n  \n  return(border)\n}\n\nget_streets = function(city, border) {\n  \n  #&gt; Encontra o nome da Unidade Federativa\n  nome_uf = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(name_state)\n  #&gt; Monta o nome do local\n  name_place = stringr::str_glue(\"{city}, {nome_uf}, Brazil\")\n  #&gt; Monta a query\n  place = opq(bbox = getbb(name_place))\n  \n  #&gt; Importa todas as principais vias da cidade\n  # streets = add_osm_feature(\n  #   place,\n  #   key = \"highway\",\n  #   value = c(\n  #     \"motorway\", \"primary\", \"motorway_link\", \"primary_link\",\n  #     \"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\",\n  #     \"residential\"\n  #     )\n  # )\n  \n  streets = add_osm_feature(place, key = \"highway\")\n  \n  #&gt; Converte o dado\n  streets = streets %&gt;%\n    osmdata_sf() %&gt;%\n    .$osm_lines %&gt;%\n    select(osm_id, name) %&gt;%\n    st_transform(crs = 4674)\n  \n  #&gt; Enconrtra a intersecção entre as estradas e o limites do município\n  streets_border = st_intersection(streets, border)\n  \n  # out = list(streets = streets, streets_border = streets_border)\n  \n  return(streets_border)\n  \n}\n\nget_elevation = function(border, z = 8) {\n  \n  altitude = elevatr::get_elev_raster(border, z = z, clip = \"bbox\")\n  altitude = raster::rasterToPolygons(altitude)\n  altitude = st_as_sf(altitude)\n  names(altitude)[1] = \"elevation\"\n  \n  altitude = st_transform(altitude, crs = 4674)\n  altitude = suppressWarnings(st_intersection(altitude, border))\n  altitude = filter(altitude, st_is_valid(altitude))\n  \n  return(altitude)\n  \n}\n\nadd_jenks_breaks = function(shp, k = 7, round = TRUE, r = 0) {\n  #&gt; Classifica os dados de altitude em k grupos segundo o algo. de Jenks\n  jbreaks = BAMMtools::getJenksBreaks(shp$elevation, k = k)\n  #&gt; Arredonda os números para chegar numa legenda menos quebrada\n  if (round) {\n    jbreaks = round(jbreaks, r)\n  }\n  #&gt; Cria a coluna 'jenks_group' que classifica cada valor num grupo\n  shp = mutate(shp, jenks_group = cut(elevation, jbreaks))\n  \n  #&gt; Verifica se todas as observações tem um grupo\n  check = any(is.na(shp$jenks_group))\n  if (check) {\n    warning(\"Some observations have failed to be grouped\")\n  }\n  \n  #&gt; Transforma os groups em legendas\n  labels = get_jenks_labels(jbreaks)\n  \n  #&gt; Retorna o output numa lista\n  out = list(shp = shp, labels = labels)\n  return(out)\n  \n}\n\nget_jenks_labels = function(x) {\n  labels = paste(x, x[-1], sep = \"–\")\n  labels[1] = paste(\"&lt;\", x[2])\n  labels[length(labels)] = paste(\"&gt;\", max(x))\n  return(labels)\n}\n\nget_streets_altitude = function(altitude, streets) {\n  \n  stopifnot(any(colnames(altitude) %in% \"jenks_group\"))\n  \n  #&gt; Get all groups\n  groups = levels(altitude$jenks_group)\n  \n  #&gt; For each group get the full polygon and join with streets\n  join_streets = function(group) {\n    \n    poly = altitude %&gt;%\n      filter(jenks_group == group) %&gt;%\n      st_union(.) %&gt;%\n      st_as_sf() %&gt;%\n      st_make_valid()\n    \n    joined = suppressWarnings(st_intersection(streets, poly))\n    \n    return(joined)\n    \n  }\n  #&gt; Apply the function to all groups\n  street_levels = purrr::map(groups, join_streets)\n  #&gt; Bind all results together\n  out = bind_rows(street_levels, .id = \"level\")\n  \n  return(out)\n  \n}\n\nmap_plot = function(shp, labels, title, showtext = TRUE) {\n  \n  colors = viridis::plasma(n = length(labels) + 1)\n  colors = colors[-length(colors)]\n  \n  font = ifelse(showtext == TRUE, \"Roboto Condensed\", \"sans\")\n  \n  plot =\n    ggplot(data = shp) +\n    geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n    scale_color_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    scale_fill_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n    ggtitle(title) +\n    ggthemes::theme_map() +\n    coord_sf() +\n    theme(\n      plot.title = element_text(\n        size = 30,\n        hjust = 0.5,\n        family = font\n      ),\n      legend.title = element_text(\n        size = 20,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.text = element_text(\n        size = 14,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.position = \"top\",\n      legend.direction = \"horizontal\",\n      plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n    )\n  \n  return(plot)\n  \n}\n\nmap_altitude = function(city, k, z) {\n  \n  #&gt; Importa o shape do limite do município\n  message(\"Importando os limites do município: \", city)\n  city_border = get_border(city)\n  #&gt; Importa as principais vias da cidade e junta com o limite do muni\n  message(\"Importando as vias.\")\n  city_street = get_streets(city, city_border)\n  #&gt; Importa a altitude da cidade\n  message(\"Importando a altitude.\")\n  city_elevation = suppressMessages(get_elevation(city_border, z = z))\n  #&gt; Classifica a altitude em grupos\n  message(\"Classificando e juntando os shapefiles.\")\n  jenks = add_jenks_breaks(city_elevation, k = k)\n  city_elevation = jenks[[\"shp\"]]\n  labels = jenks[[\"labels\"]]\n  #&gt; Junta a altitude (agrupada) com as vias\n  city_street_elevation = get_streets_altitude(city_elevation, city_street)\n  \n  #&gt; Monta o mapa final\n  message(\"Gerando o mapa final.\")\n  plot = map_plot(city_street_elevation, labels = labels, title = city)\n  message(\"Feito.\")\n  #&gt; Retorna o output numa lista\n  out = list(\n    shp = city_street_elevation,\n    streets = city_street,\n    elevation = city_elevation,\n    plot = plot\n  )\n  \n  return(out)\n  \n}\n\nexport_citymap = function(city, w = 14, h = 16, ...) {\n  \n  plot = map_altitude(city, ...)$plot\n  \n  if (is.numeric(city)) {\n    name_city = cities_brasil |&gt; \n      filter(code_muni == city) |&gt; \n      pull(name_muni)\n  } else if (is.character(city)) {\n    name_city = city\n  }\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.svg\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.png\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n}\n\nsafe_export = purrr::safely(export_citymap)\n\nparams = tribble(\n  ~city,       ~z, ~k,\n  #----------------#---#---#\n  \"São Paulo\",       8,  7,\n  \"Rio de Janeiro\",  9,  7,\n  \"Brasília\",        8,  7,\n  \"Fortaleza\",      11,  7,\n  \"Salvador\",       10,  7,\n  \"Belo Horizonte\", 10,  7,\n  \"Manaus\",          6,  7,\n  \"Curitiba\",       10,  7,\n  \"Recife\",         10,  7,\n  \"Goiânia\",         8,  7,\n  \"Porto Alegre\",    8,  7,\n  \"Belém\",           9,  7,\n  \"Guarulhos\",       9,  7,\n  \"Campinas\",        9,  7,\n  \"São Luís\",        9,  7,\n  \"Maceió\",         11,  8,\n  \"Campo Grande\",    7,  7,\n  \"São Gonçalo\",     8,  7,\n  \"Teresina\",        10,  7,\n  \"João Pessoa\",     9,  7,\n  \"Joinville\",       9,  7\n  )\n\n#&gt; Exportar todas as cidades listadas acima\n#pmap(params, safe_export)\n#&gt; Exportar uma cidade em particular\n#safe_export(\"Brasília\", z = 8, k = 7)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html",
    "title": "Estético: Escalas e Cores",
    "section": "",
    "text": "Este post é o primeiro da série de tutoriais que não vai tratar de um tipo de gráfico ou, mais especificamente, de um tipo de geom. Escalas, no contexto do ggplot2 são tanto os eixos do gráfico como as suas cores, quando usa-se a função aes para mapear alguma variável em cores. As funções scale_* controlam todos os aspectos das escalas, incluindo as cores e as suas respectivas legendas.\nAlguns aspectos mais detalhados das escalas, como a fonte do texto, o tamanho, etc. são controlados por uma função mais específica theme. Em alguns exemplos deste post eu apresento como utilizar esta função, mas uma apresentação mais formal fica postergada para outro momento. A função theme é talvez a mais burocrática e complexa do pacote ggplot2.\nO código abaixo lista os pacotes necessários para acompanhar este post.\n\n#&gt; Os pacotes necessários para acompanhar este post\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(readr)\n#&gt; Importa os dados limpos da Zona OD.\npod &lt;- read_csv(\"https://github.com/viniciusoike/restateinsight/raw/main/posts/ggplot2-tutorial/table_pod.csv\")\n#&gt; Seleciona apenas zonas de São Paulo com população acima de zero\npod &lt;- filter(pod, code_muni == 36, pop &gt; 0)\n\nPara este post, além das bases de dados que já acompanham o pacote ggplot2 também uso a base da Pesquisa Origem e Destino (POD) de São Paulo, de 2018. A POD é uma pesquisa feita pela companhia Metrô de São Paulo que compila informações sobre os deslocamentos diários da população da Região Metropolitana de São Paulo. Além de dados sobre mobilidade, a POD também reúne uma série de informações socioeconômicas. Os dados são agregados a nível de Zona Origem e Destino ou Zona OD; as zonas OD são subconjuntos de distritos de São Paulo e das demais cidades da Região Metropolitana e, na prática, são como bairros de cada cidade1.\nEu mesmo tratei a base e as funções utilizadas para montar esta tabela estão disponíveis no repositório {tidypod} do meu GitHub.\nNeste post vou explorar apenas as zonas OD da cidade de São Paulo, excluindo as zonas “não-residenciais”, como a Cidade Universitária (campus da USP)."
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#tipos-de-variáveis",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#tipos-de-variáveis",
    "title": "Estético: Escalas e Cores",
    "section": "Tipos de variáveis",
    "text": "Tipos de variáveis\nO ggplot, grosso modo, divide as variáveis em contínuas e discretas. As variáveis contínuas, em geral, são numéricas e podem assumir qualquer valor; já as variáveis discretas costumam ser “categóricas” e são “contáveis”. O preço de um imóvel, por exemplo, é uma variável contínua. Já uma variável que categoriza um imóvel entre “casa” e “apartamento” é uma variável discreta.\nEsta lógica é aplicada nas funções que controlam os eixos x e y de um gráfico.\n\nscale_y_continuous()\nscale_y_discrete()\nscale_x_continuous()\nscale_x_discrete()\n\nUma lógica muito similar se aplicas às principais funções que controlam as cores e a legenda de cores de um gráfico:\n\nscale_color_continuous()\nscale_color_discrete()\nscale_fill_continuous()\nscale_fill_discrete()\n\nMesmo quando modificamos outros aspectos estéticos do gráfico como size e alpha temos:\n\nscale_alpha_continuous()\nscale_alpha_discrete()\nscale_size_continuous()\nscale_size_discrete()\n\n\nVariáveis contínuas\nUma variável contínua costuma representar algum número. No R há várias formas de armazenar números, mas isto não costuma ser muito relevante para a tarefa de visualização dos dados. Na maior parte dos casos, basta garantir que a coluna numérica em questão seja um número usando is.numeric() ou as.numeric().\nComo mencionado acima, exemplos comuns de variáveis contínuas são: preço de um imóvel, salário de um indivíduo, a taxa de inflação num mês, etc.\n\n\nVariáveis discretas\nUma variável discreta costuma representar uma categoria. No R existe uma classe especial de variável para armazenar este tipo de dado chamada factor. Um factor é um vetor de texto ou de números que segue uma ordem. Além de ter uma ordem, cada elemento pode ter um label.\n\nfactor(x = c(...), levels = c(...), labels = c(...))\n\nPara se definir um factor basta usar a função homônima. Note que na ausência de uma ordem explicitamente definida, o R organiza o vetor em ordem alfabética. Se, ao invés de um vetor de texto tivéssemos usado um vetor de números, eles teriam sido ordenados no sentido ascedente (do menor para o maior).\nPara acessar a ordem do factor pode-se usar a função order() ou, mais especificamente, a função levels().\n\n#&gt; Criando um factor \nmedalhas &lt;- factor(c(\"ouro\", \"prata\", \"bronze\"))\n#&gt; [1] ouro   prata  bronze\n#&gt; Levels: bronze ouro prata\n\n#&gt; Conferindo a ordem dos elementos\norder(medalhas)\n#&gt; [1] 3 1 2\nlevels(medalhas)\n#&gt; [1] \"bronze\" \"ouro\"   \"prata\" \n\nO código abaixo recria o factor, deixando mais explícito a estrutura deste tipo de objeto. Note que o argumento levels e labels não precisam ser repetidos.\n\n#&gt; Criando um factor \nmedalhas &lt;- factor(\n  c(\"ouro\", \"prata\", \"bronze\", \"bronze\", \"ouro\", \"bronze\"),\n  levels = c(\"bronze\", \"prata\", \"ouro\"),\n  labels = c(\"Bronze\", \"Prata\", \"Ouro\")\n  )\n\nmedalhas\n#&gt; [1] Ouro   Prata  Bronze Bronze Ouro   Bronze\n#&gt; Levels: Bronze Prata Ouro\n\nTrabalhar com factors pode ser uma tarefa bastante frustrante. Neste sentido, recomendo muito o uso do pacote forcats, que provê uma série de funções fct_* que facilitam muito a manipulação deste tipo de objeto. Os exemplos abaixo mostram algumas das funções mais úteis deste pacote.\n\ndf &lt;- data.frame(\n  x = c(5, 2, 3),\n  y = factor(c(\"bronze\", \"prata\", \"ouro\"))\n)\n\n#&gt; Troca a ordem do factor segundo algum outro vetor\nfct_reorder(df$y, df$x)\n#&gt; [1] bronze prata  ouro  \n#&gt; Levels: prata ouro bronze\n\n#&gt; Troca os labels do factor usando uma função\nfct_relabel(df$y, toupper)\n#&gt; [1] BRONZE PRATA  OURO  \n#&gt; Levels: BRONZE OURO PRATA\n\n#&gt; Troca os labels do factor manualmente\nfct_recode(df$y, \"Bronze\" = \"bronze\")\n#&gt; [1] Bronze prata  ouro  \n#&gt; Levels: Bronze ouro prata\n\n#&gt; Conta a ocorrência de cada factor\nfct_count(df$y)\n#&gt; A tibble: 3 × 2\n#&gt;   f          n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 bronze     1\n#&gt; 2 ouro       1\n#&gt; 3 prata      1\n\nPor fim, vale comentar brevemente sobre uma particularidade de um factor criado a partir de uma variável numérica. Para converter um factor de texto em character basta usar as.character(x). Para converter de volta um factor de números é preciso usar as.numeric(as.character(x)).\n\nx &lt;- c(1, 10, 2, 5, 1)\ny &lt;- as.factor(x)\n\n#&gt; Por padrão, as.numeric retorna a ordem do factor. Equivalente a order()\nas.numeric(y)\n#&gt; [1] 1 4 2 3 1\n\n#&gt; Para converter de volta no número original\nas.numeric(as.character(y))\n#&gt; [1]  1 10  2  5  1"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#escalas-o-básico",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#escalas-o-básico",
    "title": "Estético: Escalas e Cores",
    "section": "Escalas: o básico",
    "text": "Escalas: o básico\n\nEscalas Contínuas\nVamos começar com um gráfico simples que mostra a renda domiciliar média da Zona OD no eixo-x e o número médio de carros por domicílio no eixo-y.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNo caso do gráfico acima, ambas as variáveis são contínuas, portanto, para alterar algum dos eixos usa-se as funções scale_x_continuous() e scale_y_continuous(). Estas funções tem 5 argumentos principais: name, breaks, labels, limits e expand.\nO argumento name define o título do eixo. Alternativamente, pode-se usar a função labs, como fizemos em posts anteriores.\n\n#&gt; Define o título de cada eixo usando as funções scale\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(name = \"Renda Domiciliar Média (R$)\") +\n  scale_y_continuous(name = \"Automóveis por domicílio\")\n\n#&gt; Define o título de cada eixo usando a função labs\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  labs(x = \"Renda Domiciliar Média (R$)\", y = \"Automóveis por domicílio\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara controlar as ‘quebras’ do eixo-x (os pontos onde aparece cada número) usa-se o argumento breaks.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 20000, 2500))\n\n\n\n\n\n\n\n\nPor padrão, o número exibido no gráfico é igual ao argumento fornecido em breaks, mas pode-se alterar isto usando labels. Para ser mais preciso: breaks define a posição onde o labels vai ser exibido.\nNo exemplo abaixo uso o fato do salário mínimo, à época da pesquisa, ser de R$954.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(\n    breaks = 954 * c(2, 4, 6, 10, 15),\n    labels = stringr::str_glue(\"{c(2, 4, 6, 10, 15)} S.M.\")\n    )\n\n\n\n\n\n\n\n\nO argumento labels pode ser qualquer texto, desde que ele tenha o mesmo número de elementos que o argumento breaks. O pacote scales oferece algumas funções label_* pré-definidas que auxiliam a formatar as escalas. O exemplo abaixo mostra como usar a função label_dollar para formatar o eixo-x.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(0, 20000, 2500),\n    labels = label_dollar(big.mark = \".\")\n    )\n\n\n\n\n\n\n\n\nNa minha experiência, as funções mais úteis do pacote são:\n\nlabel_number(): usado para formar números de maneira geral\nlabel_percent(): usado para formatar números expressados percentualmente\nlabel_dollar(): usado para formatar números que representam dinheiro\n\nPara seguir o padrão brasileiro, utiliza-se big.mark = \".\" e decimal.mark = \",\".\nPara dar um “zoom-in” no gráfico pode-se alterar o argumento limits. Este argumento recebe um par de números para definir o número máximo e mínimo que deve ser plotado. Para deixar o eixo “livre” basta definir o valor como NA.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(limits = c(3000, 5000)) +\n  scale_y_continuous(limits = c(NA, 1))\n\n\n\n\n\n\n\n\nPor fim, o argumento expand diminui ou aumenta a distância entre o gráfico e o limite dos eixos. A aplicação mais comum disso é para reduzir o espaço em branco que “sobra” em alguns gráficos.\nO par de histogramas abaixo mostra a distribuição do número médio de automóveis por domicílio entre as zonas OD.\n\nggplot(pod, aes(x = car_rate)) +\n  geom_histogram(bins = 12, color = \"white\")\n\nggplot(pod, aes(x = car_rate)) +\n  geom_histogram(bins = 12, color = \"white\") +\n  scale_x_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEscalas discretas\nEscalas discretas funcionam praticamente da mesma forma. Uma distinção importante é que, no caso de uma variável discreta ou categórica, o eixo - por padrão - vai plotar o label da variável.\nO código abaixo encontra os 5 distritos mais populosos de São Paulo.\n\n#&gt; Seleciona os cinco distritos mais populosos\ndstr_pop &lt;- pod |&gt; \n  #&gt; Soma a variável pop em cada distrito\n  summarise(total_pop = sum(pop), .by = \"name_district\") |&gt; \n  #&gt; Encontra os cinco valores mais elevados \n  slice_max(total_pop, n = 5) |&gt; \n  #&gt; Converte a variável `name_district` para factor\n  mutate(name_district = factor(name_district))\n\nggplot(dstr_pop, aes(x = name_district, y = total_pop)) +\n  geom_col()\n\n\n\n\n\n\n\n\nCaso não se queira alterar o tipo da variável é possível definir os labels diretamente na função scale_x_discrete().\n\nggplot(dstr_pop, aes(x = name_district, y = total_pop)) +\n  geom_col() +\n  scale_x_discrete(\n    labels = c(\"Capão Redondo\", \"Grajaú\", \"Jd. Ângela\", \"Jd. São Luis\",\n               \"Sapopemba\")\n    )\n\n\n\n\n\n\n\n\nPor fim, vale notar que o argumento labels aceita uma função. Neste caso, fornece-se apenas o nome da função, sem argumentos explícitos. O gráfico abaixo mostra quatro exemplos.\n\nbase_plot &lt;- ggplot(dstr_pop, aes(x = name_district, y = total_pop)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n#&gt; Texto maísculo\nbase_plot + scale_x_discrete(labels = stringr::str_to_upper)\n#&gt; Texto minúsculo\nbase_plot + scale_x_discrete(labels = stringr::str_to_lower)\n#&gt; Texto em formato de 'título'\nbase_plot + scale_x_discrete(labels = stringr::str_to_title)\n#&gt; Quebras de linha automáticas no texto\nbase_plot + scale_x_discrete(labels = \\(x) stringr::str_wrap(x, width = 8))\n\n\n\n\n\n\n\n\n\n\nNo exemplo acima, utiliza-se a função stringr::str_to_upper para converter o texto do eixo x para maiúsculo.\nNo caso de uma função que precisa de um argumento adicional, como é o caso da função stringr::str_wrap é preciso criar uma ‘função anônima’. Uma função anônima funciona exatamente como uma função convencional e permite que um argumento adicional seja inserido.\nA sintaxe para definir uma função anônima é simplesmente function(x) g(x, ...) onde g(x) é alguma função qualquer. Por exemplo. function(x) mean(x, na.rm = TRUE). Alternativamente, é possível definir uma função anônima simplesmente com \\(x)."
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#um-pouco-mais-de-escalas",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#um-pouco-mais-de-escalas",
    "title": "Estético: Escalas e Cores",
    "section": "Um pouco mais de escalas",
    "text": "Um pouco mais de escalas\nAlém das funções scale_* há também algumas mais específicas. O código abaixo apresenta a função scale_x_log10() que, como o nome sugere, aplica uma transformação log na variável x.\nUma das vantagens de usar esta função, ao invés de transformar os dados usando a função log(), ou mesmo de usar trans = 'log' dentro de scale_x_continuous(), é que as quebras do eixo-x fiquem num formato num formato mais bonito como se vê abaixo.\nDe maneira geral, a função scale_x_log10() é útil quando há variância crescente nos dados ou a variável segue algum tipo de crescimento exponencial.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_log10()\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutra função interessante que eventualmente pode ser útil é a scale_*_reverse() que inverte a direção dos dados. Esta função é muito mais prática do que, por exemplo, trocar o sinal do dado original e aí utilizar o argumento labels para ajustar os números.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_count() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nPor fim, outra função interessante é a scale_*_binned(), que ajuda a discretizar uma varíalvel contínua. Ela funciona de maneira similar a um historgrama, argupando uma variável contínua em grupos: isto facilita a observação de padrões nos dados. No caso do gráfico abaixo, vê-se que o grupo mais comum é de zonas com renda entre R\\$4000 e R\\$6000 com número médio de automóveis por domicílios entre 0,6 e 0,8.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_count() +\n  scale_x_binned() +\n  scale_y_binned()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#datas",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#datas",
    "title": "Estético: Escalas e Cores",
    "section": "Datas",
    "text": "Datas\nUm dado em formato de data é significativamente mais complicado do que as variáveis contínuas e discretas que vimos acima. Existem inúmeros formatos distintos para se apresentar datas que variam de local para local; além disso, pode ser necessário lidar com fusos horários, feriados, anos bissextos, etc.\nIdealmente, toda coluna com datas deve sempre estar no formato YYYY-MM-DD, isto é, 2014-01-15 (15 de janeiro de 2014). Para converter um character em Date basta usar a função as.Date. Se a data não estiver no formato YYYY-MM-DD será necessário especificar o formato usando o argumento format. Para casos mais complexos recomendo o uso do pacote {lubridate}.\n\nx &lt;- c(\"2014-01-01\", \"2014-02-01\")\ny &lt;- c(\"01/01/2014\", \"01/02/2014\")\n\nas.Date(x)\n#&gt; [1] \"2014-01-01\" \"2014-02-01\"\nas.Date(y, format = \"%d/%m/%Y\")\n#&gt; [1] \"2014-01-01\" \"2014-02-01\"\n\nComo datas têm uma classe especial elas, por conseguinte, têm também algumas funções dedicadas. A função scale_x_date tem dois argumentos principais2:\n\ndate_breaks: que aceita valores como “1 year”, “3 months”, etc.\ndate_labels: que aceita valores como “%Y%m%d”, “%Y%b”, etc.\n\nVou apresentar esta função de maneira breve, diretamente atráveis de exemplos. O exemplo abaixo mostra o funcionamento geral desta função.\n\nggplot(economics, aes(x = date, y = psavert)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"5 year\", date_labels = \"%Y\")\n\n\n\n\n\n\n\n\nO próximo exemplo mostra como exibir o número do ano junto com a nome abreviado do mês. Note como o uso de \\n quebra a linha no eixo-x.\n\nprices_austin &lt;- txhousing %&gt;%\n  filter(city == \"Austin\", year %in% 2007:2011) %&gt;%\n  mutate(date = lubridate::make_date(year, month))\n\nggplot(prices_austin, aes(x = date, y = sales)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y\\n%b\")\n\n\n\n\n\n\n\n\nPara escolher datas específicas, usa-se o argumento convencional breaks.\n\ndatas &lt;- c(as.Date(\"2007-01-01\"), as.Date(\"2008-07-30\"), as.Date(\"2010-03-01\"))\n\nggplot(prices_austin, aes(x = date, y = sales)) +\n  geom_line() +\n  scale_x_date(breaks = datas, date_labels = \"%Y\\n%b\")\n\n\n\n\n\n\n\n\nO exemplo abaixo pula alguns passos, já que ainda não se apresentou formalmente a função theme. Essencialmente, a função theme diminui o número de linhas verticais no fundo do gráfico e gira o texto do eixo-x em 90 graus.\n\nprices_austin10 &lt;- txhousing %&gt;%\n  filter(city == \"Austin\", year %in% 2010:2011) %&gt;%\n  mutate(date = lubridate::make_date(year, month))\n\nggplot(prices_austin10, aes(x = date, y = sales)) +\n  geom_line() +\n  scale_x_date(\n    breaks = seq(as.Date(\"2010-01-01\"), as.Date(\"2011-12-01\"), by = \"month\"),\n    date_labels = \"%Y-%m\"\n    ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\n\n\n\n\nA tabela abaixo resume como funciona a codificação dos padrões de datas. Vale notar que os inputs %b e %B usam o nome dos meses definidos ou pelo sistema operacional ou pelo padrão de locale do R. O mesmo vale para %A e outras que imprimem algum texto.\nPara consultar o padrão do seu computador veja Sys.getlocale(). Para trocar o padrão usa-se Sys.setlocale(), mas recomendo evitar este tipo de comando a não ser que você saiba o que está fazendo. É importante reforçar que não basta somente definir um date_labels apropriado, é preciso também que o dado esteja no formato correto.\n\nCódigos de datas\n\n\nInput\nLabel\n\n\n\n\n%Y\nAno completo (e.g. 2010) (0000-9999)\n\n\n%y\nAno abreviação (e.g. 10) (00-99)\n\n\n%m\nMês em número (e.g. 11) (01-12)\n\n\n%b\nMês abreviação (e.g. Jan) (Jan-Dec)\n\n\n%B\nMês completo (e.g. January) (January-December)\n\n\n%d\nDia do mês (e.g. 02) (01-31)\n\n\n%a\nDia da semana, abreviação (Mon-Sun)\n\n\n%A\nDia da semana, completo (Monday-Sunday)\n\n\n%I\nHora, no padrão 12 horas (01-12)\n\n\n%H\nHora, no padrão 24 horas (00-23)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#cores-o-básico",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#cores-o-básico",
    "title": "Estético: Escalas e Cores",
    "section": "Cores: o básico",
    "text": "Cores: o básico\n\nVariáveis discretas\nNovamente, vamos começar com um exemplo simples. O código abaixo seleciona algumas Zonas OD e ranqueia elas segundo a variável prop_educ_superior que é o percentual de indivíduos (naquela Zona) com ensino superior.\n\nzonas &lt;- c(\"Vila Mariana\", \"Paraíso\", \"Saúde\", \"Jabaquara\", \"Grajaú\")\n\nsubpod &lt;- pod %&gt;%\n  filter(name_zone %in% zonas) %&gt;%\n  mutate(\n    name_zone = factor(name_zone),\n    name_zone = fct_reorder(name_zone, prop_educ_superior)\n    )\n\nUsando o argumento aes sabemos que é possível mapear uma cor diferente para cada uma das regiões como no gráfico abaixo.\n\nggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone))\n\n\n\n\n\n\n\n\nPara trocar as cores usamos a função scale_fill_manual. Note que uso a função scale_fill_* pois quero trocar as cores do fill. Vale notar que, neste caso, a função scale_fill_discrete atingiria o mesmo resultado.\n\ncores &lt;- c(\"#C0D1B6\", \"#A3C9A8\", \"#84B59F\", \"#69A297\", \"#50808E\")\n\nggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) +\n  scale_fill_manual(values = cores)\n\n\n\n\n\n\n\n\nNote que os mesmos argumentos name, labels, etc. continuam valendo.\n\nggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) +\n  scale_fill_manual(\n    name = \"ZONAS\",\n    values = cores,\n    labels = toupper\n    )\n\n\n\n\n\n\n\n\nPara controlar os detalhes da legenda de cores usa-se a função guide em conjunto com a função guide_legend. Os exemplos abaixo mostram algumas das customizações possíveis. Para mais detalhes vale conferir a página de ajuda da função guide_legend.\n\nbase_plot &lt;- ggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) + \n  scale_fill_manual(name = \"Zona\", values = cores) +\n  labs(x = NULL, y = NULL) +\n  scale_x_discrete(labels = c(\"GRA\", \"JAB\", \"SAU\", \"PAR\", \"VMR\"))\n\n#&gt; Remove a legenda\nbase_plot + guides(fill = \"none\")\n#&gt; Posiciona o título da legenda em baixo\nbase_plot + guides(fill = guide_legend(title.position = \"bottom\"))\n#&gt; Centraliza o texto da legenda\nbase_plot + guides(fill = guide_legend(\n  label.position = \"left\", label.hjust = 0.5, title.hjust = 0.5\n))\n#&gt; Inverte a disposição da legenda\nbase_plot + guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\n\nComo mencionado acima, pode-se usar funções com cores pré-definidas. Abaixo mostro alguns exemplos.\n\nbase_plot &lt;- ggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) +\n  labs(x = NULL, y = NULL) +\n  scale_x_discrete(labels = c(\"GRA\", \"JAB\", \"SAU\", \"PAR\", \"VMR\"))\n\n#&gt; Usando ColorBrewer\nbase_plot + scale_fill_brewer(type = \"qual\", palette = 4)\n#&gt; Escala de cinza\nbase_plot + scale_fill_grey()\n#&gt; Usando Ghibli, cores do filme Princesa Mononoke\nbase_plot + ghibli::scale_fill_ghibli_d(\"MononokeMedium\")\n#&gt; Usando MetBrewer, cores do artista Hokusai\nbase_plot + MetBrewer::scale_fill_met_d(\"Hokusai1\")\n\n\n\n\n\n\n\n\n\n\nPor fim, mostro um exemplo onde as cores podem tanto ajudar a enxergar tendências de longo prazo como também algum tipo de padrão sazonal nos dados. O gráfico de linha abaixo mostra o preço mediano de venda de imóveis na cidade de Austin a cada mês durante o período 2000-2014.\nRepare no uso da função scale_x_continuous para mostrar o nome abreviado de cada mês no eixo-x e o uso de as.factor para garantir que o R interprete a variável year como categórica e não como contínua.\n\naustin &lt;- filter(txhousing, city == \"Austin\", year &lt; 2015)\n\nggplot(austin, aes(x = month, y = median)) +\n  geom_line(aes(color = as.factor(year))) +\n  scale_x_continuous(breaks = 1:12, labels = month.abb) +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\nVariáveis contínuas\nA mesma lógica apresentada acima se aplica a variáveis contínuas. Infelizmente, algumas das aplicações mais úteis de escalas de cores contínuas envolvem tipos de gráficos que ainda não vimos, como geom_tile() e geom_sf().\nO exemplo abaixo mostra a relação entre renda e escolaridade, olhando especificamente para a taxa de indivíduos com ensino superior. Note o uso da função scale_y_log10.\nA função scale_color_continuous não é particularmente útil pois, ao contrário do caso discreto, em que podíamos escolher as cores manualmente usando a função scale_color_discrete ou scale_color_manual, não existe uma maneira de “escolher” as cores no caso contínuo.\nO problema acontece porque teríamos que escolher “infinitas” cores, ou melhor, todo um gradiente de cores. Para definir um gradiente de cores com facilidade, usamos a função scale_color_gradient. Esta função define um gradiente a partir dos argumentos low e high.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  scale_color_gradient(low = \"blue\", high = \"orange\")\n\n\n\n\n\n\n\n\nPara construir um gradiente “divergente”, que destaca valores longe da média, por exemplo, pode-se usar scale_color_gradient2. O exemplo abaixo usa a função scale para “normalizar” a densidade populacional.\nO gráfico agora destaca em azul/roxo as Zonas com alta densidade populacional e em vermelho as Zonas com baixa densidade populacional; as Zonas com densidade próximas da média ficam em branco.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = scale(pop_density))) +\n  scale_y_log10() +\n  scale_color_gradient2(limits = c(-4, 4))\n\n\n\n\n\n\n\n\nUma função muito útil para trabalhar este tipo de dado é a scale_*_distiller(), que cria gradientes de cores a partir das paletas de cores do ColorBrewer, que vimos anteriormente. A cor de cada ponto no gráfico abaixo ilustra a densidade populacional de cada Zona em que os pontos mais escuros são as Zonas mais densas.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  scale_color_distiller(palette = 3, direction = 1)\n\n\n\n\n\n\n\n\nA principal vantagem de trabalhar com esta função é a facilidade em testar e gerar boas escalas de cores. Os exemplos abaixo mostram algumas das aplicações. Para conhecer mais vale consultar ?scale_color_distiller.\n\nbase_plot &lt;- ggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  guides(color = \"none\")\n\n#&gt; Seleciona uma paleta de cores usando número e tipo\nbase_plot + scale_color_distiller(palette = 1, type = \"sequential\")\n#&gt; Seleciona uma paleta de cores usando uma abreviação (BluesGreen)\nbase_plot + scale_color_distiller(palette = \"BuGn\")\n#&gt; Seleciona uma paleta de cores usando uma abreviação (YellowOrangeRed)\nbase_plot + scale_color_distiller(palette = \"YlOrRd\")\n#&gt; Seleciona uma paleta de cores e inverte a direção\nbase_plot + scale_color_distiller(palette = 1, direction = 1)\n\n\n\n\n\n\n\n\n\n\nNovamente, os argumentos das funções scale_* são compartilhados. No exemplo abaixo, mostro como usar breaks dentro de scale_color_distiller.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  scale_color_distiller(\n    name = \"Dens. Pop\",\n    breaks = seq(50, 400, 50),\n    palette = 3,\n    direction = 1\n    )\n\n\n\n\n\n\n\n\nA função scale_*_binned permite uma visualização um pouco mais simplificada de dados contínuos ao agrupar eles em grupos. Esta função não permite customizar a escolha dos grupos, mas, em geral, ela funciona muito bem. Em casos mais complexos, vale mais a pena agrupar os dados antes de visualizá-los, e então tratá-los como dados discretos. Outra opção é usar uma função que permita maior controle como scale_color_steps ou scale_color_gradient.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density), alpha = 0.7) +\n  scale_y_log10() +\n  scale_color_binned(type = \"viridis\")\n\n\n\n\n\n\n\n\nA função scale_color_binned tem apenas duas opções de cores: \"gradient\" e \"viridis\". Contudo, ela também aceita escalas customizadas de outras funções. No exemplo abaixo mostro como utilizar uma escala do MetBrewer.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density), alpha = 0.7) +\n  scale_y_log10() +\n  scale_color_binned(type = \\(x) MetBrewer::scale_color_met_c(name = \"VanGogh3\"))\n\n\n\n\n\n\n\n\nPor fim, para controlar os detalhes da legenda, novamente usa-se a função guides mas agora em conjunto com guide_colorbar.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density), alpha = 0.7) +\n  scale_y_log10() +\n  scale_color_binned(type = \"viridis\") +\n  guides(color = guide_colorbar(reverse = TRUE))\n\n\n\n\n\n\n\n\nO {ggplot2} é um pacote muito flexível, oferece todo tipo de visualização imaginável. Como resultado, podemos (como já vimos em vários casos) gerar todo tipo de gráfico sem sentido. O exemplo abaixo mostra novamente a série da taxa de poupança, mas agora a cor da linha é proporcional à taxa de desemprego naquele mês.\n\nggplot(economics, aes(x = date, y = psavert)) +\n  geom_line(aes(color = unemploy/pop)) +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\nEm geral, usa-se cores em gráficos de linha para ajudar a distinguir entre diferentes séries de tempo. Pode-se usar cores diferentes numa única série para enfatizar a sua mudança no tempo. O gráfico abaixo mostra a série das “anomalias de temperatura” nas últimos décadas. Por anomalia de temperatura, entende-se, o desvio da temperatura média anual em relação à média histórica (1951-1980)4.\n\ngtemp_both &lt;- astsa::gtemp_both\n# Converte o objeto para data.frame\ndf &lt;- data.frame(\n  date = as.numeric(time(gtemp_both)),\n  temp = as.numeric(gtemp_both)\n  )\n\nggplot(df, aes(x = date, y = temp, color = temp)) + \n  geom_line() +\n  scale_color_viridis_c(option = \"inferno\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#footnotes",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#footnotes",
    "title": "Estético: Escalas e Cores",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale lembrar, que a cidade de São Paulo não possui uma divisão oficial de bairros, apenas de distritos e subprefeituras.↩︎\nExiste também a função scale_y_date mas datas quase sempre são apresentadas no eixo-x.↩︎\nNão conheço boas referências sobre teoria de cores em português. Atualmente, o ChatGPT pode fornecer boas paletas de cores: “Quero uma paleta de cores profissional com tons de laranja. Esta paleta de cores será utilizada dentro do R. Quero o retorno em vetores de tamanhos de 3 a 8 elementos com cores em formato hexadecimal. Retorne os vetores numa lista.”.↩︎\nEsta base de dados é exportada em conjunto com o pacote astsa. Para mais informações consulte o site da NASA, que é a fonte original dos dados e que, também, disponibiliza uma versão atualizada da série.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Aqueles que acompanham as venturas da capital gaúcha bem devem saber da polêmica Avenida Castelo Branco, que dá entrada à cidade, para quem vem da Região Metropolitana. Em 2014, a câmara de vereadores de Porto Alegre aprovou a mudança da Av. Castelo Branco para Av. da Legalidade e da Democracia. A Avenida da Legalidade, como ficou conhecida, durou pouco tempo: em 2018 a justiça considerou inválida a lei que alterou o nome da avenida e ela voltou a se chamar Castelo Branco, agora Av. Presidente Castelo Branco, como se querendo debochar da mudança anterior.\nNa minha vivência na cidade, sempre reparei na quantidade de ruas e avenidas que homenageavam não somente os presidentes da ditadura militar, mas membros do exército em geral. A principal via do boêmio bairro da Cidade Baixa chama-se Rua General Lima e Silva; a avenida que liga a Protásio com a 24, Av. Coronel Lucas de Oliveira; um longo trecho da terceira perimetral, Av. Coronel Aparício Borges; no querido Bom Fim, General João Telles; no Centro Histórico, Gen. Vitorino, Gen. Andrade Neves, Gen. Câmara, Gen. Salustiano, Cel. Fernando Machado e tantos outros.\nParti então para a empreitada de classificar todas as ruas da cidade e entender quais os nomes que formam os logradouros de Porto Alegre. O resultado está no mapa abaixo. Para ver em mais detalhes, selecione “Abrir imagem em nova aba” e use o zoom.\n\n\n\n\n\n\n\n\nA tarefa foi mais difícil do que esperava. De imediato, nota-se que a maior parte das ruas têm nome de pessoas, cerca de 65%. O restante das ruas ou são “coisas”, como Av. Icaraí, Av. Ipiranga, etc. ou são ruas sem nome como Beco A, Rua 1, etc. Estas ruas sem nome concentram-se ou em aglomerados subnormais (favelas) ou em condomínios fechados. Por fim, temos ruas que têm nomes de outros lugares, como a Av. Nova York, Rua Europa, etc.\nO exército realmente tem muitas ruas, em torno de 185, ou 3% do total; é mais do que o nome de ruas que têm os políticos (35) e até mesmo do que as figuras religiosas que têm 155. Há menos ruas com nomes de políticos do que eu esperava; as poucas ruas, contudo, costumam ser bastante importantes, como a Av. João Pessoa, Av. Getúlio Vargas e Av. Protásio Alves. Nossos padres, freis e papas têm muitas ruas, mas a maioria delas são pequenas; a exceção é a Av. Padre Cacique, na Zona Sul.\n\n\n\n\n\nComo mencionei acima, o Centro Histórico concentra muitas ruas com nomes de generais, coroneis e marechais.\n\n\n\n\n\nAs ruas sem nome são bastante aglomeradas espacialmente. Na sua maioria, estas ruas são de condomínios fechados ou de aglomerados subnormais. A foto abaixo mostra a esquina da Rua C com a Rua Oito, no bairro Bom Jesus. As ruas em volta seguem o mesmo padrão: Rua 11, Rua Doze, Rua P, etc.\n\n\n\n\n\nAlgumas partes da cidade parecem temáticas. Na Zona Norte, por exemplo, no Bairro São Geraldo, há um enorme número de ruas contíguas com nomes de locais: Av. Pará, Av. Amazonas, Av. Bahia, Av. Ceará, Av. França, Av. Madrid, etc. No Partenon, a Rua Chile faz esquina com a R. Valparaíso e é paralela com a R. Buenos Aires.\n\n\n\n\n\nOs nomes de personalidades históricas estão espalhados pela cidade e não parece haver um padrão. Os escritores Machado de Assis, Graciliano Ramos, Eça de Queiroz, Olavo Bilac e outros estão cada um em um canto. Já Raimundo Correa, Alfonso Celso e Vicente de Carvalho (todos poetas) estão reunidos no entorno da Praça Japão; durante um trecho, também, Castro Alves e Casemiro de Abreu são paralelos. A Av. Érico Veríssimo, na sua longa extensão, faz esquina com a Olavo Bilac e eventualmente encontra-se com a Av. José de Alencar.\nMuitas ruas têm nomes de profissões, como R. Doutor Flores ou R. Professor Fitzgerald. A profissão mais comum é a de doutor (cerca de 120 ruas) seguida por professor (cerca de 80 ruas). Também temos bastante engenheiros, como na Av. Engenheiro Alfredo Corrêa Daudt. No meio das ruas temos até uma Av. Economista Nilo Wulff, no Bairro Restinga, que foi um dos criados do Conselho de Economia do Rio Grande do Sul. As ruas com nomes de profissão estão espalhadas por toda a cidade. No Bairro Tristeza, na Zona Sul, há várias delas reunidas, várias ruas com nome de Doutor.\n\n\n\n\n\n\n\n\nClassificar o nome das ruas é uma tarefa nada trivial. Primeiro, porque as categorias que eu me propus a usar não são mututamente excludentes. A Av. Senador Salgado Filho, por exemplo, é nome de um município, refere-se a uma profissão (senador), é nome de um político e, de maneira geral, é nome de uma personalidade histórica.\nA Av. Bento Gonçalves também é um pouco ambígua, afinal é o nome de um município importante; neste caso, contudo, tanto a avenida como a cidade homenageam Bento Gonçalves da Silva, tenente-coronel, líder da Revolução Farroupilha. Além disso, a Lei Ordinária No 1 de março de 1936, deixa evidente que o homenageado era o General. Infelizmente, são poucas as ruas e avenidas que estão propriamente documentadas online. Em alguns casos não há registros de quem era a pessoa homenageada ou há mesmo várias pessoas com nome similar.\nPor fim, algumas ruas são simplesmente difíceis de classificar. A Travessa Azevedo, por exemplo. Quem terá sido este Azevedo? Na dúvida, ficou como personalidade histórica.\nPara a minha surpesa algumas ruas tem nomes duplicados. A BR-290 que liga a cidade ao litoral tem o nome Estrada Marechal Osório no Google Maps, mas tem nome Rodovia Osvaldo Aranha no OpenStreetMaps. Até onde me lembro sempre ouvi as pessoas chamando ela de “freeway”. Não consegui encontrar algum tipo de material oficial na prefeitura para sanar a dúvida.\nNão encontrei nenhum tipo de “dicionário” ou lista com personalidades gaúchas. Sem uma boa lista que junta nomes com descrições fica difícil aplicar algum tipo de metodologia que envolva aprendizado de máquina. A classificação foi feita com uso extensivo de regex; a revisão foi na base de tentativa e erro, pois há incontáveis exceções a todo tipo de regra. Na minha classificação as categorias “coringa” são “personalidade histórica” e “coisa”. Até por isso, estas são as duas categorias com maior chance de estar superestimadas.\n\n\n\nComo sempre, o código para fazer o mapa segue abaixo:\n\n\nCode\n# Classificar o nome das ruas de Poa #\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggtext)\nsysfonts::font_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext::showtext_auto()\n\n# Import Data -------------------------------------------------------------\n\n## geobr -------------------------------------------------------------------\n\n# City border\npoa &lt;- geobr::read_municipality(4314902)\n\n## osmdata -----------------------------------------------------------------\n\n# Define bbox\nbbox &lt;- getbb(\"Porto Alegre Brazil\")\n# Base query\nqr &lt;- opq(bbox)\n\n# Add feature requests to query\n\n# All roads\nqr_roads &lt;- add_osm_feature(qr, key = \"highway\")\n\n# Only big roads\nqr_big_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\")\n)\n\n# Only medium roads\nqr_med_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\")\n)\n\n# Only small roads\nqr_small_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"residential\", \"living_street\", \"unclassified\", \"service\",\n            \"footway\")\n)\n\n# Download\nroads &lt;- osmdata_sf(q = qr_roads)\nroads &lt;- roads$osm_lines\nroads &lt;- st_transform(roads, crs = 4674)\nroads &lt;- st_join(roads, poa)\nroads &lt;- filter(roads, !is.na(code_muni))\n\n\nbig_streets &lt;- osmdata_sf(q = qr_big_streets)\nmed_streets &lt;- osmdata_sf(q = qr_med_streets)\nsmall_streets &lt;- osmdata_sf(q = qr_small_streets)\n\n#&gt; Get street names\nsnames &lt;- roads$name\n\n#&gt; Remove duplicate names and missing values\nsnames &lt;- unique(snames)\nsnames &lt;- na.omit(snames)\n\n# Convert to tibble\ndictionary &lt;- tibble(\n  street_name = snames\n)\n\n# Streets - classify ------------------------------------------------------\n\n#&gt; Group into categories: historical personalities, names of other cities or \n#&gt; states, jobs, holiday, religious figure, army, nameless\n\n#&gt; Get names of all cities and states in Brazil\nmuni &lt;- sidrar::get_sidra(4709, geo = \"City\", variable = 93)\n\nname_muni &lt;- muni |&gt; \n  janitor::clean_names() |&gt; \n  filter(valor &gt; 1e5) |&gt; \n  tidyr::separate(municipio, into = c(\"name_muni\", \"abb\"), sep = \" - \") |&gt; \n  pull(name_muni) |&gt; \n  unique()\n \nstate &lt;- geobr::read_state()\nstate_name &lt;- state$name_state\n\nstate_lower &lt;- c(\n  \"Rio Grande do Sul\", \"Rio de Janeiro\", \"Rio Grande do Norte\",\n  \"Mato Grosso do Sul\"\n  )\n\nstate_name &lt;- c(state_name, state_lower)\n\ngeonames &lt;- c(name_muni, state_name)\ngeostring &lt;- paste(glue::glue(\"({geonames})\"), collapse = \"|\")\n\n#&gt; Common titles for army position, liberal professions, and religious\n#&gt; personalities\n\nposto_exercito &lt;- c(\n  \"Marechal\", \"Almirante\", \"General\", \"Comandante\", \"Coronel\", \"Cabo\",\n  \"Capitão\", \"Brigadeiro\", \"Tenente\", \"(Castello Branco)\",\n  \"(Costa e Silva)\", \"(Ernesto Geisel)\", \"PM\", \"Major\"\n  )\n\n#&gt; Common prefixes for job titles\nprofissao &lt;- c(\n  \"Engenheir\", \"Doutor\", \"Profess\", \"Desembargador\", \"Economista\", \"Jornalista\", \"Escrivão\", \"Contabilista\"\n  )\n\n#&gt; Common names for religious figures\nsantidade &lt;- c(\"Frei\", \"Santo\", \"Santa\", \"São\", \"Padre\", \"Papa\", \"Reverendo\")\n\npolitico &lt;- c(\"Vereador\", \"Deputado\", \"Governador\", \"Senador\", \"Presidente\")\n\n#&gt; Collapse strings\nposto_exercito &lt;- paste(posto_exercito, collapse = \"|\")\nprofissao &lt;- paste(profissao, collapse = \"|\")\nsantidade &lt;- paste(paste0(santidade, \" \"), collapse = \"|\")\npolitico &lt;- paste(politico, collapse = \"|\")\n\n# Proxy for closed condominiums / favelas\n\ncardinais &lt;- c(\n  \"Um\", \"Dois\", \"Três\", \"Quatro\", \"Cinco\", \"Seis\", \"Sete\", \"Oito\",\n  \"Nove\", \"Dez\", \"Onze\", \"Doze\", \"Treze\", \"Catorze\", \"Quatorze\",\n  \"Quinze\", \"Dezesseis\", \"Dezesete\", \"Dezoito\", \"Dezenove\", \"Vinte\",\n  \"Vinte e Um\", \"Vinte e Dois\", \"Vinte e Três\", \"Vinte e Quatro\",\n  \"Vinte e Cinco\", \"Vinte e Seis\", \"Vinte e Sete\", \"Vinte e Oito\",\n  \"Vinte e Nove\", \"Trinta\", \"Quarenta\", \"Cinquenta\", \"Sessenta\",\n  \"Setenta\", \"Oitenta\", \"Noventa\", \"Cem\"\n  )\n\ncardinais &lt;- paste(paste0(\"(Rua \", cardinais, \")\"), collapse = \"|\")\n\n#&gt; \"nameless\" streets\nname_vias &lt;- c(\n  \"Alameda\", \"Avenida\", \"Acesso\", \"Beco\", \"Caminho\", \"Passagem\", \"Via\", \"Viela\"\n)\n\nrx_vias &lt;- paste(name_vias, \"[A-Z0-9]+[A-Z]?$\")\n\nrua_sem_nome &lt;- c(\"[0-9].+\", rx_vias, cardinais)\n\nrua_sem_nome &lt;- paste(glue::glue(\"({rua_sem_nome})\"), collapse = \"|\")\n\nrua_sem_nome &lt;- paste(rua_sem_nome, cardinais, sep = \"|\")\n\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = case_when(\n      str_count(street_name, \"\\\\w+\") &gt; 2 ~ \"Personalidade Histórica\",\n      str_count(street_name, \"\\\\w+\") &lt;= 2 ~ \"Coisa\",\n      TRUE ~ \"Outro\"\n    ),\n    class = case_when(\n      str_detect(street_name, geostring) ~ \"Nome de Cidade/UF\",\n      str_detect(street_name, politico) ~ \"Político\",\n      str_detect(street_name, posto_exercito) ~ \"Exército\",\n      str_detect(street_name, profissao) ~ \"Profissão\",\n      str_detect(street_name, santidade) ~ \"Figura Religiosa\",\n      str_detect(street_name, \"[0-9] de\") ~ \"Feriado\",\n      str_detect(street_name, rua_sem_nome) ~ \"Rua sem nome\",\n      TRUE ~ class\n    )\n  )\n\npers &lt;- c(\n  \"Carlos Gomes\", \"Protásio Alves\", \"Salgado Filho\", \"Mário Tavares Haussen\",\n  \"Donário Braga\", \"Joracy Camargo\", \"Goethe\", \"Mozart\", \"Schiller\",\n  \"Edgar Pires de Castro\", \"Plínio Brasil Milano\", \"Santos Dumont\"\n  )\n\nexercito &lt;- c(\n  \"Bento Gonçalves\", \"Luís Carlos Prestes\", \"Presidente Castello Branco\",\n  \"João Antônio da Silveira\"\n  )\n\npoliticos &lt;- c(\n  \"Getúlio Vargas\", \"João Pessoa\", \"Protásio Alves\", \"Loureiro da Silva\"\n  )\n\ncoisas &lt;- c(\n  \"Azenha\", \"Rua da Conceição\", \"Túnel da Conceição\", \"Elevada da Conceição\",\n  \"Viaduto da Conceição\", \"Ipiranga\", \"Beira Rio\", \"Rio Jacuí\", \" Banco\",\n  \"Lago das \", \"Lago do\", \"Rio dos Frades\", \"Rio Pardo\", \"Rio Claro\",\n  \"Rio dos Sinos\", \"Rio Negro\", \"Rio Grande\", \"Rio Tejo\", \"Rio Maria\",\n  \"Rio Verde\", \"Rio Solimoes\", \"Rio Xingu\", \"Rio Tapajos\", \"Avenida da Cavalhada\",\n  \"Estrada do Varejão\", \"Estrada da Taquara\", \"Sarandi\"\n  )\n\nlocais &lt;- c(\n  \"Chicago\", \"Madri\", \"Nova York\", \"Nova Zelândia\", \"Quito\", \"Brasil\",\n  \"Europa\", \"Estados Unidos\", \"Japão\", \"Itália\", \"Polônia\", \"Caracas\",\n  \"Buenos Aires\"\n)\n\npers &lt;- paste(glue::glue(\"({pers})\"), collapse = \"|\")\ncoisas &lt;- paste(glue::glue(\"({coisas})\"), collapse = \"|\")\nlocais &lt;- paste(glue::glue(\"({locais})\"), collapse = \"|\")\npoliticos &lt;- paste(glue::glue(\"({politicos})\"), collapse = \"|\")\nexercito &lt;- paste(glue::glue(\"({exercito})\"), collapse = \"|\")\n\n# Ajustes manuais\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = if_else(str_detect(street_name, pers), \"Personalidade Histórica\", class),\n    class = if_else(str_detect(street_name, coisas), \"Coisa\", class),\n    class = if_else(str_detect(street_name, locais), \"Nome de Cidade/UF\", class),\n    class = if_else(str_detect(street_name, politicos), \"Político\", class),\n    class = if_else(str_detect(street_name, exercito), \"Exército\", class)\n  )\n\ndictionary |&gt; \n  count(class, sort = TRUE) |&gt; \n  mutate(share = n / sum(n) * 100)\n\n# Streets ---------------------------------------------------------------\n\ns1 &lt;- big_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns2 &lt;- med_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns3 &lt;- small_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\n# Plot ------------------------------------------------------------------\ncores &lt;- c(\n  \"coisa\" = \"#33a02c\",\n  \"exercito\" = \"#b15928\",\n  \"feriado\" = \"#fb9a99\",\n  \"religioso\" = \"#e41a1c\",\n  \"nome_cidade\" = \"#1f77b4\",\n  \"personalidade\" = \"#ff7f00\",\n  \"politico\" = \"#a6cee3\",\n  \"profissao\" = \"#984ea3\",\n  \"sem_nome\" = \"#e78ac3\"\n  )\n\nbg_color &lt;- \"#F5F5F5\"\n\np1 &lt;- ggplot() +\n  geom_sf(\n    data = filter(s1, !is.na(class)),\n    aes(color = class),\n    linewidth = 0.8,\n    key_glyph = draw_key_rect\n  ) +\n  geom_sf(\n    data = filter(s2, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.35\n  ) +\n  geom_sf(\n    data = filter(s3, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.15\n  ) +\n  coord_sf(\n    ylim = c(-30.124, -29.9691),\n    xlim = c(-51.265, -51.135)\n  ) +\n  scale_colour_manual(name = \"\", values = unname(cores)) +\n  labs(\n    title = \"**Origem do Nome de Ruas em Porto Alegre**\",\n    caption = \"Fonte: OSM. Cores: ColorBrewer. Autor: @viniciusoike\"\n  ) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = bg_color, colour = bg_color),\n    panel.background = element_rect(fill = bg_color, colour = bg_color),\n    #&gt; Plot Margin\n    plot.margin = margin(t = 1.5, r = 0.5, l = 0.5, b = 1, unit = \"cm\"),\n    #&gt; Textual elements\n    text = element_text(family = \"Gill Sans\", size = 8),\n    plot.title = element_markdown(\n      family = \"Gill Sans\",\n      size = 20,\n      hjust = 0.5\n    ),\n    panel.border = element_rect(colour = \"gray20\", fill = \"transparent\"),\n    #&gt; Legend \n    legend.position = c(0.16, 0.8),\n    #legend.position = \"right\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.box.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.text = element_text(size = 8),\n    legend.margin = margin(t = 0.5, b = 0.5, unit = \"cm\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#uma-avenida-polêmica",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#uma-avenida-polêmica",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Aqueles que acompanham as venturas da capital gaúcha bem devem saber da polêmica Avenida Castelo Branco, que dá entrada à cidade, para quem vem da Região Metropolitana. Em 2014, a câmara de vereadores de Porto Alegre aprovou a mudança da Av. Castelo Branco para Av. da Legalidade e da Democracia. A Avenida da Legalidade, como ficou conhecida, durou pouco tempo: em 2018 a justiça considerou inválida a lei que alterou o nome da avenida e ela voltou a se chamar Castelo Branco, agora Av. Presidente Castelo Branco, como se querendo debochar da mudança anterior.\nNa minha vivência na cidade, sempre reparei na quantidade de ruas e avenidas que homenageavam não somente os presidentes da ditadura militar, mas membros do exército em geral. A principal via do boêmio bairro da Cidade Baixa chama-se Rua General Lima e Silva; a avenida que liga a Protásio com a 24, Av. Coronel Lucas de Oliveira; um longo trecho da terceira perimetral, Av. Coronel Aparício Borges; no querido Bom Fim, General João Telles; no Centro Histórico, Gen. Vitorino, Gen. Andrade Neves, Gen. Câmara, Gen. Salustiano, Cel. Fernando Machado e tantos outros.\nParti então para a empreitada de classificar todas as ruas da cidade e entender quais os nomes que formam os logradouros de Porto Alegre. O resultado está no mapa abaixo. Para ver em mais detalhes, selecione “Abrir imagem em nova aba” e use o zoom."
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-o-mapa",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-o-mapa",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "A tarefa foi mais difícil do que esperava. De imediato, nota-se que a maior parte das ruas têm nome de pessoas, cerca de 65%. O restante das ruas ou são “coisas”, como Av. Icaraí, Av. Ipiranga, etc. ou são ruas sem nome como Beco A, Rua 1, etc. Estas ruas sem nome concentram-se ou em aglomerados subnormais (favelas) ou em condomínios fechados. Por fim, temos ruas que têm nomes de outros lugares, como a Av. Nova York, Rua Europa, etc.\nO exército realmente tem muitas ruas, em torno de 185, ou 3% do total; é mais do que o nome de ruas que têm os políticos (35) e até mesmo do que as figuras religiosas que têm 155. Há menos ruas com nomes de políticos do que eu esperava; as poucas ruas, contudo, costumam ser bastante importantes, como a Av. João Pessoa, Av. Getúlio Vargas e Av. Protásio Alves. Nossos padres, freis e papas têm muitas ruas, mas a maioria delas são pequenas; a exceção é a Av. Padre Cacique, na Zona Sul.\n\n\n\n\n\nComo mencionei acima, o Centro Histórico concentra muitas ruas com nomes de generais, coroneis e marechais.\n\n\n\n\n\nAs ruas sem nome são bastante aglomeradas espacialmente. Na sua maioria, estas ruas são de condomínios fechados ou de aglomerados subnormais. A foto abaixo mostra a esquina da Rua C com a Rua Oito, no bairro Bom Jesus. As ruas em volta seguem o mesmo padrão: Rua 11, Rua Doze, Rua P, etc.\n\n\n\n\n\nAlgumas partes da cidade parecem temáticas. Na Zona Norte, por exemplo, no Bairro São Geraldo, há um enorme número de ruas contíguas com nomes de locais: Av. Pará, Av. Amazonas, Av. Bahia, Av. Ceará, Av. França, Av. Madrid, etc. No Partenon, a Rua Chile faz esquina com a R. Valparaíso e é paralela com a R. Buenos Aires.\n\n\n\n\n\nOs nomes de personalidades históricas estão espalhados pela cidade e não parece haver um padrão. Os escritores Machado de Assis, Graciliano Ramos, Eça de Queiroz, Olavo Bilac e outros estão cada um em um canto. Já Raimundo Correa, Alfonso Celso e Vicente de Carvalho (todos poetas) estão reunidos no entorno da Praça Japão; durante um trecho, também, Castro Alves e Casemiro de Abreu são paralelos. A Av. Érico Veríssimo, na sua longa extensão, faz esquina com a Olavo Bilac e eventualmente encontra-se com a Av. José de Alencar.\nMuitas ruas têm nomes de profissões, como R. Doutor Flores ou R. Professor Fitzgerald. A profissão mais comum é a de doutor (cerca de 120 ruas) seguida por professor (cerca de 80 ruas). Também temos bastante engenheiros, como na Av. Engenheiro Alfredo Corrêa Daudt. No meio das ruas temos até uma Av. Economista Nilo Wulff, no Bairro Restinga, que foi um dos criados do Conselho de Economia do Rio Grande do Sul. As ruas com nomes de profissão estão espalhadas por toda a cidade. No Bairro Tristeza, na Zona Sul, há várias delas reunidas, várias ruas com nome de Doutor."
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-fazer-o-mapa",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-fazer-o-mapa",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Classificar o nome das ruas é uma tarefa nada trivial. Primeiro, porque as categorias que eu me propus a usar não são mututamente excludentes. A Av. Senador Salgado Filho, por exemplo, é nome de um município, refere-se a uma profissão (senador), é nome de um político e, de maneira geral, é nome de uma personalidade histórica.\nA Av. Bento Gonçalves também é um pouco ambígua, afinal é o nome de um município importante; neste caso, contudo, tanto a avenida como a cidade homenageam Bento Gonçalves da Silva, tenente-coronel, líder da Revolução Farroupilha. Além disso, a Lei Ordinária No 1 de março de 1936, deixa evidente que o homenageado era o General. Infelizmente, são poucas as ruas e avenidas que estão propriamente documentadas online. Em alguns casos não há registros de quem era a pessoa homenageada ou há mesmo várias pessoas com nome similar.\nPor fim, algumas ruas são simplesmente difíceis de classificar. A Travessa Azevedo, por exemplo. Quem terá sido este Azevedo? Na dúvida, ficou como personalidade histórica.\nPara a minha surpesa algumas ruas tem nomes duplicados. A BR-290 que liga a cidade ao litoral tem o nome Estrada Marechal Osório no Google Maps, mas tem nome Rodovia Osvaldo Aranha no OpenStreetMaps. Até onde me lembro sempre ouvi as pessoas chamando ela de “freeway”. Não consegui encontrar algum tipo de material oficial na prefeitura para sanar a dúvida.\nNão encontrei nenhum tipo de “dicionário” ou lista com personalidades gaúchas. Sem uma boa lista que junta nomes com descrições fica difícil aplicar algum tipo de metodologia que envolva aprendizado de máquina. A classificação foi feita com uso extensivo de regex; a revisão foi na base de tentativa e erro, pois há incontáveis exceções a todo tipo de regra. Na minha classificação as categorias “coringa” são “personalidade histórica” e “coisa”. Até por isso, estas são as duas categorias com maior chance de estar superestimadas."
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#código",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#código",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Como sempre, o código para fazer o mapa segue abaixo:\n\n\nCode\n# Classificar o nome das ruas de Poa #\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggtext)\nsysfonts::font_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext::showtext_auto()\n\n# Import Data -------------------------------------------------------------\n\n## geobr -------------------------------------------------------------------\n\n# City border\npoa &lt;- geobr::read_municipality(4314902)\n\n## osmdata -----------------------------------------------------------------\n\n# Define bbox\nbbox &lt;- getbb(\"Porto Alegre Brazil\")\n# Base query\nqr &lt;- opq(bbox)\n\n# Add feature requests to query\n\n# All roads\nqr_roads &lt;- add_osm_feature(qr, key = \"highway\")\n\n# Only big roads\nqr_big_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\")\n)\n\n# Only medium roads\nqr_med_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\")\n)\n\n# Only small roads\nqr_small_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"residential\", \"living_street\", \"unclassified\", \"service\",\n            \"footway\")\n)\n\n# Download\nroads &lt;- osmdata_sf(q = qr_roads)\nroads &lt;- roads$osm_lines\nroads &lt;- st_transform(roads, crs = 4674)\nroads &lt;- st_join(roads, poa)\nroads &lt;- filter(roads, !is.na(code_muni))\n\n\nbig_streets &lt;- osmdata_sf(q = qr_big_streets)\nmed_streets &lt;- osmdata_sf(q = qr_med_streets)\nsmall_streets &lt;- osmdata_sf(q = qr_small_streets)\n\n#&gt; Get street names\nsnames &lt;- roads$name\n\n#&gt; Remove duplicate names and missing values\nsnames &lt;- unique(snames)\nsnames &lt;- na.omit(snames)\n\n# Convert to tibble\ndictionary &lt;- tibble(\n  street_name = snames\n)\n\n# Streets - classify ------------------------------------------------------\n\n#&gt; Group into categories: historical personalities, names of other cities or \n#&gt; states, jobs, holiday, religious figure, army, nameless\n\n#&gt; Get names of all cities and states in Brazil\nmuni &lt;- sidrar::get_sidra(4709, geo = \"City\", variable = 93)\n\nname_muni &lt;- muni |&gt; \n  janitor::clean_names() |&gt; \n  filter(valor &gt; 1e5) |&gt; \n  tidyr::separate(municipio, into = c(\"name_muni\", \"abb\"), sep = \" - \") |&gt; \n  pull(name_muni) |&gt; \n  unique()\n \nstate &lt;- geobr::read_state()\nstate_name &lt;- state$name_state\n\nstate_lower &lt;- c(\n  \"Rio Grande do Sul\", \"Rio de Janeiro\", \"Rio Grande do Norte\",\n  \"Mato Grosso do Sul\"\n  )\n\nstate_name &lt;- c(state_name, state_lower)\n\ngeonames &lt;- c(name_muni, state_name)\ngeostring &lt;- paste(glue::glue(\"({geonames})\"), collapse = \"|\")\n\n#&gt; Common titles for army position, liberal professions, and religious\n#&gt; personalities\n\nposto_exercito &lt;- c(\n  \"Marechal\", \"Almirante\", \"General\", \"Comandante\", \"Coronel\", \"Cabo\",\n  \"Capitão\", \"Brigadeiro\", \"Tenente\", \"(Castello Branco)\",\n  \"(Costa e Silva)\", \"(Ernesto Geisel)\", \"PM\", \"Major\"\n  )\n\n#&gt; Common prefixes for job titles\nprofissao &lt;- c(\n  \"Engenheir\", \"Doutor\", \"Profess\", \"Desembargador\", \"Economista\", \"Jornalista\", \"Escrivão\", \"Contabilista\"\n  )\n\n#&gt; Common names for religious figures\nsantidade &lt;- c(\"Frei\", \"Santo\", \"Santa\", \"São\", \"Padre\", \"Papa\", \"Reverendo\")\n\npolitico &lt;- c(\"Vereador\", \"Deputado\", \"Governador\", \"Senador\", \"Presidente\")\n\n#&gt; Collapse strings\nposto_exercito &lt;- paste(posto_exercito, collapse = \"|\")\nprofissao &lt;- paste(profissao, collapse = \"|\")\nsantidade &lt;- paste(paste0(santidade, \" \"), collapse = \"|\")\npolitico &lt;- paste(politico, collapse = \"|\")\n\n# Proxy for closed condominiums / favelas\n\ncardinais &lt;- c(\n  \"Um\", \"Dois\", \"Três\", \"Quatro\", \"Cinco\", \"Seis\", \"Sete\", \"Oito\",\n  \"Nove\", \"Dez\", \"Onze\", \"Doze\", \"Treze\", \"Catorze\", \"Quatorze\",\n  \"Quinze\", \"Dezesseis\", \"Dezesete\", \"Dezoito\", \"Dezenove\", \"Vinte\",\n  \"Vinte e Um\", \"Vinte e Dois\", \"Vinte e Três\", \"Vinte e Quatro\",\n  \"Vinte e Cinco\", \"Vinte e Seis\", \"Vinte e Sete\", \"Vinte e Oito\",\n  \"Vinte e Nove\", \"Trinta\", \"Quarenta\", \"Cinquenta\", \"Sessenta\",\n  \"Setenta\", \"Oitenta\", \"Noventa\", \"Cem\"\n  )\n\ncardinais &lt;- paste(paste0(\"(Rua \", cardinais, \")\"), collapse = \"|\")\n\n#&gt; \"nameless\" streets\nname_vias &lt;- c(\n  \"Alameda\", \"Avenida\", \"Acesso\", \"Beco\", \"Caminho\", \"Passagem\", \"Via\", \"Viela\"\n)\n\nrx_vias &lt;- paste(name_vias, \"[A-Z0-9]+[A-Z]?$\")\n\nrua_sem_nome &lt;- c(\"[0-9].+\", rx_vias, cardinais)\n\nrua_sem_nome &lt;- paste(glue::glue(\"({rua_sem_nome})\"), collapse = \"|\")\n\nrua_sem_nome &lt;- paste(rua_sem_nome, cardinais, sep = \"|\")\n\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = case_when(\n      str_count(street_name, \"\\\\w+\") &gt; 2 ~ \"Personalidade Histórica\",\n      str_count(street_name, \"\\\\w+\") &lt;= 2 ~ \"Coisa\",\n      TRUE ~ \"Outro\"\n    ),\n    class = case_when(\n      str_detect(street_name, geostring) ~ \"Nome de Cidade/UF\",\n      str_detect(street_name, politico) ~ \"Político\",\n      str_detect(street_name, posto_exercito) ~ \"Exército\",\n      str_detect(street_name, profissao) ~ \"Profissão\",\n      str_detect(street_name, santidade) ~ \"Figura Religiosa\",\n      str_detect(street_name, \"[0-9] de\") ~ \"Feriado\",\n      str_detect(street_name, rua_sem_nome) ~ \"Rua sem nome\",\n      TRUE ~ class\n    )\n  )\n\npers &lt;- c(\n  \"Carlos Gomes\", \"Protásio Alves\", \"Salgado Filho\", \"Mário Tavares Haussen\",\n  \"Donário Braga\", \"Joracy Camargo\", \"Goethe\", \"Mozart\", \"Schiller\",\n  \"Edgar Pires de Castro\", \"Plínio Brasil Milano\", \"Santos Dumont\"\n  )\n\nexercito &lt;- c(\n  \"Bento Gonçalves\", \"Luís Carlos Prestes\", \"Presidente Castello Branco\",\n  \"João Antônio da Silveira\"\n  )\n\npoliticos &lt;- c(\n  \"Getúlio Vargas\", \"João Pessoa\", \"Protásio Alves\", \"Loureiro da Silva\"\n  )\n\ncoisas &lt;- c(\n  \"Azenha\", \"Rua da Conceição\", \"Túnel da Conceição\", \"Elevada da Conceição\",\n  \"Viaduto da Conceição\", \"Ipiranga\", \"Beira Rio\", \"Rio Jacuí\", \" Banco\",\n  \"Lago das \", \"Lago do\", \"Rio dos Frades\", \"Rio Pardo\", \"Rio Claro\",\n  \"Rio dos Sinos\", \"Rio Negro\", \"Rio Grande\", \"Rio Tejo\", \"Rio Maria\",\n  \"Rio Verde\", \"Rio Solimoes\", \"Rio Xingu\", \"Rio Tapajos\", \"Avenida da Cavalhada\",\n  \"Estrada do Varejão\", \"Estrada da Taquara\", \"Sarandi\"\n  )\n\nlocais &lt;- c(\n  \"Chicago\", \"Madri\", \"Nova York\", \"Nova Zelândia\", \"Quito\", \"Brasil\",\n  \"Europa\", \"Estados Unidos\", \"Japão\", \"Itália\", \"Polônia\", \"Caracas\",\n  \"Buenos Aires\"\n)\n\npers &lt;- paste(glue::glue(\"({pers})\"), collapse = \"|\")\ncoisas &lt;- paste(glue::glue(\"({coisas})\"), collapse = \"|\")\nlocais &lt;- paste(glue::glue(\"({locais})\"), collapse = \"|\")\npoliticos &lt;- paste(glue::glue(\"({politicos})\"), collapse = \"|\")\nexercito &lt;- paste(glue::glue(\"({exercito})\"), collapse = \"|\")\n\n# Ajustes manuais\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = if_else(str_detect(street_name, pers), \"Personalidade Histórica\", class),\n    class = if_else(str_detect(street_name, coisas), \"Coisa\", class),\n    class = if_else(str_detect(street_name, locais), \"Nome de Cidade/UF\", class),\n    class = if_else(str_detect(street_name, politicos), \"Político\", class),\n    class = if_else(str_detect(street_name, exercito), \"Exército\", class)\n  )\n\ndictionary |&gt; \n  count(class, sort = TRUE) |&gt; \n  mutate(share = n / sum(n) * 100)\n\n# Streets ---------------------------------------------------------------\n\ns1 &lt;- big_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns2 &lt;- med_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns3 &lt;- small_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\n# Plot ------------------------------------------------------------------\ncores &lt;- c(\n  \"coisa\" = \"#33a02c\",\n  \"exercito\" = \"#b15928\",\n  \"feriado\" = \"#fb9a99\",\n  \"religioso\" = \"#e41a1c\",\n  \"nome_cidade\" = \"#1f77b4\",\n  \"personalidade\" = \"#ff7f00\",\n  \"politico\" = \"#a6cee3\",\n  \"profissao\" = \"#984ea3\",\n  \"sem_nome\" = \"#e78ac3\"\n  )\n\nbg_color &lt;- \"#F5F5F5\"\n\np1 &lt;- ggplot() +\n  geom_sf(\n    data = filter(s1, !is.na(class)),\n    aes(color = class),\n    linewidth = 0.8,\n    key_glyph = draw_key_rect\n  ) +\n  geom_sf(\n    data = filter(s2, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.35\n  ) +\n  geom_sf(\n    data = filter(s3, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.15\n  ) +\n  coord_sf(\n    ylim = c(-30.124, -29.9691),\n    xlim = c(-51.265, -51.135)\n  ) +\n  scale_colour_manual(name = \"\", values = unname(cores)) +\n  labs(\n    title = \"**Origem do Nome de Ruas em Porto Alegre**\",\n    caption = \"Fonte: OSM. Cores: ColorBrewer. Autor: @viniciusoike\"\n  ) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = bg_color, colour = bg_color),\n    panel.background = element_rect(fill = bg_color, colour = bg_color),\n    #&gt; Plot Margin\n    plot.margin = margin(t = 1.5, r = 0.5, l = 0.5, b = 1, unit = \"cm\"),\n    #&gt; Textual elements\n    text = element_text(family = \"Gill Sans\", size = 8),\n    plot.title = element_markdown(\n      family = \"Gill Sans\",\n      size = 20,\n      hjust = 0.5\n    ),\n    panel.border = element_rect(colour = \"gray20\", fill = \"transparent\"),\n    #&gt; Legend \n    legend.position = c(0.16, 0.8),\n    #legend.position = \"right\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.box.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.text = element_text(size = 8),\n    legend.margin = margin(t = 0.5, b = 0.5, unit = \"cm\")\n  )"
  },
  {
    "objectID": "press.html",
    "href": "press.html",
    "title": "Imprensa",
    "section": "",
    "text": "Em certas ocasiões, sou convidado para participar de matérias sobre mercado imobiliário na imprensa. Em outros casos, estudos que fiz ganham repercussão nos meios de comunicação. Aqui destaco algumas das publicações mais relevantes.\n\n\n\n[BBC] Nômades digitais e aluguel em dólar: por que moradores estão sendo expulsos de seus bairros na América Latina\n[BBC] Cuánto se han disparado los alquileres en las mayores ciudades de América Latina y qué posibilidades hay de que bajen\n[BBC] O que explica disparada de aluguéis nas maiores cidades da América Latina?\n[BBC] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[CBN] Entenda o movimento de alta dos aluguéis e de queda no valor de compra de imóveis\n[Folha de SP] Aluguel de apartamento com 1 quarto cai em SP, diz pesquisa\n[InvestNews] Aluguel em SP e RJ tem a maior alta nos últimos 3 anos, segundo Quinto Andar\n[Forbes] Aluguéis em SP sobem mais do que a inflação no 1ºtrimestre\n[G1] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[G1] Mercado de imóveis de luxo cresce no Brasil e preços passam dos R$ 40 milhões; veja apartamentos por dentro\n[G1 RS] Microapartamentos: aluguel médio de imóvel com até 30m² em Porto Alegre é de R$ 860\n[Rede Minas TV] A BUSCA POR MICROAPARTAMENTOS É REFLEXO DE UMA NOVA DINÂMICA SOCIAL\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[Veja SP] São Paulo teve aumento de 15,5% no aluguel em 2022\n\n\n\n\n\n[Abecip] Rio é a 6ª capital mais cara para comprar imóvel na América Latina\n[El Economista] CDMX quiere ser el hogar de los nómadas digitales con ayuda de Airbnb\n[Estadão] Preço de Aluguel de Microapartamentos em SP bate recorde após 6a alta seguida\n[Folha de SP] Tamanho mínimo de casa vai de 8 a 25 metros quadrados em capitais brasileiras\n[G1] Comprar imóvel é mais barato no Brasil em relação à América Latina, mas valor ainda pesa no bolso das famílias, diz pesquisa\n[Imobi Report] Em alta ou em queda? Entenda o momento dos microapartamentos\n[Real Estate Market] CDMX y Buenos Aires tienen el alquiler más caro en LATAM\n[R7] Porto Alegre, Curitiba, Salvador e BH são as cidades mais baratas para morar na América Latina\n[Nacion MX] ESTUDIO EN LATAM NOS DA PANORAMA DE REAL ESTATE EN MÉXICO\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[QA] Mercado residencial na América Latina: QuintoAndar faz estudo inédito em 12 cidades de seis países\n[QA] Tendências do mercado imobiliário: o que a economia nos aponta?\n\n\n\n\n\n[Folha de SP] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP, diz pesquisa\n[Abecip] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP\n[G1 SP] Casal com dois filhos e renda mediana em SP só consegue comprar imóvel com mais de 40m² longe do Centro"
  },
  {
    "objectID": "press.html#section",
    "href": "press.html#section",
    "title": "Imprensa",
    "section": "",
    "text": "[BBC] Nômades digitais e aluguel em dólar: por que moradores estão sendo expulsos de seus bairros na América Latina\n[BBC] Cuánto se han disparado los alquileres en las mayores ciudades de América Latina y qué posibilidades hay de que bajen\n[BBC] O que explica disparada de aluguéis nas maiores cidades da América Latina?\n[BBC] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[CBN] Entenda o movimento de alta dos aluguéis e de queda no valor de compra de imóveis\n[Folha de SP] Aluguel de apartamento com 1 quarto cai em SP, diz pesquisa\n[InvestNews] Aluguel em SP e RJ tem a maior alta nos últimos 3 anos, segundo Quinto Andar\n[Forbes] Aluguéis em SP sobem mais do que a inflação no 1ºtrimestre\n[G1] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[G1] Mercado de imóveis de luxo cresce no Brasil e preços passam dos R$ 40 milhões; veja apartamentos por dentro\n[G1 RS] Microapartamentos: aluguel médio de imóvel com até 30m² em Porto Alegre é de R$ 860\n[Rede Minas TV] A BUSCA POR MICROAPARTAMENTOS É REFLEXO DE UMA NOVA DINÂMICA SOCIAL\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[Veja SP] São Paulo teve aumento de 15,5% no aluguel em 2022"
  },
  {
    "objectID": "press.html#section-1",
    "href": "press.html#section-1",
    "title": "Imprensa",
    "section": "",
    "text": "[Abecip] Rio é a 6ª capital mais cara para comprar imóvel na América Latina\n[El Economista] CDMX quiere ser el hogar de los nómadas digitales con ayuda de Airbnb\n[Estadão] Preço de Aluguel de Microapartamentos em SP bate recorde após 6a alta seguida\n[Folha de SP] Tamanho mínimo de casa vai de 8 a 25 metros quadrados em capitais brasileiras\n[G1] Comprar imóvel é mais barato no Brasil em relação à América Latina, mas valor ainda pesa no bolso das famílias, diz pesquisa\n[Imobi Report] Em alta ou em queda? Entenda o momento dos microapartamentos\n[Real Estate Market] CDMX y Buenos Aires tienen el alquiler más caro en LATAM\n[R7] Porto Alegre, Curitiba, Salvador e BH são as cidades mais baratas para morar na América Latina\n[Nacion MX] ESTUDIO EN LATAM NOS DA PANORAMA DE REAL ESTATE EN MÉXICO\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[QA] Mercado residencial na América Latina: QuintoAndar faz estudo inédito em 12 cidades de seis países\n[QA] Tendências do mercado imobiliário: o que a economia nos aponta?"
  },
  {
    "objectID": "press.html#section-2",
    "href": "press.html#section-2",
    "title": "Imprensa",
    "section": "",
    "text": "[Folha de SP] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP, diz pesquisa\n[Abecip] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP\n[G1 SP] Casal com dois filhos e renda mediana em SP só consegue comprar imóvel com mais de 40m² longe do Centro"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html",
    "title": "Pipes",
    "section": "",
    "text": "A partir da versão 4.1.0, o R passou a oferecer o operador |&gt; chamado de pipe (literalmente, cano)1. Este operador foi fortemente inspirado no operador homônimo %&gt;% do popular pacote magrittr. Neste post, explico como utilizar o pipe nativo e como ele difere do pipe do magrittr.\nO operador pipe (em ambos os casos), essencialmente, ordena uma função composta.\nLembrando um pouco sobre funções compostas: a expressão abaixo mostra a aplicação de três funções onde primeiro aplica-se a função f sobre x, depois a função g e, por fim, a função h. Lê-se a função de dentro para fora.\n\\[\nh(g(f(x))) = \\dots\n\\]\nPara tornar o exemplo mais concreto considere o exemplo abaixo onde calcula-se a média geométrica de uma sequência de números aleatórios.\nA média geométrica é dada pela expressão:\n\\[\n\\overline{x} = (\\prod_{i = 1}^{n}x_{i})^{\\frac{1}{n}} = \\text{exp}(\\frac{1}{n}\\sum_{i = 1}^{n}\\text{log}(x_{i}))\n\\]\n\nx &lt;- rnorm(n = 100, mean = 10)\n#&gt; Calcula a média geométrica\nexp(mean(log(x)))\n\n[1] 10.221\n\n\nUsando a mesma notação acima, aplica-se primeiro a função log (f), depois a função mean (g) e, por fim, a função exp (h). Usando o operador pipe, pode-se reescrever a expressão da seguinte forma.\n\nx |&gt; log() |&gt; mean() |&gt; exp()\n\n[1] 10.221\n\n\nNote que o resultado da função vai sendo “carregado” da esquerda para a direita sucessivamente. Para muitos usuários, a segunda sintaxe é mais intuitiva e/ou fácil de ler. No segundo código a ordem em que o nome das funções aparecem coincide com a ordem da sua aplicação.\nPor fim, note que o uso de várias funções numa mesma linha de código também nos poupa de ter de criar objetos intermediários como no exemplo abaixo.\n\nlog_x &lt;- log(x)\nlog_media &lt;- mean(log_x)\nmedia_geometrica_x &lt;- exp(log_media)\n\nmedia_geometrica_x\n\n[1] 10.221\n\n\nOs exemplos acima funcionaram sem problemas porque usou-se o operador pipe para “abrir” uma função composta. O argumento de cada função subsequente é o resultado da função antecedente: funciona como uma linha de montagem, em que cada nova etapa soma-se ao resultado da etapa anterior.\nQuando o resultado da função anterior não vai diretamente no primeiro argumento da função subsequente, precisa-se usar o operador _ (underline/underscore)2. Este operador serve como um placeholder: indica onde que o resultado da etapa anterior deve entrar. No exemplo abaixo, uso o placeholder para colocar a base de dados filtrada no argumento data dentro da função lm.\n\ncarros_4 &lt;- subset(mtcars, cyl == 4)\nfit &lt;- lm(mpg ~ wt, data = carros_4)\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _)\n\nPor fim, temos o caso das funções anônimas3. Uma função anônima é simplesmente uma função sem nome que é chamada uma única vez. Infelizmente, a sintaxe de um pipe com uma função anônima é bastante carregada.\n\nobjeto |&gt; (\\(x, y, z, ...) {define função})()\n\n# Nova sintaxe de funções anônimas (similar a lambda no Python)\nobjeto |&gt; (\\(x, y) {x^2 + y^2})()\n# Antiga sintaxe de funções anônimas\nobjeto |&gt; (function(x, y) {x^2 + y^2})()\n\nO exemplo repete o código acima, mas agora usa uma função anônima para pegar o R2 ajustado da regressão.\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _) |&gt; \n  (\\(x) {summary(x)$adj.r.squared})()\n\n\n\n\nImagine agora que se quer calcular o erro absoluto médio de uma regressão. Lembre-se que o EAM é dado por\n\\[\n\\text{EAM} = \\frac{1}{N}\\sum_{i = 1}^{N}|e_{i}|\n\\]\nonde \\(e_{i}\\) é o resíduo da regressão. O código abaixo mostra como fazer isto usando pipes.\n\n#&gt; Estima uma regressão qualquer\nfit &lt;- lm(mpg ~ wt, data = mtcars)\n\n#&gt; Calcula o erro absoluto médio\nfit |&gt; residuals() |&gt; abs() |&gt; mean()\n\n[1] 2.340642\n\n\nNote, contudo, que a situação fica um pouco mais complicada no caso em que se quer calcular a raiz do erro quadrado médio.\n\\[\n\\text{REQM} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^{N}(e_{i})^2}\n\\]\nNa sintaxe convencional temos\n\nsqrt(mean(residuals(fit)^2))\n\n[1] 2.949163\n\n\nO problema é que a exponenciação acontece via um operador e não uma função. Nenhum dos exemplos abaixo funciona.\n\nfit |&gt; residuals() |&gt; ^2 |&gt; mean() |&gt; sqrt()\n\nError: &lt;text&gt;:1:23: unexpected '^'\n1: fit |&gt; residuals() |&gt; ^\n                          ^\n\n\n\nfit |&gt; residuals()^2 |&gt; mean() |&gt; sqrt()\n\nError: function '^' not supported in RHS call of a pipe\n\n\nPara chegar no mesmo resultado, novamente precisa-se usar uma sintaxe bastante esotérica que envolve passar o resultado de residuals para uma função anônima.\n\nfit |&gt; residuals() |&gt; (\\(y) {sqrt(mean(y^2))})()\n\n[1] 2.949163\n\n\n\n\n\nAssim, apesar de muito útil, o operador pipe tem suas limitações. O operador sempre espera encontrar uma função à sua direita; a única maneira de seguir |&gt; com um operador é criando uma função anônima, cuja sintaxe é um pouco carregada. Pode-se resumir os principais fatos sobre o operador pipe:\n\nSimplifica funções compostas. Na expressão x |&gt; f |&gt; g o operador |&gt; aplica a função f sobre o objeto x usando x como argumento de f. Depois, aplica a função g sobre o resultado de f(x). Isto é equivalente a g(f(x)).\nEvita a definição de objetos intermediários. O uso de pipes evita que você precise “salvar” cada passo intermediário da aplicação de funções. Isto deixa seu espaço de trabalho mais limpo e também consome menos memória.\nPlaceholder. Quando o objeto anterior não serve como o primeiro argumento da função subsequente, usa-se o placeholder para indicar onde ele deve ser inserido. x |&gt; f(y = 2, data = _).\nFunção anônima. Em casos mais complexos, é necessário montar uma função anônima usando x |&gt; (\\(y) {funcao})().\n\n\n\n\n\n\nO uso mais comum de pipes é junto com funções do tidyverse, que foram desenvolvidas com este intuito.\n\nlibrary(tidyverse)\n\nAs funções do tidyverse (quase) sempre recebem um data.frame como primeiro argumento; isto facilita a construção de código usando pipe, pois basta encadear as funções em sequência.\n\nfiltered_df &lt;- filter(mtcars, wt == 2)\ngrouped_df &lt;- group_by(filtered_df, cyl)\ntbl &lt;- summarise(grouped_df, avg = mean(mpg), count = n())\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg))\n\nA leitura do código fica mais “gramatical”: pegue o objeto mtcars filtre as linhas onde wt &gt; 2 depois agrupe pela variável cyl e, por fim, tire uma média de mpg.\nPode-se terminar um pipe com uma chamada para um plot em ggplot2 para uma rápida visualização dos resultados\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg)) |&gt; \n  ggplot(aes(x = as.factor(cyl), y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNão se recomenda fazer longas sequências de pipes, pois o código pode acabar muito confuso para quem está lendo. O exemplo abaixo mostra justamente isto.\n\nlibrary(realestatebr)\n\nabecip &lt;- get_abecip_indicators(cached = TRUE)\n\nabecip$units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) |&gt; \n  filter(category == \"units\", type != \"total\") |&gt; \n  ggplot(aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nÉ recomendável quebrar o código acima em passos distintos. Além de ficar mais organizado, pode-se salvar objetos úteis como a tabela agrupada por trimestre, antes de se aplicar o filtro de unidades. A tabela final também fica salva num objeto, permitindo que se faça outros gráficos e análises com estes dados.\n\nunits &lt;- abecip$units\n\n#&gt; Converte em long e agrega os dados por trimestre\ntab_quarter &lt;- units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) \n\n#&gt; Filtra apenas dados de unidades e retira o 'total'\ntab_units &lt;- tab_quarter |&gt; \n  filter(category == \"units\", type != \"total\")\n\n#&gt; Faz o gráfico\nggplot(tab_units, aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\nO pacote sf também funciona bem com pipes pois há vários casos em que se quer aplicar múltiplas funções num mesmo objeto.\n\n# Transforma um data.frame num objeto espacial (pontos)\n# depois faz a interseção dos pontos num polígono e\n# por fim limpa as geometrias\n\ndat |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  st_join(poly) |&gt; \n  filter(!is.na(gid)) |&gt; \n  st_make_valid()\n\nO exemplo abaixo é emprestado do pacote censobr e mostra como combinar a manipulação de dados do dplyr com objetos espaciais manipulados via sf.\n\nlibrary(censobr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(mapview)\n\n# Importa alguns dados do Censo IBGE 2010\npop &lt;- read_population(\n  year = 2010,\n  columns = c(\"code_weighting\", \"abbrev_state\", \"V0010\")\n  )\n# Calcula a população total das áreas de ponderação no Rio de Janeiro\ndf &lt;- pop |&gt;\n      filter(abbrev_state == \"RJ\") |&gt;\n      group_by(code_weighting) |&gt;\n      summarise(total_pop = sum(V0010)) |&gt;\n      collect()\n\n# Import o shape das áreas de ponderação do Censo\nareas &lt;- read_weighting_area(3304557, showProgress = FALSE)\n\nareas |&gt; \n  # Converte o CRS da geometria\n  st_transform(crs = 4326) |&gt; \n  # \"Limpa\" as geometrias\n  st_make_valid() |&gt; \n  # Junta com os dados do Censo\n  left_join(df, by = \"code_weighting\") |&gt; \n  # Visualiza os dados num mapa interativo\n  mapview(zcol = \"total_pop\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#introdução",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#introdução",
    "title": "Pipes",
    "section": "",
    "text": "A partir da versão 4.1.0, o R passou a oferecer o operador |&gt; chamado de pipe (literalmente, cano)1. Este operador foi fortemente inspirado no operador homônimo %&gt;% do popular pacote magrittr. Neste post, explico como utilizar o pipe nativo e como ele difere do pipe do magrittr.\nO operador pipe (em ambos os casos), essencialmente, ordena uma função composta.\nLembrando um pouco sobre funções compostas: a expressão abaixo mostra a aplicação de três funções onde primeiro aplica-se a função f sobre x, depois a função g e, por fim, a função h. Lê-se a função de dentro para fora.\n\\[\nh(g(f(x))) = \\dots\n\\]\nPara tornar o exemplo mais concreto considere o exemplo abaixo onde calcula-se a média geométrica de uma sequência de números aleatórios.\nA média geométrica é dada pela expressão:\n\\[\n\\overline{x} = (\\prod_{i = 1}^{n}x_{i})^{\\frac{1}{n}} = \\text{exp}(\\frac{1}{n}\\sum_{i = 1}^{n}\\text{log}(x_{i}))\n\\]\n\nx &lt;- rnorm(n = 100, mean = 10)\n#&gt; Calcula a média geométrica\nexp(mean(log(x)))\n\n[1] 10.221\n\n\nUsando a mesma notação acima, aplica-se primeiro a função log (f), depois a função mean (g) e, por fim, a função exp (h). Usando o operador pipe, pode-se reescrever a expressão da seguinte forma.\n\nx |&gt; log() |&gt; mean() |&gt; exp()\n\n[1] 10.221\n\n\nNote que o resultado da função vai sendo “carregado” da esquerda para a direita sucessivamente. Para muitos usuários, a segunda sintaxe é mais intuitiva e/ou fácil de ler. No segundo código a ordem em que o nome das funções aparecem coincide com a ordem da sua aplicação.\nPor fim, note que o uso de várias funções numa mesma linha de código também nos poupa de ter de criar objetos intermediários como no exemplo abaixo.\n\nlog_x &lt;- log(x)\nlog_media &lt;- mean(log_x)\nmedia_geometrica_x &lt;- exp(log_media)\n\nmedia_geometrica_x\n\n[1] 10.221\n\n\nOs exemplos acima funcionaram sem problemas porque usou-se o operador pipe para “abrir” uma função composta. O argumento de cada função subsequente é o resultado da função antecedente: funciona como uma linha de montagem, em que cada nova etapa soma-se ao resultado da etapa anterior.\nQuando o resultado da função anterior não vai diretamente no primeiro argumento da função subsequente, precisa-se usar o operador _ (underline/underscore)2. Este operador serve como um placeholder: indica onde que o resultado da etapa anterior deve entrar. No exemplo abaixo, uso o placeholder para colocar a base de dados filtrada no argumento data dentro da função lm.\n\ncarros_4 &lt;- subset(mtcars, cyl == 4)\nfit &lt;- lm(mpg ~ wt, data = carros_4)\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _)\n\nPor fim, temos o caso das funções anônimas3. Uma função anônima é simplesmente uma função sem nome que é chamada uma única vez. Infelizmente, a sintaxe de um pipe com uma função anônima é bastante carregada.\n\nobjeto |&gt; (\\(x, y, z, ...) {define função})()\n\n# Nova sintaxe de funções anônimas (similar a lambda no Python)\nobjeto |&gt; (\\(x, y) {x^2 + y^2})()\n# Antiga sintaxe de funções anônimas\nobjeto |&gt; (function(x, y) {x^2 + y^2})()\n\nO exemplo repete o código acima, mas agora usa uma função anônima para pegar o R2 ajustado da regressão.\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _) |&gt; \n  (\\(x) {summary(x)$adj.r.squared})()"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#limitações",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#limitações",
    "title": "Pipes",
    "section": "",
    "text": "Imagine agora que se quer calcular o erro absoluto médio de uma regressão. Lembre-se que o EAM é dado por\n\\[\n\\text{EAM} = \\frac{1}{N}\\sum_{i = 1}^{N}|e_{i}|\n\\]\nonde \\(e_{i}\\) é o resíduo da regressão. O código abaixo mostra como fazer isto usando pipes.\n\n#&gt; Estima uma regressão qualquer\nfit &lt;- lm(mpg ~ wt, data = mtcars)\n\n#&gt; Calcula o erro absoluto médio\nfit |&gt; residuals() |&gt; abs() |&gt; mean()\n\n[1] 2.340642\n\n\nNote, contudo, que a situação fica um pouco mais complicada no caso em que se quer calcular a raiz do erro quadrado médio.\n\\[\n\\text{REQM} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^{N}(e_{i})^2}\n\\]\nNa sintaxe convencional temos\n\nsqrt(mean(residuals(fit)^2))\n\n[1] 2.949163\n\n\nO problema é que a exponenciação acontece via um operador e não uma função. Nenhum dos exemplos abaixo funciona.\n\nfit |&gt; residuals() |&gt; ^2 |&gt; mean() |&gt; sqrt()\n\nError: &lt;text&gt;:1:23: unexpected '^'\n1: fit |&gt; residuals() |&gt; ^\n                          ^\n\n\n\nfit |&gt; residuals()^2 |&gt; mean() |&gt; sqrt()\n\nError: function '^' not supported in RHS call of a pipe\n\n\nPara chegar no mesmo resultado, novamente precisa-se usar uma sintaxe bastante esotérica que envolve passar o resultado de residuals para uma função anônima.\n\nfit |&gt; residuals() |&gt; (\\(y) {sqrt(mean(y^2))})()\n\n[1] 2.949163"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#resumo",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#resumo",
    "title": "Pipes",
    "section": "",
    "text": "Assim, apesar de muito útil, o operador pipe tem suas limitações. O operador sempre espera encontrar uma função à sua direita; a única maneira de seguir |&gt; com um operador é criando uma função anônima, cuja sintaxe é um pouco carregada. Pode-se resumir os principais fatos sobre o operador pipe:\n\nSimplifica funções compostas. Na expressão x |&gt; f |&gt; g o operador |&gt; aplica a função f sobre o objeto x usando x como argumento de f. Depois, aplica a função g sobre o resultado de f(x). Isto é equivalente a g(f(x)).\nEvita a definição de objetos intermediários. O uso de pipes evita que você precise “salvar” cada passo intermediário da aplicação de funções. Isto deixa seu espaço de trabalho mais limpo e também consome menos memória.\nPlaceholder. Quando o objeto anterior não serve como o primeiro argumento da função subsequente, usa-se o placeholder para indicar onde ele deve ser inserido. x |&gt; f(y = 2, data = _).\nFunção anônima. Em casos mais complexos, é necessário montar uma função anônima usando x |&gt; (\\(y) {funcao})()."
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#aplicações-comuns",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#aplicações-comuns",
    "title": "Pipes",
    "section": "",
    "text": "O uso mais comum de pipes é junto com funções do tidyverse, que foram desenvolvidas com este intuito.\n\nlibrary(tidyverse)\n\nAs funções do tidyverse (quase) sempre recebem um data.frame como primeiro argumento; isto facilita a construção de código usando pipe, pois basta encadear as funções em sequência.\n\nfiltered_df &lt;- filter(mtcars, wt == 2)\ngrouped_df &lt;- group_by(filtered_df, cyl)\ntbl &lt;- summarise(grouped_df, avg = mean(mpg), count = n())\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg))\n\nA leitura do código fica mais “gramatical”: pegue o objeto mtcars filtre as linhas onde wt &gt; 2 depois agrupe pela variável cyl e, por fim, tire uma média de mpg.\nPode-se terminar um pipe com uma chamada para um plot em ggplot2 para uma rápida visualização dos resultados\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg)) |&gt; \n  ggplot(aes(x = as.factor(cyl), y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNão se recomenda fazer longas sequências de pipes, pois o código pode acabar muito confuso para quem está lendo. O exemplo abaixo mostra justamente isto.\n\nlibrary(realestatebr)\n\nabecip &lt;- get_abecip_indicators(cached = TRUE)\n\nabecip$units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) |&gt; \n  filter(category == \"units\", type != \"total\") |&gt; \n  ggplot(aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nÉ recomendável quebrar o código acima em passos distintos. Além de ficar mais organizado, pode-se salvar objetos úteis como a tabela agrupada por trimestre, antes de se aplicar o filtro de unidades. A tabela final também fica salva num objeto, permitindo que se faça outros gráficos e análises com estes dados.\n\nunits &lt;- abecip$units\n\n#&gt; Converte em long e agrega os dados por trimestre\ntab_quarter &lt;- units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) \n\n#&gt; Filtra apenas dados de unidades e retira o 'total'\ntab_units &lt;- tab_quarter |&gt; \n  filter(category == \"units\", type != \"total\")\n\n#&gt; Faz o gráfico\nggplot(tab_units, aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\nO pacote sf também funciona bem com pipes pois há vários casos em que se quer aplicar múltiplas funções num mesmo objeto.\n\n# Transforma um data.frame num objeto espacial (pontos)\n# depois faz a interseção dos pontos num polígono e\n# por fim limpa as geometrias\n\ndat |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  st_join(poly) |&gt; \n  filter(!is.na(gid)) |&gt; \n  st_make_valid()\n\nO exemplo abaixo é emprestado do pacote censobr e mostra como combinar a manipulação de dados do dplyr com objetos espaciais manipulados via sf.\n\nlibrary(censobr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(mapview)\n\n# Importa alguns dados do Censo IBGE 2010\npop &lt;- read_population(\n  year = 2010,\n  columns = c(\"code_weighting\", \"abbrev_state\", \"V0010\")\n  )\n# Calcula a população total das áreas de ponderação no Rio de Janeiro\ndf &lt;- pop |&gt;\n      filter(abbrev_state == \"RJ\") |&gt;\n      group_by(code_weighting) |&gt;\n      summarise(total_pop = sum(V0010)) |&gt;\n      collect()\n\n# Import o shape das áreas de ponderação do Censo\nareas &lt;- read_weighting_area(3304557, showProgress = FALSE)\n\nareas |&gt; \n  # Converte o CRS da geometria\n  st_transform(crs = 4326) |&gt; \n  # \"Limpa\" as geometrias\n  st_make_valid() |&gt; \n  # Junta com os dados do Censo\n  left_join(df, by = \"code_weighting\") |&gt; \n  # Visualiza os dados num mapa interativo\n  mapview(zcol = \"total_pop\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#footnotes",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#footnotes",
    "title": "Pipes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara a lista completa de mudanças veja News and Notes.↩︎\nTecnicamente, o placeholder foi apenas introduzido na versão 4.2.0 como uma melhoria em relação ao pipe nativo implementado anteriormente. “In a forward pipe |&gt; expression it is now possible to use a named argument with the placeholder _ in the rhs call to specify where the lhs is to be inserted. The placeholder can only appear once on the rhs.”. Link original.↩︎\nA notação abaixo de função anônima, usando \\(x), também foi introduzida na versão 4.1.0 do R. Antigamente, para se definir uma função era necessário usar function(x).↩︎\nVale notar que este comportamento foi introduzido também no placeholder do pipe nativo a partir da versão 4.3.0 do R. Contudo, este comportamento ainda está em fase experimental. “As an experimental feature the placeholder _ can now also be used in the rhs of a forward pipe |&gt; expression as the first argument in an extraction call, such as _$coef.”. Link Original.↩︎\nUma quantidade enorme de pacotes utiliza o magrittr como dependência. Veja a página do CRAN.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html",
    "title": "Apêndice: manipular para enxergar",
    "section": "",
    "text": "Este tutorial, ao contrário da série “ggplot2: do básico ao intermediário” já assume que se tenha um entendimento razoável de R. O material aqui serve mais para consultar/relembrar ou aprender um truque novo. Da maneira como está escrito, não é adequado para uma primeira leitura. Grosso modo, a referência aqui é Hadley (2017) capítulos 4, 5, 10-12."
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#tabelas",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#tabelas",
    "title": "Apêndice: manipular para enxergar",
    "section": "Tabelas",
    "text": "Tabelas\nO objeto central da análise de dados é o data.frame. Um data.frame é uma tabela bidimensional que contém informações: em geral, cada coluna é uma variável diferente e cada linha é uma observação. Este objeto possui propriedades bastante simples:\n\nComprimento fixo. O número de linhas de um data.frame é fixo, assim todas as colunas têm o mesmo comprimento.\nHomogeneidade. Cada coluna de um data.frame é homogênea, isto é, contém um dado de um único tipo. Assim, uma mesma coluna não pode misturar um string e um número, um factor e um string, etc.\nNomes. Cada coluna tem um nome (único e idiomático). Este nome é utilizado para fazer refrência a esta coluna.\n\nEstas três características garantem a funcionalidade e consistência de um data.frame. O comprimento fixo e a homogeneidade, em particular, tornam este tipo de objeto muito conveniente e previsível.\n\nConstruindo tabelas\nPara construir um data.frame basta chamar a função homônima e declarar as suas colunas seguindo as três propriedades acima. Nos exemplos abaixo, ao invés da função data.frame vou utilizar a função tibble que é, essencialmente, equivalente, mas que possui algumas pequenas vantagens. No restante do texto as palavras tibble e data.frame serão utilizadas como sinônimas.\nNo primeiro exemplo crio uma tabela com três linhas e duas colunas.\n\ndados &lt;- tibble(\n  cidade = c(\"Porto Alegre\", \"São Paulo\", \"Salvador\"),\n  pop22 = c(1.332, 11.451, 2.418)\n)\n\nPara visualizar o resultado basta chamar o objeto por nome ou usar a função print. Uma das vantanges do tibble é de mostrar a classe de cada coluna, onde chr indica character (caractere), isto é, um string e dbl indica double, isto é, um número3.\n\ndados\n\n# A tibble: 3 × 2\n  cidade       pop22\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Porto Alegre  1.33\n2 São Paulo    11.5 \n3 Salvador      2.42\n\n\nPode-se também criar a tabela a partir de vetores/objetos previamente declarados.\n\ncidades &lt;- c(\"Porto Alegre\", \"São Paulo\", \"Salvador\")\npopulacao &lt;- c(1.332, 11.451, 2.418)\n\ndados &lt;- tibble(\n  nome_cidade = cidades,\n  pop22 = populacao\n)\n\nQuando alguma das colunas não tiver o mesmo comprimento das demais, o R vai tentar “reciclar” os valores desta coluna. Em geral, isto vai causar um erro, mas em alguns casos pode funcionar. No caso abaixo o valor \"Brasil\" (de comprimento unitário) é repetido três vezes para “caber” dentro da tabela.\n\ndados &lt;- tibble(\n  cidade = c(\"Porto Alegre\", \"São Paulo\", \"Salvador\"),\n  pop22 = c(1.332, 11.451, 2.418),\n  pais = \"Brasil\"\n)\n\ndados\n\n# A tibble: 3 × 3\n  cidade       pop22 pais  \n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; \n1 Porto Alegre  1.33 Brasil\n2 São Paulo    11.5  Brasil\n3 Salvador      2.42 Brasil\n\n\n\n\nPropriedades de tabelas\nToda coluna de um data.frame possui nomes. Para acessar os nomes usa-se names.\n\nnames(dados)\n#&gt; [1] \"cidade\" \"pop22\"  \"pais\"\n\nOs nomes das colunas sempre devem ser únicos. Aqui, há uma pequena vantagem em utilizar o tibble. Mesmo no caso em que se tenta criar uma tabela com nomes idênticos, a função data.frame evita que isto acontece, mas emite nenhum tipo de alerta sobre o que está acontecendo.\n\ntab &lt;- data.frame(\n  a = c(1, 2, 3),\n  a = c(\"a\", \"b\", \"c\")\n)\n\ntab\n\n  a a.1\n1 1   a\n2 2   b\n3 3   c\n\n\nA função tibble é um pouco mais exigente e retorna um erro neste caso.\n\ntab &lt;- tibble(\n  a = c(1, 2, 3),\n  a = c(\"a\", \"b\", \"c\")\n)\n\nError in `tibble()`:\n! Column name `a` must not be duplicated.\nUse `.name_repair` to specify repair.\nCaused by error in `repaired_names()`:\n! Names must be unique.\n✖ These names are duplicated:\n  * \"a\" at locations 1 and 2.\n\n\nPara extrair uma coluna de um data.frame temos duas opções. A mais simples e direta é utilizar o operador $ e chamar o nome da coluna como se fosse um objeto. A segunda opção é utilizar [[ e chamar o nome da coluna como um string4.\n\n#&gt; Extraindo uma coluna \n\ndados$cidade\n#&gt; [1] \"Porto Alegre\" \"São Paulo\"    \"Salvador\"\n\ndados[[\"cidade\"]]\n#&gt; [1] \"Porto Alegre\" \"São Paulo\"    \"Salvador\"\n\n\n\nImportando tabelas\nRaramente vamos declarar todas as observações de uma tabela. Na prática, é muito mais comum importar uma tabela de alguma fonte externa como de uma planilha de Excel ou de um arquivo csv. Para cada tipo de arquivo existe uma função read_* diferente. Importar dados costuma ser uma tarefa frustrante por três motivos:\n\nHá muitos arquivos para se importar.\nÉ difícil fazer o R encontrar o arquivo.\nOs arquivos têm problemas (valores corrompidos, linhas vazias, etc.)\n\nOs dois primeiros problemas são simples de se resolver. Pode-se importar múltiplos arquivos ao mesmo tempo usando um loop; importar todos os arquivos dentro de uma mesma pasta é trivial, desde que os arquivos sigam o mesmo padrão.\nGarantir que o R consiga encontrar os arquivos também é simples. Idealmente, todos os arquivos externos devem estar organizados dentro de uma pasta chamada dados ou data e deve-se chamar estes dados usando funções read_*. Uma boa prática é sempre usar “caminhos relativos” ao invés de caminhos absolutos.\n\n#&gt; Ruim\ndat &lt;- read_csv(\"/Users/viniciusoike/Documents/GitHub/projeto/data/income.csv\")\n#&gt; Bom \ndat &lt;- read_csv(\"data/income.csv\")\n#&gt; Ainda melhor\ndat &lt;- read_csv(here::here(\"data/income.csv\"))\n\nO terceiro problema é muito mais complexo e vai exigir mais conhecimento e prática. Em geral, resolve-se a maior parte dos problemas usando algum dos argumentos dentro da função read_* como:\n\nskip: Pula as primeiras k linhas.\nna: Define quais valores devem ser interpretados como valores ausentes.\ncol_types: Permite que se declare explicitamente qual o tipo de dado (numérico, data, texto) que está armazenado em cada coluna.\ncol_names ou name_repair: O primeiro permite que se declare explicitamente o nome que cada coluna vai ter dentro do R enquanto o segundo permite que se use uma função que renomeia as colunas.\nlocale: Permite selecionar diferentes tipos de padrão de local. Em geral, usa-se locale = locale(\"pt_BR\").\nrange: Este argumento só vale no caso de planilhas de Excel e permite que se importe uma seleção específica da planilha (e.g. “D4:H115”)\n\nO código abaixo mostra um exemplo particularmente tenebroso. O título da segunda coluna inclui símbolos como $ e /; as datas estão em português com o mês escrito por extenso e em formato dia-mês-ano; os números usam a vírgula (ao invés do ponto) para separar o decimal e o valor ausente é sinalizado com “X”.\n\n#&gt; Input de um csv sujo\ndados &lt;-\n'Data; Valor (R$/m2)\n\"01-maio-2020\";22,3\n\"01-junho-2020\";21,5\n\"06-julho-2021\";X\n\"07-novembro-2022\";22'\n\n#&gt; Lendo o arquivo\ndf &lt;- read_delim(\n  #&gt; Substitui esta linha pelo 'path' até o csv\n  I(dados),\n  delim = \";\",\n  #&gt; Usa , como separador decimal; lê meses em português (e.g. maio, junho, etc.)\n  locale = locale(decimal_mark = \",\", date_names = \"pt\", date_format = \"%d-%B-%Y\"),\n  #&gt; Interpreta X como valores ausentes (NA)\n  na = \"x\",\n  #&gt; Renomeia as colunas\n  name_repair = janitor::clean_names\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#rename",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#rename",
    "title": "Apêndice: manipular para enxergar",
    "section": "rename",
    "text": "rename\nPara renomear as colunas de data.frame usa-se a função rename (renomear) com rename(tbl, novo_nome = velho_nome). Vale lembrar que para checar os nomes da tabela usa-se names(tbl).\n\n#&gt; Renomear colunas\ntbl_renamed &lt;- rename(tbl, codigo_municipio = code_muni, pop = population)\n\nOs nomes das colunas de um data.frame devem\n\nSer únicos (não-duplicados) e evitar caracteres maiúsculos.\nNão devem incluir caracteres especiais (e.g. !*&@%), nem começar com um número ou caractere especial.\nEvitar espaços em branco, que devem ser substituídos por _ ou omitidos (e.g. PIB Agro deve ser reescrito como pibAgro ou pib_agro.\n\nTambém é possível renomear colunas com auxílio de um vetor ou lista e usando ou all_of (todos) ou any_of (algum/alguns). O exemplo abaixo mostra a lógica geral: temos um vetor que indica o novo nome e o antigo nome de cada coluna que se quer trocar. Caso se queira trocar exatamente todos os nomes indicados no vetor usa-se all_of (mais rigoroso), caso se queira trocar todos os nomes indicados no vetor, ignorando os casos que não batem com nomes de colunas existentes, usa-se any_of.\n\nnew_names &lt;- c(\n  \"codigo_municipio\" = \"code_muni\",\n  \"pop\" = \"population\",\n  \"pop_rate\" = \"population_growth_rate\"\n  )\n\ntbl_renamed &lt;- rename(tbl, all_of(new_names))\n\nNo exemplo abaixo incluo uma “nova coluna” chamada unit que desejo renomear para unidade. Usando any_of o retorno é exatamente igual ao caso acima, pois a função ignora a coluna inexistente unit.\n\nnew_names &lt;- c(\n  \"codigo_municipio\" = \"code_muni\",\n  \"pop\" = \"population\",\n  \"pop_rate\" = \"population_growth_rate\",\n  \"unidade\" = \"unit\"\n)\n\ntbl_renamed &lt;- rename(tbl, any_of(new_names))\n\nJá a função all_of é mais rigorosa e retorna um erro indicando que a coluna unit não existe.\n\ntbl_renamed &lt;- rename(tbl, all_of(new_names))\n\nError in `all_of()`:\n! Can't rename columns that don't exist.\n✖ Column `unit` doesn't exist.\n\n\nO uso de um vetor externo para renomear colunas é conveniente não somente porque permite melhor organizar o código; na prática, este vetor externo funciona como um dicionário de variáveis que pode ser inclusive utilizado para tratar várias bases de dados ou mesmo em outros códigos. Além disso, tratar o nome das colunas como strings é útil pois nos permite transformar este dado mais facilmente.\nPor fim, pode-se aplicar uma função para renomear as colunas usando rename_with.\n\ntbl_renamed &lt;- rename_with(tbl, toupper)\nnames(tbl_renamed)\n\n [1] \"CODE_MUNI\"              \"NAME_MUNI\"              \"CODE_STATE\"            \n [4] \"NAME_STATE\"             \"ABBREV_STATE\"           \"CODE_REGION\"           \n [7] \"NAME_REGION\"            \"POPULATION\"             \"POPULATION_GROWTH\"     \n[10] \"POPULATION_GROWTH_RATE\" \"CITY_AREA\"              \"POPULATION_DENSITY\"    \n[13] \"HOUSEHOLDS\"             \"DWELLERS_PER_HOUSEHOLD\" \"PIB\"                   \n[16] \"PIB_SHARE_UF\"           \"PIB_TAXES\"              \"PIB_ADDED_VALUE\"       \n[19] \"PIB_AGRICULTURE\"        \"PIB_INDUSTRIAL\"         \"PIB_SERVICES\"          \n[22] \"PIB_GOVMT_SERVICES\"    \n\n\nUma dica final para rapidamente renomear colunas é a função janitor::clean_names que obedece aos três princípios elencados acima. Ela pode ser utilizada diretamente num data.frame como se vê no exemplo abaixo.\n\ntest_df &lt;- as.data.frame(matrix(ncol = 6))\nnames(test_df) &lt;- c(\"firstName\", \"ábc@!*\", \"% successful (2009)\",\n                    \"REPEAT VALUE\", \"REPEAT VALUE\", \"\")\njanitor::clean_names(test_df)\n#&gt;   first_name abc percent_successful_2009 repeat_value repeat_value_2  x\n#&gt; 1         NA  NA                      NA           NA             NA NA"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#select",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#select",
    "title": "Apêndice: manipular para enxergar",
    "section": "select",
    "text": "select\nA função select serve para selecionar colunas num data.frame, permitindo-se trabalhar com uma versão menor dos dados. Similarmente à função rename é possível selecionar colunas diretamente select(tbl, coluna_1, coluna_2, …) ou selecionando os nomes via um vetor de strings com auxílio de any_of of all_of.\n\n#&gt; Seleciona diretamente as colunas\nsel_tbl &lt;- select(tbl, code_muni, pib, pib_agriculture)\n\n#&gt; Cria um vetor de nomes\ncolunas &lt;- c(\"code_muni\", \"pib\", \"pib_agriculture\")\n#&gt; Seleciona as colunas baseado no vetor\nsel_tbl &lt;- select(tbl, all_of(colunas))\n\nsel_tbl\n\n# A tibble: 5,570 × 3\n   code_muni     pib pib_agriculture\n       &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n 1   1100015  570272          203394\n 2   1100023 2818049          199723\n 3   1100031  167190           81177\n 4   1100049 2519353          236215\n 5   1100056  600670           94758\n 6   1100064  366931           88923\n 7   1100072  268381          155648\n 8   1100080  261978           80684\n 9   1100098  666331          137802\n10   1100106  984586           59384\n# ℹ 5,560 more rows\n\n\nPara remover uma coluna, basta usar o sinal de menos na frente do nome.\n\n#&gt; Seleciona diretamente as colunas\nsel_tbl &lt;- select(tbl, -pib, -city_area, -population_density)\n\n#&gt; Cria um vetor de nomes\ncolunas &lt;- c(\"pib\", \"city_area\", \"population_density\")\n#&gt; Seleciona as colunas baseado no vetor\nsel_tbl &lt;- select(tbl, -all_of(colunas))\n\nsel_tbl\n\n# A tibble: 5,570 × 19\n   code_muni name_muni             code_state name_state abbrev_state\n       &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       \n 1   1100015 Alta Floresta D'Oeste         11 Rondônia   RO          \n 2   1100023 Ariquemes                     11 Rondônia   RO          \n 3   1100031 Cabixi                        11 Rondônia   RO          \n 4   1100049 Cacoal                        11 Rondônia   RO          \n 5   1100056 Cerejeiras                    11 Rondônia   RO          \n 6   1100064 Colorado do Oeste             11 Rondônia   RO          \n 7   1100072 Corumbiara                    11 Rondônia   RO          \n 8   1100080 Costa Marques                 11 Rondônia   RO          \n 9   1100098 Espigão D'Oeste               11 Rondônia   RO          \n10   1100106 Guajará-Mirim                 11 Rondônia   RO          \n   code_region name_region population population_growth population_growth_rate\n         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n 1           1 Norte            21495             -2897                  -1.05\n 2           1 Norte            96833              6480                   0.58\n 3           1 Norte             5363              -950                  -1.35\n 4           1 Norte            86895              8321                   0.84\n 5           1 Norte            15890             -1139                  -0.58\n 6           1 Norte            15663             -2928                  -1.42\n 7           1 Norte             7519             -1264                  -1.29\n 8           1 Norte            12627             -1051                  -0.66\n 9           1 Norte            29397               668                   0.19\n10           1 Norte            39386             -2270                  -0.47\n   households dwellers_per_household pib_share_uf pib_taxes pib_added_value\n        &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1       7695                   2.79         1.11     35109          535163\n 2      34768                   2.77         5.46    295656         2522393\n 3       1967                   2.73         0.32      7237          159953\n 4      31919                   2.71         4.88    274451         2244902\n 5       5873                   2.69         1.16     89923          510747\n 6       5991                   2.61         0.71     24075          342856\n 7       2840                   2.64         0.52     10200          258181\n 8       4161                   3.01         0.51      9276          252702\n 9      10463                   2.8          1.29     58285          608046\n10      11803                   3.3          1.91    139461          845125\n   pib_agriculture pib_industrial pib_services pib_govmt_services\n             &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n 1          203394          20716       150192             160860\n 2          199723         404752      1207405             710513\n 3           81177           5438        28667              44671\n 4          236215         275537      1157344             575806\n 5           94758          23582       276755             115652\n 6           88923          24322       118529             111082\n 7          155648          10847        34749              56937\n 8           80684           6205        48318             117495\n 9          137802          54521       213513             202211\n10           59384          41650       443005             301086\n# ℹ 5,560 more rows\n\n\nPode-se também selecionar várias colunas ao mesmo tempo se elas estiverem em sequência (uma ao lado da outra). O código abaixo, por exemplo, seleciona a coluna code_muni e todas as colunas entre pib e pib_added_value (inclusive).\n\nsel_tbl &lt;- select(tbl, code_muni, pib:pib_added_value)\n\nsel_tbl\n\n# A tibble: 5,570 × 5\n   code_muni     pib pib_share_uf pib_taxes pib_added_value\n       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1   1100015  570272         1.11     35109          535163\n 2   1100023 2818049         5.46    295656         2522393\n 3   1100031  167190         0.32      7237          159953\n 4   1100049 2519353         4.88    274451         2244902\n 5   1100056  600670         1.16     89923          510747\n 6   1100064  366931         0.71     24075          342856\n 7   1100072  268381         0.52     10200          258181\n 8   1100080  261978         0.51      9276          252702\n 9   1100098  666331         1.29     58285          608046\n10   1100106  984586         1.91    139461          845125\n# ℹ 5,560 more rows\n\n\nTambém é possível renomear e selecionar ao mesmo tempo.\n\nsel_tbl &lt;- select(tbl, codigo_municipio = code_muni, pib)\n\nsel_tbl\n\n# A tibble: 5,570 × 2\n   codigo_municipio     pib\n              &lt;dbl&gt;   &lt;dbl&gt;\n 1          1100015  570272\n 2          1100023 2818049\n 3          1100031  167190\n 4          1100049 2519353\n 5          1100056  600670\n 6          1100064  366931\n 7          1100072  268381\n 8          1100080  261978\n 9          1100098  666331\n10          1100106  984586\n# ℹ 5,560 more rows\n\n\nPor fim, existem algumas funções auxiliares que facilitam a seleção de múltiplas colunas. Vou apresentar apenas três destas funções:\n\nstarts_with/ends_with - selecionam colunas que começam ou terminam com determindo string.\nwhere - seleciona colunas de uma determinada classe (numeric, factor, etc.)\nmatches - seleciona colunas com base num match, usando regex. Esta função é mais geral e engloba as duas primeiras.\n\nO código abaixo mostra alguns exemplos simples. Vou omitir as saídas do código, max experimente reproduzir o resultado.\n\n#&gt; Seleciona code_muni mais as colunas que começam com 'pib'\nselect(tbl, code_muni, starts_with(\"pib\"))\n#&gt; Seleciona as colunas que terminam com 'muni'\nselect(tbl, ends_with(\"muni\"))\n#&gt; Seleciona code_muni mais as colunas que contêm números\nselect(tbl, code_muni, where(is.numeric))\n\n#&gt; Seleciona as colunas que tem o padrão '_texto_'\nselect(tbl, matches(\"_[a-z].+_\"))\n#&gt; Selciona todas as colunas que começam com 'pib'\nselect(tbl, matches(\"^pib\"))\n#&gt; Seleciona todas as colunas que terminam com 'muni'\nselect(tbl, matches(\"muni$\"))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#filter",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#filter",
    "title": "Apêndice: manipular para enxergar",
    "section": "filter",
    "text": "filter\nA função filter serve para filtrar as linhas de um data.frame segundo alguma condição lógica.\n\nfiltered_tbl &lt;- filter(tbl, population_growth &lt; 0)\n\nfiltered_tbl\n\n# A tibble: 2,399 × 22\n   code_muni name_muni                code_state name_state abbrev_state\n       &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       \n 1   1100015 Alta Floresta D'Oeste            11 Rondônia   RO          \n 2   1100031 Cabixi                           11 Rondônia   RO          \n 3   1100056 Cerejeiras                       11 Rondônia   RO          \n 4   1100064 Colorado do Oeste                11 Rondônia   RO          \n 5   1100072 Corumbiara                       11 Rondônia   RO          \n 6   1100080 Costa Marques                    11 Rondônia   RO          \n 7   1100106 Guajará-Mirim                    11 Rondônia   RO          \n 8   1100114 Jaru                             11 Rondônia   RO          \n 9   1100130 Machadinho D'Oeste               11 Rondônia   RO          \n10   1100148 Nova Brasilândia D'Oeste         11 Rondônia   RO          \n   code_region name_region population population_growth population_growth_rate\n         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n 1           1 Norte            21495             -2897                  -1.05\n 2           1 Norte             5363              -950                  -1.35\n 3           1 Norte            15890             -1139                  -0.58\n 4           1 Norte            15663             -2928                  -1.42\n 5           1 Norte             7519             -1264                  -1.29\n 6           1 Norte            12627             -1051                  -0.66\n 7           1 Norte            39386             -2270                  -0.47\n 8           1 Norte            50591             -1414                  -0.23\n 9           1 Norte            30707              -428                  -0.12\n10           1 Norte            15679             -4195                  -1.96\n   city_area population_density households dwellers_per_household     pib\n       &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n 1      7067               3.04       7695                   2.79  570272\n 2      1314               4.08       1967                   2.73  167190\n 3      2783               5.71       5873                   2.69  600670\n 4      1451              10.8        5991                   2.61  366931\n 5      3060               2.46       2840                   2.64  268381\n 6      4987               2.53       4161                   3.01  261978\n 7     24857               1.58      11803                   3.3   984586\n 8      2944              17.2       18947                   2.66 1665068\n 9      8509               3.61      10841                   2.81  700317\n10      1703               9.21       5798                   2.7   403370\n   pib_share_uf pib_taxes pib_added_value pib_agriculture pib_industrial\n          &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n 1         1.11     35109          535163          203394          20716\n 2         0.32      7237          159953           81177           5438\n 3         1.16     89923          510747           94758          23582\n 4         0.71     24075          342856           88923          24322\n 5         0.52     10200          258181          155648          10847\n 6         0.51      9276          252702           80684           6205\n 7         1.91    139461          845125           59384          41650\n 8         3.23    189006         1476062          223881         195776\n 9         1.36     35830          664487          226106          35782\n10         0.78     26898          376472          123270          22561\n   pib_services pib_govmt_services\n          &lt;dbl&gt;              &lt;dbl&gt;\n 1       150192             160860\n 2        28667              44671\n 3       276755             115652\n 4       118529             111082\n 5        34749              56937\n 6        48318             117495\n 7       443005             301086\n 8       712461             343944\n 9       150365             252233\n10       100806             129836\n# ℹ 2,389 more rows\n\n\nOs principais opereadores lógicos no R:\n\n“Maior que”, “Menor que”: &gt;, &lt;, &gt;=, &lt;=\nE/ou: &, |\n“Negação”: !\n“Igual a”: ==\n“Dentro de”: %in%\n\nExistem alguns outros operadores, mas estes costumam resolver 95% dos casos. O exemplo abaixo mostra como filtrar linhas baseado num string. Note que quando se usa múltiplos strings é preciso usar o %in%.\n\nfilter(tbl, name_muni == \"São Paulo\")\n\n# A tibble: 1 × 22\n  code_muni name_muni code_state name_state abbrev_state code_region name_region\n      &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;      \n1   3550308 São Paulo         35 São Paulo  SP                     3 Sudeste    \n  population population_growth population_growth_rate city_area\n       &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;\n1   11451245            197742                   0.15      1521\n  population_density households dwellers_per_household       pib pib_share_uf\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1              7528.    4307693                   2.65 748759007         31.5\n  pib_taxes pib_added_value pib_agriculture pib_industrial pib_services\n      &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1 124349146       624409861           61896       58077784    520357969\n  pib_govmt_services\n               &lt;dbl&gt;\n1           45912212\n\nfilter(tbl, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\"))\n\n# A tibble: 2 × 22\n  code_muni name_muni      code_state name_state     abbrev_state code_region\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;\n1   3304557 Rio de Janeiro         33 Rio De Janeiro RJ                     3\n2   3550308 São Paulo              35 São Paulo      SP                     3\n  name_region population population_growth population_growth_rate city_area\n  &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;\n1 Sudeste        6211423           -109023                  -0.14      1200\n2 Sudeste       11451245            197742                   0.15      1521\n  population_density households dwellers_per_household       pib pib_share_uf\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1              5175.    2437059                   2.53 331279902         44.0\n2              7528.    4307693                   2.65 748759007         31.5\n  pib_taxes pib_added_value pib_agriculture pib_industrial pib_services\n      &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1  59975014       271304888          105065       36666723    180098159\n2 124349146       624409861           61896       58077784    520357969\n  pib_govmt_services\n               &lt;dbl&gt;\n1           54434942\n2           45912212\n\n\nPara negar a igualdade, basta usar o operador !. No caso do operador %in% há duas maneiras válidas de negá-lo: pode-se colocar o ! no começo da expressão ou colocar a expressão inteira dentro de um parêntesis. Eu tendo a preferir a segunda sintaxe.\n\n#&gt; Remove todas as cidades da região Sudeste\nfilter(tbl, name_region != \"Sudeste\")\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(tbl, !name_region %in% c(\"Sudeste\", \"Norte\"))\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(tbl, !(name_region %in% c(\"Sudeste\", \"Norte\")))\n\nEm geral, não é preciso utilizar o E (&), já que pode-se colocar várias condições lógicas dentro de uma mesma chamada para função filter.\n\nfilter(\n  tbl,\n  name_region == \"Nordeste\",\n  !(name_state %in% c(\"Pernambuco\", \"Piauí\")),\n  !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n  )\n\nNo caso de relações de grandeza, pode-se colocar um número absoluto, mas também pode-se usar alguma função. No exemplo abaixo filtra-se apenas os municípios com PIB acima da média, por exemplo.\n\nfilter(tbl, pib &gt; mean(pib))\nfilter(tbl, population &gt;= 1000000)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#arrange",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#arrange",
    "title": "Apêndice: manipular para enxergar",
    "section": "arrange",
    "text": "arrange\nA função arrange é talvez a mais simples e serve para rearranjar as linhas de um data.frame. Em geral, ela é utilizada mais para fins estéticos ou exploratórios, como para ordenar as cidades pelo maior PIB, ou menor população. Contudo, no caso de uma tabela que contenha séries de tempo, pode ser importante validar que as observações estão na ordem correta.\n\ntbl_arranged &lt;- arrange(tbl, pib)\n\ntbl_arranged\n\n# A tibble: 5,570 × 22\n   code_muni name_muni                  code_state name_state         \n       &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;              \n 1   2209450 Santo Antônio dos Milagres         22 Piauí              \n 2   2510659 Parari                             25 Paraíba            \n 3   3166600 Serra da Saudade                   31 Minas Gerais       \n 4   2414902 Viçosa                             24 Rio Grande Do Norte\n 5   2206308 Miguel Leão                        22 Piauí              \n 6   3147501 Passabém                           31 Minas Gerais       \n 7   2501153 Areia de Baraúnas                  25 Paraíba            \n 8   3115607 Cedro do Abaeté                    31 Minas Gerais       \n 9   5201207 Anhanguera                         52 Goiás              \n10   2504850 Coxixola                           25 Paraíba            \n   abbrev_state code_region name_region  population population_growth\n   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 PI                     2 Nordeste           2138                79\n 2 PB                     2 Nordeste           1720              -122\n 3 MG                     3 Sudeste             833                18\n 4 RN                     2 Nordeste           1822               204\n 5 PI                     2 Nordeste           1318                65\n 6 MG                     3 Sudeste            1600              -166\n 7 PB                     2 Nordeste           2005              -178\n 8 MG                     3 Sudeste            1081              -129\n 9 GO                     5 Centro Oeste        924               -96\n10 PB                     2 Nordeste           1824                53\n   population_growth_rate city_area population_density households\n                    &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n 1                   0.31        34              63.6         606\n 2                  -0.57       208               8.28        644\n 3                   0.18       336               2.48        337\n 4                   0.99        38              48.1         636\n 5                   0.42        93              14.1         406\n 6                  -0.82        94              17.0         610\n 7                  -0.71       114              17.6         673\n 8                  -0.94       283               3.82        449\n 9                  -0.82        56              16.6         358\n10                   0.25       174              10.5         734\n   dwellers_per_household   pib pib_share_uf pib_taxes pib_added_value\n                    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1                   3.53 16741         0.03       396           16345\n 2                   2.67 20839         0.03       903           19936\n 3                   2.46 21055         0          555           20500\n 4                   2.86 21254         0.03       739           20515\n 5                   3.23 21627         0.04      1454           20173\n 6                   2.61 21854         0          817           21037\n 7                   2.98 22128         0.03      1047           21082\n 8                   2.41 22133         0          636           21497\n 9                   2.58 22362         0.01      1312           21050\n10                   2.49 22544         0.03       742           21801\n   pib_agriculture pib_industrial pib_services pib_govmt_services\n             &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n 1             471            776         2545              12552\n 2            1888            769         4692              12587\n 3            5970            908         4249               9373\n 4             768            738         5355              13654\n 5             614           2042         5336              12181\n 6            2582            873         5838              11745\n 7             982            893         3735              15472\n 8            4116            875         5633              10872\n 9            2768            964         7164              10154\n10            1907           1218         4849              13827\n# ℹ 5,560 more rows\n\n\nO padrão da função é de sempre ordenar de maneira crescente (do menor para o maior). Para inverter este comportamento pode-se usar a função desc ou o sinal de menos.\n\n#&gt; Ordena as cidades por PIB em ordem decrescente (maior ao menor)\narrange(tbl, desc(pib))\n#&gt; Ordena as cidades por PIB em ordem decrescente (maior ao menor)\narrange(tbl, -pib)\n\nA função arrange ordena strings em ordem alfabética e Datas em ordem cronológica."
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#mutate",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#mutate",
    "title": "Apêndice: manipular para enxergar",
    "section": "mutate",
    "text": "mutate\nA função mutate cria novas colunas. Em geral, cria-se uma nova coluna com base nas colunas pré-existentes, mas a expressão é bastante geral na forma mutate(tbl, nova_coluna = …). Novamente, vou omitir as saídas para poupar espaço.\n\n#&gt; Cria uma coluna onde todas as entradas são iguais a 1\nmutate(tbl, id = 1)\n#&gt; Cria a coluna 'lpib' igual ao logaritmo natural do 'pib'\nmutate(tbl, lpib = log(pib))\n#&gt; Cria a coluna hh igual a 'household' dividido por 1 milhão\nmutate(tbl, hh = household / 1e6)\n\nUm fato conveniente da função mutate é que ela vai criando as colunas sequencialmente, assim é possível fazer diversas transformações numa mesma chamada à função. No caso abaixo, pode-se criar a variável lpibpc a partir das colunas lpib e lpop.\n\nmutate(tbl,\n  lpib = log(pib),\n  lpop = log(population),\n  #&gt; Criando uma variável a partir de duas colunas criadas anteriormente\n  lpibpc = lpib - lpop,\n  pibserv = pib_services + pib_govmt_services,\n  lpibs = log(pibserv)\n  )\n\nPor fim, é possível transformar múltiplas colunas simultaneamente usando a função across da seguinte maneira: across(colunas, função). Para indicar quais colunas quer-se transformar podemos usar a mesma lógica da função select: isto é, declarando o nome das colunas, usando col1:col2, ou mesmo uma função como starts_with/matches, etc.\n\ntbl |&gt; \n  mutate(across(pib:pib_services, log)) |&gt; \n  select(pib:pib_services)\n\n# A tibble: 5,570 × 7\n     pib pib_share_uf pib_taxes pib_added_value pib_agriculture pib_industrial\n   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n 1  13.3        0.104     10.5             13.2            12.2           9.94\n 2  14.9        1.70      12.6             14.7            12.2          12.9 \n 3  12.0       -1.14       8.89            12.0            11.3           8.60\n 4  14.7        1.59      12.5             14.6            12.4          12.5 \n 5  13.3        0.148     11.4             13.1            11.5          10.1 \n 6  12.8       -0.342     10.1             12.7            11.4          10.1 \n 7  12.5       -0.654      9.23            12.5            12.0           9.29\n 8  12.5       -0.673      9.14            12.4            11.3           8.73\n 9  13.4        0.255     11.0             13.3            11.8          10.9 \n10  13.8        0.647     11.8             13.6            11.0          10.6 \n   pib_services\n          &lt;dbl&gt;\n 1         11.9\n 2         14.0\n 3         10.3\n 4         14.0\n 5         12.5\n 6         11.7\n 7         10.5\n 8         10.8\n 9         12.3\n10         13.0\n# ℹ 5,560 more rows\n\n\n\n#&gt; Aplica uma transformação log em todas as colunas entre pib e pib_services\nmutate(tbl, across(pib:pib_services, log))\n#&gt; Aplica uma transformação log em todas as colunas que começam com pib\nmutate(tbl, across(starts_with(\"pib\"), log))\n#&gt; Divide por pib e multiplica por 100 todas as colunas entre pib_taxes e\n#&gt; pib_govmt_services\nmutate(tbl, across(pib_taxes:pib_govmt_services, ~.x / pib * 100))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#summarise-e-group_by",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#summarise-e-group_by",
    "title": "Apêndice: manipular para enxergar",
    "section": "summarise e group_by",
    "text": "summarise e group_by\n\nUso básico\nAs funções summarise5 e group_by são (quase) sempre utilizadas em conjunto e servem para resumir ou “sumarizar” os dados. A função group_by agrupa os dados segundo alguma coluna. No caso da nossa base de cidades, poderíamos agrupar os dados por estado ou região, por exemplo. A função summarise aplica transformações nestes dados agrupados: pode-se, por exemplo, calcular a população total de cada estado, o PIB per capita médio de cada região, etc.\nA tabela abaixo calcula a população total de cada região e ordena os dados, de maneira decrescente, segundo a população. A partir de agora começo a usar mais o pipe nos códigos.\n\ntbl |&gt; \n  #&gt; Agrupa por região\n  group_by(name_region) |&gt; \n  #&gt; Soma o total da população (dentro de cada região)\n  summarise(pop = sum(population)) |&gt; \n  #&gt; Rearranja o resultado final\n  arrange(desc(pop))\n\n# A tibble: 5 × 2\n  name_region       pop\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Sudeste      84847187\n2 Nordeste     54644582\n3 Sul          29933315\n4 Norte        17349619\n5 Centro Oeste 16287809\n\n\n\n\nUm pouco mais de group_by\nA função group_by agrupa os dados de um tibble e permite que se faça operações sobre estes grupos. Pode-se, por exemplo, calcular o share da população de cada cidade, dentro do seu estado usando mutate. No caso abaixo, eu calculo o share percentual e mostro o resultado para a capital paulista. Vê-se que a capital tem cerca de 11,45 milhão de habitantes, equivalente a 25,78% da população do estado.\n\ntbl_share_pop &lt;- tbl |&gt; \n  group_by(name_state) |&gt; \n  mutate(pop_share = population / sum(population) * 100)\n\ntbl_share_pop |&gt; \n  filter(name_muni == \"São Paulo\") |&gt; \n  select(name_muni, population, pop_share)\n\n# A tibble: 1 × 4\n# Groups:   name_state [1]\n  name_state name_muni population pop_share\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 São Paulo  São Paulo   11451245      25.8\n\n\nSimilarmente, pode-se combinar outras funções como filter. O código abaixo filtra somente as cidades que possuem população acima da média do seu estado. Vale comparar os resultados e entender as diferenças entre cada uma das tabelas\n\n#&gt; Filtra cidades que possuem população acima da média do seu estado\ntbl_pop_grouped &lt;- tbl |&gt; \n  group_by(name_state) |&gt; \n  filter(population &gt; mean(population))\n\n#&gt; Filtra cidades que possuem população acima da média do país\ntbl_pop_ungrouped &lt;- tbl |&gt; \n  filter(population &gt; mean(population))\n\n\n\nUm pouco mais de summarise\nÉ possível fazer várias novas colunas num mesmo summarise. Assim como em mutate também é possível fazer transformações com colunas que foram criadas anteriormente na mesma função. No caso abaixo eu calculo o PIB e população totais de cada estado do nordeste e depois calculo o PIB per capita baseado nestes valores agregados.\n\ntbl |&gt; \n  filter(name_region == \"Nordeste\") |&gt; \n  group_by(name_state) |&gt; \n  summarise(\n    pib_uf = sum(pib) * 1000,\n    pop_uf = sum(population),\n    pibpc_uf = pib_uf / pop_uf\n    ) |&gt; \n  arrange(pibpc_uf)\n\n# A tibble: 9 × 4\n  name_state                pib_uf   pop_uf pibpc_uf\n  &lt;chr&gt;                      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Maranhão            106915961000  6775152   15781.\n2 Piauí                56391259000  3269200   17249.\n3 Paraíba              70292036000  3974495   17686.\n4 Ceará               166914529000  8791688   18985.\n5 Alagoas              63202350000  3127511   20209.\n6 Sergipe              45409659000  2209558   20551.\n7 Pernambuco          193307324000  9058155   21341.\n8 Bahia               305320808000 14136417   21598.\n9 Rio Grande Do Norte  71577110000  3302406   21674.\n\n\nA função summarise é bastante potente. O exemplo abaixo filtra as cidades de médio-grande porte (acima de 100.000 habitantes), agrupa os dados por estado e faz:\n\nO total (soma) da população (cidades com mais de 100.000 habitantes, por estado).\nCalcula a média da população (entre as cidades com mais de 100.000 habitantes, por estado).\nCalcula a população máxima (idem).\nCalcula os quintis da distribuição da população (idem).\nFaz uma regressão linear entre a população e o PIB (idem).\nFaz uma regressão linear entre a população e o PIB e extrai o R2 (idem).\nConta o número de cidades (idem).\n\n\ntbl_summary &lt;- tbl |&gt; \n  filter(population &gt; 100000) |&gt; \n  group_by(name_state) |&gt; \n  summarise(\n    pop_uf = sum(population),\n    pop_avg = mean(population),\n    pop_max = max(population),\n    pop_ntile = list(quantile(population, probs = c(0.2, 0.4, 0.6, 0.8))),\n    reg = list(lm(population ~ pib)),\n    reg_r2 = summary(lm(population ~ pib))$r.squared,\n    count = n()\n    ) |&gt; \n  arrange(desc(count))\n\nNem tudo o que fiz acima faz muito sentido, mas ilustra a capacidade da função summarise de gerar informação e a flexibilidade de um tibble para armazenar diferentes tipos de output. Note que as funções foram todas executadas dentro dos respectivos grupos.\nNo código abaixo pode-se verificar os quintis da população das cidades de Minas Gerais.\n\ntbl_summary |&gt; \n  filter(name_state == \"Minas Gerais\") |&gt; \n  pull(pop_ntile)\n\n[[1]]\n     20%      40%      60%      80% \n111694.6 129821.8 169058.0 333014.8 \n\n\nJá no código abaixo pode-se verificar o resultado da regressão entre população e PIB feita somente nos municípios grandes de São Paulo.\n\nreg_sp &lt;- filter(tbl_summary, name_state == \"São Paulo\")[[\"reg\"]]\nreg_sp &lt;- reg_sp[[1]]\n\nsummary(reg_sp)\n\n\nCall:\nlm(formula = population ~ pib)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-535435  -28068    8337   55289  240897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.330e+04  1.619e+04   3.911 0.000199 ***\npib         1.511e-02  1.851e-04  81.617  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 137300 on 76 degrees of freedom\nMultiple R-squared:  0.9887,    Adjusted R-squared:  0.9886 \nF-statistic:  6661 on 1 and 76 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAdendos técnicos\nVale notar que a função group_by trasnforma um tibble num grouped_df. Para desfazer esta transformação é preciso usar ungroup. De fato, é uma boa prática sempre utilizar ungroup depois de usar um group_by. Por exemplo, no caso em que se calcula o share percentual da população de cada município em seu respectivo estado, é importante desagrupar os dados.\n\ntbl_share_pop &lt;- tbl |&gt; \n  group_by(name_state) |&gt; \n  mutate(pop_share = population / sum(population) * 100) |&gt; \n  ungroup()\n\nEsta prática serve para evitar erros potenciais e, infelizmente, é necessária em alguns casos, como quando se quer combinar duas bases distintas. Atualmente, existe uma sintaxe experimental, fortemente inspirada na sintaxe do data.table, que desagrupa os dados por padrão. No caso do código abaixo não é preciso utilizar ungroup().\nPessoalmente, gosto bastante desta sintaxe, mas como ela ainda está em fase experimental, é melhor esperar um pouco para utilizá-la.\n\ntbl_share_pop &lt;- tbl |&gt; \n  mutate(\n    pop_share = population / sum(population) * 100,\n    .by = \"name_state\")\n\nEste mesmo .by pode ser utilizado dentro de summarise, filter, etc."
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#contra-exemplos",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#contra-exemplos",
    "title": "Apêndice: manipular para enxergar",
    "section": "Contra-exemplos",
    "text": "Contra-exemplos\nVamos começar explorando alguns contra-exemplos de dados que não estão em formato “tidy”.\n\nVendas de casas e apartamentos\nA tabela abaixo segue um formato tipicamente encontrando em planilhas de Excel. A primeira coluna define: (vendas de) apartamentos, casas e o total. Cada coluna subsequente representa um mês diferente; os valores de cada linha representam o número de vendas de cada tipo em cada mês.\n\ndat &lt;- tibble(\n  nome = c(\"Apartamentos\", \"Casas\", \"Total\"),\n  `2022-01` = c(900, 100, 1000),\n  `2022-02` = c(850, 120, 970),\n  `2022-03` = c(875, 125, 1000),\n  `2022-04` = c(920, 100, 1020),\n)\n\ndat\n\n# A tibble: 3 × 5\n  nome         `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos       900       850       875       920\n2 Casas              100       120       125       100\n3 Total             1000       970      1000      1020\n\n\nNote que:\n\nCada coluna não é uma variável. A maior parte das colunas são datas, que deveriam estar todas numa única coluna.\nCada linha não é uma observação. Cada linha é uma série de valores de observações que varia mês a mês.\n\n\n\nVendas e Alugueis de apartamentos e casas\nEsta segunda tabela é uma versão piorada da versão acima.\n\ndat2 &lt;- tibble(\n  nome = c(\"Apartamentos\", \"Casas\", \"Total\", \"Apartamentos\", \"Casas\", \"Total\"),\n  tipo = c(\"Venda\", \"Venda\", \"Venda\", \"Aluguel\", \"Aluguel\", \"Aluguel\"),\n  `2022-01` = c(900, 100, 1000, 50, 100, 150),\n  `2022-02` = c(850, 120, 970, 60, 80, 140),\n  `2022-03` = c(875, 125, 1000, 70, 90, 160),\n  `2022-04` = c(920, 100, 1020, 50, 50, 100),\n)\n\ndat2\n\n# A tibble: 6 × 6\n  nome         tipo    `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos Venda         900       850       875       920\n2 Casas        Venda         100       120       125       100\n3 Total        Venda        1000       970      1000      1020\n4 Apartamentos Aluguel        50        60        70        50\n5 Casas        Aluguel       100        80        90        50\n6 Total        Aluguel       150       140       160       100\n\n\nNote que:\n\nO nome das colunas mistura variáveis e valores.\nA linha continua não sendo uma observação.\n\n\n\nTeste AB\nA tabela abaixo mostra um teste AB num formato (bem) problemático. No experimento, Bernardo e Álvares estão no grupo de tratamento, enquanto Fernando e Ricardo estão no grupo controle.\n\ndat3 &lt;- tibble(\n  id = c(\"Bernardo\", \"Álvares\", \"Fernando\", \"Ricardo\"),\n  controle = c(NA, NA, 7.2, 5.1),\n  tratamento = c(6.4, 5.5, NA, NA)\n)\n\ndat3\n\n# A tibble: 4 × 3\n  id       controle tratamento\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Bernardo     NA          6.4\n2 Álvares      NA          5.5\n3 Fernando      7.2       NA  \n4 Ricardo       5.1       NA  \n\n\nNote que:\n\nNem ‘controle’ e nem ‘tratamento’ são variáveis, já que são valores de uma mesma variável “qual grupo que está o indivíduo”.\n\nAlternativamente, pode-se ter:\n\ndat31 &lt;- tibble(\n  grupo = c(\"controle\", \"tratamento\"),\n  `Bernardo` = c(NA, 6.4),\n  `Álvares` = c(NA, 5.5),\n  `Fernando` = c(7.2, NA),\n  `Ricardo` = c(5.1, NA)\n)\n\ndat31\n\n# A tibble: 2 × 5\n  grupo      Bernardo Álvares Fernando Ricardo\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 controle       NA      NA        7.2     5.1\n2 tratamento      6.4     5.5     NA      NA  \n\n\nAgora ‘controle’ e ‘tratamento’ estão corretamente dentro de uma mesma coluna, mas cada coluna representa um indivíduo diferente e não uma variável. “Bernardo” não é uma variável e sim um valor (nome do indivíduo).\n\n\nCidades\nA tabela abaixo mostra dados hipóteticos de PIB e população de duas cidades.\n\ndat4 &lt;- tibble(\n  nome_cidade = c(\"São Paulo\", \"Porto Alegre\"),\n  pib_2020 = c(1000, 500),\n  pib_2021 = c(1200, 700),\n  pop_2020 = c(1100, 110),\n  pop_2021 = c(1200, 120)\n)\n\ndat4\n\n# A tibble: 2 × 5\n  nome_cidade  pib_2020 pib_2021 pop_2020 pop_2021\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 São Paulo        1000     1200     1100     1200\n2 Porto Alegre      500      700      110      120\n\n\nNote que:\n\nHá mais de uma variável por coluna: a coluna pib_2020 indica duas informações: o ano da observação e o que está sendo mensurado (PIB).\n\n\n\nCasas e Apartamentos\nNesta tabela o nome das colunas mistura variáveis e valores.\n\ndata &lt;- tibble(\n  cidade = c(\"A\", \"B\", \"C\"),\n  financiado_apto_2020 = c(1, 2, 3),\n  vista_apto_2020 = c(2, 2, 2),\n  permuta_casa_2020 = c(3, 1, 2),\n  vista_casa_2020 = c(1, 1, 1)\n)\n\ndata\n\n# A tibble: 3 × 5\n  cidade financiado_apto_2020 vista_apto_2020 permuta_casa_2020 vista_casa_2020\n  &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1 A                         1               2                 3               1\n2 B                         2               2                 1               1\n3 C                         3               2                 2               1"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#organizando",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#organizando",
    "title": "Apêndice: manipular para enxergar",
    "section": "Organizando",
    "text": "Organizando\nSendo bastante franco, acho difícil explicar a intuição por trás das funções pivot_longer e pivot_wider. No caso da primeira função tem-se:\n\ndat |&gt; \n  pivot_longer(\n    cols = ...,\n    #&gt; Argumentos opcionais\n    names_to = \"name\",\n    values_to = \"value\"\n  )\n\nonde cols indica quais colunas devem ser convertidas em formato longitudinal. Este argumento é bastante flexível e segue as mesmas regras da função select. Por exemplo:\n\ndat |&gt; pivot_longer(cols = -date)\ndat |&gt; pivot_longer(cols = starts_with(\"pib\"))\ndat |&gt; pivot_longer(cols = c(\"x1\", \"x2\"))\n\nJá a função pivot_wider é mais exigente:\n\ndat |&gt; \n  pivot_wider(\n    id_cols = ...,\n    names_from = ...,\n    values_from = ...\n  )\n\nO primeiro argumento indica qual coluna identifica unicamente os valores; o segundo argumento indica quais valores devem ser convertidos em colunas (variáveis); o terceiro argumento indica quais valores devem ser convertido em valores (sim, é isto mesmo). A função “desfaz” o que a pivot_longer “faz”, mas também pode fazer novas tabelas e também condensar informação. Pra piorar a situação, ambas as funções tem vários argumentos opcionais, que muitas vezes são super úteis.\nComo num jogo, explicar as regras em voz-alta parece torná-lo mais complicado do que é. A melhor dica que posso dar é que se pratique bastante. Alternativamente, considere também as funções:\n\ndata.table::melt ou reshape2::melt. É preciso escolher as colunas “identificadoras” e as colunas de “mensuração”. Eu penso no “id” como o que identifica unicamente cada linha e “measure” como o que identifica o que está sendo mensurado. Por exemplo: na tabela abaixo temos o preços de duas ações em dois dias distintos.\n\n\ntab &lt;- tibble(\n  data = c(as.Date(\"2023-05-04\"), as.Date(\"2023-05-05\")),\n  PETR4 = c(23.02, 24),\n  CYRE3 = c(15.54, 15.97)\n)\n\ntab\n\n# A tibble: 2 × 3\n  data       PETR4 CYRE3\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2023-05-04  23.0  15.5\n2 2023-05-05  24    16.0\n\n\nPara converter em formato “tidy” ou longitudinal, declara-se quais as colunas identificam a observação e quais as colunas indicam o que está sendo mensurado.\n\nreshape2::melt(tab, id.vars = \"data\", measure.vars = c(\"PETR4\", \"CYRE3\"))\n\n        data variable value\n1 2023-05-04    PETR4 23.02\n2 2023-05-05    PETR4 24.00\n3 2023-05-04    CYRE3 15.54\n4 2023-05-05    CYRE3 15.97\n\n\nNeste caso particular, é útil saber que o argumento measure.vars pode ser omitido\n\nreshape2::melt(tab, id.vars = \"data\")\n\n        data variable value\n1 2023-05-04    PETR4 23.02\n2 2023-05-05    PETR4 24.00\n3 2023-05-04    CYRE3 15.54\n4 2023-05-05    CYRE3 15.97\n\n\n\nO contrário destas funções é data.table::dcast ou reshape2::dcast, cuja sintaxe é um pouco mais estranha, mas bastante intuitiva.\n\n\nlong &lt;- reshape2::melt(tab, id.vars = \"data\")\n\nreshape2::dcast(long, data ~ variable)\n\n        data PETR4 CYRE3\n1 2023-05-04 23.02 15.54\n2 2023-05-05 24.00 15.97\n\n\n\ntidyr::gather que é a versão antiga de pivot_longer. Pessoalmente, sempre achei esta função bastante confusa, mas, ela talvez seja mais intuitiva para você.\n\n\ngather(tab, \"ticker\", \"price\", -data)\n\n# A tibble: 4 × 3\n  data       ticker price\n  &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 2023-05-04 PETR4   23.0\n2 2023-05-05 PETR4   24  \n3 2023-05-04 CYRE3   15.5\n4 2023-05-05 CYRE3   16.0\n\n\n\nlong &lt;- gather(tab, \"ticker\", \"price\", -data)\nspread(long, \"ticker\", \"price\")\n\n# A tibble: 2 × 3\n  data       CYRE3 PETR4\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2023-05-04  15.5  23.0\n2 2023-05-05  16.0  24  \n\n\nNeste caso simples, o par gather/spread parece bastante conveniente, mas em casos mais complexos esta sintaxe limitada torna-se bastante problemática.\n\nExemplo 1\n\ndat\n\n# A tibble: 3 × 5\n  nome         `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos       900       850       875       920\n2 Casas              100       120       125       100\n3 Total             1000       970      1000      1020\n\n\nEste caso é bem simples, pois todas as colunas, exceto a primeira, são uma única variável (as datas do mês). A função rename é usada somente para deixar mais evidente que a primeira coluna contém a tipologia do que foi vendido. O argumento values_to também é opcional.\n\ndat |&gt; \n  rename(tipologia = nome) |&gt; \n  pivot_longer(cols = -tipologia, names_to = \"data\", values_to = \"unidades\")\n\n# A tibble: 12 × 3\n   tipologia    data    unidades\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n 1 Apartamentos 2022-01      900\n 2 Apartamentos 2022-02      850\n 3 Apartamentos 2022-03      875\n 4 Apartamentos 2022-04      920\n 5 Casas        2022-01      100\n 6 Casas        2022-02      120\n 7 Casas        2022-03      125\n 8 Casas        2022-04      100\n 9 Total        2022-01     1000\n10 Total        2022-02      970\n11 Total        2022-03     1000\n12 Total        2022-04     1020\n\n\nNote que agora:\n\nCada linha é uma observação: 900 é o valor de apartamentos vendidos em 2022-01.\nCada coluna é uma variável: a primeira coluna define a ‘tipologia’, a segunda coluna define a ‘data’ e a terceira coluna é o número de ‘unidades’.\n\nPor fim, para ser mais completo pode-se converter a data num formato padrão.\n\ndat |&gt; \n  rename(tipologia = nome) |&gt; \n  pivot_longer(cols = -tipologia, names_to = \"data\", values_to = \"unidades\") |&gt; \n  mutate(data = readr::parse_date(data, format = \"%Y-%m\"))\n\n# A tibble: 12 × 3\n   tipologia    data       unidades\n   &lt;chr&gt;        &lt;date&gt;        &lt;dbl&gt;\n 1 Apartamentos 2022-01-01      900\n 2 Apartamentos 2022-02-01      850\n 3 Apartamentos 2022-03-01      875\n 4 Apartamentos 2022-04-01      920\n 5 Casas        2022-01-01      100\n 6 Casas        2022-02-01      120\n 7 Casas        2022-03-01      125\n 8 Casas        2022-04-01      100\n 9 Total        2022-01-01     1000\n10 Total        2022-02-01      970\n11 Total        2022-03-01     1000\n12 Total        2022-04-01     1020\n\n\n\n\nExemplo 2\n\ndat2\n\n# A tibble: 6 × 6\n  nome         tipo    `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos Venda         900       850       875       920\n2 Casas        Venda         100       120       125       100\n3 Total        Venda        1000       970      1000      1020\n4 Apartamentos Aluguel        50        60        70        50\n5 Casas        Aluguel       100        80        90        50\n6 Total        Aluguel       150       140       160       100\n\n\nO segundo exemplo é quase idêntico ao primeiro.\n\ndat2 |&gt; \n  rename(tipologia = nome, mercado = tipo) |&gt; \n  pivot_longer(\n    cols = -c(tipologia, mercado),\n    names_to = \"data\",\n    values_to = \"unidades\"\n    ) |&gt; \n  print(n = 24)\n\n# A tibble: 24 × 4\n   tipologia    mercado data    unidades\n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n 1 Apartamentos Venda   2022-01      900\n 2 Apartamentos Venda   2022-02      850\n 3 Apartamentos Venda   2022-03      875\n 4 Apartamentos Venda   2022-04      920\n 5 Casas        Venda   2022-01      100\n 6 Casas        Venda   2022-02      120\n 7 Casas        Venda   2022-03      125\n 8 Casas        Venda   2022-04      100\n 9 Total        Venda   2022-01     1000\n10 Total        Venda   2022-02      970\n11 Total        Venda   2022-03     1000\n12 Total        Venda   2022-04     1020\n13 Apartamentos Aluguel 2022-01       50\n14 Apartamentos Aluguel 2022-02       60\n15 Apartamentos Aluguel 2022-03       70\n16 Apartamentos Aluguel 2022-04       50\n17 Casas        Aluguel 2022-01      100\n18 Casas        Aluguel 2022-02       80\n19 Casas        Aluguel 2022-03       90\n20 Casas        Aluguel 2022-04       50\n21 Total        Aluguel 2022-01      150\n22 Total        Aluguel 2022-02      140\n23 Total        Aluguel 2022-03      160\n24 Total        Aluguel 2022-04      100\n\n\n\n\nExemplo 3\n\ndat3\n\n# A tibble: 4 × 3\n  id       controle tratamento\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Bernardo     NA          6.4\n2 Álvares      NA          5.5\n3 Fernando      7.2       NA  \n4 Ricardo       5.1       NA  \n\n\n\ndat3 |&gt; \n  pivot_longer(-id, names_to = \"grupo\", values_to = \"valor\")\n\n# A tibble: 8 × 3\n  id       grupo      valor\n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 Bernardo controle    NA  \n2 Bernardo tratamento   6.4\n3 Álvares  controle    NA  \n4 Álvares  tratamento   5.5\n5 Fernando controle     7.2\n6 Fernando tratamento  NA  \n7 Ricardo  controle     5.1\n8 Ricardo  tratamento  NA  \n\n\n\n\nExemplo 4\n\ndat4\n\n# A tibble: 2 × 5\n  nome_cidade  pib_2020 pib_2021 pop_2020 pop_2021\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 São Paulo        1000     1200     1100     1200\n2 Porto Alegre      500      700      110      120\n\n\nNeste exemplo pode-se separar as colunas facilmente usando names_sep e names_to.\n\ndat4 |&gt; \n  pivot_longer(\n    cols = -nome_cidade,\n    names_sep = \"_\",\n    names_to = c(\"variavel\", \"ano\"),\n    values_to = \"valor\"\n    )\n\n# A tibble: 8 × 4\n  nome_cidade  variavel ano   valor\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n1 São Paulo    pib      2020   1000\n2 São Paulo    pib      2021   1200\n3 São Paulo    pop      2020   1100\n4 São Paulo    pop      2021   1200\n5 Porto Alegre pib      2020    500\n6 Porto Alegre pib      2021    700\n7 Porto Alegre pop      2020    110\n8 Porto Alegre pop      2021    120\n\n\n\n\nExemplo 5\n\ndata\n\n# A tibble: 3 × 5\n  cidade financiado_apto_2020 vista_apto_2020 permuta_casa_2020 vista_casa_2020\n  &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1 A                         1               2                 3               1\n2 B                         2               2                 1               1\n3 C                         3               2                 2               1\n\n\nConfesso que só coloquei este exemplo aqui para mostrar que é possível usar dois pivot_longer em sequência para chegar num resultado útil.\n\ndata |&gt; \n  pivot_longer(\n    cols = -cidade,\n    names_sep = \"_\",\n    names_to = c(\".value\", \"tipologia\", \"ano\")\n  ) |&gt; \n  pivot_longer(\n    cols = financiado:permuta,\n    names_to = \"forma_pagamento\",\n    values_to = \"unidades\"\n  )\n\n# A tibble: 18 × 5\n   cidade tipologia ano   forma_pagamento unidades\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1 A      apto      2020  financiado             1\n 2 A      apto      2020  vista                  2\n 3 A      apto      2020  permuta               NA\n 4 A      casa      2020  financiado            NA\n 5 A      casa      2020  vista                  1\n 6 A      casa      2020  permuta                3\n 7 B      apto      2020  financiado             2\n 8 B      apto      2020  vista                  2\n 9 B      apto      2020  permuta               NA\n10 B      casa      2020  financiado            NA\n11 B      casa      2020  vista                  1\n12 B      casa      2020  permuta                1\n13 C      apto      2020  financiado             3\n14 C      apto      2020  vista                  2\n15 C      apto      2020  permuta               NA\n16 C      casa      2020  financiado            NA\n17 C      casa      2020  vista                  1\n18 C      casa      2020  permuta                2"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#footnotes",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#footnotes",
    "title": "Apêndice: manipular para enxergar",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara saber mais sobre pipes e a diferença entre o novo pipe nativo |&gt; e o pipe %&gt;% do magrittr veja meu post sobre o assunto.↩︎\nNo fundo, isto é ainda mais um incentivo para aprender inglês.↩︎\nA classe mais geral de números do R é a numeric. Aqui, “double” faz referência à precisão do número, que é um “double-precision value”, que equivale a uma precisão de 53 bits, com aplitude de \\(2\\times 10^{-308}\\) a \\(2\\times 10^{-308}\\). Em geral, a maioria dos números (com exceção de inteiros) é armazenada desta forma.↩︎\nDe fato, esta é a mesma sintaxe que se utiliza para extrair um elemento de uma lista. Isto acontece pois um data.frame é essencialmente, uma lista com um pouco mais de estrutura. Isto pode ser verificado usando typeof em um objeto data.frame.↩︎\nO dplyr também aceita a função summarize com ‘z’ ao invés de ‘s’. As funções são exatamente iguais e podem ser intercambiadas sem maiores problemas.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html",
    "href": "posts/general-posts/2023-09-happiness/index.html",
    "title": "Life Satisfaction and GDP per capita",
    "section": "",
    "text": "In this tutorial post I will replicate this graph, from OurWorldInData (OWID) using ggplot2. The original graph is available at OWID website. The plot shows the correlation between GDP per capita and self-reported happiness. The income data comes from the World Bank and is in 2017 constant PPP dollars: this ensures the data is comparable across countries since it accounts for both inflation and different costs of living. The happiness data comes from the World Happiness Report.\n\nThis plot has several attractive features including how colors are used to represent continents and how size is used to show the population of each country.\nTo follow this tutorial make sure all of the packages below are installed.\n\n#&gt; Packages needed to replicate the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(showtext)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n\n\n\nThe first step is acquiring the data. Luckily, the csv data is readily available at the OWID website. For convenience I stored a smaller version of the dataset in my Github. The code below imports the data directly into the R session.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nThe data shows both the GDP per capita and the Happiness Indicator for several countries across many years. The code below selects only the most recent that is available for each country.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#adicionando-os-paises",
    "href": "posts/general-posts/2023-09-happiness/index.html#adicionando-os-paises",
    "title": "Satisfação com a vida e felicidade",
    "section": "Adicionando os paises",
    "text": "Adicionando os paises\n\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\nbase_plot &lt;- ggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_text_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#escalas-e-cores",
    "href": "posts/general-posts/2023-09-happiness/index.html#escalas-e-cores",
    "title": "Satisfação com a vida e felicidade",
    "section": "Escalas e cores",
    "text": "Escalas e cores\n\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\nbase_plot &lt;- base_plot +\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  scale_y_continuous(breaks = 3:7) +\n  scale_size_continuous(range = c(1, 15)) +\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  guides(\n    color = \"none\",\n    size = \"none\",\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#elementos-textuais",
    "href": "posts/general-posts/2023-09-happiness/index.html#elementos-textuais",
    "title": "Satisfação com a vida e felicidade",
    "section": "Elementos textuais",
    "text": "Elementos textuais\n\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\nbase_plot &lt;- base_plot +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#fonte",
    "href": "posts/general-posts/2023-09-happiness/index.html#fonte",
    "title": "Satisfação com a vida e felicidade",
    "section": "Fonte",
    "text": "Fonte\nOlhando o c’odigo fonte da p’agina\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nfont_add_google(\"Lato\", \"Lato\")\n\nshowtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#theme",
    "href": "posts/general-posts/2023-09-happiness/index.html#theme",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Theme",
    "text": "Theme\nThis step really does the magic.\nFor the theme, I use theme_minimal as a template, since its fairly similar to the OWID graphic. I start by removing the minor panel grids from the background and changing the major panel grids. Then, I alter the textual elements of the graphic and make minor tweaks to the legend and plot margins.\nAll of the text is in different shades of gray with exception of the title, the axis titles, and the legend text which are all in plain black. Figuring out the sizes of the text is mostly a trial and error process. Almost all of the text is small except the main title and the text on the axes.\n\nbase_plot +\n  theme_minimal() +\n  theme(\n    #&gt; Background grid\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(linetype = 3, color = \"#DDDDDD\"),\n    \n    #&gt; Text elements (change font)\n    text = element_text(family = \"Lato\"),\n    title = element_text(family = \"Lato\"),\n    #&gt; Caption, title, and subtitle\n    plot.caption = element_text(color = \"#777777\", hjust = 0, size = 8),\n    plot.title = element_text(\n      color = \"#444444\",\n      family = \"Playfair Display\",\n      size = 18),\n    plot.subtitle = element_text(color = \"#666666\", size = 11),\n    #&gt; Axis text\n    axis.title = element_text(color = \"#000000\", size = 9),\n    axis.text = element_text(color = \"#666666\", size = 12),\n    #&gt; Legend\n    legend.key.size = unit(5, \"pt\"),\n    legend.position = \"right\",\n    legend.text = element_text(size = 10),\n    #&gt; Margin\n    plot.margin = margin(rep(10, 4))\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#highlighting-the-countries",
    "href": "posts/general-posts/2023-09-happiness/index.html#highlighting-the-countries",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Highlighting the countries",
    "text": "Highlighting the countries\nThis part is mostly manual labor. I create a simple vector with the names of all countries highlighted in the original plot. This vector allows me to create a dummy variable that indicates whether the name of the country should be plotted or not. For increased flexibility I store this as an auxiliar tibble called dftext. In the end, this didn’t make much of a difference but it can be helpful in cases where one needs finer control over the text that is plotted.\nI ggrepel to avoid overlapping the text labels.\n\n#&gt; Countries to highlight\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\n#&gt; Auxiliar tibble with names of countries to highlight\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\n#&gt; Creates a base plot with the bubbles plus the text labels\nbase_plot &lt;- ggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_text_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#scales-and-colors",
    "href": "posts/general-posts/2023-09-happiness/index.html#scales-and-colors",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Scales and colors",
    "text": "Scales and colors\nThe x-axis of the original plot is in a logarithmic scale and its labels highlight specific values (1000, 2000, 5000, …, 100000). The numbers are formatted with a comma and dollar sign. The y-axis is much more straightforward since the numbers are simple integers ranging from 3 to 7.\nThe default size of the bubbles in ggplot is small so I use scale_size_continuous to increase them. I have no idea how to emulate the original size legend (the circle within a circle) so I omit it.\nBoth the interior color of the bubbles and the text follow a particular color scheme. I got the exact colors of the original plot by exporting it to SVG. By default, the legend key inherits its colors. So the legend shows slightly transparent round circles. To get solid colored squares I override this default behavior.\n\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\nbase_plot &lt;- base_plot +\n  #&gt; Adds labels to the log scale\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  #&gt; Adds labels to the y-axis scale\n  scale_y_continuous(breaks = 3:7) +\n  #&gt; Increases the size of the bubbles\n  scale_size_continuous(range = c(1, 15)) +\n  #&gt; Adds colors to the bubbles and text labels\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  #&gt; Removes the size and color legend\n  guides(\n    color = \"none\",\n    size = \"none\",\n    #&gt; Override default behaviour to get solid colors\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#text-elements",
    "href": "posts/general-posts/2023-09-happiness/index.html#text-elements",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Text elements",
    "text": "Text elements\nThe code below inserts the other textual elements of the plot.\n\n#&gt; Caption\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n#&gt; Subtitle\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\n#&gt; Adds textual elements to base plot\nbase_plot &lt;- base_plot +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#font",
    "href": "posts/general-posts/2023-09-happiness/index.html#font",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Font",
    "text": "Font\nLooking at the source code of the page it seems that most of the text is displayed in Lato while the titles are displayed in Playfair Display. I import both fonts using font_add_google and load them using showtext.\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nfont_add_google(\"Lato\", \"Lato\")\n\nshowtext::showtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#bubbles",
    "href": "posts/general-posts/2023-09-happiness/index.html#bubbles",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Bubbles",
    "text": "Bubbles\nThe most essential aspect of this plot is summarized in the code below. The plot shows each country as bubble, where the size of the bubble is proportional to its population. The position of each bubble shows the country’s GDP per capita (on the horizontal axis) and average life satisfaction (on the vertical axis). Finally, the color of each bubble corresponds to the continent of the country. Since many of the observations overlap the original plot uses a bit of transparency.\nNote that I use shape = 21 to get a special circle with two colors. The color argument controls the circle’s border while the fill argument controls the circle’s interior color.\n\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#the-data",
    "href": "posts/general-posts/2023-09-happiness/index.html#the-data",
    "title": "Life Satisfaction and GDP per capita",
    "section": "",
    "text": "The first step is acquiring the data. Luckily, the csv data is readily available at the OWID website. For convenience I stored a smaller version of the dataset in my Github. The code below imports the data directly into the R session.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nThe data shows both the GDP per capita and the Happiness Indicator for several countries across many years. The code below selects only the most recent that is available for each country.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")"
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html",
    "title": "Preços de Imóveis e Demografia",
    "section": "",
    "text": "Já vi em alguns lugares uma suposta ligação entre fatores demográficos e tendências de longo prazo no mercado imobiliário. Intuitivamente, alguns dos principais motivadores para comprar ou vender um imóvel estão ligados a fatores demográficos: nascimentos, casamentos, divórcios ou óbitos.\nEste tipo de análise omite fatores importantes como renda, condições de financiamento e o contexto geral da economia. Ainda assim, fiquei curioso para ver se havia algum padrão entre tendências demográficas mais simples e o comportamento dos preços."
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html#todos-os-países",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html#todos-os-países",
    "title": "Preços de Imóveis e Demografia",
    "section": "Todos os países",
    "text": "Todos os países\nEsta análise é ainda bastante preliminar. Ainda que a demografia seja um motor para a demanda imobiliária, outros fatores como oferta de moradia e condições de crédito são importantes demais parecem serem omitidos.\nO gráfico abaixo compara a população em 2010/2020 com os preços em 2010/2020. Os países ao lado direito do gráfico, são os países onde houve crescimento populacional. Os países na parte de cima do gráfico são os países onde houve crescimento real do preço dos imóveis. Na média da amostra, destacada como WLD, houve crescimento de ambos.\nPaíses que estão muito para cima como Chile (CHL), Índia (IND) e Estônia (EDT) estão com imóveis muito “caros”. Já em países com França e Finlândia tanto a população como o nível de preço dos imóveis cresceram muito pouco. No caso da Grécia, tanto a população como o preço dos imóveis diminuiu nos últimos dez anos.\nO gráfico não me surpreendeu muito, mas esperava que os EUA estivessem mais para cima no gráfico e que o Brasil estivesse ao menos do lado positivo do eixo-x. Isso me sugere que a impressão de que os imóveis no Brasil são ou estão caros tem muito mais a ver com a baixa renda da população.\n\n\nCode\np5 &lt;- \n  ggplot(\n    data = na.omit(tbl_wide),\n    aes(x = index_pop, y = index_house)\n    ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, color = colors[1]) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(aes(label = iso3c, color = highlight)) +\n  scale_x_continuous(breaks = seq(-10, 30, 5)) +\n  scale_y_continuous(breaks = seq(-30, 90, 15)) +\n  scale_color_manual(values = c(\"black\", colors[1])) +\n  guides(color = \"none\") +\n  labs(\n    title = \"Real House Prices x Population (2010/20)\",\n    x = \"Population (2010/2020)\",\n    y = \"RPPI (2010/2020)\",\n    caption = \"Source: Real House Price Indexes (BIS), Population (UN).\") +\n  theme_vini\n\np5"
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html#footnotes",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html#footnotes",
    "title": "Preços de Imóveis e Demografia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHong Kong: https://exame.com/economia/as-raizes-economicas-dos-protestos-de-hong-kong/↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html",
    "href": "posts/general-posts/repost-aquecimento-global/index.html",
    "title": "Aquecimento Global",
    "section": "",
    "text": "Uma recente edição da revista inglesa The Economist exibe uma série de listras coloridas em sua capa. Elas formam um degradê que vai de um azul escuro até um vermelho intenso. Cada listra representa a temperatura de um ano e a linha do tempo vai desde o 1850 até o presente. A mensagem é bastante clara: o planeta esta cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Esta imagem é creditada a Ed Hawkings, editor do Climate Lab Book.\nPara ser preciso, a imagem não plota a temperatura de cada ano, mas sim o quanto cada ano se desvia da temperatura média do período 1971-2000. Isto é, anos acima dessa média têm um valor positivo, valores abaixo dessa média, valores negativos. Esta é uma forma bastante comum de representar este tipo de dado climático. De imediato, quando vi a imagem me ocorreu que seria bastante simples reproduzir uma versão aproximada dela usando o R."
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html#o-código",
    "href": "posts/general-posts/repost-aquecimento-global/index.html#o-código",
    "title": "Aquecimento Global",
    "section": "O código",
    "text": "O código\nO código necessário para gerar a imagem é bastante enxuto. Vou descrever em linhas gerais o que ele faz:\nPrimeiro carrego dois pacotes (linhas 1, 2), depois a série de temperatura (linha 3), faço algumas transformações nos dados (linhas 4, 5) e, por fim, ploto os dados (linhas 6, 7, 8). O resultado inicial já é bastante satisfatório e a partir destas poucas linhas de código pode-se chegar num resultado muito próximo ao da imagem original. Vale notar que a imagem fica um pouco diferente da original porque eu uso uma base de dados diferente.\n\n# Carrega pacotes\nlibrary(ggplot2)\nlibrary(astsa)\n# Carrega a base de dados 'xglobtemp'\ndata(\"xglobtemp\")\n# Converte o objeto para data.frame\ndf &lt;- data.frame(ano = as.numeric(time(xglobtemp)),\n temp = as.numeric(xglobtemp))\n\n# Monta o gráfico\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")"
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html#os-detalhes-do-código",
    "href": "posts/general-posts/repost-aquecimento-global/index.html#os-detalhes-do-código",
    "title": "Aquecimento Global",
    "section": "Os detalhes do código",
    "text": "Os detalhes do código\nVou explicar cada linha de código para ser didático. O R funciona, grosso modo, como um repositório de pacotes: cada pacote contem funções e, às vezes, bases de dados. O primeiro pacote que carrego é o ggplot2. Ele serve para fazer visualizações de dados. O pacote astsa traz várias funções para fazer análise de séries de tempo, mas eu carrego ele somente para usar a base de dados xglobtemp, que traz informação sobre a temperatura anual da terra coletada pela NASA.\nO objeto xglobtemp é uma série de tempo (um objeto da classe ts), que tem alguns atributos especiais. Um deles pode ser acesado pela função time que extrai um vetor numérico com as datas desta série de tempo. No código abaixo mostro os primeiros dez valores do time(xglobtemp).\n\nclass(xglobtemp)\n\n[1] \"ts\"\n\ntime(xglobtemp)[1:10]\n\n [1] 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n\n\nPara extrair somente os valores da série, uso a função as.numeric, que converte o vetor de ts para numeric (numérico). Este tipo de função é bastante comum já que frequentemente é preciso trocar a classe de um objeto. O objetivo destes primeiros passos é de inserir as informações do xglobtemp num data.frame em que a data aparece na primeira coluna e os valores da série são armazenados na segunda coluna. O procedimento pode parecer um tanto trabalhoso (e acho que é mesmo), mas é o jeito. Um data.frame é como uma tabela com dados. Este é um objeto bastante típico em análise de dados e é necessário para usar a função ggplot que vai fazer o gráfico. Abaixo pode-se ver as primeiras linhas desta tabela.\n\nhead(df)\n\n   ano  temp\n1 1880 -0.20\n2 1881 -0.11\n3 1882 -0.10\n4 1883 -0.20\n5 1884 -0.28\n6 1885 -0.31\n\n\nAgora que tenho os dados no formato apropriado posso usar o ggplot. O argumento que pode ser um pouco confuso é o aes. Nele especifica-se quais dados serão mapeados no gráfico. Depois disso adicionamos um geom. Há vários tipos de geom (geom_line, geom_bar, geom_histogram, etc.) e cada um deles produz uma imagem diferente. O geom_tile faz um pequeno quadrado. Para que a função consiga desenhar o quadrado é preciso informar uma variável x e uma variável y. Além disso, também especifico fill = temp. O fill se refere à cor que vai preencher (fill) o quadrado. Como especifico fill = temp a cor do quadrado vai representar a variável temp (temperatura).\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile()\n\n\n\n\n\n\n\n\nO resultado é exatamente como o esperado, mas ainda é preciso mudar a escala de cores. Faço isto com o scale_fill_gradient2. Aqui cada termo tem um signficado: scale_fill pois estamos mudando a escala do fill (outra opção seria scale_color que muda a escala do color). scale_fill_gradient pois queremos um gradiente (degradê) de cores. Por fim, o 2 é adicionado no final pois queremos um escala que diferencie dois grupos distintos: temperaturas acima da média em vermelho, temperaturas abaixo da média em azul. A escala de cores é determinada pelos argumentos low, mid e high.\nOs valores negativos serão coloridos pelo low, os próximos de zero pelo mid e os valores grandes pelo high. Abaixo escrevo as cores em hexa-decimal, mas elas podem ser lidas, essencialmente, como: azul-escuro, cinza-azulado-claro e vermelho-escuro.\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")\n\n\n\n\n\n\n\n\nComo comentei acima, pode-se melhorar o gráfico acima adicionando outros elementos e detalhes. A versão final que fiz do gráfico fica no código abaixo.\n\n# Pacote para carregar fontes externas no R\n# Necessário para utilizar 'Georgia' no gráfico\nlibrary(extrafont)\n# Data.frames auxiliares para plotar as anotações de texto\ndf_aux_title &lt;- data.frame(x = 1930, y = 0, label = \"The Climate Issue\")\ndf_aux_anos &lt;- data.frame(\n  label = c(1880, 1920, 1960, 2000),\n  x = c(1890, 1925, 1960, 1995)\n  )\n\nggplot() +\n  geom_tile(data = df, aes(x = ano, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 6,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = 1950, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 11,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_gradientn(\n    colors = c(\"#213A82\", \"#3B60CE\", \"#8DA2E2\", \"#DE2E02\", \"#9d0208\")\n   ) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n    )"
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html",
    "href": "posts/general-posts/repost-arima-no-r/index.html",
    "title": "Séries de Tempo no R",
    "section": "",
    "text": "Neste post vou explorar um pouco das funções base do R para montar um modelo SARIMA. O R vem “pré-equipado” com um robusto conjunto de funções para lidar com séries de tempo. Inclusive, como se verá, existe uma class específica de objeto para trabalhar com séries de tempo."
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html#modelagem-sarima",
    "href": "posts/general-posts/repost-arima-no-r/index.html#modelagem-sarima",
    "title": "Séries de Tempo no R",
    "section": "Modelagem SARIMA",
    "text": "Modelagem SARIMA\nAqui a ideia é experimentar com alguns modelos simples. Em especial, o modelo que Box & Jenkins sugerem para a série é de um SARIMA (0, 1, 1)(0, 1, 1)[12] da forma\n\\[\n(1 - \\Delta)(1 - \\Delta^{12})y_{t} = \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t_1} + \\Theta_{1}\\varepsilon_{t-12}\n\\]\nA metodologia correta para a análise seria primeiro fazer testes de raiz unitária para avaliar a estacionaridade da série. Mas só de olhar para as funções de autocorrelação e autocorrelação parcial, fica claro que há algum componente sazonal e que a série não é estacionária.\n\n\n\n\n\n\n\n\n\n\nTeste de raiz unitária\nApenas a título de exemplo, faço um teste Dickey-Fuller (ADF), bastante geral, com constante e tendência temporal linear. Para uma boa revisão metodológica de como aplicar testes de raiz unitária, em partiular o teste ADF, consulte o capítulo de séries não-estacionárias do livro Applied Econometric Time Series do Enders\nAqui, a escolha ótima do lag é feita usando o critério BIC (também conhecido como Critério de Schwarz). Não existe uma função que aplica o teste ADF no pacote base o R. A implementação é feita na função ur.df do pacote urca.\nA estatísitica de teste mais relevante é a tau3 e vê-se, surpreendentemente, que se rejeita a hipótese nula de raiz unitária. As estatísticas phi2 e phi3 são testes-F da significânica conjunta dos termos de constante e de tendência temporal. As estatísticas de teste são convencionais e seguem a notação do livro do Enders citado acima e também do clássico livro do Hamilton.\n\nlibrary(urca)\nadf_test &lt;- ur.df(y, type = \"trend\", selectlags = \"BIC\", lags = 13)\nsummary(adf_test)\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090139 -0.022382 -0.002417  0.021008  0.110003 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.8968049  0.3999031   2.243 0.026859 *  \nz.lag.1      -0.1809364  0.0842729  -2.147 0.033909 *  \ntt            0.0016886  0.0008609   1.961 0.052263 .  \nz.diff.lag1  -0.2588927  0.1134142  -2.283 0.024301 *  \nz.diff.lag2  -0.0986455  0.1070332  -0.922 0.358665    \nz.diff.lag3  -0.0379799  0.1045583  -0.363 0.717097    \nz.diff.lag4  -0.1392651  0.0981271  -1.419 0.158560    \nz.diff.lag5  -0.0283998  0.0963368  -0.295 0.768686    \nz.diff.lag6  -0.1326313  0.0889223  -1.492 0.138581    \nz.diff.lag7  -0.1096365  0.0865862  -1.266 0.208019    \nz.diff.lag8  -0.2348880  0.0829892  -2.830 0.005497 ** \nz.diff.lag9  -0.0926604  0.0843594  -1.098 0.274344    \nz.diff.lag10 -0.2053937  0.0789245  -2.602 0.010487 *  \nz.diff.lag11 -0.1081091  0.0786801  -1.374 0.172127    \nz.diff.lag12  0.6633101  0.0752086   8.820 1.54e-14 ***\nz.diff.lag13  0.3197783  0.0883636   3.619 0.000443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04011 on 114 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8621 \nF-statistic: 54.75 on 15 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -2.147 4.9781 3.4342 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n\n\nPela análise do correlograma do resíduo da regressão, fica claro que ainda há autocorrelação. Novamente, o mais correto seria aplicar o teste Ljung-Box sobre os resíduos para verificar a presença de autocorrelação conjunta nas primeiras k defasagens, mas este não é o foco deste post. Esta pequena digressão exemplifica como a aplicação destes testes em séries de tempo pode não ser tão direto/simples.\n\n\n\n\n\n\n\n\n\nOs gráficos abaixo mostram o correlograma da série após tirarmos a primeira diferença e a primeira diferença sazonal. A partir da análise destes correlogramas poderíamos inferir algumas especificações alternativas para modelos SARIMA e aí, poderíamos escolher o melhor modelo usando algum critério de informação.\nA metodologia Box & Jenkins de análise de séries de tempo tem, certamente, um pouco de arte e feeling. Não é tão imediato entender como devemos proceder e, na prática, faz sentido experimentar com vários modelos alternativos de ordem baixa como SARIMA(1, 1, 1)(0, 1, 1), SARIMA(2, 1, 0)(0, 1, 1), etc.\n\n\n\n\n\n\n\n\n\n\n\nOs modelos\nPara não perder muito tempo experimentando com vários modelos vou me ater a três modelos diferentes. Uma função bastante útil é a auto.arima do pacote forecast que faz a seleção automática do melhor modelo da classe SARIMA/ARIMA/ARMA.\nEu sei que o Schumway/Stoffer, autores do ótimo Time Series Analysis and Its Applications, tem um post crítico ao uso do auto.arima. Ainda assim, acho que a função tem seu mérito e costuma ser um bom ponto de partida para a sua análise. Quando temos poucas séries de tempo para analisar, podemos nos dar ao luxo de fazer a modelagem manualmente, mas quando há centenas de séries, é muito conveniente poder contar com o auto.arima.\nComo o auto.arima escolhe o mesmo modelo do Box & Jenkins eu experimento com uma especificação diferente. Novamente, a título de exemplo eu comparo ambos os modelos SARIMA com uma regressão linear simples que considera uma tendência temporal linear e uma série de dummies sazonais. O modelo é algo da forma\n\\[\ny_{t} = \\alpha_{0} + \\alpha_{1}t + \\sum_{i = 1}^{11}\\beta_{i}s_{i} + \\varepsilon_{t}\n\\]\nOnde \\(s_{i}\\) é uma variável indicadora igual a 1 se \\(t\\) corresponder ao mês \\(i\\) e igual a 0 caso contrário. Vale notar que não podemos ter uma dummy para todos os meses se não teríamos uma matriz de regressores com colinearidade perfeita!\nAqui vou contradizer um pouco o espírito do post novamente para usar o forecast. O ganho de conveniência vem na hora de fazer as previsões. Ainda assim, indico como estimar os mesmos modelos usando apenas funções base do R.\n\n# Usando o forecast\nlibrary(forecast)\nmodel1 &lt;- auto.arima(train)\nmodel2 &lt;- Arima(train, order = c(1, 1, 1), seasonal = c(1, 1, 0))\nmodel3 &lt;- tslm(train ~ trend + season)\n\n\n# Usando apenas funções base\nmodel2 &lt;- arima(\n  trains,\n  order = c(1, 1, 1),\n  seasonal = list(order = c(1, 1, 1), period = 12)\n)\n\n# Extrai uma tendência temporal linear \ntrend &lt;- time(train)\n# Cria variáveis dummies mensais\nseason &lt;- cycle(train)\nmodel3 &lt;- lm(train ~ trend + season)\n\nA saída dos modelos segue abaixo. As saídas dos modelos SARIMA não são muito interessantes. Em geral, não é muito comum avaliar nem a significância e nem o sinal dos coeficientes, já que eles não têm muito valor interpretativo. Uma coisa que fica evidente dos dois modelos abaixo é que o primeiro parece melhor ajustado aos dados pois tem valores menores em todos os critérios de informação considerados.\n\nmodel1\n\nSeries: train \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.3424  -0.5405\ns.e.   0.1009   0.0877\n\nsigma^2 = 0.001432:  log likelihood = 197.51\nAIC=-389.02   AICc=-388.78   BIC=-381\n\n\n\nmodel2\n\nSeries: train \nARIMA(1,1,1)(1,1,0)[12] \n\nCoefficients:\n         ar1      ma1     sar1\n      0.0668  -0.4518  -0.4426\ns.e.  0.3046   0.2825   0.0875\n\nsigma^2 = 0.001532:  log likelihood = 195.14\nAIC=-382.28   AICc=-381.89   BIC=-371.59\n\n\nJá o modelo de regressão linear tem uma saída mais interessante. Note que, por padrão, o primeiro mês foi omitido e seu efeito aparece no termo constante. Na tabela abaixo, vemos que há um efeito positivo e significativo, por exemplo, nos meses 6-8 (junho a agosto), que coincidem com o período de férias de verão no hemisfério norte. Já, em novembro (mês 11) parece haver uma queda na demanda por passagens aéreas.\nNote que o R quadrado da regressão é extremamente elevado e isso é um indício de que algo está errado. Isto, muito provavelmente é resultado da não-estacionaridade da série.\n\nbroom::tidy(model3)\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic   p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  4.71     0.0191    247.     2.81e-149\n 2 trend        0.0106   0.000145   73.2    3.09e- 93\n 3 season2     -0.0134   0.0245     -0.549  5.84e-  1\n 4 season3      0.120    0.0245      4.91   3.32e-  6\n 5 season4      0.0771   0.0245      3.15   2.14e-  3\n 6 season5      0.0675   0.0245      2.75   6.93e-  3\n 7 season6      0.191    0.0245      7.81   4.23e- 12\n 8 season7      0.288    0.0245     11.7    6.32e- 21\n 9 season8      0.278    0.0245     11.4    4.36e- 20\n10 season9      0.143    0.0245      5.82   6.13e-  8\n11 season10     0.00108  0.0245      0.0441 9.65e-  1\n12 season11    -0.141    0.0245     -5.77   7.97e-  8\n13 season12    -0.0248   0.0245     -1.01   3.14e-  1\n\n\nDe fato, olhando para a função de autocorrelação do resíduo do modelo de regressão linear, fica evidente que há autocorrelação. Uma forma de contornar isso seria de incluir um termo ARMA no termo de erro. Novamente, este não é o foco do post e vamos seguir normalmente.\n\nacf(resid(model3))"
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html#comparando-as-previsões",
    "href": "posts/general-posts/repost-arima-no-r/index.html#comparando-as-previsões",
    "title": "Séries de Tempo no R",
    "section": "Comparando as previsões",
    "text": "Comparando as previsões\nA maneira mais prática de trabalhar com vários modelos ao mesmo tempo é agregando eles em listas e aplicando funções nessas listas.\nAbaixo eu aplico a função forecast para gerar as previsões 24 períodos a frente nos três modelos. Depois, eu extraio somente a estimativa pontual de cada previsão.\n\nmodels &lt;- list(model1, model2, model3)\nyhat &lt;- lapply(models, forecast, h = 24)\nyhat_mean &lt;- lapply(yhat, function(x) x$mean)\n\nComparamos a performance de modelos de duas formas: (1) olhando para medidas de erro (o quão bem o modelo prevê os dados do test) e (2) olhando para critérios de informação (o quão bem o modelo se ajusta aos dados do train).\nOs critérios de informação têm todos uma interpretação bastante simples: quanto menor, melhor. Tipicamente, o AIC tende a escolher modelos sobreparametrizados enquanto o BIC tende a escolher modelos mais parcimoniosos.\nJá a comparação de medidas de erro não é tão simples. Pois ainda que um modelo tenha, por exemplo, um erro médio quadrático menor do que outro, não é claro se esta diferença é significante. Uma maneira de testar isso é via o teste Diebold-Mariano, que compara os erros de previsão de dois modelos. Implicitamente, contudo, ele assume que a diferença entre os erros de previsão é covariância-estacionária (também conhecido como estacionário de segunda ordem ou fracamente estacionário). Dependendo do contexto, esta pode ser uma hipótese mais ou menos razoável.\n\ncompute_error &lt;- function(model, test) {\n  y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    y - yhat\n}\n\ncompute_error_metrics &lt;- function(model, test) {\n\n    y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    error &lt;- y - yhat\n    \n    # Raiz do erro quadrado medio\n    rmse &lt;- sqrt(mean(error^2))\n    # Erro medio absoluto\n    mae &lt;- mean(abs(error))\n    # Erro medio percentual\n    mape &lt;- mean(abs(100 * error / y))\n    # Root mean squared scaled error\n    rmsse &lt;- sqrt(mean(error^2 / snaive(train)$mean))\n\n    # Devolve os resultados num list\n    list(rmse = rmse, mae = mae, mape = mape, rmsse = rmsse)\n    \n\n}\n\ncompute_ics &lt;- function(model) {\n\n    # Extrai criterios de informacao\n    aic  &lt;- AIC(model)\n    bic  &lt;- BIC(model)\n\n    # Devolve os resultados num list\n    list(aic = aic, bic = bic)\n\n} \n\nfcomparison &lt;- lapply(yhat, function(yhat) compute_error_metrics(yhat, test))\nicc &lt;- lapply(models, compute_ics)\n\ncomp_error &lt;- do.call(rbind.data.frame, fcomparison)\nrownames(comp_error) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\ncomp_ics &lt;- do.call(rbind.data.frame, icc)\nrownames(comp_ics) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\n\nA tabela abaixo mostra os critérios AIC e BIC para os três modelos. Em ambos os casos, o SARIMA(0, 1, 1)(0, 1, 1)[12] parece ser o escolhido. Este tipo de feliz coincidência não costuma acontecer frequentemente na prática, mas neste caso ambos os critérios apontam para o mesmo modelo.\n\ncomp_ics\n\n                    aic       bic\nAutoArima     -389.0155 -380.9970\nManual SARIMA -382.2807 -371.5894\nOLS           -342.2930 -303.2681\n\n\nNa comparação de medidas de erro, o SARIMA(0, 1, 1)(0, 1, 1)[12] realmente tem uma melhor performance, seguido pelo OLS e pelo SARIMA(1, 1, 1)(1, 1, 0)[12].\n\ncomp_error\n\n                    rmse        mae     mape      rmsse\nAutoArima     0.09593236 0.08959921 1.463477 0.03939512\nManual SARIMA 0.11688549 0.10780460 1.762201 0.04806577\nOLS           0.10333715 0.09384411 1.549261 0.04266214\n\n\nSerá que esta diferença é significante? Vamos comparar os modelos SARIMA. Pelo teste DM ela é sim. Lembre-se que o teste DM é, essencialmente, um teste Z de que \\(e_{1} - e_{2} = 0\\) ou \\(e_{1} = e_{2}\\), onde os valores são a média dos erros de previsão dos modelos.\n\nerrors &lt;- lapply(yhat, function(yhat) compute_error(yhat, test))\ne1 &lt;- errors[[1]]\ne2 &lt;- errors[[2]]\n\ndm.test(e1, e2, power = 2)\n\n\n    Diebold-Mariano Test\n\ndata:  e1e2\nDM = -4.4826, Forecast horizon = 1, Loss function power = 2, p-value =\n0.0001691\nalternative hypothesis: two.sided\n\n\nVale notar que o teste DM serve para comparar os erros de previsão de quaisquer modelos. Como o teste não faz qualquer hipótese sobre “de onde vem” os erros de previsão, ele pode ser utilizado livremente. Vale lembrar também que este teste não deve ser utilizado para escolher o melhor modelo, já que ele compara apenas a capacidade preditiva de dois modelos alternativos.\nOutro ponto, também complicado, é de qual a medida de erro que se deve escolher. O teste DM implicitamente usa o erro médio quadrático, mas há várias outras alternativas. Uma breve discussão pode ser vista aqui.\nPor fim, o gráfico abaixo mostra as previsões dos modelos alternativos contra a série real.\n\nplot(test, ylim = c(5.8, 6.5), col = \"#8ecae6\", lwd = 2, type = \"o\")\nlines(yhat_mean[[1]], col = \"#ffb703\")\nlines(yhat_mean[[2]], col = \"#fb8500\")\nlines(yhat_mean[[3]], col = \"#B86200\")\ngrid()\nlegend(\"topleft\", lty = 1,\n       legend = c(\"Test\", \"AutoArima\", \"Arima\", \"OLS\"),\n       col = c(\"#8ecae6\", \"#ffb703\", \"#fb8500\", \"#B86200\"))"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "A inflação voltou a ser uma pauta, não só no Brasil, mas no mundo todo, nos últimos meses. No Brasil, a combinação de câmbio desvalorizado, desajustes logísticos, crise hídrica e choques de preços externos, culminaram no maior nível de inflação desde 2002.\nMesmo em países avançados, os níveis de inflação estão em altas históricas. Nos Estados Unidos, por exemplo, o nível do CPI está no valor mais alto desde o final dos anos 1970.\nVisualizar a magnitude da inflação no Brasil pode ser um pouco desafiador. A série do IPCA é calculada desde 1979. O número de cidades avaliadas pelo índice cresceu no tempo: nos primeiros anos o índice contemplava Rio de Janeiro, Porto Alegre, Belo Horizonte, Recife, São Paulo, Brasília, Belém, Fortaleza, Salvador e Curitiba. Em 1991, Goiânia entra no índice e, mais recentemente, em 2014, Vitória e Campo Grande também entraram no cômputo do índice.\nMais importante do que a variação no número das cidades, é o período hiperinflacionário da década de 1980. Os números da inflação são incomparavelmente mais altos do que os atuais. Como regra, os cortes temporais mais relevantes para enxergar a inflação são Jul/94 (Plano Real), Jul/99 (Regime de Metas de Inflação), Mai/00 (Lei de Responsabilidade Fiscal) e Mai/03 (pós choque de 2002).\nNeste post vou mostrar o comportamento da inflação desde 1999.\n\n\n\n# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)\n\n\n\n\nImporto os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  inner_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)\n\n\n\n\nO cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nUma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso.\n\n\n\n\nOutra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com uma linha de densidade sobreposta.\nNote como o os variações mensais do IPCA estão concentradas entre 0,25 e 0,5 e como os “outliers” à direita são mais comuns do que os “outliers” à esquerda. Isto é, eventos de alta inflação são mais frequentes do que de desinflação.\nConhecer a distribução dos dados costuma ser uma informação relevante para a previsão da variável.\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#pacotes",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#pacotes",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#importando-os-dados",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#importando-os-dados",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Importo os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  inner_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#inflação",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#inflação",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "O cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#no-longo-prazo",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#no-longo-prazo",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Uma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso."
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#enxergando-a-distribuição",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#enxergando-a-distribuição",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Outra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com uma linha de densidade sobreposta.\nNote como o os variações mensais do IPCA estão concentradas entre 0,25 e 0,5 e como os “outliers” à direita são mais comuns do que os “outliers” à esquerda. Isto é, eventos de alta inflação são mais frequentes do que de desinflação.\nConhecer a distribução dos dados costuma ser uma informação relevante para a previsão da variável.\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html",
    "title": "OLS com matrizes",
    "section": "",
    "text": "Uma forma instrutiva de entender o modelo de regressão linear é expressando ele em forma matricial. Os cursos introdutórios de econometria costumam omitir esta abordagem e expressam todas as derivações usando somatórios, deixando a abordagem matricial para cursos mais avançados. É bastante simples computar uma regressão usando apenas matrizes no R.\nDe fato, um dos objetos fundamentais do R é a e muitas das operações matriciais (decomposições, inversa, transposta, etc.) já estão implementadas em funções base. Uma var \\(k\\) .\nNeste post vou mostrar como fazer uma regressão linear usando somente matrizes no R. Além disso, vou computar algumas estatísticas típicas (t, F)\nO modelo linear é da forma\n\\[\ny_{t} = x^\\intercal_{t} \\beta + e_{t}\n\\]\nonde \\(x^\\intercal\\) é o vetor transposto de \\(x\\) . É importante sempre ter em mente a dimensão destes vetores. O vetor \\(y_{t}\\) é \\(n\\times1\\) onde \\(n\\) representa o número de observações na amostra. O vetor \\(\\beta\\) é \\(k\\times1\\) onde \\(k\\) é o número de regressores (ou variáveis explicativas). Como há \\(n\\) observações para cada uma dos \\(k\\) regressores, \\(x\\) é \\(k\\times1\\) ; o detalhe é que \\(x = (1 \\, \\,x_{1} \\, \\dots \\,x_{k-1})\\) , onde cada \\(x_{i}\\) é \\(n\\times 1\\) e \\(1\\) é um vetor de uns \\(n\\times1\\) . Finalmente, \\(e_{t}\\) é \\(n\\times1\\) . Temos então que:\n\\[\n\\begin{pmatrix}\ny_{1} \\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}=\n\\begin{pmatrix}\n1\\\\\nx_{1}\\\\\n\\vdots\\\\\nx_{k-1}\n\\end{pmatrix}^\\intercal\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k-1}\n\\end{pmatrix}+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n\\]\nonde:\n\\[\n\\begin{bmatrix}\nx_{1} & = (x_{11} & x_{12} & x_{13} & \\dots & x_{1n})\\\\\nx_{2} & = (x_{21} & x_{22} & x_{23} & \\dots & x_{2n})\\\\\nx_{3} & = (x_{31} & x_{32} & x_{33} & \\dots & x_{3n})\\\\\n\\vdots\\\\\nx_{k-1} & = (x_{(k-1)1} & x_{(k-1)2} & x_{(k-1)3} & \\dots & x_{(k-1)n})\n\\end{bmatrix}\n\\]\nQueremos encontrar o vetor \\(\\hat{\\beta}\\) que minimiza o a soma do quadrado dos erros, isto é, que minimiza\n\\[\nS(\\beta) = \\sum_{t = 1}^{T}(y_{t} - x^\\intercal_{t}\\beta)^{2}\n\\]\nEncontramos o ponto crítico derivando a expressão acima e igualando-a a zero. O resultado é o conhecido estimador de mínimos quadrados:\n\\[\n\\hat{\\beta} = \\left ( \\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} \\right )^{-1} \\sum_{t = 1}^{T}x_{t}y_{t}\n\\]\nPara reescrever as equações acima usando matrizes usamos os seguintes fatos:\n\\[\n\\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} = X^\\intercal X\n\\]\nonde \\(X\\) é uma matriz \\(n \\times k\\)\n\\[\n\\sum_{t = 1}^{T}x_{t}y_{t} = X^\\intercal y\n\\]\nonde \\(X^\\intercal y\\) é \\(k \\times 1\\) . Lembre-se que uma hipótese do modelo linear é de que \\(X\\) é uma matriz de posto completo, logo \\(X^\\intercal X\\) possui inversa e podemos escrever:\n\\[\n\\hat{\\beta} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "title": "OLS com matrizes",
    "section": "Exemplo: salário (Wooldridge)",
    "text": "Exemplo: salário (Wooldridge)\nComo exemplo vou usar um exemplo clássico de livro texto de econometria: uma regressão de salário (rendimento) contra algumas variáveis explicativas convencionais: anos de educação, sexo, anos de experiência, etc. As bases de dados do livro Introductory Econometrics estão disponíveis no pacote wooldridge. O código abaixo carrega a base de dados .\n\nlibrary(wooldridge)\n# Carrega a base\ndata(wage2)\n# Remove os valores ausentes (NAs)\nsal &lt;- na.omit(wage2)\n\n\n\n\n\n\n\n\nwage\nhours\nIQ\nKWW\neduc\nexper\ntenure\nage\nmarried\nblack\nsouth\nurban\nsibs\nbrthord\nmeduc\nfeduc\nlwage\n\n\n\n\n1\n769\n40\n93\n35\n12\n11\n2\n31\n1\n0\n0\n1\n1\n2\n8\n8\n6.645091\n\n\n3\n825\n40\n108\n46\n14\n11\n9\n33\n1\n0\n0\n1\n1\n2\n14\n14\n6.715383\n\n\n4\n650\n40\n96\n32\n12\n13\n7\n32\n1\n0\n0\n1\n4\n3\n12\n12\n6.476973\n\n\n5\n562\n40\n74\n27\n11\n14\n5\n34\n1\n0\n0\n1\n10\n6\n6\n11\n6.331502\n\n\n7\n600\n40\n91\n24\n10\n13\n0\n30\n0\n0\n0\n1\n1\n2\n8\n8\n6.396930\n\n\n9\n1154\n45\n111\n37\n15\n13\n1\n36\n1\n0\n0\n0\n2\n3\n14\n5\n7.050990\n\n\n10\n1000\n40\n95\n44\n12\n16\n16\n36\n1\n0\n0\n1\n1\n1\n12\n11\n6.907755\n\n\n11\n930\n43\n132\n44\n18\n8\n13\n38\n1\n0\n0\n0\n1\n1\n13\n14\n6.835185\n\n\n14\n1318\n38\n119\n24\n16\n7\n2\n28\n1\n0\n0\n1\n3\n1\n10\n10\n7.183871\n\n\n15\n1792\n40\n118\n47\n16\n9\n9\n34\n1\n0\n0\n1\n1\n1\n12\n12\n7.491087\n\n\n\n\n\n\n\nA base traz 663 observações de 17 variáveis. A função str é útil para entender a estrutura dos dados.\n\n# Dimensão da base (# linhas  # colunas)\ndim(sal)\n\n[1] 663  17\n\n# Descrição da base\nstr(sal)\n\n'data.frame':   663 obs. of  17 variables:\n $ wage   : int  769 825 650 562 600 1154 1000 930 1318 1792 ...\n $ hours  : int  40 40 40 40 40 45 40 43 38 40 ...\n $ IQ     : int  93 108 96 74 91 111 95 132 119 118 ...\n $ KWW    : int  35 46 32 27 24 37 44 44 24 47 ...\n $ educ   : int  12 14 12 11 10 15 12 18 16 16 ...\n $ exper  : int  11 11 13 14 13 13 16 8 7 9 ...\n $ tenure : int  2 9 7 5 0 1 16 13 2 9 ...\n $ age    : int  31 33 32 34 30 36 36 38 28 34 ...\n $ married: int  1 1 1 1 0 1 1 1 1 1 ...\n $ black  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ south  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urban  : int  1 1 1 1 1 0 1 0 1 1 ...\n $ sibs   : int  1 1 4 10 1 2 1 1 3 1 ...\n $ brthord: int  2 2 3 6 2 3 1 1 1 1 ...\n $ meduc  : int  8 14 12 6 8 14 12 13 10 12 ...\n $ feduc  : int  8 14 12 11 8 5 11 14 10 12 ...\n $ lwage  : num  6.65 6.72 6.48 6.33 6.4 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n - attr(*, \"na.action\")= 'omit' Named int [1:272] 2 6 8 12 13 19 20 21 31 36 ...\n  ..- attr(*, \"names\")= chr [1:272] \"2\" \"6\" \"8\" \"12\" ...\n\n\nO modelo proposto é o abaixo:\n\\[\n\\text{lwage}_{t} = \\beta_{0} + \\beta_{1}\\text{educ}_{t} + \\beta_{2}\\text{exper}_{t} + \\beta_{3}\\text{exper}^{2}_{t} + \\beta_{4}\\text{tenure}_{t} + \\beta{5}\\text{married}_{t} + u_{t}\n\\]\nonde:\n\nlwage = logaritmo natural do salário\neduc = anos de educação\nexper = anos de experiência (trabalhando)\ntenure = anos trabalhando com o empregador atual\nmarried = dummy (1 = casado, 0 = não-casado)\n\nHá 6 coeficientes para estimar logo \\(k = 6\\) . Além disso, como há \\(663\\) observações temos que \\(n = 663\\) . A matriz de “dados” é da forma:\n\\[\nX = \\begin{bmatrix}\n1 & 12 & 11 & 121 & 2 & 1\\\\\\\\\n1 & 14 & 11 & 121 & 9 & 1\\\\\\\\\n1 & 12 & 13 & 169 & 7 & 1\\\\\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\n1 & 13 & 10 & 100 & 3 & 1\n\\end{bmatrix}_{663\\times 6}\n\\]\nO código abaixo monta a matriz \\(X\\) acima. As funções head e tail podem ser usadas para verificar as primeiras e últimas linhas da matriz. Vale esclarecer dois pontos sobre o código a ser usado neste post. Primeiro, quando se cria um objeto usando o operador &lt;- pode-se forçar o R a imprimir o seu valor colocando a expressão entre parêntesis. Por exemplo, teste (x &lt;- 3). O segundo ponto é que o operador de multiplicação matricial é %*%.\n\n# Define alguns valores úteis: \n## N = número de observações\n## k = número de regressores\n## const = vetor com 1^\\intercals (uns)\nN &lt;- 663; k &lt;- 6; const &lt;- rep(1, 663)\n# Monta a matriz de observações da regressão\nX &lt;- cbind(const, sal$educ, sal$exper, sal$exper^2, sal$tenure, sal$married)\nX &lt;- as.matrix(X)\n# Define o nome das colunas da matriz de observações\ncolnames(X) &lt;- c(\"const\", \"educ\", \"exper\", \"exper2\", \"tenure\", \"married\")\n# Função para verificar as primeiras linhas da matriz X\nhead(X)\n\n     const educ exper exper2 tenure married\n[1,]     1   12    11    121      2       1\n[2,]     1   14    11    121      9       1\n[3,]     1   12    13    169      7       1\n[4,]     1   11    14    196      5       1\n[5,]     1   10    13    169      0       0\n[6,]     1   15    13    169      1       1\n\n# Função para verificar as últimas linhas da matriz X\ntail(X)\n\n       const educ exper exper2 tenure married\n[658,]     1   12     9     81      2       1\n[659,]     1   16     8     64     10       1\n[660,]     1   12    11    121      3       1\n[661,]     1   12     9     81      3       1\n[662,]     1   16    10    100      9       1\n[663,]     1   13    10    100      3       1\n\n\nLembrando que o problema de mínimos quadrados é de encontrar os valores de \\(\\beta\\) que minimizam a soma dos erros ao quadrado.\n\\[\n\\underset{\\beta}{\\text{Min }} e^\\intercal e\n\\]\nAbrindo mais a expressão acima:\n\\[\n\\begin{align}\n  e^\\intercal e & = (y - X\\beta )^\\intercal(y - X\\beta ) \\\\\\\\\n      & = y^\\intercal y - y^\\intercal X\\beta - \\beta ^\\intercal X^\\intercal y + \\beta ^\\intercal X^\\intercal X \\beta \\\\\\\\\n      & = y^\\intercal y - 2 y^\\intercal X \\beta + \\beta^\\intercal X^\\intercal X \\beta\n\\end{align}\n\\]\nDerivando em relação a \\(\\beta\\) e igualando a zero chega-se no estimador de MQO\n\\[\n\\beta_{\\text{MQO}} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]\nO código abaixo computa \\(\\beta_{\\text{MQO}}\\) . Note que os parêntesis por fora da expressão forçam o R a imprimir o valor do objeto. Além disso, como estamos multiplicando matrizes/vetores usamos %*%.\n\n# Define o vetor y (log do salário)\ny &lt;- sal$lwage\n# Computa a estimativa para os betas\n(beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y)\n\n                [,1]\nconst   5.3563606713\neduc    0.0767258959\nexper   0.0104985672\nexper2  0.0002881339\ntenure  0.0091039254\nmarried 0.2002468574\n\n\nOs valores estimados dos betas são reportados na tabela abaixo.\n\ntabela &lt;- as.data.frame(round(beta, 4))\ncolnames(tabela) &lt;- c(\"Coeficiente estimado\")\nround(beta, 4) %&gt;%\n  kable(align = \"c\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\nconst\n5.3564\n\n\neduc\n0.0767\n\n\nexper\n0.0105\n\n\nexper2\n0.0003\n\n\ntenure\n0.0091\n\n\nmarried\n0.2002"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html#resíduo-e-variância",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html#resíduo-e-variância",
    "title": "OLS com matrizes",
    "section": "Resíduo e variância",
    "text": "Resíduo e variância\nO resíduo do modelo é simplesmente a diferença entre o observado \\(y_{t}\\) e o estimado \\(\\hat{y_{t}}\\) . Isto é,\n\\[\n\\hat{e}_{t} = y_{t} - \\hat{y}_{t} = y_{t} - x_{t}^\\intercal\\hat{\\beta}\n\\]\nou, de forma equivalente,\n\\[\n\\hat{e} = y - X\\hat{\\beta}\n\\]\n\n# Computa o resíduo da regressão\nu_hat &lt;- y - X %*% beta\n\nUsando o histograma pode-se visualizar a distribuição dos resíduos.\n\nhist(u_hat, breaks = 30, freq = FALSE, main = \"Histograma dos resíduos\")\n\n\n\n\nO estimador da variância é dado por:\n\\[\n\\hat{\\sigma}^{2} = \\frac{1}{N-k}\\sum_{t = 1}^{N}\\hat{e}_{t}^{2}\n\\]\nSubstituindo os valores calculados acima chegamos em:\n\\[\n\\hat{\\sigma}^{2} = \\frac{\\hat{e}^\\intercal\\hat{e}}{N-k} = 0.1403927\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html",
    "href": "posts/general-posts/repost-definindo-objetos/index.html",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Há dois operadores para definir um objeto no R: = e &lt;-. A maior parte dos usuários parece preferir o último apesar dele parecer um tanto inconveniente. Em teclados antigos, havia uma tecla específica com o símbolo &lt;-, mas em teclados ABNT modernos ele exige três teclas para ser escrito.\nPara contornar este incômodo é comum criar um atalho no teclado para esse símbolo; o RStudio, por exemplo, tem um atalho usando a teclas Alt e - em conjunto. Mas ainda assim fica a questão: por que não utilizar o =? A resposta curta é que o símbolo &lt;- é a melhor e mais consistente forma de definir objetos R. Na prática, contudo, há poucas diferenças entre as expressões e elas dificilmente vão fazer alguma diferença. Podemos começar com um exemplo bastante simples para entender estas diferenças.\n\n\nO código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1\n\n\n\n\n\nEu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%.\n\n\n\nNo geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html#qual-a-diferença",
    "href": "posts/general-posts/repost-definindo-objetos/index.html#qual-a-diferença",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "O código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1"
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html#mais-um-adendo",
    "href": "posts/general-posts/repost-definindo-objetos/index.html#mais-um-adendo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Eu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%."
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html#resumo",
    "href": "posts/general-posts/repost-definindo-objetos/index.html#resumo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "No geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html",
    "href": "posts/general-posts/repost-emv-no-r/index.html",
    "title": "EMV no R",
    "section": "",
    "text": "A estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é consistente (converge para o valor verdadeiro), normalmente assintótico (distribuição assintórica segue uma normal padrão) e eficiente (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.\nA intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros \\(\\mu\\) e \\(\\sigma\\) de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido “gerada” por uma normal).\nHá vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no R. Neste post vou me ater apenas a dois pacotes: o optimx e o maxLik. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.\nVale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função optim do R. Os dois pacotes simplesmente trazem algumas comodidades.\n\nlibrary(maxLik)\nlibrary(optimx)\n# Para reproduzir os resultados\nset.seed(33)"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-optimx",
    "href": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-optimx",
    "title": "EMV no R",
    "section": "Usando o pacote optimx",
    "text": "Usando o pacote optimx\nA função optim já é bastante antiga e um novo pacote, chamado optimx, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são optimx e optimr. Mais informações sobre o pacote podem ser encontradas aqui.\nA sintaxe das funções é muito similar à sintaxe original do optim. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead\n\nsummary(fit &lt;- optimx(par = 1, fn = ll_pois, x = amostra))\n\n                p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nNelder-Mead 4.9375 2202.837     32     NA    NA        0 TRUE TRUE 0.001\nBFGS        4.9380 2202.837     34      9    NA        0 TRUE TRUE 0.002\n\n\nUma das principais vantagens do optimx é a possibilidade de usar vários métodos de otimização numérica numa mesma função.\n\nfit &lt;- optimx(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n  method = c(\"nlm\", \"BFGS\", \"Rcgmin\", \"nlminb\")\n  )\n\nfit\n\n             p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nnlm    4.937998 2202.837     NA     NA     8        0 TRUE TRUE 0.001\nBFGS   4.938000 2202.837     34      9    NA        0 TRUE TRUE 0.002\nRcgmin 4.937999 2202.837    708    112    NA        1 TRUE TRUE 0.039\nnlminb 4.938000 2202.837     10     12     9        0 TRUE TRUE 0.001\n\n\nComo este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos."
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-maxlik",
    "href": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-maxlik",
    "title": "EMV no R",
    "section": "Usando o pacote maxLik",
    "text": "Usando o pacote maxLik\nA função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, não é preciso montar a função com sinal de menos como fizemos acima; segundo, ela já calcula erros-padrão e estatísticas-t dos coeficientes estimados. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas aqui.\nPara usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o dpois que implementa a função densidade da Poisson. O argumento log = TRUE retorna as probabilidades \\(p\\) como \\(log(p)\\).\n\nll_pois &lt;- function(x, theta) {\n    ll &lt;- dpois(x, theta, log = TRUE)\n    return(sum(ll))\n}\n\nO comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.\n\nsummary(fit &lt;- maxLik(ll_pois, start = 1, x = amostra))\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 8 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07617   64.83  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função optimx. Derivando a função de log-verossimilhança:\n\\[\n\\begin{align}\n  \\frac{\\partial \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda} & = \\frac{1}{\\lambda}\\sum_{k = 1}^{n}x_{k} - n \\\\\n  \\frac{\\partial^2 \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda^2} & = -\\frac{1}{\\lambda^2}\\sum_{k = 1}^{n}x_{k}\n\\end{align}\n\\]\nO código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.\n\ngrad_pois &lt;- function(x, theta) {\n  (1 / theta) * sum(x) - length(x)\n  }\n\nhess_pois &lt;- function(x, theta) {\n    -(1 / theta^2) * sum(x)\n}\n\nfit2 &lt;- maxLik(\n  ll_pois,\n  grad = grad_pois,\n  hess = hess_pois,\n  start = 1,\n  x = amostra\n  )\n\nsummary(fit2)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 7 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07027   70.27  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#consistência",
    "href": "posts/general-posts/repost-emv-no-r/index.html#consistência",
    "title": "EMV no R",
    "section": "Consistência",
    "text": "Consistência\nVamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição \\(N(2, 3)\\); computamos as estimativas para \\(\\mu\\) e \\(\\sigma\\) e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.\n\nSimular uma amostra segundo uma distribuição.\nEstimata os parâmetros da distribuição.\nCalcula a variância assintótica dos estimadores.\nRepete 5000 vezes os passos 1-3.\n\nO código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.\n\nr &lt;- 5000\nn &lt;- 1000\n\nestimativas &lt;- matrix(ncol = 4, nrow = r)\n\nfor(i in 1:r) {\n    x &lt;- rnorm(n = n, mean = 2, sd = 3)\n    \n    fit &lt;- optimr(\n      par = c(1, 1),\n      fn = ll_norm,\n      method = \"BFGS\",\n      hessian = TRUE\n      )\n    # Guarda o valor estimado do parâmetro\n    estimativas[i, 1:2] &lt;- fit$par\n    estimativas[i, 3:4] &lt;- diag(n * solve(fit$hess))\n}\n\nA consistência dos estimadores \\(\\hat{\\theta}_{MV}\\) significa que eles aproximam os valores verdadeiros do parâmetros \\(\\theta_{0}\\) à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande \\(\\mathbb{N} \\to \\infty\\) então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros \\(\\hat{\\theta}_{\\text{MV}} \\to \\theta_{0}\\)\nO código abaixo calcula a média das estimativas para cada parâmetro - lembrando que \\(\\mu_{0} = 2\\) e que \\(\\sigma_{0} = 3\\). Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).\n\n# | fig-width: 10\npar(mfrow = c(1, 2))\n# Consistência dos estimadores de MV\nmu &lt;- estimativas[, 1]; sigma &lt;- estimativas[, 2]\nmean(mu)\n\n[1] 2.000883\n\nmean(sigma)\n\n[1] 2.997335\n\nhist(mu, main = bquote(\"Estimativas para \"~~mu), freq = FALSE, xlim = c(1.5, 2.5))\nabline(v = 2, col = \"indianred\")\nhist(sigma, main = bquote(\"Estimativas para \"~~sigma), freq = FALSE, xlim = c(2.7, 3.3))\nabline(v = 3, col = \"indianred\")"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#normalmente-assintótico",
    "href": "posts/general-posts/repost-emv-no-r/index.html#normalmente-assintótico",
    "title": "EMV no R",
    "section": "Normalmente assintótico",
    "text": "Normalmente assintótico\nDizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:\n\\[\nz_{\\theta} = \\sqrt{N}\\frac{\\hat{\\theta}_{MV} - \\theta}{\\sqrt{\\text{V}_{ast}}} \\to \\mathbb{N}(0, 1)\n\\]\nonde \\(\\text{V}_{ast}\\) é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\nNo loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\n# Normalidade assintótica\n\n# Define objetos para facilitar a compreensão\nmu_hat &lt;- estimativas[, 1]\nsigma_hat &lt;- estimativas[, 2]\nvar_mu_hat &lt;- estimativas[, 3]\nvar_sg_hat &lt;- estimativas[, 4]\n\n# Centra a estimativa\nmu_centrado &lt;- mu_hat - 2 \nsigma_centrado &lt;- sigma_hat - 3\n# Computa z_mu z_sigma\nmu_normalizado &lt;- sqrt(n) * mu_centrado / sqrt(var_mu_hat)\nsigma_normalizado &lt;- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)\n\n\n# Monta o gráfico para mu\n\n# Eixo x\ngrid_x &lt;- seq(-3, 3, 0.01)\n\nhist(\n  mu_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[mu]),\n  freq = FALSE,\n  xlim = c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[mu]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n\n\n\n\n\n\n\n\n\n# Monta o gráfico para sigma2\nhist(\n  sigma_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[sigma]),\n  freq = FALSE,\n  xlim =c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[sigma]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#escolha-de-valores-inicias",
    "href": "posts/general-posts/repost-emv-no-r/index.html#escolha-de-valores-inicias",
    "title": "EMV no R",
    "section": "Escolha de valores inicias",
    "text": "Escolha de valores inicias\nComo comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.\nO exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.\n\n# sensível a escolha de valores inicias\nx &lt;- rnorm(n = 1000, mean = 15, sd = 4)\nfit &lt;- optim(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n\n$par\n[1] 618.6792 962.0739\n\n$value\n[1] 7984.993\n\n$counts\nfunction gradient \n     107      100 \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]\n[1,]  0.001070703 -0.0013531007\n[2,] -0.001353101  0.0001884928\n\n\nNote que as estimativas estão muito distantes dos valores corretos \\(\\mu = 15\\) e \\(\\sigma = 4\\). Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.\nO código abaixo usa os momentos empíricos como valores inicias para \\(\\mu\\) e \\(\\sigma\\):\n\\[\n\\begin{align}\n  \\mu_{inicial} & = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\\\\n  \\sigma_{inicial} & = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} (x_{i} - \\mu_{inicial})^2}\n\\end{align}\n\\]\n\n(chute_inicial &lt;- c(mean(x), sqrt(var(x))))\n\n[1] 14.859702  3.930849\n\n(est &lt;- optimx(par = chute_inicial, fn = ll_norm))\n\n                  p1       p2    value fevals gevals niter convcode kkt1 kkt2\nNelder-Mead 14.85997 3.929097 2787.294     47     NA    NA        0 TRUE TRUE\nBFGS        14.85970 3.928884 2787.294     15      2    NA        0 TRUE TRUE\n            xtime\nNelder-Mead 0.001\nBFGS        0.001\n\n\nAgora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função optimx se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.\nNote como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método bobyqa retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos nlminb e Nelder-Mead ambos retornam os valores corretos.\n\n# Usando outros métodos numéricos\noptimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = c(\"BFGS\", \"Nelder-Mead\", \"CG\", \"nlminb\", \"bobyqa\")\n  )\n\n                   p1         p2    value fevals gevals niter convcode  kkt1\nBFGS        618.67917 962.073907 7984.993    107    100    NA        1  TRUE\nNelder-Mead  14.85571   3.929621 2787.294     83     NA    NA        0  TRUE\nCG           46.43586 628.570987 7358.601    204    101    NA        1  TRUE\nnlminb       14.85970   3.928883 2787.294     23     47    19        0  TRUE\nbobyqa       15.20011   8.993240 3211.556    109     NA    NA        0 FALSE\n             kkt2 xtime\nBFGS        FALSE 0.007\nNelder-Mead  TRUE 0.001\nCG          FALSE 0.008\nnlminb       TRUE 0.001\nbobyqa      FALSE 0.033\n\n\nVale notar também alguns detalhes técnicos da saída. Em particular, convcode == 0 significa que o otimizador conseguiu convergir com sucesso, enquanto convcode == 1 indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.\nJá o kkt1 e kkt2 verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte necessária do teorema enquanto a segunda condição verifica a parte suficiente. Note que o bobyqa falha em ambas as condições (pois ele não é feito para este tipo de problema).\nOs métodos que retornam os melhores valores, o Nelder-Mead e nlminb são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.\nMais detalhes sobre os métodos podem ser encontrados na página de ajuda da função ?optimx."
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html",
    "title": "Usando fontes com showtext no R",
    "section": "",
    "text": "Criar boas visualizações é parte importante de qualquer análise de dados.\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar e o mesmo vale para visualizações com dados. A fonte do texto ajuda a transmitir informação e pode comunicar, por exemplo, maior sobriedade, profissionalismo, etc.\nO pacote showtext, desenvolvido por yixuan, facilita a importação e o uso de fontes em gráficos no R. O pacote funciona com uma variedade de extensões de fontes, não sendo limitado como o extrafont, por exemplo, a arquivos .ttf."
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html#base-r",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html#base-r",
    "title": "Usando fontes com showtext no R",
    "section": "Base R",
    "text": "Base R\nPara modificar a fonte dos elementos textuais dos gráficos feitos com o plot() é preciso ajustar o argumento family. Usando as funções base do R, este argumento aparece dentro da função par (que configura vários parâmetros dos gráficos).\nO código abaixo mostra como trocar a fonte do gráfico.\n\n# Define a fonte padrão do gráfico\npar(family = \"alice\")\n# Monta um scatter plot de exemplo\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30)\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nNote que todos os objetos textuais (título, legenda, etc.) são convertidos para a mesma fonte. Caso se queira fontes diferentes para estes elementos é preciso especificá-los adequadamente. Por exemplo, para trocar somente a fonte do título\ntitle(\"nome_do_titulo\", family = \"nome_fonte\")\n\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\", family = \"RobCond\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30, family = \"Montserrat\")\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nVale notar que, uma vez definida a fonte usando a função par, todos os gráficos subsequentes vão usar esta fonte. Para trocar a fonte é preciso usar a função par novamente. Além da fonte também é possível trocar a ênfase (e.g. face = c(\"bold\", \"italic\")) e também o tamanho da letra (e.g. cex.axis = 1.5, cex.main = 2)."
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html#ggplot2",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html#ggplot2",
    "title": "Usando fontes com showtext no R",
    "section": "ggplot2",
    "text": "ggplot2\nTambém é possível trocar a fonte de gráficos feitos com outros pacotes, como o ggplot2. O exemplo abaixo monta um gráfico similar ao que foi feito acima. Modifica-se a fonte dentro da função theme. Esta função é um tanto particular, então vale a pena discorrer um pouco sobre ela. Ela é basicamente usada para modificar elementos do gráfico. Há quatro elementos principais, dos quais só nos interessa um: o element_text. São seis os principais elementos textuais que pode-se modificar:\n\naxis.text - texto dos eixos (em geral, os números do eixo);\naxis.title - nome do eixo (e.g. “Milhas por galão” no exemplo acima);\nlegend.text - texto da legenda;\nlegend.title - título da legenda;\nplot.title - título do gráfico;\ntext - todos os acima.\n\nPode-se ser mais específico com o texto dos eixos usando axis.text.x e axis.text.y, por exemplo. O último dos elementos listados acima funciona como um “coringa”, ele serve para modificar de uma vez só todos os elementos textuais de um gráfico. No exemplo abaixo modifico somente o text.\n\nlibrary(ggplot2)\n\np &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE) +\n  labs(\n    x = \"Peso (ton.)\",\n    y = \"Milhas por galão\",\n    title = \"Eficiência e peso de carros\",\n    subtitle = \"Regressão entre o peso de diferentes carros e sua eficiência energética\",\n    caption = \"Fonte: Motor Trend US Magazine 1974\"\n    )\n\n\np + theme(text = element_text(family = \"Montserrat\", size = 10))\n\n\n\n\n\n\n\n\nO próximo exemplo mostra como modificar alguns dos diferentes elementos do gráfico. Aproveito a variável cyl (cilindradas) para diferenciar os carros em três grupos para que o gráfico agora tenha uma legenda.\n\np +\n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme(\n    # Modifica o texto (números) dos eixos x e y\n    axis.text = element_text(family = \"alice\"),\n    # Modifica o título do eixo (i.e. Milhas por galão)\n    axis.title = element_text(family = \"Montserrat\"),\n    # Modifica o título da legenda (Cilindros)\n    legend.title = element_text(family = \"HelveticaNeue\"),\n    # Modifica o texto da legenda (i.e. 4, 6, 8)\n    legend.text = element_text(family = \"HelveticaNeue\"),\n    # Modifica o título do gráfico\n    plot.title = element_text(family = \"RobCond\", size = 20)\n    )\n\n\n\n\n\n\n\n\nTalvez o jeito mais sensato de usar fontes com o ggplot2 seja primeiro especificar uma fonte padrão para o gráfico usando text e depois calibrar as exceções. Os elementos textuais como axis.title e legend.text copiam as propriedades definidas em text.\nNo exemplo abaixo defino que todos os elementos textuais ser escritos em Arial simples em tamanho 12 na cor \"gray20\". Depois disso defino que o título deve ter mais destaque com Arial em negrito (bold) num tamanho maior e numa cor mais escura. Por fim, defino que o rodapé do gráfico seja escrito em fonte menor e em itálico.\n\ntheme_custom &lt;- theme_light() +\n  theme(\n    # Modifica todos os elementos textuais do gráfico\n    text = element_text(family = \"Arial\", size = 12, color = \"gray20\"),\n    # Modifica apenas o título\n    plot.title = element_text(face = \"bold\", size = 14, color = \"gray10\"),\n    # Modifica apenas a nota no rodapé\n    plot.caption = element_text(face = \"italic\", size = 8)\n  )\n\np + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n\n\n\n\n\n\n\nPor último, também pode ser interessante usar fontes diferentes para representar dados diferentes. Isto é possível usando o argumento family dentro do aes. Da mesma forma, seria possível também representar grupos de dados diferentes com tamanhos de fontes diferentes ou mesmo destacar algum grupo específico com itálico.\n\nnomes &lt;- row.names(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_text(aes(label = nomes, family = c(\"Arial\", \"alice\", \"Montserrat\")[cyl]))"
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "title": "Usando fontes com showtext no R",
    "section": "Anexo: problemas com DPI e RMarkdown",
    "text": "Anexo: problemas com DPI e RMarkdown\nApesar de muito conveniente, o showtext não é inteiramente desprovido de problemas. Dois problemas que enfrento com alguma recorrência são diferenças de DPI na hora de exportar gráficos e problemas com RMarkdown.\nO problema com o RMarkdown é mais simples. Em versões antigas do RMarkdown e do showtext era necessário adicionar um argumento fig.showtext = TRUE em todos os chunks em que um gráfico usando showtext fosse renderizado. Alternativamente, podia-se modificar esta opção globalmente inserido o seguinte código no início do documento RMarkdown.\n\nknitr::opts_chunk$set(\n  fig.showtext = TRUE,\n  fig.retina = 1\n  )\n\nAcredito, mas não tenho certeza, de que este problema sumiu em versões mais recentes dos pacotes, pois com frequência eu esqueço de adicionar estes argumentos mas não encontro problemas na prática.\nO problema com o DPI na hora de exportar gráficos é mais complicado. Por problemas de DPI quero dizer quando o showtext “desenha” o texto num DPI diferente do ggplot2. O resultado é que o texto fica ou grande ou pequeno demais. Por padrão o showtext utiliza o DPI em 96.\nVamos montar um gráfico para ilustrar o problema.\n\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exportar o gráfico em alta resolução\nggsave(\"meu_grafico.jpeg\", grafico, dpi = 300)\n\nQuando vamos abrir o arquivo que foi exportado temos o resultado abaixo.\n\nComo a imagem foi salva com DPI mais elevado o texto fica menor do que deveria; em casos mais extremos o texto fica minúsculo a ponto de ser ilegível. Há duas formas de tentar contornar este problema: (1) reduzir o DPI dentro de ggsave; ou (2) modificar o DPI do showtext.\nVamos tentar a primeira solução: modificar o ggsave para o DPI padrão do showtext. Agora o texto está maior mas a proporção dos elementos está péssima! O resultado é pior do que o problema inicial.\n\n# Exportar a imagem num dpi menor\nggsave(\"meu_grafico_96.jpeg\", grafico, dpi = 96)\n\n\n\n\n\n\nA segunda solução é modificar as opções internas do showtext. Isto é bastante simples e pode ser feito com o showtext_opts(dpi = 300) e chamando novamente a função showtext_auto().\n\n# Ajusta o DPI do showtext\nshowtext_opts(dpi = 300)\n# \"Ativa\" o showtext novamente\nshowtext_auto()\n\n# Refaz o gráfico (isto é importante!)\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exporta um novo gráfico\nggsave(\"meu_grafico_300.jpeg\", grafico, dpi = 300)\n\nAgora o tamanho do texto está correto e a imagem como um todo está em alta resolução. Um problema é que a imagem ficou bastante grande, mas isto pode ser ajustado variando os argumentos width e height da função ggsave.\n\nVale notar que, a depender do seu sistema operacional, modificar o DPI padrão do showtext pode distorcer os gráficos dentro do R ou RStudio. Na prática o melhor workflow pode ser de modificar o DPI do showtext apenas no momento de exportar os gráficos."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html",
    "href": "posts/general-posts/2023-09-tidyverse/index.html",
    "title": "A filosofia do Tidyverse",
    "section": "",
    "text": "O tidyverse é um metapacote, ou conjunto de pacotes. Pode-se pensar no tidyverse como uma família de pacotes, unidos por uma filosofia comum; grosso modo, os pacotes que compõem o tidyverse tem o mesmo objetivo: facilitar a manipulação de dados. Estes pacotes criaram uma nova forma de se escrever código, que substitui boa parte ou mesmo todas as funções base do R. Atualmente, o tidyverse parece estar se consolidando como a variante dominante do R. De fato, a maior parte dos pacotes do tidyverse consta na lista dos mais baixados no repositório CRAN.\nO tidyverse é intimamente ligado com o estatístico Hadley Wickham, criador ou co-criador da maioria dos seus pacotes, e autor do influente artigo em que ele define o que é “tidy” data. Ele também é autor de diversos livros didáticos como R for Data Science e ggplot2: Elegant graphics for Data Analysis, que ajudaram a popularizar o tidyverse.\nWickham também tem posição de liderança dentro da Posit (antigamente conhecida como RStudio), que mantém o GUI mais popular de R e que patrocina inúmeras atividades vinculadas com o aprendizado de R, que costumam enfatizar os pacotes do tidyverse. De fato, tornou-se lugar comum começar a se ensinar R pelo tidyverse como se vê pela prevalência de cursos no Coursera ou Datacamp.\nQuando se olha para a curta história do tidyverse é difícil explicar o porquê do seu enorme sucesso, mas é fato que este conjunto de pacotes se tornou um dialeto dominante dentro da comunidade do R. As funções do tidyverse tem algumas vantagens importantes sobre o base-R.\n\n\nAs funções do tidyverse possuem uma característica ausente na maior parte das funções base do R: consistência. As funções do tidyverse oferecem consistência sintática: o nome da funções segue certas convenções e a ordem dos argumentos segue regras previsíveis.\nUm exemplo imediato é o pacote stringr, que serve para manipulação de strings. Todas as funções deste pacote começam com prefixo str_ e seus argumentos seguem a lógica: string e pattern como em str_detect(string, pattern)1. Além disso, as funções são mais otimizadas em relação às funções base do R.\nO purrr faz algo similar, ao simplificar a família de funções apply em diversas funções map_*. Neste caso, além da consistência sintática, as funções map_* também garantem a consistência do output, em termos da classe do objeto que é retornado como resultado da função. Isto é uma grande vantagem, especialmente quando comparado com a função sapply que “simplifica” o output de maneiras às vezes imprevisíveis.\nEm termos de eficiência, o tidyverse costuma ganhar das funções equivalentes em base-R. O dplyr/tidyr, de maneira geral, garante manipulações de dados muito mais velozes2, assim como o readr importa dados mais rapidamente3. As funções map também tem paralelos simples na família future_ do pacote furrr, que permite usar processamento paralelo no R.\n\n\n\nHá muito material de apoio para tidyverse: livros, materiais didáticos, posts em blogs, respostas em fóruns, etc. Como citado acima, o próprio Posit produz inúmeros materiais didáticos e livros que ajudam a aprender e a ensinar tidyverse. Na medida em que o tidyverse consolida-se como o dialeto dominante isto tende a se tornar um ciclo virtuoso.\nO R é uma linguagem bastante versátil, que reúne pesquisadores de campos distintos. Recentemente, parece haver uma convergência para o tidyverse. O campo de séries de tempo, por exemplo, agora tem o tidyquant, fable e modeltime que utilizam os princípios do tidyverse. Com o tempo, deve-se observar movimentos similares de outros campos.\n\n\n\nO tidyverse oferece funções que se aplicam a cada uma das etapas de uma análise de dados. Neste sentido, ele vai de ponta-a-ponta, cobrindo importação, limpeza, modelagem e visualização de dados. A natureza autocontida do tidyverse é bastante atraente pois oferece um caminho seguro para novatos no R, especialmente para quem tem interesse em ciência de dados.\n\n\n\nO conhecimento no R muitas vezes é bastante horizontal. Cada pacote novo traz funções diferentes, que funcionam de novas maneiras e este conhecimento adquirido nem sempre se traduz para outras tarefas. Já sintaxe do dplyr é bastante geral, pode ser utilizada em vários contextos. O dbplyr, por exemplo, é um backend para databases (como BigQuery, PostgreSQL, etc.) que usa a sintaxe do dplyr como frontend. O mesmo acontece com dtplyr/tidytable que permite usar a sintaxe do dplyr junto com a eficiência do data.table. Até para dados complexos já existe o pacote srvyr que usa o survey como backend.\n\n\n\nEste último ponto é bastante mais contencioso. Eu acredito que o tidyverse é mais fácil do que base-R. Eu tenho um conhecimento razoável de base-R e avançado tanto de tidyverse como de data.table. Na minha opinião, a lógica do tidyverse de usar o nome das colunas de um data.frame como objetos é muito poderosa e intuitiva. Não só torna o código mais legível como também evita uma sintaxe carregada com operadores estranhos como $.\nA integração com pipes também simplifica muito o workflow da análise de dados. Com o tempo, a leitura de um código em pipes torna-se natural. Por fim, fazer funções com tidyverse também é muito fácil. Especialmente no caso de funções simples, a sintaxe {{x}} e !!x facilita bastante e, de maneira geral, considero mais simples programar usando princípios “tidy” do que programar usando base-R."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html#o-tidyverse-em-números",
    "href": "posts/general-posts/2023-09-tidyverse/index.html#o-tidyverse-em-números",
    "title": "A filosofia do Tidyverse",
    "section": "O tidyverse em números",
    "text": "O tidyverse em números\nOlhando para as estatísticas do CRAN, vê-se que os pacotes do tidyverse são muito relevantes dentro do ecossistema. Os dados compilados abaixo mostram o retrato dos pacotes do CRAN em 05 de setembro de 2023, quando havia cerca de 19.800 pacotes ativos.\nAtualmente, cerca de um terço dos pacotes no CRAN dependem de algum dos pacotes core do tidyverse. O crescimento desta razão tem sido crescente: de todos os pacotes ativos em 2023, 40% dependem diretamente do tidyverse. Note que no gráfico abaixo, o ano de publicação reflete o ano da versão mais recente de cada pacote. Assim, pacotes ativos cuja última atualização foi anterior a 2016 dificilmente vão possuir alguma dependência com os pacotes do tidyverse já que a maioria deles não existia nesta época.\n\n\n\n\n\n\n\n\n\nVale lembrar que há três tipos de “dependência” entre pacotes no R: imports, depends e suggests. Tipicamente, se um pacote A usa algumas funções de outro pacote B, então o pacote A importa (imports) o pacote B. Isto é, ele assume que o usuário tenha o pacote B instalado. Já a relação depends é mais estrita: se um pacote A depends de um pacote C então os pacotes são carregados conjuntamente quando se chama library()4. Por fim se um pacote A usa um pacote D, em algum contexto específico, mas não requer que o usuário tenha o pacote D instalado, então o pacote A suggests o pacote D5.\nOlhando os dados por pacote vê-se que o ggplot2 e dplyr são os mais populares."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html#a-filosofia-do-tidyverse",
    "href": "posts/general-posts/2023-09-tidyverse/index.html#a-filosofia-do-tidyverse",
    "title": "A filosofia do Tidyverse",
    "section": "A filosofia do tidyverse",
    "text": "A filosofia do tidyverse\nA filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes”6 que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático. A tarefa de renomear colunas, criar variáveis e calcular uma média nos grupos torna-se “linear” no mesmo sentido em que uma frase com sujeito-verbo-objeto é linear.\n\nPipes\nO pipe, essencialmente, carrega o resultado de uma função adiante numa cadeia de comandos: objeto |&gt; função1 |&gt; função2 |&gt; função3. Isto tem duas vantagens: primeiro, evita que você use funções compostas que são lidas “de dentro para fora” como exp(mean(log(x))); e, segundo, dispensa a criação de objetos intermediários “inúteis” que estão ali somente para segurar um valor que não vai ser utilizado mais adiante.\n\nmodel &lt;- lm(log(AirPassengers) ~ time(AirPassengers))\n\n#&gt; Função composta\nmean(exp(fitted(model)))\n#&gt; Usando pipes\nmodel |&gt; fitted() |&gt; exp() |&gt; mean()\n#&gt; Usando objetos intermediários\nx1 &lt;- fitted(model)\nx2 &lt;- exp(x1)\nx3 &lt;- mean(x2)\n\nNum contexto de manipulação de dados pode-se ter algo como o código abaixo.\n\ntab_vendas_cidade &lt;- dados |&gt; \n  #&gt; Renomeia colunas\n  rename(date = data, city = cidade, variable = vendas, value = valor) |&gt; \n  #&gt; Transforma colunas\n  mutate(\n    value = value / 1000,\n    date = readr::parse_date(date, format = \"%Y-%b%-d\", locale = readr::locale(\"pt\")),\n    year = lubridate::year(date)\n    ) |&gt; \n  #&gt; Agrupa pela coluna year e calcula algumas estatísticas\n  group_by(year) |&gt; \n  summarise(\n    total = sum(value),\n    count = n()\n  )\n\nEm base-R o mesmo código ficaria algo como o descrito abaixo.\n\nnames(dados) &lt;- c(\"date\", \"city\", \"variable\", \"value\")\n\ndados$value &lt;- dados$value / 1000\ndados$date &lt;- readr::parse_date(\n  dados$date, format = \"%Y-%b%-d\", locale = readr::locale(\"pt\")\n  )\ndados$year &lt;- lubridate::year(dados$date)\n\ntab_vendas_cidade &lt;- tapply(\n  dados$value,\n  dados$city,\n  \\(x) {data.frame(total = sum(x), count = length(x))}\n  )\n\nHá um tempo atrás argumentava-se contra o uso de “pipes”, pois estes dificultavam a tarefa de encontrar bugs no código. Isto continua sendo parcialmente verdade, mas as funções do tidyvserse atualmente têm mensagens de erro bastante ricas e permitem encontrar a fonte do erro com relativa facilidade. Ainda assim, não se recomenda encadear funções em excesso, i.e., pipes com 10 funções ou mais7.\n\n\nFunções\nOutra filosofia do tidyverse é de que tarefas rotineiras devem ser transformadas em funções específicas. Neste sentido, os pacotes dplyr, tidyr e afins são recheados de funções, às vezes com nomes muito semelhantes e com usos redundantes. As funções starts_with e ends_with, por exemplo, são casos específicos da função matches. Há funções que permitem até duas formas de grafia como summarise e summarize. Outras como slice_min e slice_max são convenientes mas são literalmente: arrange + slice.\nSomando somente os dois principais pacotes, dplyr e tidyr, há 360 funções disponíveis. Contraste isto com o data.table que permite fazer 95% das transformações de dados somente com dt[i, j, by = c(), .SDcols = cols].\nMesmo as funções base do R costumam ser mais sucintas do que códigos em tidyverse. No exemplo abaixo, a função tapply consegue o mesmo resultado que o código mais extenso feito com dplyr.\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(cyl))\n\nAs vantagens do tidyverse se tornam mais evidentes com o tempo. De fato, o pacote permite abstrações muito poderosas, e eventualmente, pode-se fazer um código centenas de vezes mais sucinto combinando as suas funções. Em outros casos, as funções do tidyverse são simplesmente muito convenientes.\nTome a starts_with, por exemplo, que seleciona as colunas que começam de uma certa forma. Suponha uma tabela simples em que há múltiplas colunas cujos nomes começam com a letra “x”. O código em tidyverse é muito mais limpo que o código em base-R.\n\ndf &lt;- df |&gt; \n  select(date, starts_with(\"x\"))\n\ndf &lt;- df[, c(\"date\", names(df)[grep(\"^x\", names(df))])]\ndf &lt;- df[, c(\"date\", names(df)[stringr::str_detect(names(df), \"^x\")])]\n\nO exemplo abaixo é inspirado neste post, que mostra como calcular lags de uma série de tempo que esteja em um data.frame. Calcular defasagens de uma série de tempo é uma tarefa um pouco árdua quando se usa somente funções base. O código abaixo mostra não somente a elegância do tidyverse mas também a facilidade em se criar funções a partir do tidyverse.\n\ncalculate_lags &lt;- function(df, var, lags) {\n  \n map_lag &lt;- lags |&gt; map(~partial(lag, n = .x))\n out &lt;- df |&gt;\n   mutate(\n     across(.cols = {{var}},\n            .fns = map_lag,\n            .names = \"{.col}_lag{lags}\")\n     )\n \n return(out)\n}\n\ndf &lt;- data.frame(\n  date = time(AirPassengers),\n  value = as.numeric(AirPassengers)\n)\n\ndf |&gt; calculate_lags(value, 1:3) |&gt; head()\n#       date value value_lag1 value_lag2 value_lag3\n# 1 1949.000   112         NA         NA         NA\n# 2 1949.083   118        112         NA         NA\n# 3 1949.167   132        118        112         NA\n# 4 1949.250   129        132        118        112\n# 5 1949.333   121        129        132        118\n# 6 1949.417   135        121        129        132\n\n\n\nDesvantagens\nO lado negativo da abordagem “gramatical” é que para não-falantes de inglês muitas destas vantagens são despercebidas8 e o resultado é somente um código “verborrágico”, cheio de funções. Além disso, pode-se argumentar que há ambiguidades inerentes na linguagem. A função filter, por exemplo, é utilizada para filtrar as linhas de um data.frame, mas podia, igualmente, chamar-se select, que selecionaria as linhas de um data.frame. A função select, contudo, é usada para selecionar as colunas de um data.frame.\nUm fato particularmente irritante do tidyverse é a frequência com que os pacotes mudam. Na maior parte das vezes, as mudanças são positivas, mas isto faz com que o código escrito em tidyverse não seja sustentável ao longo do tempo.\nEu demorei um bom tempo para entender as funções tidyr::gather e tidyr::spread e, atualmente, ambas foram descontinuadas e substituídas pelas funções pivot_longer e pivot_wider9. As funções mutate_if, mutate_at e similares do dplyr foram todas suprimidas pela sinataxe mais geral do across. A função tidyr::separate agora está sendo substituída por separate_wider_position e separate_wider_delim.\nMesmo um código bem escrito há poucos anos atrás tem grandes chances de não funcionar mais porque as funções foram alteradas ou descontinuadas. Em 2021, Wickham discutiu este problema abertamente numa palestra. Desde então, o tidyverse tem melhorado a sua política de manutenção de funções.\nA velocidade e eficiência das funções do tidyverse pode ser um problema, mas atualmente existem diversas boas soluções como o já citado tidytable. Particularmente, são raras as situações em que a velocidade do tidyverse me incomoda.\nAtualmente, parece haver um consenso crescente de que a melhor forma de começar a aprender R é começando pelo tidyverse; esta visão não é livre de críticos como de Norm Matloff, professor de estatística da UC Davis. Essencialmente, Matloff considera que o tidyverse é muito complexo para iniciantes: há muitas funções para se aprender e o incentivo à programação funcional torna o código muito abstrato. O tidyverse também esconde o uso do base-R e não ensina operadores básicos como [[ e $. Matloff também considera que “pipes” prejudicam o aprendizado pois dificultam a tarefa de encontrar a fonte dos erros no código."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html#footnotes",
    "href": "posts/general-posts/2023-09-tidyverse/index.html#footnotes",
    "title": "A filosofia do Tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs funções base gsub, grep e grepl, por exemplo seguem o padrão pattern, string. Já a função strsub usa o padrão string, pattern. Para mais diferenças entre as funções base para manipulação de texto e o stringr consulte este material.↩︎\nExistem diversos benchmarks que atestam os ganhos do dplyr em relação ao base-R. Veja, por exemplo, este comparativo. Apesar disto, o dplyr é menos eficiente que seu concorrente data.table. Existem algumas alternativas como dtplyr e, mais recentemente, tidytable, que fornecem a velocidade do data.table com a sintaxe do dplyr.↩︎\nSimilarmente ao dplyr, o readr também perde para seu concorrente data.table::fread. Contudo, o pacote vroom oferece uma alternativa mais veloz ao readr dentro do universo tidyverse.↩︎\nO pacote ggforce, por exemplo, depends do ggplot2. Isto significa que library(ggforce) automaticamente carrega o pacote ggplot2.↩︎\nEm geral, os pacotes sugeridos (suggests) são listados para os desenvolvedores do pacote ou utilizados para testes e exemplos. Você provavelmente já deve ter visto algum exemplo que começa com: if (require(\"pacote\")) { … }. O pacote nnet, por exemplo, não depende nem importa outros pacotes, mas utiliza o pacote MASS em seus exemplos; assim, o pacote nnet suggests o pacote MASS. Também existem casos onde um pacote tem mais capacidades ou melhor performance quando os pacotes sugeridos estão instalados; nestes casos, recomenda-se instalar o pacote com install.packages(dependencies = TRUE)`.↩︎\nPara saber mais sobre pipes e a diferença entre o novo pipe nativo |&gt; e o pipe |&gt; do magrittr veja meu post sobre o assunto.↩︎\nPara mais sobre pipes consulte o meu post sobre o assunto.↩︎\nNo fundo, isto é ainda mais um incentivo para aprender inglês.↩︎\nTecnicamente, elas foram “superseded”, ou suplatandas. Isto significa que elas continuam existindo exatamente da forma como sempre existiram e que não receberão mais atualizações.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html",
    "title": "Importando arquivos, visualizando linhas",
    "section": "",
    "text": "Uma das minhas maiores dificuldades quando comecei a mexer com R era conseguir importar a minha base de dados. Em geral, eu tinha um ou vários arquivos .csv ou planilhas .xlsx que precisavam ser importador para o R. Eu tinha três dificuldades\n\nSaber qual a função que eu precisava usar.\nEscrever o caminho até o arquivo específico (qualquer errinho e já não funcionava!).\nCalibrar os argumentos na hora de importar.\n\nDepois de muito esforço eu conseguia importar os dados, mas logo surgia ourto problema: o nome das variáveis vinha num formato muito ruim para trabalhar. No R, idealmente as variáveis são todas:\n\nMinúsculas\nNão tem acento nem caracteres especiais como $, %, etc.\nNão tem espaços\nNão começa com números\n\nIsso evita inúmeros problemas e facilita muito na hora de escrever o código.\nPor fim, eu tinha bastante dificuldade de “enxergar os dados” mesmo depois de ter importado eles. No caso de uma planilha de Excel eu poderia simplesmente abrir ela e explorar um pouco. Já se a base de dados fosse em formato dta ou sav isso já não era tão simples, pois eu não tinha Stata e nem SPSS no meu computador.\nTudo isso me desanimava quando comecei a mexer com R e vejo que isso é um daqueles obstáculos idiotas que acabam segurando muita gente de fazer a transição para o R. É o tipo de problema (que parece) simples, mas que na verdade é difícil de resolver e que faz com que você se sinta burro e fique frustrado.\nNeste post vou te dar algumas dicas de como lidar com todos estes passos. É o post que eu gostaria de poder enviar para mim mesmo no passado."
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#a-solução-correta",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#a-solução-correta",
    "title": "Importando arquivos, visualizando linhas",
    "section": "A solução correta",
    "text": "A solução correta\nA tabela abaixo resume as principais funções que você provavelmente vai ter que usar.\n\n\n\n\n\n\n\n\n\nFormato\nExtensao\nImportar\nExportar\n\n\n\n\nExcel\nxls, xlsx\nreadxl::read_excel()\nxlsx::write.xlsx()\n\n\nSeparados\ncsv, tsv, psv, csvy\ndata.table::fread() ou readr\ndata.table::fwrite() ou readr\n\n\nStata\ndta\nhaven::read_dta()\nhaven::write_dta()\n\n\nSPSS\nsav\nhaven::read_sav()\nhaven::write_sav()\n\n\nShapefiles\nshp, geosjon, gpkg, etc.\nsf::st_read()\nsf::st_write()\n\n\n\nOBS: Caso não esteja familiarizado com esta sintaxe, aqui readxl é o nome do pacote, read_excel() é a funcão e o :: indica que eu quero a função read_excel do pacote readxl. Em geral, é comum omitir a parte do nomepacote:: porque acaba sendo desnecessário (exceto no caso de conflitos de funções que têm o mesmo nome).\n\nSobre csvs\nVale uma nota: existem várias alternativas para importar arquivos csv, várias alternativas mesmo. Eu recomendo evitar as funções base e usar as funções equivalentes do readr. Então, por exemplo, ao invés de usar read.csv use readr::read_csv. Esta é uma boa decisão por três motivos:\n\nAs funções do readr são consideravelmente mais rápidas e versáteis do que as funções base equivalentes.\nAs funções read_* compartilham uma sintaxe padronizada e costumam ter argumentos muito similares. Assim as funções do readr são muito parecidas com as funções do pacote readxl e haven. O combo readr + readxl + haven resolve o problema em 95% dos casos.\nTodos estes pacotes e funções já estão bem integrados ao universo tidyverse1.\n\nSe velocidade começar a ser um problema, pode-se experimentar também com o pacote vroom que permite importar arquivos csv mais rapidamente. A função vroom::vroom também compartilha da sintaxe das funções read_*.\nUm típico problema com arquivos csv é que os delimitadores e separadores variam de país para país. Arquivos csv de fontes brasileiras costumam ser separadas por ; e usam a , como quebra de decimal, ao contrário dos csv de fontes dos EUA que usam a , como separador e . como quebra de decimal. Os arquivos no padrão EUA devem ser lidos com read_csv enquanto os arquivos no padrão brasileiro devem ser lidos com read_csv22.\nApesar de todas as vantagens listadas acima, ainda vale recomendar o data.table::fread()3 na hora de importar qualquer arquivo “separado” (csv, tsv, psv, etc.). Esta função é extremamente rápida, aloca os dados na memória de maneira eficiente e simplesmente funciona. Mesmo sem nenhum argumento adicional ela é muito boa na hora de adivinhar o tipo de separador utilizado e o tipo de dado em cada coluna.\nPor fim, apesar de ter recomendado o data.table::fwrite na tabela acima, vale notar que funções como readr::write_excel_csv2() podem ser muito úteis caso seu objetivo seja exportar um csv que vai ser consumido por um usuário brasileiro numa planilha de Excel.\n\n\nMais controle\nA prática faz a perfeição na hora de importar arquivos problemáticos. Como comentei acima, uma das vantagens de se ater ao combo readr + readxl + haven é que os argumentos adicionais destas funções seguem o mesmo padrão.\n\nskip = k: Pula as primeiras k linhas.\nna: Define quais valores devem ser interpretados como valores ausentes.\ncol_types: Permite que se declare explicitamente qual o tipo de dado (numérico, data, texto) que está armazenado em cada coluna.\ncol_names ou name_repair: O primeiro permite que se declare explicitamente o nome que cada coluna vai ter dentro do R enquanto o segundo permite que se use uma função que renomeia as colunas.\nlocale: Permite selecionar diferentes tipos de padrão de local. Em geral, usa-se locale = locale(\"pt\").\nrange: Este argumento só vale no caso de planilhas de Excel e permite que se importe uma seleção específica da planilha (e.g. “D4:H115”)\n\nO código abaixo mostra como importar um csv bastante sujo. Veremos detalhes sobre a função janitor::clean_names mais adiante.\n\n#&gt; Input de um csv sujo\ndados &lt;-\n'Data; Valor (R$/m2)\n\"01-maio-2020\";22,3\n\"01-junho-2020\";21,5\n\"06-julho-2021\";X\n\"07-novembro-2022\";22'\n\n#&gt; Lendo o arquivo\ndf &lt;- read_delim(\n  #&gt; Substitua esta linha pelo 'path' até o csv\n  I(dados),\n  delim = \";\",\n  #&gt; Usa , como separador decimal; lê meses em português (e.g. maio, junho, etc.)\n  locale = locale(decimal_mark = \",\", date_names = \"pt\", date_format = \"%d-%B-%Y\"),\n  #&gt; Interpreta X como valores ausentes (NA)\n  na = \"X\",\n  #&gt; Renomeia as colunas\n  name_repair = janitor::clean_names\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#importando-arquivos-o-atalho",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#importando-arquivos-o-atalho",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Importando arquivos: o atalho",
    "text": "Importando arquivos: o atalho\nUma função muito prática que inicialmente contorna todos estes problemas é a rio::import().\nA função rio::import() simplesmente importa seus dados e funciona com boa parte das extensões mais populares. Na prática, ela é uma função “facilitadora”. Por baixo dos panos, ela está chamando a função correta para o caso específico.\nEste site mostra exatamente qual função de qual pacote ele utiliza para importar os dados. A lista é bem completa e inclui bases do Minitab, Matlab, EViews, etc. Spoiler: data.table::fread() é utilizada para importar arquivos csv, psv e tsv.\nO par da função rio::import é a rio::export e serve justamente para exportar bases de dados do R para o formato desejado.\n\ndata &lt;- rio::import(\"data/meus_dados.dta\")\n\nrio::export(mtcars, \"data/mtcars.csv\")\n\nO único problema desta função é quando seus arquivos não estão num formato muito bacana e argumentos adicionais são necessários. É possível fornecer estes argumentos à função, mas é difícil saber quais são os argumentos, já que não se sabe qual função está sendo chamada. Assim, é preciso consultar a documentação (?rio::import) para verificar qual função está sendo utilizada e aí consultar a documentação desta função."
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#escrevendo-paths-relativos-com-here",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#escrevendo-paths-relativos-com-here",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Escrevendo paths relativos com here",
    "text": "Escrevendo paths relativos com here\nUm problema bem sério que eu enfrentava nos meus códigos era escrever o path até os arquivos externos. Primeiro, eu achava muito trabalhoso escrever ele inteiro. Depois, quando eu mandava meu código para outra pessoa, ou quando eu mesmo ia executar o meu código em outro computador, nada funcionava!\nO primeiro passo para lidar com isso é trabalhar com projetos do RStudio4. O melhor workflow é sempre começar seu trabalho num projeto novo e deixar todos os arquivos necessários neste mesmo diretório em pastas com nomes simples como data, report, graphics, etc.\nO segundo passo é utilizar paths relativos. Paths relativos, ao contrário de paths absolutos, começam no seu diretório da seguinte forma: \"data/subpasta/meus_dados.xlsx\". A pasta data está dentro da pasta do projeto: isto é indicado implicitamente.\nÉ bem diferente de um path absoluto: /Users/nome_do_usuario/Documentos/meus_projetos/Projeto 1/data/subpasta/meus_dados.xlsx.\nUsar paths absolutos no seu código é garantir que a única pessoa que conseguirá reproduzir ele com sucesso será você, e unicamente no computador em que você escreveu ele (isso se você não formatar ele!).\nAqui entra o pacote here, um pacotinho muito simples, centrado em uma única função homônima. A função here funciona de duas maneiras bastante simples\n\nlibrary(here)\nlibrary(haven)\n\nmeus_dados &lt;- read_dta(here(\"data/subpasta/minha_base.dta\"))\nmeus_dados &lt;- read_dta(here(\"data\", \"subpasta\", \"minha_base.dta\"))\n\nA segunda forma de sintaxe é muito útil na hora de criar paths. Isto será muito conveniente depois, quando formos importar vários arquivos de uma mesma pasta.\nA seguinte ilustração que serve de capa do projeto do here resume muito bem a sua utilidade.\n\n\n\n\n\nExiste um mal hábito dissemeniado de incluir uma linha com setwd(\"insira_seu_diretorio\") no início de todo código. Eu garanto que todo tipo de problema imaginável e inimaginável acontece com pessoas que fazem isso.\nO here simplesmente funciona e funciona com tudo. Ele é especialmente útil na hora de escrever scripts em RMarkdown e Quarto. O pacote here é talvez o único que esteja presente em todos os meus projetos e em todos os meus códigos.\nOutra dica boa para manter seus projetos organizados é de evitar colocar espaços ou caracteres especiais no nome das suas pastas. Em geral, o R consegue lidar bem com isso, mas volta e meia este mau hábito pode gerar problemas desnecessários e inesperados."
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#importando-todos-os-arquivos-de-de-uma-pasta",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#importando-todos-os-arquivos-de-de-uma-pasta",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Importando todos os arquivos de de uma pasta",
    "text": "Importando todos os arquivos de de uma pasta\nEste é um problema bastante recorrente e que é fácil de resolver usando here e funções base.\nImagine que você tem vários arquivos .csv numa pasta e os arquivos estão na seguinte estrutura: Dados/inflacao/2012/ e aí cada csv individual é um arquivo mensal (com nomes potencialmente fora de padrão) com os dados de inflação mensal por produto. Algo como 2012_jan.csv, 2012fevereiro.csv, etc.\nA estrutura do código para importar tudo isso no R é bastante simples.\n\nlibrary(here)\n# Define o diretório\ndir &lt;- here(\"Dados/inflacao/2012\")\n# Encontra o nome de todos os arquivos com extensão csv nesta pasta\nfilenames &lt;- list.files(dir, pattern = \"\\\\.csv$\")\n# Define o path até cada um dos arquivos\npathfiles &lt;- here(dir, filenames)\n# Importa todos os csv usando fread\ndata &lt;- lapply(pathfiles, data.table::fread)\n\n# Opcionalmente, empilha todos os resultados e cria uma coluna que identifica\n# de qual o arquivo a observação pertence\nnames(data) &lt;- basename(pathfiles)\nempilhado &lt;- data.table::rbindlist(data, idcol = \"nome_arquivo\")\n\nUma maneira ainda mais sucinta de escrever o código seria omitindo os objetos intermediários e simplesmente empilhando o resultado final.\n\n# Define o path até cada um dos arquivos\npathfiles &lt;- list.files(\n  here(\"Dados/inflacao/2012\"),\n  pattern = \"\\\\.csv$\",\n  full.names = TRUE\n  )\n# Importa todos os csv usando fread\nfiles &lt;- lapply(pathfiles, data.table::fread)\n# Empilha todos os resultados\ndat &lt;- data.table::rbindlist(files)"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#olhando-seus-dados-no-excel",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#olhando-seus-dados-no-excel",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Olhando seus dados no Excel",
    "text": "Olhando seus dados no Excel\nPor fim, vou deixar uma função customizada bem divertida, que permite você rapidamente dar uma espiada nos seus dados no bom e velho Excel. A função abaixo cria um arquivo temporário a partir da sua base de dados no R e abre isso no Excel.\nEvidentemente, é preciso ter o Excel instalado para que o código funcione.\n\nshow_in_excel &lt;- function(.data) {\n  if (interactive()) {\n    tmp &lt;- paste0(tempfile(), \".xlsx\")\n    writexl::write_xlsx(.data, tmp)\n    browseURL(tmp)\n  }\n  .data\n}\n\nshow_in_excel(mtcars)\n\nA figura abaixo mostra o resultado do código\n\n\n\n\n\n\n\n\n\nNovamente, como a função aceita um data.frame como argumento é bem fácil de colocá-la no final de um pipe. Esta função é bastante útil quando você precisa rapidamente compartilhar algum resultado ou tabela com alguém.\n\nmtcars |&gt; \n  filter(cyl &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(peso_medio = mean(wt)) |&gt; \n  show_in_excel()"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#footnotes",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#footnotes",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara saber mais sobre o tidyverse consulte o meu post sobre A Filosofia do Tidyverse.↩︎\nAlternativamente, pode-se usar a função read_delim, que é mais geral e permite especificar quais símbolos são usados como delimitador e separador de números.↩︎\nDisclaimer importante: depois de revisar este texto eu já não recomendo tão fortemente o data.table::fread por um movito bobo e simples. Atualmente, o data.table tem uma classe própria para datas chamada IDate. Esta classe é útil se você pretende fazer todas as suas análises usando as funções do data.table como shift, hour, etc. Contudo, se você pretende usar outros pacotes comuns de séries de tempo será necessário converter para Date todas as vezes. Além disso, como eu já estou bastante habituado a usar o pacote lubridate para manipular datas, não vejo muita vantagem em utilizar as funções do data.table.↩︎\nEvidentemente, projetos não são exclusivos ao RStudio; também é possível trabalhar com um workflow de projeto com VSCode, por exemplo. Para usuários que usam majoritariamente o R, contudo, o RStudio facilita muito a vida.↩︎\nEstas palavras são “nomes reservados” dentro do R e jamais devem ser utilizados na hora de definir um objeto ou o nome de uma coluna. Para consultar a lista de nomes reservados, veja help(\"Reserved\").↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-comandos-simples/index.html#importando-todos-os-arquivos-de-uma-pasta",
    "href": "posts/general-posts/2023-09-comandos-simples/index.html#importando-todos-os-arquivos-de-uma-pasta",
    "title": "Importando arquivos, visualizando linhas",
    "section": "Importando todos os arquivos de uma pasta",
    "text": "Importando todos os arquivos de uma pasta\nEste é um problema bastante recorrente e que é fácil de resolver usando here e funções base.\nImagine que você tem vários arquivos .csv numa pasta e os arquivos estão na seguinte estrutura: Dados/inflacao/2012/ e aí cada csv individual é um arquivo mensal (com nomes potencialmente fora de padrão) com os dados de inflação mensal por produto. Algo como 2012_jan.csv, 2012fevereiro.csv, etc.\nA estrutura do código para importar tudo isso no R é bastante simples.\n\nlibrary(here)\n# Define o diretório\ndir &lt;- here(\"Dados/inflacao/2012\")\n# Encontra o nome de todos os arquivos com extensão csv nesta pasta\nfilenames &lt;- list.files(dir, pattern = \"\\\\.csv$\")\n# Define o path até cada um dos arquivos\npathfiles &lt;- here(dir, filenames)\n# Importa todos os csv usando fread\ndata &lt;- lapply(pathfiles, data.table::fread)\n\n# Opcionalmente, empilha todos os resultados e cria uma coluna que identifica\n# de qual o arquivo a observação pertence\nnames(data) &lt;- basename(pathfiles)\nempilhado &lt;- data.table::rbindlist(data, idcol = \"nome_arquivo\")\n\nUma maneira ainda mais sucinta de escrever o código seria omitindo os objetos intermediários e simplesmente empilhando o resultado final. Note que, para que as bases de dados sejam empilhadas é necessário que o nome e o tipo das colunas seja compatível. Isto pode ser melhor controlado declarando o tipo das colunas na função fread.\n\n# Define o path até cada um dos arquivos\npathfiles &lt;- list.files(\n  here(\"Dados/inflacao/2012\"),\n  pattern = \"\\\\.csv$\",\n  full.names = TRUE\n  )\n# Importa todos os csv usando fread\nfiles &lt;- lapply(pathfiles, data.table::fread)\n# Empilha todos os resultados\ndat &lt;- data.table::rbindlist(files)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html",
    "title": "Estético: Tipografia e temas",
    "section": "",
    "text": "Este post encerra a discussão de elementos “estéticos” de gráficos. Primeiro apresento, brevemente, uma discussão sobre tipografias e como utilizar fontes em gráficos de ggplot2. Depois, entro numa discussão mais detalhada sobre a função theme, que controla todos os aspectos “temáticos” do gráfico, isto é, todos os aspectos que não envolvem diretamente algum dado: o tamanho do título, a posição da legenda, a cor do fundo, etc.\nEste é talvez o post mais burocrático de todos. A função theme permite um controle fino sobre o gráfico, mas exige instruções igualmente detalhadas. Assim, o código, em termos de linhas, começa a crescer muito. Um simples gráfico de linhas, que podia ser feito em 5 ou 6 linhas, torna-se uma tarefa complexa, que pode ultrapassar 50 linhas de código. O lado positivo disto é que estas configurações podem então ser replicadas em múltiplos gráficos. Isto não é apenas um ganho estético, mas também permite padronizar a identidade visual dos gráficos.\nAssim, o esforço de customizar o gráfico é feito uma única vez e depois pode ser replicado em todos os gráficos subsequentes."
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#fontes-no-r",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#fontes-no-r",
    "title": "Estético: Tipografia e temas",
    "section": "Fontes no R",
    "text": "Fontes no R\nPara adicionar fontes em gráficos vamos utilizar o pacote showtext.\n\nlibrary(sysfonts)\nlibrary(showtext)\n\nNo código abaixo eu importo a fonte Montserrat.\n\nfont_add_google(\"Montserrat\", \"Montserrat\")\n\nO gráfico abaixo mostra como ficam as letras.\n\n#&gt; Carrega as fontes\nshowtext_auto()\n\ndat &lt;- expand.grid(x = 1:5, y = 6:1)\ndat$z &lt;- c(letters, 1, 2, 3, 4)\n\nggplot(dat, aes(x = x, y = y, label = z)) +\n  geom_text(family = \"Montserrat\") +\n  theme_void()\n\n\n\n\n\n\n\n\nNo segundo exemplo mostro como adicionar a fonte Helvetica. Neste caso, será necessário que a fonte esteja instalada em seu computador.\n\nfont_add(\"Helvetica\", \"Helvetica.ttc\")\n\n\nggplot(dat, aes(x = x, y = y, label = z)) +\n  geom_text(family = \"Helvetica\") +\n  theme_void()\n\n\n\n\n\n\n\n\nNa seção seguinte mostro como utilizar estas fontes em outras partes do gráfico."
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#temas-completos",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#temas-completos",
    "title": "Estético: Tipografia e temas",
    "section": "Temas completos",
    "text": "Temas completos\nUm tema completo é um conjunto de especificações temáticas que pode ser aplicado diretamente num gráfico. É a opção mais simples e direta de customização: no exemplo abaixo aplico o tema minimalista usando theme_minimal().\nO gráfico abaixo usa a mesma base da Pesquisa Origem e Destino (POD) do post anterior e mostra o número absoluto de carros no eixo-x e o número de carros por domicílio no eixo-y.\n\n#&gt; Importa os dados limpos da Zona OD.\npod &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/posts/ggplot2-tutorial/table_pod.csv\"\n  )\n#&gt; Seleciona apenas zonas de São Paulo com população acima de zero\npod &lt;- dplyr::filter(pod, code_muni == 36, pop &gt; 0)\n\n\nggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  scale_color_brewer(type = \"qual\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nO tema theme_minimal já vem carregado no pacote ggplot2. Abaixo mostro alguns temas diferentes aplicados ao mesmo gráfico. Para não sobrecarregar a visualização eu omito a legenda.\n\nbase_plot &lt;- ggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  scale_x_continuous(labels = scales::label_number(big.mark = \".\")) +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  guides(color = \"none\") +\n  labs(x = \"Carros\", y = \"Carros por habitante\")\n\n#&gt; Tema \"clássico\" padrão\nbase_plot + theme_classic()\n#&gt; Tema preto e branco padrão\nbase_plot + theme_bw()\n#&gt; Tema claro padrão\nbase_plot + theme_light()\n#&gt; Tema completamente vazio (útil para mapas)\nbase_plot + theme_void()\n\n\n\n\n\n\n\n\n\n\nO pacote ggthemes traz algumas funções theme_* adicionais. Além disso, ele também traz escalas de cores, que combinam com estes temas.\n\nlibrary(ggthemes)\n\nNo exemplo abaixo uso theme_pander e scale_color_pander em conjunto.\n\nggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  scale_x_continuous(labels = scales::label_number(big.mark = \".\")) +\n  guides(color = \"none\") +\n  labs(title = \"Pander\", x = \"Carros\", y = \"Carros por habitante\") + \n  scale_color_pander() +\n  theme_pander()\n\n\n\n\n\n\n\n\nNovamente, os gráficos abaixo mostram algumas das opções disponíveis no pacote ggthemes. Estes temas “imitam” algumas publicações famosas como The Economist e FiveThirtyEight. Há também temas que imitam a identidade visual de “softwares” estatísticos como Stata, Excel, Google Docs, etc.\n\nbase_plot &lt;- ggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  scale_x_continuous(labels = scales::label_number(big.mark = \".\")) +\n  guides(color = \"none\") +\n  labs(x = \"Carros\", y = \"Carros por habitante\")\n\n#&gt; Tema que imita a The Economist\nbase_plot + scale_color_economist() + theme_economist()\n#&gt; Tema que imita o Excel\nbase_plot + scale_color_excel_new() + theme_excel_new()\n#&gt; Tema que imita o Google Docs\nbase_plot + scale_color_gdocs() + theme_gdocs()\n#&gt; Tema que imita o FiveThirtyEight\nbase_plot + scale_color_fivethirtyeight() + theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\n\nHá também outros pacotes que exportam escalas e temas pré-definidos. O pacote hrbrthemes, por exemplo, oferece boas opções e já traz algumas tipografias como Roboto Condensed e Arial. Para conhecer mais sobre o pacote consulte o seu repositório no GitHub. Acredito que os temas deste pacote funcionam bem para contextos formais em geral, sejam acadêmicos ou empresariais.\n\nlibrary(hrbrthemes)\n\nbase_plot +\n  hrbrthemes::scale_color_ft() +\n  hrbrthemes::theme_ft_rc()\n\n\n\n\n\n\n\n\nOutro pacote interessante é o cowplot, que oferece uma visualização simples e limpa. Acredito que o tema funciona bem para publicações acadêmicas em geral.\n\nlibrary(cowplot)\n\nbase_plot + \n  cowplot::theme_half_open(font_size = 10, font_family = \"mono\") +\n  cowplot::background_grid()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#os-elementos-principais",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#os-elementos-principais",
    "title": "Estético: Tipografia e temas",
    "section": "Os elementos principais",
    "text": "Os elementos principais\nPara entender como estes temas são construídos precisa-se entender os elementos temáticos do gráfico em maiores detalhes. São quatro os principais elementos temáticos de um gráfico: fundo, eixos, legenda e margens.\nNas próximas seções vamos explorar estes elementos.\n\nFundo\nPor “fundo” quer-se dizer tudo o que fica atrás do gráfico principal. Num gráfico, temos dois “fundos”: o fundo do gráfico como um todo (plot.background) e o fundo do “painel” onde fica o gráfico (panel.background).\nO exemplo abaixo tenta mostrar a diferença entre estes dois “fundos”. Aqui, as cores seguem o mesmo padrão de geom_col, o argumento color define a borda do objeto enquanto fill define a cor que preenche o objeto. Como fundo do gráfico coloquei uma margem azul espessa e um preenchimento em cinza-escuro (gray40). Como fundo do painel coloquei um “off-white” e uma linha de contorno escura (gray10).\n\nggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  guides(color = \"none\") +\n  theme(\n    plot.background = element_rect(\n      fill = \"gray40\",\n      color = \"blue\",\n      linewidth = 5),\n    panel.background = element_rect(\n      fill = \"#f8f8f8\",\n      color = \"gray10\"\n      )\n  )\n\n\n\n\n\n\n\n\nAs linhas de grade também compõem o fundo do gráfico. Estas linhas são úteis para relacionar as observações no gráfico com os valores destacados nos eixos. O excesso de linhas de grade, contudo, pode poluir um gráfico.\nAs linhas de grade são controladas por element_line e muitos dos argumentos seguem a lógica de geom_line. Isto é, pode-se modificar o tipo de linha com linetype ou a espessura da linha com linewidth.\nNo exemplo abaixo, novamente, exagero nos argumentos para exemplificar algumas das possibilidades. As linhas “principais” que saem diretamente dos valores destacados no eixo são controladas via panel.grid.major; as linhas “intermediárias”, panel.grid.minor.\n\nggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  guides(color = \"none\") +\n  theme(\n    panel.grid.major = element_line(\n      color = \"gray10\",\n      linetype = 2\n    ),\n    panel.grid.minor = element_line(\n      color = \"red\",\n      linetype = 3\n    )\n  )\n\n\n\n\n\n\n\n\nPara omitir qualquer elemento temático usa-se element_blank. No caso abaixo, elimina-se as linhas de grade intermediárias.\n\nggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  guides(color = \"none\") +\n  theme(\n    panel.grid.major = element_line(\n      color = \"gray50\",\n      linetype = 1\n    ),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nEixos\nPor eixos, quer-se dizer não somente os elementos textuais nos eixos x e y, mas também os elementos textuais adicionais (título, subtítulo e caption4).\nOs eixos são um elemento temático bastante complexo. No post anterior, vimos como controlar alguns dos aspectos dos eixos usando funções scale. A função theme como mencionado diversas vezes permite um controle detalhado dos elementos temáticos. Para exemplificar o seu uso vamos começar com um gráfico cheio de elementos textuais.\n\nbase_plot &lt;- ggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  scale_x_continuous(labels = scales::label_number(big.mark = \".\")) +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Carros\",\n    y = \"Carros por habitante\",\n    title = \"Carros demais em SP\",\n    subtitle = \"Proporção e número absoluto de carros por Zona OD\",\n    caption = \"Fonte: POD (2017)\"\n    )\n\nbase_plot\n\n\n\n\n\n\n\n\nVamos começar trocando a fonte, cor e tamanho do título do gráfico. Note que é necessário ter a fonte Montserrat carregada, o que pode ser feito usando font_add_google(\"Montserrat\", \"Montserrat\") como visto acima.\n\nbase_plot &lt;- base_plot +\n  theme(\n    plot.title = element_text(\n      family = \"Montserrat\",\n      size = 20,\n      color = \"gray10\")\n  )\n\nbase_plot\n\n\n\n\n\n\n\n\nHá dois elementos temáticos que compõem os eixos x e y. Temos o título do eixo (“Carros por habitante”) e o texto do eixo (“0.5”, “1.0”, “1.5”). Para modificar o primeiro usa-se axis.title e para modificar o segundo, axis.text. Estes elementos temáticos são replicados em ambos os eixos; contudo, se for necessário maior controle têm-se os elementos axis.title.x, axis.title.y, axis.text.x e axis.text.y.\nNo exemplo abaixo eu deixo o título dos eixos em Roboto Condensed em negrito, tamanho 12, e em cinza-escuro. Já o texto dos eixos fica em Raleway, tamanho 10, num tom mais claro de cinza.\n\nbase_plot &lt;- base_plot +\n  theme(\n    axis.title = element_text(\n      family = \"Roboto Condensed\",\n      face = \"bold\",\n      size = 12,\n      color = \"gray10\"\n    ),\n    axis.text = element_text(\n      family = \"Raleway\",\n      size = 10,\n      color = \"gray40\"\n      )\n  )\n\nPor fim, falta apenas o subtítulo e o caption do gráfico. Note como uso hjust = 0 para alinhar o caption à esquerda. Outra opção seria hjust = 0.5 (centralizar) ou hjust = 1 (à direita).\n\nbase_plot +\n  theme(\n    plot.subtitle = element_text(size = 10, color = \"gray50\"),\n    plot.caption = element_text(size = 8, hjust = 0, color = \"gray50\")\n  )\n\n\n\n\n\n\n\n\nUma maneira mais sã de utilizar estes argumentos é definindo uma mesma fonte para o gráfico inteiro. Aqui é importante notar que os elementos temáticos “herdam” argumentos. Para modificar todos os elementos textuais de um gráfico, por exemplo, pode-se modificar apenas text = element_text() e title = element_text(). A partir disto pode-se modificar os demais elementos introduzindo exceções à regra geral.\n\nbase_plot +\n  theme(\n    text = element_text(family = \"Montserrat\", size = 10),\n    title = element_text(size = 8, color = \"gray50\"),\n    plot.title = element_text(\n      size = 20,\n      color = \"gray10\"\n      ),\n    axis.title = element_text(\n      face = \"bold\",\n      size = 12,\n      color = \"gray10\"\n    ),\n    axis.text = element_text(\n      color = \"gray40\"\n      )\n  )\n\n\n\n\n\n\n\n\n\n\nLegenda\n\nbase_plot &lt;- ggplot(pod, aes(x = cars, y = car_rate)) +\n  geom_point(aes(color = as.factor(is_cbd)), alpha = 0.5) +\n  scale_x_continuous(labels = scales::label_number(big.mark = \".\")) +\n  scale_color_brewer(\n    type = \"qual\",\n    palette = 2,\n    name = \"Dentro do CE?\",\n    labels = c(\"Não\", \"Sim\")\n    ) +\n  labs(\n    x = \"Carros\",\n    y = \"Carros por habitante\",\n    title = \"Carros demais em SP\",\n    subtitle = \"Proporção e número absoluto de carros por Zona OD\",\n    caption = \"Fonte: POD (2017)\"\n    )\n\nbase_plot\n\n\n\n\n\n\n\n\nO código abaixo mostra a maior parte das modificações que pode-se fazer com a legenda.\n\nbase_plot +\n  theme(\n    legend.position = \"bottom\",\n    legend.background = element_rect(color = \"red\"),\n    legend.key = element_rect(color = \"blue\"),\n    legend.title = element_text(family = \"Montserrat\"),\n    legend.text = element_text(color = \"gray50\")\n  )\n\n\n\n\n\n\n\n\n\n\nMargens\nPor fim, as margens do gráfico são determinados diretamente via o elemento margin. As duas principais margens a se escolher são: a margem do gráfico e a margem da legenda. A ordem dos argumentos é margem de cima, da direita, de baixo e da esquerda (sentido horário começando em cima).\nVale notar que não costuma haver motivos para modificar as margens padrão do ggplot2.\n\nbase_plot +\n  theme(\n    plot.margin = margin(5, 10, 5, 10),\n    legend.box.margin = margin(20, 20, 20, 20)\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#criando-seu-tema",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#criando-seu-tema",
    "title": "Estético: Tipografia e temas",
    "section": "Criando seu tema",
    "text": "Criando seu tema\nA motivação para aprender temas é poder criar o seu próprio tema e utilizá-lo em múltiplos gráficos. Em geral, recomenda-se começar com um tema simples e aí adicionar modificações; pode-se usar, por exemplo, o theme_minimal como ponto de partida.\nNo código abaixo eu monto o theme_vini, que modifica o theme_minimal. O meu tema utiliza Helvetica como fonte padrão (em tamanho 12). Além disso, eu removo as linhas de grade intermediárias, posiciono qualquer legenda no topo do gráfico e modifico as configurações do título do gráfico.\n\ntheme_vini &lt;- theme_bw(base_size = 12, base_family = \"Helvetica\") +\n  theme(\n    #&gt; Remove linhas de grade intermediárias\n    panel.grid.minor = element_blank(),\n    #&gt; Posiciona a legenda no topo do gráfico\n    legend.position = \"top\",\n    #&gt; Aumenta o tamanho e destaca o título\n    plot.title = element_text(size = 20, color = \"#000000\")\n  )\n\nAgora posso utilizar este tema em qualquer gráfico.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(aes(color = as.factor(cyl))) +\n  labs(title = \"Meu gráfico\") +\n  theme_vini"
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#folha-de-sp",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#folha-de-sp",
    "title": "Estético: Tipografia e temas",
    "section": "Folha de SP",
    "text": "Folha de SP\nO primeiro exemplo que escolhi vem da Folha de São Paulo em matéria publicada em outubro de 2021. A matéria foi baseada num estudo que participei que tentava mensurar a acessibilidade financeira à moradia em São Paulo. Usando preços de anúncios online e microdados de renda, mensurei a maior (ou menor) acessibilidade via um indicador chamado HAI (housing affordability index).\n\n\n\n\n\nEm linhas gerais, o HAI compara a renda média de um grupo contra o valor da parcela de financiamento de um imóvel típico. As condições de financiamento seguem as médias do mercado na época. A demanda pelo “imóvel típico” é inferida a partir do grupo. No recorte abaixo, por exemplo, assume-se que a pessoa que mora sozinha vai comprar um apartamento de um dormitório; já a família com dois filhos vai comprar um apartamento de 3 ou 4 dormitórios.\nValores próximos de 100 indicam uma acessibilidade boa, enquanto valores próximos de 0 indicam uma acessibilidade ruim.\nNeste exemplo vamos reproduzir apenas o gráfico de colunas. Para reproduzir o gráfico, copio os dados diretamente da imagem e tento chegar em tons de azul similares ao da imagem original. Não faço ideia qual a fonte que a Folha utiliza; usei a “Roboto” por ser relativamente similar e de fácil acesso.\n\n\nCode\n#&gt; Dados do HAI\ndados &lt;- tibble::tribble(\n  ~nome,             ~hai, ~type,\n  \"República\",       80.1,    1L,\n  \"Tatuapé\",         70.4,    1L,\n  \"Jabaquara\",       68.3,    1L,\n  \"Vila Mazzei\",     66.4,    1L,\n  \"Santana\",         62.8,    1L,\n  \"Jardim Brasil\",   51.9,    0L,\n  \"Belém\",           48.5,    0L,\n  \"Jardim Umarizal\", 42.1,    0L,\n  \"Parque Arariba\",  42.1,    0L,\n  \"Brasilândia\",     42.1,    0L\n)\n\ndados &lt;- dados |&gt; \n  mutate(nome = factor(nome), nome = fct_reorder(nome, hai))\n\n#&gt; Cores dos grupos\ncores &lt;- c(\"#B9D4EE\", \"#348ACA\")\n#&gt; Adiciona a fonte Roboto\nfont_add_google(\"Roboto\", \"Roboto\")\n\n\nA primeira versão do gráfico contém apenas o essencial da imagem. Temos um gráfico de colunas, virado na horizontal, com labels de texto. Além disso, as cores estão variando por grupo e temos uma legenda de cores. Sem utilizar a função theme o resultado do gráfico fica próximo, mas ainda muito distante do original.\n\n\nCode\nggplot(dados) +\n  geom_col(aes(x = nome, y = hai, fill = as.factor(type)), width = 0.5) +\n  geom_text(aes(x = nome, y = hai + 5, label = hai), color = \"#000000\") +\n  coord_flip() +\n  labs(x = NULL, y = NULL) +\n  scale_fill_manual(\n    name = \"\",\n    values = c(\"#B9D4EE\", \"#348ACA\"),\n    labels = c(\n      \"Para um casal com dois\\nfilhos e renda mediana**\",\n      \"Para quem mora sozinho\\ne tem renda mediana**\")\n  )\n\n\n\n\n\n\n\n\n\nO código abaixo tenta chegar num resultado próximo ao da imagem original. Essencialmente, precisamos:\n\nAjustar a cor do fundo.\nRemover as linhas de grade.\nRemover todas as informações dos eixos.\nAjustar a posição da legenda.\nMudar a fonte e a cor do texto.\n\nAlém destas mudanças, também deixo os números em negrito e uso a vírgula como separador de decimal. O resultado final segue abaixo. Eu utilizo theme_minimal como um template inicial.\n\n\nCode\nggplot(dados) +\n  geom_col(\n    aes(x = nome, y = hai, fill = as.factor(type)),\n    width = 0.5\n    ) +\n  geom_text(\n    aes(x = nome, y = hai + 5, label = format(hai, decimal.mark = \",\")),\n    size = 4,\n    vjust = 0.5,\n    family = \"Roboto\",\n    color = \"#000000\",\n    fontface = \"bold\") +\n  scale_y_continuous(expand = c(0, 0), limits = c(NA, 95)) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, title = \"\") +\n  scale_fill_manual(\n    name = \"\",\n    values = c(\"#B9D4EE\", \"#348ACA\"),\n    labels = c(\n      \"Para um casal com dois\\nfilhos e renda mediana**\",\n      \"Para quem mora sozinho\\ne tem renda mediana**\")\n  ) +\n  theme_minimal() +\n  theme(\n    #&gt; Fundo branco\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    #&gt; Remove as linhas de grade\n    panel.grid = element_blank(),\n    #&gt; Aplica a fonte Roboto\n    legend.text = element_text(family = \"Roboto\", color = \"#000000\"),\n    #&gt; Ajusta o texto no eixo-y\n    axis.text.y = element_text(\n      family = \"Roboto\",\n      color = \"#000000\",\n      size = 12,\n      vjust = 0.4),\n    #&gt; Remove o texto no eixo-x\n    axis.text.x = element_blank(),\n    #&gt; Aumenta. margem superior para dar espaço para a legenda\n    plot.margin = margin(t = 40, r = 5, b = 5, l = 5),\n    #&gt; Ajusta a posição e direção da legenda\n    legend.position = c(0.15, 1.1),\n    legend.direction = \"horizontal\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#financial-times",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#financial-times",
    "title": "Estético: Tipografia e temas",
    "section": "Financial Times",
    "text": "Financial Times\nO segudo exemplo vem do Financial Times e foi publicado março de 2021. O gráfico abaixo mostra a evolução trimestral do preço dos imóveis em alguns países da OCDE. Mais especificamente, os valores representam a variação real dos índices de preços imobiliários dos respectivos países; para comparar a evolução entre os países, os valores foram indexados no valor do primeiro trimestre de 2000.\nEssencialmente, o gráfico mostra como o preço dos imóveis subiu muito no Reino Unido e o autor argumenta que é necessário aumentar a oferta de moradia para conter a pressão sobre os preços.\n\n\n\n\n\nPara reproduzir este gráfico, primeiro importo as séries via o pacote OECD. É preciso filtrar os países, limpar as datas e indexar os valores.\n\n\nCode\nlibrary(OECD)\n\ndataset &lt;- \"HOUSE_PRICES\"\nhp &lt;- get_dataset(dataset)\ncountries &lt;- c(\"CAN\", \"DEU\", \"USA\", \"ITA\", \"GBR\", \"FRA\", \"ESP\")\n\ndat &lt;- hp |&gt; \n  janitor::clean_names() |&gt; \n  filter(ind == \"RHP\", cou %in% countries, stringr::str_length(time) &gt; 4) |&gt; \n  mutate(\n    date = zoo::as.Date(zoo::as.yearqtr(time, format = \"%Y-Q%q\")),\n    obs_value = as.numeric(obs_value)\n    ) |&gt; \n  filter(date &gt;= as.Date(\"2000-01-01\"), date &lt;= as.Date(\"2022-10-01\")) |&gt; \n  select(country = cou, date, index = obs_value)\n\ndat &lt;- dat |&gt; \n  mutate(reindex = index / first(index) * 100, .by = \"country\") \n\ncountry_order &lt;- dat |&gt; \n  filter(date == max(date)) |&gt; \n  arrange(desc(reindex)) |&gt; \n  pull(country)\n\ndat &lt;- dat |&gt; \n  mutate(country = factor(country, levels = country_order))\n\n\nNa sua essência, as duas linhas de código abaixo reproduzem o gráfico do Financial Times.\n\nggplot(dat, aes(x = date, y = reindex, color = country)) +\n  geom_line()\n\n\n\n\n\n\n\n\nPara recriar o gráfico vou precisar das cores das linhas. Tentei encontrar cores parecidas, mas os códigos abaixo não devem ser idênticos aos do gráfico original. Além disso, também preciso do nome - por extenso - dos países.\n\n#&gt; Cores\ncores &lt;- c(\n  \"#1A48B0\", \"#EB5F8E\", \"#73DAE4\", \"#A1BC4B\", \"#2F8CC9\", \"#7B052D\", \"#BBB7B4\")\n#&gt; Nomes dos países\ncountry_labels &lt;- c(\n  \"Canada\", \"UK\", \"France\", \"US\", \"Spain\", \"Germany\", \"Italy\"\n  )\n\nAs quebras no eixo-x são um pouco difíceis de emular, pois elas fogem do comportamento padrão do ggplot2. Assim, eu preciso definir ela manualmente e ainda fazer um pequeno “hack”: essencialmente, eu crio um vetor que destaca os anos “cheios” (2000, 2005, … 2020) e coloca valores vazios nos anos intermediários.\n\ndate_breaks &lt;- seq(as.Date(\"2000-01-01\"), as.Date(\"2022-01-01\"), by = \"year\")\ndate_labels &lt;- c(date_breaks[c(1, 6, 11, 16, 21)])\nlabels_year &lt;- format(date_labels, \"%Y\")\nlabs &lt;- c(sapply(labels_year, function(x) {c(x, rep(\"\", 4))}))\nlabs &lt;- labs[1:length(date_breaks)]\n\nlabs\n\n [1] \"2000\" \"\"     \"\"     \"\"     \"\"     \"2005\" \"\"     \"\"     \"\"     \"\"    \n[11] \"2010\" \"\"     \"\"     \"\"     \"\"     \"2015\" \"\"     \"\"     \"\"     \"\"    \n[21] \"2020\" \"\"     \"\"    \n\n\nUsando todos os conhecimentos adquiridos nos posts anteriores, pode-se criar a visualização abaixo. Note que o resultado já é bastante satisfatório. Eu mantive a ordem das cores, mas como a ordem dos países mudou com a atualização dos dados, elas não batem com as dos países no gráfico original.\n\n\nCode\nggplot(dat, aes(x = date, y = reindex, color = country)) +\n  geom_line(linewidth = 1) + \n  scale_color_manual(name = \"\", values = cores, labels = country_labels) +\n  scale_y_continuous(position = \"right\") +\n  scale_x_date(breaks = date_breaks, labels = labs) +\n  labs(\n    title = \"The divergent paths of house prices across countries\",\n    subtitle = \"Real house prices (Q1 2000 = 100)\",\n    caption = \"Source: OECD (replica FT)\",\n    x = NULL,\n    y = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPara chegar num resultado mais próximo do original é necessário mexer em vários elementos temáticos. Como fonte, usei a Gill Sans.\n\n\nCode\nggplot(dat, aes(x = date, y = reindex, color = country)) +\n  geom_line(linewidth = 1.1) + \n  scale_color_manual(name = \"\", values = cores, labels = country_labels) +\n  scale_y_continuous(position = \"right\") +\n  scale_x_date(breaks = date_breaks, labels = labs) +\n  labs(\n    title = \"The divergent paths of house prices across countries\",\n    subtitle = \"Real house prices (Q1 2000 = 100)\",\n    caption = \"Source: OECD (replica)\",\n    x = NULL,\n    y = NULL) +\n  guides(color = guide_legend(nrow = 1)) +\n  theme_minimal() +\n  theme(\n    #&gt; Muda a cor do fundo do gráfico\n    plot.background = element_rect(fill = \"#FEF1E4\", color = NA),\n    #&gt; Remove todos as linhas de grade intermediárias\n    panel.grid.minor = element_blank(),\n    #&gt; Remove as linhas de grade \"verticais\" que partem do eixo-x\n    panel.grid.major.x = element_blank(),\n    #&gt; Altera a cor das linhas de grade \"horizontais\" que partem do eixo-y\n    panel.grid.major.y = element_line(color = \"#EAE3DF\"),\n    \n    #&gt; Altera a fonte e a cor de todos os elementos textuais\n    text = element_text(family = \"Gill Sans\", color = \"#686261\"),\n    #&gt; Ajusta o título do gráfico para ser maior e em preto\n    plot.title = element_text(size = 20, color = \"#000000\"),\n    #&gt; Ajusta o tamanho da legenda\n    plot.subtitle = element_text(size = 12),\n    #&gt; Ajusta a posição da \"Fonte\"\n    plot.caption = element_text(hjust = 0),\n    #&gt; Altera o tamanho e a cor do texto nos eixos\n    axis.text = element_text(size = 11, color = \"#6B6865\"),\n    \n    #&gt; Aumenta as margens do gráfico\n    plot.margin = margin(rep(10, 4)),\n    \n    #&gt; Muda a cor do \"tiquezinho\" no eixo-x e deixa ele mais comprido\n    axis.ticks.x = element_line(color = \"#EADFD8\"),\n    axis.ticks.length = unit(7, \"pt\"),\n    \n    #&gt; Altera a posição da legenda\n    legend.position = c(0.35, 1)\n    \n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/7-themes-fonts.html#footnotes",
    "href": "posts/ggplot2-tutorial/7-themes-fonts.html#footnotes",
    "title": "Estético: Tipografia e temas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNeste post, mostro em maiores detalhes como utilizar o pacote showtext para importar e usar fontes no R.↩︎\nUma boa referência sobre a história da tipografia The Story of the World’s Most Famous Font: Helvetica↩︎\nhttps://henrywang.nl/ggplot2-theme-elements-demonstration/↩︎\nUso o termo em inglês para evitar confusão com a legenda, que se refere à legenda de cores. Uma tradução livre seria: nota de rodapé.↩︎\nVale notar o pacote ggimprensa de Bruno Mioto.↩︎"
  },
  {
    "objectID": "posts/shiny-apps/atlas-brasil.html",
    "href": "posts/shiny-apps/atlas-brasil.html",
    "title": "Atlas Brasil",
    "section": "",
    "text": "Sobre o aplicativo\nLink para o aplicativo\nEste aplicativo ajuda a visualizar uma série de indicadores socioeconômicos e demográficos nas regiões metropolitanas do Brasil. Os dados são do Atlas Brasil e o aplicativo foi inteiramente construído usando {shiny}. O código do aplicativo e alguns detalhes sobre a sua construção estão disponíveis no repositório do GitHub (em inglês).\nFiz este aplicativo em inglês então deixo o about abaixo.\n\n\nAbout the app\nThe Atlas of Human Development is a comprehensive collection of development indicators in Brazil. It provides access to information that reveals socioeconomic realities and inequalities. The data is compiled from IBGE’s decennial Census and yearly PNAD/C survey. It is the result of a collaborative effort between PNUD (UN), IPEA, and FJP.\nThe interactive map displays both regions and UDHs (human development units) for the major metropolitan regions of Brazil in 2000 and 2010. Income values have been adjusted for inflation up to January 2023. The map options allow for different forms of aggregation and color palettes. The ranking tool ranks metro regions and includes more recent data from PNAD. Finally, the download data tool provides a convenient way to download all of the data used in this app.\nThe construction of this app required extensive data cleaning, classification, and standardization. I chose a smaller subset of variables to keep the app manageable and to avoid overwhelming the user with options. For reference, the complete Atlas UDH dataset contains almost 230 variables. This smaller subset of variables also allowed me to better integrate the different Atlas datasets. In the future, I will provide more details about this process on my blog."
  },
  {
    "objectID": "posts/general-posts/2023-10-wv-nascimentos-brasil/index.html",
    "href": "posts/general-posts/2023-10-wv-nascimentos-brasil/index.html",
    "title": "Weekly Viz: Nascimentos no Brasil",
    "section": "",
    "text": "Tende-se a pensar que a data de nascimento de um indivíduo é algo completamente aleatório. Afinal, ninguém escolhe precisamente quando vai nascer. Alguns atribuem significado profundo à data de nascimento: a depender do horário, dia e mês a pessoa terá tendências a ser mais de uma forma do que outra. Nascer no mês impróprio pode ser um mal negócio para a vida toda.\nJá na cultura popular é comum especular que os nascimentos seguem alguns ciclos da vida. As estações do ano regulam as safras de comida, a temperatura, a disposição para sair de casa e, muito acreditam, o desejo sexual. Os feriados, as festividades, o carnaval, as vitórias no campeonato de futebol, tudo isso - imagina-se - tem algum efeito sobre a natalidade no país, nove meses no futuro.\nNo campo da economia, pode-se especular que os ciclos de crescimento econômico e, sobretudo, os ciclos de desemprego devem ter algum efeito sobre os nascimentos.\n\n\nUsando dados do IBGE, mais especificamente das Estatísticas do Registro Civil, pode-se calcular o número total de nascimentos em cada mês desde 2003. Grosso modo, nos últimos vinte anos, março, abril e maio foram os três meses com maior número de nascimentos (27,23%). Já os meses do final do ano, outubro, novembro e dezembro foram os meses com menor número de nascimentos (22,95%).\n\n\n\n\n\n\n\n\n\nOlhando para a série mensal do nascimentos fica mais fácil de ver o padrão dos dados. De 2003 a 2015, o número de nascidos permaneceu estável entre 230 e 240 mil. A recessão econômica de 2015-17 parece ter tido um efeito severo sobre a natalidade: a série cai do seu pico, acima de 260 mil, para o menor valor registrado até agora, abaixo de 200 mil. Após o término da recessão, a série se recupera parcialmente, mas registra tendência de queda desde a metade de 2018.\n\n\n\n\n\n\n\n\n\nNão há muita diferença entre o gráfico abaixo, que mostra o número absoluto de nascimentos a cada mês, e o primeiro. Uma distinção é que fica mais fácil perceber como os números de 2020 e 2021 são menores que os demais da série histórica."
  },
  {
    "objectID": "posts/general-posts/2023-10-wv-nascimentos-brasil/index.html#o-mês-de-nascimento",
    "href": "posts/general-posts/2023-10-wv-nascimentos-brasil/index.html#o-mês-de-nascimento",
    "title": "Weekly Viz: Nascimentos no Brasil",
    "section": "",
    "text": "Usando dados do IBGE, mais especificamente das Estatísticas do Registro Civil, pode-se calcular o número total de nascimentos em cada mês desde 2003. Grosso modo, nos últimos vinte anos, março, abril e maio foram os três meses com maior número de nascimentos (27,23%). Já os meses do final do ano, outubro, novembro e dezembro foram os meses com menor número de nascimentos (22,95%).\n\n\n\n\n\n\n\n\n\nOlhando para a série mensal do nascimentos fica mais fácil de ver o padrão dos dados. De 2003 a 2015, o número de nascidos permaneceu estável entre 230 e 240 mil. A recessão econômica de 2015-17 parece ter tido um efeito severo sobre a natalidade: a série cai do seu pico, acima de 260 mil, para o menor valor registrado até agora, abaixo de 200 mil. Após o término da recessão, a série se recupera parcialmente, mas registra tendência de queda desde a metade de 2018.\n\n\n\n\n\n\n\n\n\nNão há muita diferença entre o gráfico abaixo, que mostra o número absoluto de nascimentos a cada mês, e o primeiro. Uma distinção é que fica mais fácil perceber como os números de 2020 e 2021 são menores que os demais da série histórica."
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html",
    "title": "Cidades Brasil",
    "section": "",
    "text": "O objetivo deste post é conseguir gerar o gráfico abaixo com uma única linha de código. Isto será possível, pois todo o processamento dos dados será feito por funções auxiliares. A inspiração do mapa vem diretamente de BlakeRMills, criador do pacote {MetBrewer}.\nAbaixo segue a lista de todos os pacotes utilizados neste post. Não é necessário chamar todos eles com library mas é preciso ter todos eles instalados1. Com exceção dos três primeiros, usaremos somente uma ou duas funções de cada um dos pacotes.\n\n#&gt; Principais pacotes\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(osmdata)\nlibrary(elevatr)\nlibrary(raster)\nlibrary(geobr)\n#&gt; Pacotes auxiliares\nlibrary(purrr)\nlibrary(furrr)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(BAMMtools)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(ggthemes)\nlibrary(viridis)\nlibrary(showtext)"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#baixando-os-dados",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#baixando-os-dados",
    "title": "Cidades Brasil",
    "section": "Baixando os dados",
    "text": "Baixando os dados\nO objetivo é gerar o mapa de altitude das principais cidades do Brasil. Ao invés de ranquear as cidades por área, eu prefiro ordená-las por tamanho de população. Uma maneira fácil de conseguir esta informação é buscando ela diretamente na Wikipedia. No link temos uma tabela com código do IBGE, nome do município, nome do estado e população total em 2022.\nÉ bastante simples importar esta tabela no R. O código abaixo, interpreta a página com base na url, encontra todas as tabelas2 e escolhe especificamente a tabela principal.\n\nurl = \"https://pt.wikipedia.org/wiki/Lista_de_municípios_do_Brasil_por_população\"\n\ntab = xml2::read_html(url) |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(2)\n\nhead(tab)\n\n# A tibble: 6 × 5\n  Posição `Código IBGE` Município      `Unidade federativa` População \n  &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;                &lt;chr&gt;     \n1 1º      3550308       São Paulo      São Paulo            11 451 245\n2 2º      3304557       Rio de Janeiro Rio de Janeiro       6 211 423 \n3 3º      5300108       Brasília       Distrito Federal     2 817 068 \n4 4º      2304400       Fortaleza      Ceará                2 428 678 \n5 5º      2927408       Salvador       Bahia                2 418 005 \n6 6º      3106200       Belo Horizonte Minas Gerais         2 315 560 \n\n\nEste dado está um pouco sujo então eu limpo a tabela para facilitar nosso trabalho. Após limpar os dados, eu seleciono as 200 cidades mais populosas.\n\n\nCode\nas_numeric_char = Vectorize(function(x) {\n  ls = stringr::str_extract_all(x, \"[[:digit:]]\")\n  y = paste(ls[[1]], collapse = \"\")\n  as.numeric(y)\n})\n\nclean_tab = tab |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    code_muni = codigo_ibge,\n    name_muni = municipio,\n    rank = posicao,\n    name_state = unidade_federativa,\n    pop = populacao\n  ) |&gt; \n  filter(name_muni != \"Brasil\") |&gt; \n  mutate(\n    code_muni = as.numeric(code_muni),\n    pop = as_numeric_char(pop),\n    rank = rank(-pop),\n    name_muni = stringr::str_to_title(name_muni)\n  )\n\ntop200 = slice_max(clean_tab, pop, n = 200)"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade",
    "title": "Cidades Brasil",
    "section": "Shape da cidade",
    "text": "Shape da cidade\nAgora que temos uma lista de cidades podemos importar o shapefile com os limites territoriais do município do Recife. Graças ao pacote {geobr} isto é muito simples. A função read_municipality faz justamente isto e precisa apenas do código de 7 dígitos do IBGE.\n\n#&gt; Encontra o código do IBGE de Recife\ncode_muni = top200 |&gt; \n  filter(name_muni == \"Recife\") |&gt; \n  pull(code_muni)\n\n#&gt; Importa o shape dos limites do município\nborder = geobr::read_municipality(code_muni, showProgress = FALSE)\n\n#&gt; Um mapa simples para checar o resultado\nggplot(border) +\n  geom_sf() +\n  ggtitle(\"Limites do município\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias",
    "title": "Cidades Brasil",
    "section": "Principais vias",
    "text": "Principais vias\nO segundo passo é importar o shape das principais vias da cidade. Aqui, uso o pacote {osmdata}. O código abaixo importa as vias como “linhas” e depois usa os limites do município para remover os segmentos de linhas que estão fora da cidade.\n\n#&gt; Define os \"limites\" da busca. Monta um bounding box em torno do Recife\nrec = opq(bbox = getbb(\"Recife, Pernambuco, Brazil\"))\n\n#&gt; Pega as principais vias\nstreets = add_osm_feature(\n  rec,\n  key = \"highway\",\n  value = c(\"primary\", \"secondary\", \"tertiary\", \"residential\")\n  )\n\n#&gt; Converte o objeto para sf (LINESTRING)\nstreets = osmdata_sf(streets)\nstreets = streets$osm_lines\nstreets = select(streets, osm_id, name)\nstreets = st_transform(streets, crs = 4674)\n#&gt; Encontra a intersecção entre as vias e os limites do município\nstreets_border = st_intersection(streets, border)\n\nPara tornar mais evidente o que está acontecendo mostro primeiro o resultado geral, com todas as vias.\n\nggplot(streets) + \n  geom_sf(linewidth = 0.15) +\n  theme_void()\n\n\n\n\n\n\n\n\nE agora o resultado após a intersecção entre as vias e os limites do município.\n\nggplot() +\n  geom_sf(data = border, fill = NA) +\n  geom_sf(data = streets_border, linewidth = 0.15, color = \"gray20\") +\n  theme_void()"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude",
    "title": "Cidades Brasil",
    "section": "Altitude",
    "text": "Altitude\nO terceiro passo é importar os dados de altitude da cidade. Isto é feito com o pacote {elevatr}. Como os dados de altitude são armazenados como raster preciso convertê-los para dados em formato de vetor3. Novamente eu faço a intersecção destes dados com os limites do município para ficar somente com os valores que nos interessam.\n\n#&gt; Importa os dados de altitude\naltitude = elevatr::get_elev_raster(border, z = 9, clip = \"bbox\")\n#&gt; Converte para 'vector'\nrec_alti = raster::rasterToPolygons(altitude)\nrec_alti = sf::st_as_sf(rec_alti)\nnames(rec_alti)[1] = \"elevation\"\n\n#&gt; Converte o CRS e intersecta com os limites do município\nrec_alti = rec_alti %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  st_intersection(border) %&gt;%\n  #&gt; Remove geometrias inválidas\n  filter(st_is_valid(.))\n\nO gráfico abaixo mostra o resultado final.\n\n#&gt; Mapa\nggplot(rec_alti) +\n  geom_sf(aes(fill = elevation)) +\n  scale_fill_viridis_c(name = \"Altitude\", option = \"inferno\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nClassificando a altitude\nPara facilitar a visualização dos dados, classifica-se eles em grupos. Eu uso o algoritmo de Jenks para classificar os dados de elevação em 7 grupos distintos. O algoritmo de Jenks, também conhecido como “quebras naturais”, é bastante utilizado com dados espaciais.\nComo todo algoritmo de clustering, o algoritmo de Jenks busca minimizar a distância intraclasse enquanto tenta maximizar a distância entre as classes; isto é, ele busca observações parecidas e juntas elas em um grupo e busca separar os grupos o máximo possível.\nA medida de distância/dissemelhança que o algoritmo usa é a soma do quadrado dos desvios (em relação à média do grupo). O algortimo busca minimizar esta “variância” em cada um dos grupos para encontrar os grupos mais “parecidos” possíveis. O número de grupos é arbitrário e precisa ser selecionado manualmente4. Eu costumo escolher algum número entre 3 e 9.\n\njbreaks = BAMMtools::getJenksBreaks(rec_alti$elevation, k = 7)\njbreaks = round(jbreaks, -1)\n\nrec_alti = rec_alti %&gt;%\n  mutate(\n    jenks_group = cut(elevation, jbreaks)\n  )\n\nO resultado do agrupamento pode ser visto no histograma abaixo, onde as linhas verticais representam as quebras entre os grupos.\n\n\nCode\nggplot(rec_alti, aes(x = elevation)) +\n  geom_histogram(bins = 40, color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = jbreaks) +\n  scale_x_continuous(limits = c(-1, NA)) +\n  labs(\n    title = \"Distribuição da altitude em Recife\",\n    subtitle = \"Linhas verticais mostram o agrupamento do algoritmo de Jenks\"\n    ) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra o formato dos dados de altitude. Temos um grid retangular que mostram a altura, em metros, da cidade do Recife.\n\n#&gt; Mapa\nggplot(rec_alti) +\n  geom_sf(aes(fill = jenks_group)) +\n  scale_fill_viridis_d(option = \"inferno\") +\n  theme_void()"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes",
    "title": "Cidades Brasil",
    "section": "Juntando as partes",
    "text": "Juntando as partes\nA próxima etapa é um pouco mais complexa então vamos proceder em passos curtos. Os dados de altitude estão agrupados em grupos, definidos pelo algoritmo de jenks. Nosso objetivo é encontrar todas as ruas que pertencem a cada um destes grupos de altitude.\n\nlevels(rec_alti$jenks_group)\n#&gt; [1] \"(-40,20]\" \"(20,30]\"  \"(30,50]\"  \"(50,70]\"  \"(70,90]\"  \"(90,120]\"\n\njgroups = levels(rec_alti$jenks_group)\n\nO mapa abaixo mostra todos as áreas com altitude dentro do grupo 6 (90, 120].\n\njgroups = levels(rec_alti$jenks_group)\n\n\nsub = rec_alti %&gt;%\n  filter(jenks_group == jgroups[6]) %&gt;%\n  st_union(.) %&gt;%\n  st_as_sf()\n\nggplot(sub) + geom_sf()\n\n\n\n\n\n\n\n\nFazendo a intereseção entre este subconjunto do grid de altitude com o shape das ruas, encontra-se somente as vias que estão neste grupo de altitude.\n\nsubstreet = streets %&gt;%\n  st_intersection(sub) %&gt;%\n  filter(st_is_valid(.))\n\nggplot() +\n  geom_sf(data = substreet, linewidth = 0.5) +\n  geom_sf(data = sub, fill = NA)\n\n\n\n\n\n\n\n\nJuntando as duas etapas num mesmo bloco de código temos as linhas abaixo. Primeiro, encontra-se o subconjunto do grid de altitude dentro de um grupo. Depois, faz-se a interseção deste grupo com as vias da cidade.\n\npoly = rec_alti %&gt;%\n  filter(jenks_group == jgroups[6]) %&gt;%\n  st_union(.) %&gt;%\n  st_as_sf()\n\njoined = streets %&gt;%\n  st_intersection(poly) %&gt;%\n  filter(st_is_valid(.))\n\nggplot(joined) + geom_sf()\n\n\n\n\n\n\n\n\nPara repetir o código acima em todos os grupos, faz-se um loop simples. Os resultados são gravados numa lista chamada streets_altitude.\n\n#&gt; Cria uma lista para gravar os resultados\nstreets_altitude = list()\n\n#&gt; For-loop em todos os elementos de jgroups\nfor (i in seq_along(jgroups)) {\n\n  #&gt; Seleciona um nível do grupo em particular  \n  group = levels(rec_alti$jenks_group)[[i]]\n  \n  #&gt; Encontra o subconjunto de do grid de altitude que corresponde a este grupo\n  poly = rec_alti %&gt;%\n    filter(jenks_group == group) %&gt;%\n    st_union(.) %&gt;%\n    st_as_sf()\n  #&gt; Faz a interseção deste subconjunto com as vias da cidade\n  joined = streets %&gt;%\n    st_intersection(poly) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    mutate(level = factor(i))\n  #&gt; Grava o resultado como elemento da lista streets_altitude\n  streets_altitude[[i]] &lt;- joined\n  \n}\n\nFeito isto, pode-se verificar visualmente o resultado.\n\nrec_streets_altitude &lt;- bind_rows(streets_altitude)\n\nggplot(rec_streets_altitude) +\n  geom_sf(aes(fill = level, color = level), linewidth = 0.2) +\n  scale_fill_viridis_d(name = \"Altitude\", option = \"inferno\") +\n  scale_color_viridis_d(name = \"Altitude\", option = \"inferno\") +\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_rect(color = NA, fill = \"gray75\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa-final",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa-final",
    "title": "Cidades Brasil",
    "section": "Mapa final",
    "text": "Mapa final\nPara montar a versão finalizada do mapa, adiciona-se alguns elementos temáticos. Para saber mais sobre elementos temáticos e a função theme, consulte meu post.\n\ncores = c(\n  \"#0D0887FF\", \"#5402A3FF\", \"#8B0AA5FF\", \"#B93289FF\", \"#DB5C68FF\", \n  \"#F48849FF\", \"#FEBC2AFF\"\n  )\n\njlabels = paste(jbreaks, jbreaks[-1], sep = \"–\")\njlabels[1] = paste(\"&lt;\", min(jbreaks[-1]))\njlabels[length(jlabels)] = paste(\"&gt;\", max(jbreaks))\n\nggplot(data = rec_streets_altitude) +\n  geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n  scale_color_manual(name = \"Altitude\", values = cores, labels = jlabels) +\n  scale_fill_manual(name = \"Altitude\", values = cores, labels = jlabels) +\n  guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n  ggtitle(\"Recife\") +\n  ggthemes::theme_map() +\n  coord_sf() +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 8),\n    panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n    plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n    legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#o-básico-de-funções",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#o-básico-de-funções",
    "title": "Cidades Brasil",
    "section": "O básico de funções",
    "text": "O básico de funções\nUma função transforma um input num output seguindo uma série de comandos. Uma função no R é composta de três elementos: (1) formals; (2) body; e (3) environment. O primeiro elemento corresponde aos argumentos da função e o segundo elemento corresponde à função, propriamente dita. Tomando um exemplo da matemática considere a função abaixo\n\\[\nf(x) = x^2 + 1\n\\]\nNeste caso, o formals seria simplesmente \\(x\\) e o body seria \\(x^2 + 1\\) . Podemos ver como isto ocorre dentro do R.\n\nf &lt;- function(x) {\n  x^2 + 1\n}\n\nformals(f)\n#&gt; $x\nbody(f)\n#&gt; {\n#&gt;    x^2 + 1\n#&gt; }\nenvironment(f)\n#&gt; &lt;environment: R_GlobalEnv&gt;\n\nO environment é o ambiente onde a função existe; geralmente, o environment é simplesmente R_GlobalEnv, o ambiente “global” onde habitam todos os objetos que você cria como, por exemplo, x &lt;- 1:5. Este aspecto é um pouco mais técnico, mas é o que permite que funções habitem dentro de funções.\nO exemplo abaixo é adaptado do livro Advanced R e ilustra como funcionam diferentes environments quando se tem funções dentro de funções.\nFunções compartimentalizam o seu ambiente de trabalho. O que acontece dentro da função, fica dentro da função. Assim, a linha y &lt;- 2 abaixo “existe” apenas dentro do contexto da função j. Os objetos definidos dentro de funções não interferem com objetos criados fora da função.\n\nprint(environment())\n#&gt; &lt;environment: R_GlobalEnv&gt;\nj &lt;- function(x) {\n  y &lt;- 2\n  print(environment())\n  #&gt; &lt;environment: 0x16eaa2320&gt;\n  function() {\n    print(environment())\n    #&gt; &lt;environment: 0x16e9f8d40&gt;\n    c(x, y)\n  }\n}\nk &lt;- j(1)\nk()\n#&gt; [1] 1 2\nprint(y)\n#&gt; Error in print(y) : object 'y' not found\n\nUma função executa um conjunto de ações sobre seus inputs dentro de um “ambiente controlado” e devolve apenas o resultado final, output, para a sessão ativa do R. Assim, funções permitem isolar partes do código e dispensar resultados intermediários.\nO próximo exemplo mostra como filtrar linhas de um data.frame. No fundo, esta função simplesmente chama dplyr::filter e fornece argumentos numa determinada maneira. Note que declaro o nome do pacote dplyr o que evita a necessidade de chamar library(dplyr) e torna a função filtrar_linhas portátil5.\n\ndados = data.frame(\n  grupo = c(\"A\", \"A\", \"A\", \"B\", \"C\"),\n  y = c(1, 3, 7, 4, 1)\n)\n\n\nfiltrar_linhas = function(df, filter_val) {\n  dplyr::filter(df, grupo == filter_val)\n}\n\nfiltrar_linhas(dados, \"B\")\n\n  grupo y\n1     B 4\n\n\nEste é um ponto importante: em geral, nossas funções simplesmente chamam outras funções prontas numa ordem específica e com argumentos específicos.\nO próximo é exemplo é um pouco mais sofisticado. Agora temos uma função que calcula a média geométrica de um vetor numérico. Neste caso há uma estrutura de if/else dentro do corpo da função que verifica o input antes de executar os cálculos. Especificamente, como a média geométrica é definida apenas para números positivos faz-se um teste para verificar se o input contém apenas números positivos.\nNote que as funções exp, mean e log são utilizadas sem o sinal :: pois estas funções do base-R. Isto é, são funções básicas, que permitem que o R funcione enquanto linguagem de programação, e estão sempre disponíveis.\nAlém disso, desta vez uso return para explicitamente declarar qual objeto a função deve retornar. Vale reforçar que o objeto z &lt;- exp(mean(log(x))) passa a existir somente dentro da função. Ou seja, ele não interfere com algum objeto z que exista ou que possa vir a existir no ambiente global.\n\ngeometric_mean &lt;- function(x) {\n  #&gt; Verifica o input\n  if (is.numeric(x) && all(x &gt; 0)) {\n    #&gt; Calcula a média geométrica\n    z &lt;- exp(mean(log(x)))\n    #&gt; Retorna o vetor z\n    return(z)\n    \n  } else {\n    #&gt; Retorna um erro\n    stop(\"Non-positive values found in x.\")\n    \n  }\n  \n}\n\ny &lt;- c(1, 4, 5, 10, 15, 6)\ngeometric_mean(y)\n#&gt; [1] 5.119318\nz &lt;- c(-1, 4, 5, 10, 15, 6)\ngeometric_mean(z)\n#&gt; Error in geometric_mean(z) : Non-positive values found in x.\n\nNa sua essência, a programação funcional enfatiza modularidade e previsibilidade. Quebra-se uma tarefa complexa em partes menores. Cada uma destas partes menores vira uma função individual com input e output bem estabelecidos. Códigos escritos desta maneira também funcionam melhor com processamento paralelo, ou seja, há também um ganho de eficiência.\nVamos reconstruir todos os passos acima usando funções."
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#shape-da-cidade-1",
    "title": "Cidades Brasil",
    "section": "Shape da cidade",
    "text": "Shape da cidade\nPor conveniência, eu repito abaixo o código que se usou para chegar no shape dos limites territoriais do Recife. Note o que acontece: (1) o primeiro passo filtra um data.frame para encontrar o código do IBGE do respectivo município; (2) usando o código, a função geobr::read_municipality() importa o shape.\nHá dois objetos importantes nesta função: a tabela que contém as informações de ‘nome da cidade’ e ‘código do ibge’; e o string com o ‘nome da cidade’.\n\ncode_muni = top200 |&gt; \n  filter(name_muni == \"Recife\") |&gt; \n  pull(code_muni)\n\nborder = geobr::read_municipality(code_muni)\n\nPara transformar o código acima numa função, basta incluir estes objetos como argumentos. Como a tabela usada é sempre a mesma coloca-se o objeto top200 dentro da função6.\n\nget_border = function(city) {\n  #&gt; Filtra a tabela top200 e encontra o código do município\n  code_muni = top200 |&gt; \n    dplyr::filter(name_muni == city) |&gt; \n    dplyr::pull(code_muni)\n  #&gt; Importa o shape do município\n  border = geobr::read_municipality(code_muni, showProgress = FALSE)\n  \n  return(border)\n  \n}\n\nA função get_border, definida acima, possui um único argumento city, o nome da cidade, e retorna um único objeto, border, um spatial data.frame que contém o shapefile com os limites territoriais do município."
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#principais-vias-1",
    "title": "Cidades Brasil",
    "section": "Principais vias",
    "text": "Principais vias\nO processo de transformar o código numa função consiste em entender quais são os elementos essenciais e circustanciais. No código abaixo o nome da cidade \"Recife, Pernambuco, Brazil\" é um elemento mutável enquanto opq(bbox = getbb(…)) é a parte essencial. Similarmente, todos os comandos abaixo de streets são essenciais, mas o nome do objeto rec é inteiramente acidental.\n\n#&gt; Define os \"limites\" da busca. Monta um bounding box em torno do Recife\nrec = opq(bbox = getbb(\"Recife, Pernambuco, Brazil\"))\n\n#&gt; Pega as principais vias\nstreets = add_osm_feature(\n  rec,\n  key = \"highway\",\n  value = c(\"primary\", \"secondary\", \"tertiary\", \"residential\")\n  )\n\n#&gt; Converte o objeto para sf (LINESTRING)\nstreets = osmdata_sf(streets)\nstreets = streets$osm_lines\nstreets = select(streets, osm_id, name)\nstreets = st_transform(streets, crs = 4674)\n#&gt; Encontra a intersecção entre as vias e os limites do município\nstreets_border = st_intersection(streets, border)\n\nVamos dividir o código acima em duas etapas. Na primeira, vamos usar o nome da cidade para encontrar o nome do estado do município. Na segunda, vamos encontrar todas as ruas da cidade selecionada.\nA função get_state é bastante simples: ela filtra a tabela que contém as informações das cidades e encontra o nome do estado do município selecionado.\n\nget_state = function(city) {\n  top200 |&gt; \n    dplyr::filter(name_muni == city) |&gt; \n    dplyr::pull(name_state)\n}\n\nget_state(\"Recife\")\n\n[1] \"Pernambuco\"\n\n\nA função get_streets usa o nome da cidade e o shape dos limites territoriais (gerado pela função get_border()) para encontrar todas as vias contidas dentro da cidade. Note como a função get_streets chama a função get_state.\n\nget_streets = function(city, border) {\n  \n  #&gt; Encontra o nome da Unidade Federativa\n  nome_uf = get_state(city)\n  #&gt; Monta o nome do local\n  name_place = stringr::str_glue(\"{city}, {nome_uf}, Brazil\")\n  #&gt; Monta a query\n  place = osmdata::opq(bbox = osmdata::getbb(name_place))\n  \n  #&gt; Importa todas as principais vias da cidade\n  streets = osmdata::add_osm_feature(\n    place,\n    key = \"highway\",\n    value = c(\"primary\", \"secondary\", \"tertiary\", \"residential\")\n    )\n  \n  #&gt; Converte o dado\n  streets = streets %&gt;%\n    osmdata::osmdata_sf() %&gt;%\n    .$osm_lines %&gt;%\n    dplyr::select(osm_id, name) %&gt;%\n    sf::st_transform(crs = 4674)\n  \n  #&gt; Enconrtra a intersecção entre as estradas e o limites do município\n  streets_border = sf::st_intersection(streets, border)\n  #&gt; Retorna o objeto streets_border\n  return(streets_border)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#altitude-1",
    "title": "Cidades Brasil",
    "section": "Altitude",
    "text": "Altitude\nO código que encontra a altitude da cidade é muito similar ao código que encontra as principais vias. Neste caso, contudo, pode ser interessante manter controle sobre o argumento z da função get_elev_raster. Este argumento controla o nível de resolução da imagem de altitude que se importa.\n\n\nCode\n#&gt; Importa os dados de altitude\naltitude = elevatr::get_elev_raster(border, z = 9, clip = \"bbox\")\n#&gt; Converte para 'vector'\nrec_alti = raster::rasterToPolygons(altitude)\nrec_alti = sf::st_as_sf(rec_alti)\nnames(rec_alti)[1] = \"elevation\"\n\n#&gt; Converte o CRS e intersecta com os limites do município\nrec_alti = rec_alti %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  st_intersection(border) %&gt;%\n  #&gt; Remove geometrias inválidas\n  filter(st_is_valid(.))\n\n\nA função get_elevation utiliza o shape dos limites do município e retorna um spatial data.frame com o grid retangular que contém os dados de altitude do município. O argumento z controla o nível de resolução e varia de 1 a 14. Quanto maior, maior será a resolução (o “zoom” da imagem), ou seja, mais detalhado (e mais pesado) será o resultado.\nNa definição da função, deixa-se o valor padrão pré-definido em z = 9. Isto significa que o argumento pode ser omitido pelo usuário.\n\nget_elevation = function(border, z = 9) {\n  #&gt; Importa os dados de altitude\n  altitude = elevatr::get_elev_raster(border, z = z, clip = \"bbox\")\n  #&gt; Converte para 'vector'\n  altitude = raster::rasterToPolygons(altitude)\n  altitude = sf::st_as_sf(altitude)\n  names(altitude)[1] = \"elevation\"\n  \n  #&gt; Converte o CRS e intersecta com os limites do município\n  altitude = sf::st_transform(altitude, crs = 4674)\n  altitude = suppressWarnings(sf::st_intersection(altitude, border))\n  altitude = dplyr::filter(altitude, sf::st_is_valid(altitude))\n  \n  return(altitude)\n  \n}\n\n\nClassificando\nPara classificar os dados utiliza-se o algoritmo de Jenks. As duas funções abaixo retornam esse agrupamento tomando um spatial data.frame como argumento principal. Eu adicionei alguns argumento adicionais que facilitam a escolha do número dos grupos e (opcionalmente) permite os limites dos grupos sejam arredondados para gerar número mais bonitos.\nA função add_jenks_breaks retorna uma lista com dois elementos: o primeiro elemento é o spatial data.frame, acrescido de uma coluna com o argupamento; o segundo elemento é um vetor character com a “legenda” do grupo, i.e., \"&gt;50\", \"50-100\", ….\n\nadd_jenks_breaks = function(shp, k = 7, round = FALSE) {\n  #&gt; Classifica os dados de altitude em k grupos segundo o algo. de Jenks\n  jbreaks = BAMMtools::getJenksBreaks(shp$elevation, k = k)\n  #&gt; Arredonda os números para chegar numa legenda menos quebrada\n  if (round) {\n    jbreaks[1] = floor(jbreaks[1])\n    jbreaks[length(jbreaks)] = ceiling(jbreaks)\n    jbreaks[2:(length(jbreaks) - 1)] = round(jbreaks)\n  }\n  #&gt; Cria a coluna 'jenks_group' que classifica cada valor num grupo\n  shp = shp |&gt; \n    dplyr::mutate(\n      jenks_group = findInterval(elevation, jbreaks, rightmost.closed = TRUE),\n      jenks_group = factor(jenks_group, labels = get_jenks_labels(jbreaks))\n    )\n  \n  #&gt; Verifica se todas as observações tem um grupo\n  check = any(is.na(shp$jenks_group))\n  if (check) {\n    warning(\"Some observations have failed to be grouped\")\n  }\n  \n  #&gt; Transforma os groups em legendas\n  labels = get_jenks_labels(jbreaks)\n  \n  #&gt; Retorna o output numa lista\n  out = list(shp = shp, labels = labels)\n  return(out)\n  \n}\n\nget_jenks_labels = function(x) {\n  labels = paste(x, x[-1], sep = \"–\")\n  labels[1] = paste(\"&lt;\", x[2])\n  labels = labels[1:(length(labels) - 1)]\n  return(labels)\n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes-1",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#juntando-as-partes-1",
    "title": "Cidades Brasil",
    "section": "Juntando as partes",
    "text": "Juntando as partes\nO código que junta o grid de altitude com as ruas usava um for-loop em cada um dos grupos. Costuma ser relativamente simples substituir loops por funções. Transforma-se a essência do loop numa função e aplica-se a esta função no objeto usando alguma função map_* ou lapply. No exemplo abaixo, a parte essencial é o código que filtra o grid de alitutude e faz a sua intersecção com o shape das ruas.\nFunções têm algumas vantanges sobre loops. Para rodar um loop quase sempre é necessário criar objetos temporários, que vão armazenar os resultados parciais do loop. Além disso, loops costumam ser mais lentos do que funções rodando em paralelo7. Loops mal construídos, em particular, podem ser bastante ineficientes.\n\n\nCode\n#&gt; Cria uma lista para gravar os resultados\nstreets_altitude &lt;- list()\n\n#&gt; For-loop em todos os elementos de jgroups\nfor (i in seq_along(jgroups)) {\n\n  #&gt; Seleciona um nível do grupo em particular  \n  group = levels(rec_alti$jenks_group)[[i]]\n  \n  #&gt; Encontra o subconjunto de do grid de altitude que corresponde a este grupo\n  poly = rec_alti %&gt;%\n    filter(jenks_group == group) %&gt;%\n    st_union(.) %&gt;%\n    st_as_sf()\n  #&gt; Faz a interseção deste subconjunto com as vias da cidade\n  joined = streets %&gt;%\n    st_intersection(poly) %&gt;%\n    filter(st_is_valid(.)) %&gt;%\n    mutate(level = factor(i))\n  #&gt; Grava o resultado como elemento da lista streets_altitude\n  streets_altitude[[i]] &lt;- joined\n  \n}\n\nrec_streets_altitude &lt;- bind_rows(streets_altitude)\n\n\nA função get_street_altitude abaixo pega o grid de altitude da cidade e o shape das principais vias e retorna um spatial data.frame único que contém as ruas da cidade agrupadas pela sua altura.\nNesta função, define-se uma função “auxiliar” join_streets que existe somente dentro do contexto da função get_streets_altitude. Aplica-se esta função em paralelo usando furrr::future_map8.\n\nget_streets_altitude = function(altitude, streets) {\n  \n  stopifnot(any(colnames(altitude) %in% \"jenks_group\"))\n  \n  #&gt; Encontra todos os grupos\n  groups = levels(altitude$jenks_group)\n  \n  #&gt; Define uma função auxiliar\n\n  #&gt; Esta função filtra o grid de altitude e faz a sua interseção\n  #&gt; com o shape das princiapis vias\n  join_streets = function(group) {\n    \n    poly = altitude %&gt;%\n      dplyr::filter(jenks_group == group) %&gt;%\n      sf::st_union(.) %&gt;%\n      sf::st_as_sf() %&gt;%\n      sf::st_make_valid()\n    \n    joined = suppressWarnings(sf::st_intersection(streets, poly))\n    \n    return(joined)\n    \n  }\n  #&gt; Aplica a função acima em todos os grupos em paralelo\n  street_levels = furrr::future_map(groups, join_streets)\n  #&gt; \"Empilha\" o objeto num único spatial data.frame\n  out = dplyr::bind_rows(street_levels, .id = \"level\")\n  \n  return(out)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#mapa",
    "title": "Cidades Brasil",
    "section": "Mapa",
    "text": "Mapa\nTransformar o mapa final numa função é bastante simples já que quase todos os argumentos das funções vão continuar exatamente iguais. A única diferença importante a se notar é na escolha da paleta de cores. Como o número de grupos de altitude pode mudar, faz sentido que a paleta de cores também se altere. Além disso, adiciono também uma opção de usar a fonte Roboto Condensed com o pacote showtext.\nNote que suprimo o uso de ggplot2:: para deixar o código menos carregado.\n\nmap_plot = function(shp, labels, title, showtext = TRUE) {\n  \n  cores = viridis::plasma(n = length(labels) + 1)\n  cores = cores[-length(cores)]\n  \n  font = ifelse(showtext == TRUE, \"Roboto Condensed\", \"sans\")\n  \n  plot =\n    ggplot(data = shp) +\n    geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n    scale_color_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = cores\n      ) +\n    scale_fill_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = cores\n      ) +\n    guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n    ggtitle(title) +\n    ggthemes::theme_map() +\n    coord_sf() +\n    theme(\n      plot.title = element_text(\n        size = 30,\n        hjust = 0.5,\n        family = font\n        ),\n      legend.title = element_text(\n        size = 20,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.text = element_text(\n        size = 14,\n        family = font,\n        color = \"gray10\"\n        ),\n      legend.position = \"top\",\n      legend.direction = \"horizontal\",\n      plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n    )\n  \n  return(plot)\n  \n}\n\n#&gt; Para adicionar a fonte\n# sysfonts::font_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\n# showtext::showtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#uma-função-final",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#uma-função-final",
    "title": "Cidades Brasil",
    "section": "Uma função final",
    "text": "Uma função final\nAgora que temos funções para cada uma das principais tarefas, podemos criar uma última função que vai executar estas funções na ordem apropriada.\nA função map_altitude utiliza apenas o nome da cidade para gerar o mapa de altitude. Opcionalmente, pode-se alterar os argumentos k, que altera o número de grupos e z que aumenta/diminui a resolução do mapa de altura.\nComo esta função executa vários passos intermediários, e alguns destes podem ser bastante demorados, eu incluo algumas mensagens que informam sobre o andamento da função. O resultado final é armazenado numa lista, que retorna alguns dos objetos intermediários como o shape de altitude.\n\nmap_altitude = function(city, k = 6, z = 7) {\n  \n  #&gt; Importa o shape do limite do município\n  message(\"Importando os limites do município: \", city)\n  city_border = get_border(city)\n  #&gt; Importa as principais vias da cidade e junta com o limite do muni\n  message(\"Importando as vias.\")\n  city_street = get_streets(city, city_border)\n  #&gt; Importa a altitude da cidade\n  message(\"Importando a altitude.\")\n  city_elevation = suppressMessages(get_elevation(city_border, z = z))\n  #&gt; Classifica a altitude em grupos\n  message(\"Classificando e juntando os shapefiles.\")\n  jenks = add_jenks_breaks(city_elevation, k = k)\n  city_elevation = jenks[[\"shp\"]]\n  labels = jenks[[\"labels\"]]\n  #&gt; Junta a altitude (agrupada) com as vias\n  city_street_elevation = get_streets_altitude(city_elevation, city_street)\n  \n  #&gt; Monta o mapa final\n  message(\"Gerando o mapa final.\")\n  plot = map_plot(city_street_elevation, labels = labels, title = city)\n  message(\"Feito.\")\n  #&gt; Retorna o output numa lista\n  out = list(\n    shp = city_street_elevation,\n    streets = city_street,\n    elevation = city_elevation,\n    plot = plot\n    )\n  \n  return(out)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#testando-a-função",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#testando-a-função",
    "title": "Cidades Brasil",
    "section": "Testando a função",
    "text": "Testando a função\nFeito tudo isto. Agora podemos testar a função com outras cidades.\n\nSão Caetano do Sul\nSão Caetano do Sul é uma boa cidade teste, porque ela é bem pequena então os resultados não demoram tanto para carregar.\n\nscs = map_altitude(\"São Caetano Do Sul\", k = 4, z = 8)\nscs$plot\n\n\n\n\n\n\n\n\nOs demais mapas gerados foram feitos em resolução bastante elevada, então os códigos podem demorar várias horas para executar. Recomendo começar testando com valores menores de z. Mesmo em alta resolução, há melhorias possíveis nos gráficos como mudanças no “enquadramento” do mapa e também na escolha das quebras da legenda. Ainda assim, o resultado é muito bom.\n\n\nFortaleza\n\nfortaleza = map_altitude(\"Fortaleza\", z = 13)\nfortaleza$plot\n\n\n\n\n\n\n\n\nOsasco\n\nosa &lt;- map_altitude(\"Osasco\", k = 7, z = 12)\nosa$plot\n\n\n\n\n\n\n\n\nBelo Horizonte\n\nmap_altitude(\"Belo Horizonte\", k = 7, z = 13)$plot\n\n\n\n\n\n\n\n\nPorto Alegre\n\nmap_altitude(\"Porto Alegre\", k = 6, z = 13)\n\n\n\n\n\n\n\n\nCuritiba\n\nmap_altitude(\"Curitiba\", k = 8, z = 11)\n\n\n\n\n\n\n\n\nBrasília\n\nmap_altitude(\"Brasília\", k = 7, z = 11)"
  },
  {
    "objectID": "posts/general-posts/2023-10-mapas-altitude/index.html#footnotes",
    "href": "posts/general-posts/2023-10-mapas-altitude/index.html#footnotes",
    "title": "Cidades Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChamar muitos pacotes numa mesma sessão pode levar a muitos conflitos entre funções com o mesmo nome. Isto não é um problema muito sério já que sempre é possível especificar nome_pacote::nome_funcao.↩︎\nNa verdade, o código encontra todos os elementos com a classe table na página.↩︎\nPara uma introdução aos tipos de objetos espaciais (raster e vector) veja Lovelace (2023).↩︎\nExistem alguns métodos que ajudam a escolher um número “ótimo” de clusters, como o “elbow method” mas vale lembrar que clustering é muito mais arte do que ciência. Clustering envolve agrupar dados semelhantes em um número finito de grupos, mas há inúmeras maneiras de definir “semelhante”; além disso, o algoritmo de clustering sempre chega num agrupamento, qualquer que seja a escolha do número de grupos. Assim, é importante frisar que estes resultados são mais explortatórios, por assim dizer.↩︎\nDeclarar explicitamente o pacote utilizado é a melhor maneira de escrever novas funções. Isto garante que a função vai funcionar em qualquer contexto onde o pacote estiver instalado. A longo prazo, isto também permite que se use a mesma função em outro projeto ou até mesmo dentro de um novo pacote. Vale notar que algumas funções base como mean, round, findInterval, etc. não precisam ser declaradas pois elas fazem parte do base-R.↩︎\nNote que isto torna a função limitada, pois temos dados somente das 200 cidades mais populosas do Brasil. Outro fator importante é que estamos usando o nome da cidade como input. Se a tabela fosse expandida para incluir todas as cidades do Brasil seria necessário alterar o argumento pois há cidades com nome duplicado.\nEste tipo de generalização está fora do escopo deste post.↩︎\nEm geral, desde que o loop seja bem feito ele não será muito lento. Para consultar boas práticas de como fazer bons loops consulte Best Coding Practices for R. Uma comparação recente da velocidade de loops no R está disponível em On the performance of for loops in R.↩︎\nAlternativamente, pode-se usar parallel::mclapply, mas esta função, infelizmente só funciona em sistemas MacOs e Linux.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-brasilia/index.html",
    "href": "posts/general-posts/2023-09-brasilia/index.html",
    "title": "Mapa de altitude de ruas de Brasília",
    "section": "",
    "text": "Inspirado num antigo post de BlakeRMills, criador do pacote {MetBrewer}, criei um mapa com a altitude das ruas em Brasília.\nEm breve farei um post com tutorial detalhado e também pretendo replicar este tipo de mapa para outras cidades interessantes. O código para replicar o gráfico está abaixo.\n\n\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(osmdata)\nlibrary(purrr)\nsf::sf_use_s2(FALSE)\nsysfonts::font_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nshowtext::showtext_auto()\n\nurl = \"https://pt.wikipedia.org/wiki/Lista_de_municípios_do_Brasil_por_população\"\n\ntab = xml2::read_html(url) |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(2)\n\nas_numeric_char = Vectorize(function(x) {\n  ls = stringr::str_extract_all(x, \"[[:digit:]]\")\n  y = paste(ls[[1]], collapse = \"\")\n  as.numeric(y)\n})\n\nclean_tab = tab |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    code_muni = codigo_ibge,\n    name_muni = municipio,\n    rank = posicao,\n    name_state = unidade_federativa,\n    pop = populacao\n  ) |&gt; \n  filter(name_muni != \"Brasil\") |&gt; \n  mutate(\n    code_muni = as.numeric(code_muni),\n    pop = as_numeric_char(pop),\n    rank = rank(-pop)\n  )\n\ntop20 = slice_max(clean_tab, pop, n = 20)\n\nget_border = function(city) {\n  \n  #&gt; Encontra o código do município\n  code_muni = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(code_muni) |&gt; \n    unique()\n  \n  stopifnot(length(code_muni) == 1)\n  \n  #&gt; Baixa o shapefile do município\n  border = geobr::read_municipality(code_muni, showProgress = FALSE)\n  \n  return(border)\n}\n\nget_streets = function(city, border) {\n  \n  #&gt; Encontra o nome da Unidade Federativa\n  nome_uf = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(name_state)\n  #&gt; Monta o nome do local\n  name_place = stringr::str_glue(\"{city}, {nome_uf}, Brazil\")\n  #&gt; Monta a query\n  place = opq(bbox = getbb(name_place))\n  \n  #&gt; Importa todas as principais vias da cidade\n  # streets = add_osm_feature(\n  #   place,\n  #   key = \"highway\",\n  #   value = c(\n  #     \"motorway\", \"primary\", \"motorway_link\", \"primary_link\",\n  #     \"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\",\n  #     \"residential\"\n  #     )\n  # )\n  \n  streets = add_osm_feature(place, key = \"highway\")\n  \n  #&gt; Converte o dado\n  streets = streets %&gt;%\n    osmdata_sf() %&gt;%\n    .$osm_lines %&gt;%\n    select(osm_id, name) %&gt;%\n    st_transform(crs = 4674)\n  \n  #&gt; Enconrtra a intersecção entre as estradas e o limites do município\n  streets_border = st_intersection(streets, border)\n  \n  # out = list(streets = streets, streets_border = streets_border)\n  \n  return(streets_border)\n  \n}\n\nget_elevation = function(border, z = 8) {\n  \n  altitude = elevatr::get_elev_raster(border, z = z, clip = \"bbox\")\n  altitude = raster::rasterToPolygons(altitude)\n  altitude = st_as_sf(altitude)\n  names(altitude)[1] = \"elevation\"\n  \n  altitude = st_transform(altitude, crs = 4674)\n  altitude = suppressWarnings(st_intersection(altitude, border))\n  altitude = filter(altitude, st_is_valid(altitude))\n  \n  return(altitude)\n  \n}\n\nadd_jenks_breaks = function(shp, k = 7, round = TRUE, r = 0) {\n  #&gt; Classifica os dados de altitude em k grupos segundo o algo. de Jenks\n  jbreaks = BAMMtools::getJenksBreaks(shp$elevation, k = k)\n  #&gt; Arredonda os números para chegar numa legenda menos quebrada\n  if (round) {\n    jbreaks = round(jbreaks, r)\n  }\n  #&gt; Cria a coluna 'jenks_group' que classifica cada valor num grupo\n  shp = mutate(shp, jenks_group = cut(elevation, jbreaks))\n  \n  #&gt; Verifica se todas as observações tem um grupo\n  check = any(is.na(shp$jenks_group))\n  if (check) {\n    warning(\"Some observations have failed to be grouped\")\n  }\n  \n  #&gt; Transforma os groups em legendas\n  labels = get_jenks_labels(jbreaks)\n  \n  #&gt; Retorna o output numa lista\n  out = list(shp = shp, labels = labels)\n  return(out)\n  \n}\n\nget_jenks_labels = function(x) {\n  labels = paste(x, x[-1], sep = \"–\")\n  labels[1] = paste(\"&lt;\", x[2])\n  labels[length(labels)] = paste(\"&gt;\", max(x))\n  return(labels)\n}\n\nget_streets_altitude = function(altitude, streets) {\n  \n  stopifnot(any(colnames(altitude) %in% \"jenks_group\"))\n  \n  #&gt; Get all groups\n  groups = levels(altitude$jenks_group)\n  \n  #&gt; For each group get the full polygon and join with streets\n  join_streets = function(group) {\n    \n    poly = altitude %&gt;%\n      filter(jenks_group == group) %&gt;%\n      st_union(.) %&gt;%\n      st_as_sf() %&gt;%\n      st_make_valid()\n    \n    joined = suppressWarnings(st_intersection(streets, poly))\n    \n    return(joined)\n    \n  }\n  #&gt; Apply the function to all groups\n  street_levels = purrr::map(groups, join_streets)\n  #&gt; Bind all results together\n  out = bind_rows(street_levels, .id = \"level\")\n  \n  return(out)\n  \n}\n\nmap_plot = function(shp, labels, title, showtext = TRUE) {\n  \n  colors = viridis::plasma(n = length(labels) + 1)\n  colors = colors[-length(colors)]\n  \n  font = ifelse(showtext == TRUE, \"Roboto Condensed\", \"sans\")\n  \n  plot =\n    ggplot(data = shp) +\n    geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n    scale_color_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    scale_fill_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n    ggtitle(title) +\n    ggthemes::theme_map() +\n    coord_sf() +\n    theme(\n      plot.title = element_text(\n        size = 30,\n        hjust = 0.5,\n        family = font\n      ),\n      legend.title = element_text(\n        size = 20,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.text = element_text(\n        size = 14,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.position = \"top\",\n      legend.direction = \"horizontal\",\n      plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n    )\n  \n  return(plot)\n  \n}\n\nmap_altitude = function(city, k, z) {\n  \n  #&gt; Importa o shape do limite do município\n  message(\"Importando os limites do município: \", city)\n  city_border = get_border(city)\n  #&gt; Importa as principais vias da cidade e junta com o limite do muni\n  message(\"Importando as vias.\")\n  city_street = get_streets(city, city_border)\n  #&gt; Importa a altitude da cidade\n  message(\"Importando a altitude.\")\n  city_elevation = suppressMessages(get_elevation(city_border, z = z))\n  #&gt; Classifica a altitude em grupos\n  message(\"Classificando e juntando os shapefiles.\")\n  jenks = add_jenks_breaks(city_elevation, k = k)\n  city_elevation = jenks[[\"shp\"]]\n  labels = jenks[[\"labels\"]]\n  #&gt; Junta a altitude (agrupada) com as vias\n  city_street_elevation = get_streets_altitude(city_elevation, city_street)\n  \n  #&gt; Monta o mapa final\n  message(\"Gerando o mapa final.\")\n  plot = map_plot(city_street_elevation, labels = labels, title = city)\n  message(\"Feito.\")\n  #&gt; Retorna o output numa lista\n  out = list(\n    shp = city_street_elevation,\n    streets = city_street,\n    elevation = city_elevation,\n    plot = plot\n  )\n  \n  return(out)\n  \n}\n\nexport_citymap = function(city, w = 14, h = 16, ...) {\n  \n  plot = map_altitude(city, ...)$plot\n  \n  if (is.numeric(city)) {\n    name_city = cities_brasil |&gt; \n      filter(code_muni == city) |&gt; \n      pull(name_muni)\n  } else if (is.character(city)) {\n    name_city = city\n  }\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.svg\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.png\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n}\n\nsafe_export = purrr::safely(export_citymap)\n\nparams = tribble(\n  ~city,       ~z, ~k,\n  #----------------#---#---#\n  \"São Paulo\",       8,  7,\n  \"Rio de Janeiro\",  9,  7,\n  \"Brasília\",        8,  7,\n  \"Fortaleza\",      11,  7,\n  \"Salvador\",       10,  7,\n  \"Belo Horizonte\", 10,  7,\n  \"Manaus\",          6,  7,\n  \"Curitiba\",       10,  7,\n  \"Recife\",         10,  7,\n  \"Goiânia\",         8,  7,\n  \"Porto Alegre\",    8,  7,\n  \"Belém\",           9,  7,\n  \"Guarulhos\",       9,  7,\n  \"Campinas\",        9,  7,\n  \"São Luís\",        9,  7,\n  \"Maceió\",         11,  8,\n  \"Campo Grande\",    7,  7,\n  \"São Gonçalo\",     8,  7,\n  \"Teresina\",        10,  7,\n  \"João Pessoa\",     9,  7,\n  \"Joinville\",       9,  7\n  )\n\n#&gt; Exportar todas as cidades listadas acima\n#pmap(params, safe_export)\n#&gt; Exportar uma cidade em particular\n#safe_export(\"Brasília\", z = 8, k = 7)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html",
    "href": "posts/ggplot2-tutorial/8-lollipop.html",
    "title": "Indo Além: Lollipops",
    "section": "",
    "text": "Os gráficos de lollipop (pirulito) consistem de barras (colunas) com círculos no topo, que representam os valores das observações. Visualmente, elas lembram pirulitos. Eles são utilizados tanto para substituir gráficos de coluna convencionais, como para destacar e comparar valores entre diferentes categorias ou momentos no tempo. Estes gráficos também funcionam bem para fazer rankings de valores, quando há várias categorias."
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#exemplo",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#exemplo",
    "title": "Indo Além: Lollipops",
    "section": "Exemplo",
    "text": "Exemplo\nPrimeiro vamos montar uma base de dados simulada. Vamos usar a função tibble para montar uma base de dados usando o código abaixo.\n\n# Monta uma base de dados usando o tibble\ndados &lt;- tibble(\n  categoria = factor(c(\"A\", \"B\", \"C\", \"D\", \"E\")),\n  vendas_2020 = c(40, 10, 15, 30, 20),\n  vendas_2021 = c(55, 50, 30, 35, 30)\n)\n\nA função factor no código acima indica que a variável categoria deve ser tratada como uma variável categórica. Temos 5 grupos distintos (A, B, C, D e E) na coluna categoria. As colunas vendas_2020 e vendas_2021 representam o total de vendas nos anos de 2020 e 2021, respectivamente. A base de dados tem a seguinte forma.\n\n\n\n\n\ncategoria\nvendas_2020\nvendas_2021\n\n\n\n\nA\n40\n55\n\n\nB\n10\n50\n\n\nC\n15\n30\n\n\nD\n30\n35\n\n\nE\n20\n30\n\n\n\n\n\n\n\nNão existe uma função geom_lollipop. Para montar um lollipop vamos combinar dois geoms: o geom_segment com o geom_point. A função geom_segment() desenha segmentos de linha e exige quatro argumentos: x, xend, y e yend, que especificam onde a linha começa e onde ela termina. Já a função geom_point() exige apenas os argumentos x e y que especificam a posição dos pontos.\nO código abaixo cria um gráfico de lollipop com a base de dados criada anteriormente.\n\n# Monta o gráifco\nggplot(data = dados) +\n  # Desenha o a linha reta\n  geom_segment(aes(x = categoria, xend = categoria, y = 0, yend = vendas_2020)) +\n  # Desenha os pontos\n  geom_point(aes(x = categoria, y = vendas_2020), size = 3)\n\nVamos destrinchar o código acima em partes.\nPrimeiro fazemos a chamada do ggplot para começar o gráfico. Incluímos o argumento data = dados pois ambas as figuras vão utilizar a mesma base de dados.\nPor força de ser didático, deixamos todos os argumentos explícitos. Assim, a função geom_segment que desenha o segmento de reta precisa de 4 argumentos x, xend, y e yend. Como queremos uma linha reta na vertical, deixamos o valor de x igual ao de xend. O valor y = 0 especifica que a linha deve iniciar no zero (no eixo-y) e yend = vendas_2020 indica que a linha deve se estender até o valor de vendas_2020.\nA função geom_point precisa apenas dos argumentos x e y que definem as coordenadas de cada ponto. Assim x = categoria indica a coordenada horizontal e y = vendas_2020 indica a coordenada vertical. O argumento size = 3 aumenta o tamanho do círculo (opcional, mas recomendado).\nO resultado do gráfico é apresentado abaixo.\n\n\n\n\n\n\n\n\n\nVale notar que os argumentos x e y são compartilhados entre tanto o geom_segment como o geom_point de tal forma que poderíamos simplificar o código, fazendo uma pequena adaptação nos argumentos y e yend da seguinte maneira:\n\n# Código mais sucinto\nggplot(data = dados, aes(x = categoria, xend = categoria, y = vendas_2020, yend = 0)) +\n  geom_segment() +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\nAté aqui, o gráfico de lollipop parece muito similar a um gráfico de colunas, com as desvantagens de ser mais complexo e de exigir mais linhas de código. O exemplo abaixo deixa isto evidente.\n\n# Código para gerar o lollipop chart\nggplot(data = dados, aes(x = categoria, xend = categoria, y = vendas_2020, yend = 0)) +\n  geom_segment() +\n  geom_point(size = 3)\n\n# Código para gerar um gráfico de colunas equivalente\nggplot(data = dados, aes(x = categoria, y = vendas_2020)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nDe fato, o gráfico de lollipop pode ser utilizado para mostrar valores, mas uma das suas principais vantanges está em poder comparar valores entre grupos. No nosso caso, temos os valores das vendas de 2020 e 2021 de cinco grupos distinos (A, B, C, D, e E). Podemos montar um gráfico que enfatiza a evolução das vendas em cada um dos grupos.\nPode-se adaptar o gráfico para ressaltar a diferença nas vendas entre os anos. Agora fica evidente, por exemplo, que as vendas aumentaram em todos os grupos entre 2020 e 2021. Além disso, o grupo B teve o maior crescimento no volume das vendas.\n\nggplot(data = dados, aes(x = categoria)) +\n  # Segmento de reta que liga os pontos\n  geom_segment(aes(xend = categoria, y = vendas_2020, yend = vendas_2021)) +\n  # Ponto (verde) que representa as vendas de 2020\n  geom_point(aes(y = vendas_2020), color = \"#2A9D8F\", size = 3) +\n  # Ponto (amarelo) que representa as vendas de 2021\n  geom_point(aes(y = vendas_2021), color = \"#E9C46A\", size = 3)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#refinando-o-gráfico",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#refinando-o-gráfico",
    "title": "Indo Além: Lollipops",
    "section": "Refinando o gráfico",
    "text": "Refinando o gráfico\nO gráfico acima é bom, mas podemos melhorá-lo modificando alguns parâmetros estéticos. Vamos fazer estas melhorias em três etapas:\n\nAlterar a cor do segmento de linha\nAjustar os eixos\nVirar o gráfico e alterar a cor do fundo\n\n\nAlterar a cor do segmento de linha\nPrimeiro vamos mudar a cor do segmento de reta com o argumento color dentro de geom_segment(). Vamos escolher um tom mais escuro de cinza.\n\nggplot(data = dados, aes(x = categoria)) +\n  geom_segment(\n    aes(xend = categoria, y = vendas_2020, yend = vendas_2021),\n    # Ajuste a cor da linha\n    color = \"gray30\") +\n  geom_point(aes(y = vendas_2020), color = \"#2A9D8F\", size = 3) +\n  geom_point(aes(y = vendas_2021), color = \"#E9C46A\", size = 3) \n\n\n\n\n\n\n\n\n\n\nAjustar os eixos\nDepois, vamos ajustar o eixo-y, usando scale_y_continuous(), para iniciar no zero modificando o argumento limits. Este argumento aceita dois valores: o primeiro valor define o limite inferior e o segundo valor define o limite superior. Para deixar qualquer um dos limites “livre” basta defini-lo como NA. Assim limits = c(0, NA) força o eixo a começar no zero e deixa o limite superior “livre” (i.e. definido automaticamente).\nJá o argumento breaks define quais os números que devem ser destacados no texto do eixo. Definimos este valor usando breaks = seq(0, 50, 10) que cria uma sequência de 0 a 50, de 10 em 10.\nO eixo-y foi automaticamente nomeado como “vendas_2020” o que pode causar certa confusão. Além disso o título do eixo-x, “categoria” é um pouco redundante. Modificamos estes nomes utilizando a função labs(). Definimos y = \"Vendas (unidades)\" e x = NULL para omitir o título no eixo-x.\n\nggplot(data = dados, aes(x = categoria)) +\n  geom_segment(\n    aes(xend = categoria, y = vendas_2020, yend = vendas_2021),\n    color = \"gray30\") +\n  geom_point(aes(y = vendas_2020), color = \"#2A9D8F\", size = 3) +\n  geom_point(aes(y = vendas_2021), color = \"#E9C46A\", size = 3) +\n  # Ajusta o eixo-y\n  scale_y_continuous(\n    # Quebras de linha\n    breaks = seq(0, 50, 10),\n    # Limites do eixo (NA = livre): força o gráfico a começar no 0\n    limits = c(0, NA)) +\n  # Altera nome/título dos eixos\n  labs(\n    x = NULL,\n    y = \"Vendas (unidades)\")\n\n\n\n\n\n\n\n\n\n\nVirar o gráfico e alterar a cor do fundo\nPor fim, vamos virar o gráfico de lado usando a função coord_flip() e, em seguida, definir um tema simples com fundo branco usando theme_white(). Para mais opções de temas consulte, por exemplo, o pacote ggthemes ou veja o post da série.\nO código abaixo reúne todas estas melhorias.\n\nggplot(data = dados, aes(x = categoria)) +\n  # Segmento de reta que liga os pontos\n  geom_segment(\n    aes(xend = categoria, y = vendas_2020, yend = vendas_2021),\n    # Ajuste a cor da linha\n    color = \"gray30\") +\n  # Ponto (verde) que representa as vendas de 2020\n  geom_point(aes(y = vendas_2020), color = \"#2A9D8F\", size = 3) +\n  # Ponto (amarelo) que representa as vendas de 2021\n  geom_point(aes(y = vendas_2021), color = \"#E9C46A\", size = 3) +\n  # Ajusta o eixo-y\n  scale_y_continuous(\n    # Quebras de linha\n    breaks = seq(0, 50, 10),\n    # Limites do eixo (NA = livre): força o gráfico a começar no 0\n    limits = c(0, NA)) +\n  # Altera nome/título dos eixos\n  labs(\n    x = NULL,\n    y = \"Vendas (unidades)\") +\n  # Inverte o gráfico (vira ele de lado)\n  coord_flip() +\n  # Tema minimalista com fundo branco\n  theme_light()\n\n\n\n\n\n\n\n\nQuem já tem certo domínio do ggplot2 deve ter percebido que falta uma legenda no gráfico acima. De fato, da maneira como nossos dados estão estruturados, não é possível ter uma legenda de cores que explique que os círculos verdes são referentes às vendas de 2020, enquanto os círculos amarelos são referentes às vendas de 2021.\nGerar esta legenda não é tão simples e exigiria, provavelmente, dois tibble: um para representar os pontos e outro para representar a reta que liga os pontos. No código abaixo conseguimos fazer isto numa simples linha de código utilizando a função tidyr::pivot_longer.\nNote que agora a chamada inicial do ggplot está vazia e especificamos o argumento data dentro de cada função geom_*. O controle da legenda de cores é feito pela função scale_color_manual().\n\n\nCode\n# Converte a base para o formato longitudinal\ntbl_pontos &lt;- tidyr::pivot_longer(dados, vendas_2020:vendas_2021)\n\nggplot() +\n  # Define o segmento de reta que liga os pontos\n  geom_segment(\n    data = dados,\n    aes(x = categoria, xend = categoria, y = vendas_2020, yend = vendas_2021)\n  ) +\n  # Desenha os pontos \n  geom_point(\n    data = tbl_pontos,\n    aes(x = categoria, y = value, color = name),\n    size = 3\n  ) +\n  # Controla as cores e a legenda\n  scale_color_manual(\n    # Título da legenda\n    name = \"Vendas no Ano\",\n    # Cores dos pontos\n    values = c(\"#2A9D8F\", \"#E9C46A\"),\n    # Texto da legenda\n    labels = c(\"2020\", \"2021\")\n  ) +\n  # Ajusta o texto do eixo-y\n  scale_y_continuous(\n    breaks = seq(0, 50, 10),\n    limits = c(0, NA)) +\n  # Altera nome/título dos eixos\n  labs(\n    x = NULL,\n    y = \"Vendas (unidades)\") +\n  # Inverte o gráfico (vira ele de lado)\n  coord_flip() +\n  # Tema minimalista com fundo branco\n  theme_light() +\n  # Coloca a legenda na parte inferior do gráfico\n  theme(\n    legend.position = \"bottom\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#formato-dos-dados",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#formato-dos-dados",
    "title": "Indo Além: Lollipops",
    "section": "Formato dos dados",
    "text": "Formato dos dados\nHá uma maneira de contornar a necessidade de ter duas bases de dados distintas (uma em formato wide e outra em formato long), substituindo a função geom_segment() pela função geom_line()1. A função geom_line() é tipicamente utilizada para desenhar gráficos de séries de tempo, mas também pode ser utilizada para desenhar simples linhas retas.\nO código abaixo exemplifica como isto pode ser feito. Note o uso do argumento group = categoria dentro da função geom_line().\nApesar de mais simples nestre caso, a função geom_line() é menos versátil para montar gráficos de lollipop. Não é possível replicar o primeiro gráfico que fizemos, com comportamento similar a um gráfico de colunas, usando esta função, por exemplo.\n\nggplot(tbl_pontos, aes(x = categoria, y = value)) +\n  geom_line(aes(group = categoria), color = \"gray30\") +\n  geom_point(aes(color = name), size = 3) +\n  coord_flip()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#alterando-a-ordem-dos-elementos",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#alterando-a-ordem-dos-elementos",
    "title": "Indo Além: Lollipops",
    "section": "Alterando a ordem dos elementos",
    "text": "Alterando a ordem dos elementos\nPor fim, um último ponto importante é a ordem da variável categórica. No primeiro gráfico que fizemos a variável categórica foi exposta da esquerda para a direita, como seria mais natural para leitores ocidentais. Contudo, quando usamos a função coord_flip() para virar o gráfico, a variável categórica agora vai de baixo para cima o que acaba sendo confuso.\nO exemplo abaixo ilustra o problema: no gráfico da direita a ordem do eixo-y começa no E e termina no A.\n\n\nCode\n# Gráfico na horizontal com a variável categórica da esquerda para a direita\nggplot(\n  data = dados,\n  aes(x = categoria, xend = categoria, y = vendas_2020, yend = 0)\n  ) +\n  geom_segment() +\n  geom_point(size = 3)\n\n# Gráfico na vertical com a variável categórica de baixo para cima\nggplot(\n  data = dados,\n  aes(x = categoria, xend = categoria, y = vendas_2020, yend = 0)\n  ) +\n  geom_segment() +\n  geom_point(size = 3) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nA maneira mais segura e consistente de remediar isto é alterar a base de dados. Especificamente, deve-se interveter a ordem da variável categórica. Há muitas maneiras de fazer isso usando as funções factor() e rev(). Felizmente, o pacote forcats também tem uma função feita justamente para inverter a ordem de uma variável factor. O código abaixo utiliza justamente a função forcats::fct_rev(). Note que agora a ordem da variável categoria está de cima para baixo, de A até E.\n\ndados &lt;- mutate(dados, categoria = forcats::fct_rev(categoria))\n\n# Agora a variável categórica está ordenada de cima para baixo\nggplot(\n  data = dados,\n  aes(x = categoria, xend = categoria, y = vendas_2020, yend = 0)\n  ) +\n  geom_segment() +\n  geom_point(size = 3) +\n  coord_flip()\n\n\n\n\n\n\n\n\nPode-se também fazer a ordem dos elementos categóricos seguir a variável numérica que se quer representar. Neste caso, usa-se a função forcats::fct_reorder.\n\ndados &lt;- mutate(\n  dados,\n  categoria = forcats::fct_reorder(categoria, vendas_2020)\n  )\n\n# Agora a variável categórica está ordenada segundo vendas_2020\nggplot(\n  data = dados,\n  aes(x = categoria, xend = categoria, y = vendas_2020, yend = 0)\n  ) +\n  geom_segment() +\n  geom_point(size = 3) +\n  coord_flip()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#expectativa-de-vida-nas-américas",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#expectativa-de-vida-nas-américas",
    "title": "Indo Além: Lollipops",
    "section": "Expectativa de vida nas Américas",
    "text": "Expectativa de vida nas Américas\nVamos montar um gráfico de lollipop que mostra a evolução da expectativa de vida nos países americanos no maior horizonte de tempo possível. Primeiro, temos de preparar os dados para nossa visualização.\nComo os dados já estão no formato longitudinal, vamos utilizar esta base para plotar os pontos do gráfico. O código abaixo usa a função filter para encontrar apenas as linhas referentes a países do continente americano (inclui América do Sul, Central e do Norte) no primeiro e o último ano disponível.\nDepois, o código inverte a ordem alfabética dos países usando forcats::fct_rev() dentro da função mutate. Isto é útil pois quando a função coord_flip() “vira” o gráfico, a ordem da variável categórica acaba ficando invertida (como nos gráficos que fizemos acima).\n\namericas &lt;- gapminder |&gt; \n  filter(\n    continent == \"Americas\",\n    year == min(year) | year == max(year)) |&gt; \n  mutate(country = forcats::fct_rev(country))\n\nPara converter os dados no formato necessário da função geom_segment usamos a função pivot_wider(). Os argumentos podem parecer confusos à primeira vista. Essencialmente, estamos indicando que cada linha deve ser um país, que as colunas devem representar os valores dos anos (1952 e 2007) e que os valores destas colunas estão em lifeExp. Vale consultar ?pivot_wider.\n\namericas_wide &lt;- tidyr::pivot_wider(\n  americas,\n  # Cada linha é um país\n  id_cols = \"country\",\n  # As novas colunas devem representar os anos de 1952 e 2007\n  names_from = \"year\",\n  # Os valores destas novas colunas estão na coluna lifeExp\n  values_from = \"lifeExp\",\n  # Concatena o stringr 'life_exp_' no nome das novas colunas\n  names_prefix = \"life_exp_\"\n  )\n\nO código completo para o gráfico segue abaixo.\n\n\nCode\nggplot() +\n  geom_segment(\n    data = americas_wide,\n    aes(x = country, xend = country, y = life_exp_1952, yend = life_exp_2007),\n    color = \"gray30\") +\n  geom_point(\n    data = americas,\n    aes(x = country, y = lifeExp, color = as.factor(year)),\n    size = 2) +\n  scale_color_brewer(name = \"\", type = \"qual\", palette = 6) +\n  labs(\n    title = \"Aumento da Expectativa de Vida\",\n    subtitle = \"Expectativa de vida ao nascer em 1952 e em 2007 nos países americanos.\",\n    caption = \"Fonte: Gapminder.\",\n    x = NULL,\n    y = \"Expectativa de vida (Anos)\") +\n  coord_flip() +\n  theme_light() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#expectativa-de-vida-em-relação-à-média",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#expectativa-de-vida-em-relação-à-média",
    "title": "Indo Além: Lollipops",
    "section": "Expectativa de vida em relação à média",
    "text": "Expectativa de vida em relação à média\nPode-se também montar um gráfico que compara a expectativa de vida nos países em relação à média mundial. O código abaixo pega as observações do ano mais recente e calcula o desvio em relação à média de cada país. Para tornar a visualização mais limpa, seleciona-se um subconjunto de 20 países.\n\ngap07 &lt;- gapminder |&gt; \n  filter(year == max(year)) |&gt; \n  mutate(\n    diff = lifeExp - mean(lifeExp),\n    country = fct_reorder(country, diff)\n    )\n\ncountry_sel &lt;- c(\n  \"Swaziland\", \"Rwanda\", \"Botswana\", \"Kenya\", \"Haiti\", \"India\", \"Brazil\",\n  \"Turkey\", \"Mexico\", \"United States\", \"Austria\", \"France\", \"Japan\", \"Taiwan\",\n  \"Argentina\", \"Sri Lanka\", \"Egypt\", \"Iraq\", \"Nigeria\", \"Afghanistan\"\n  )\n\nsub07 &lt;- filter(gap07, country %in% country_sel)\n\nNo gráfico abaixo, além de incluir uma linha vertical para sinalizar a média mundial com geom_hline, também coloco uma pequena nota de texto para destacar que a expectativa de vida média mundial era de 67 anos2.\n\n\nCode\ndftext &lt;- tibble(\n  x = \"Kenya\",\n  y = 8,\n  label = \"67 anos de vida\\né expectativa de\\nvida média do mundo.\"\n  )\n\nggplot(data = sub07) +\n  geom_segment(\n    aes(x = country, xend = country, y = diff, yend = 0),\n    color = \"gray30\"\n  ) +\n  geom_point(\n    aes(x = country, y = diff, color = continent),\n    size = 3\n  ) +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    data = dftext,\n    aes(x = x, y = y, label = label),\n    size = 3\n  ) +\n  scale_y_continuous(limits = c(-28, 28)) +\n  scale_color_manual(\n    name = \"Continente\",\n    values = c(\"#264653\", \"#e9c46a\", \"#2a9d8f\", \"#e76f51\")\n    ) +\n  labs(\n    title = \"Expectativa de Vida\",\n    subtitle = \"Expectativa de vida em 2007 nos países selecionados em relação à média mundial.\",\n    caption = \"Fonte: Gapminder.\",\n    x = NULL,\n    y = \"Expectativa de vida (Dif. em rel. a média)\") +\n  coord_flip() +\n  theme_light() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#expectativa-de-vida-por-continente",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#expectativa-de-vida-por-continente",
    "title": "Indo Além: Lollipops",
    "section": "Expectativa de vida por continente",
    "text": "Expectativa de vida por continente\nPor fim, podemos montar um lollipop similar a um gráfico de colunas. Para torná-lo mais interessante, vamos fazer um lollipop, onde o valor numérico é plotado no meio círculo.\nO código abaixo calcula a média ponderada da expectativa de vida por continente. Além disso, ordena-se o nome dos continentes e cria-se uma variável chamada label, que arredonda o resultado do cálculo. Este valor será plotado no meio do círculo.\n\ntab_continent &lt;- gap07 |&gt; \n  group_by(continent) |&gt; \n  summarise(avg = weighted.mean(lifeExp, pop)) |&gt; \n  mutate(\n    continent = fct_reorder(continent, avg),\n    label = format(round(avg, 1), decimal.mark = \",\")\n    )\n\nO gráfico combina geom_point, geom_segment e geom_text.\n\nggplot(tab_continent, aes(x = continent, y = avg, color = continent)) +\n  geom_point(size = 15) +\n  geom_segment(aes(xend = continent, yend = 0), linewidth = 2) +\n  geom_text(aes(label = label), color = \"white\", fontface = \"bold\") +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nA versão finalizada do gráfico remove elementos temáticos desnecessários para deixar o gráfico mais minimalista. Além disso, uso o coord_flip não apenas para virar o gráfico, mas também para dar um “zoom-in”. Note que o gráfico claramente não começa no zero, o que ajuda a enfatizar a diferença entre os grupos. Apesar de ser um conselho comum que se deve sempre iniciar o eixo-y no zero, há casos em que isto não faz muito sentido.\n\n\nCode\nggplot(tab_continent, aes(x = continent, y = avg, color = continent)) +\n  geom_point(size = 15) +\n  geom_segment(aes(xend = continent, yend = 0), linewidth = 2) +\n  geom_text(aes(label = label), color = \"white\", fontface = \"bold\") +\n  coord_flip(ylim = c(50, NA), expand = TRUE) +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  guides(color = \"none\") +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Expectativa de vida média por continente\",\n    subtitle = \"Média ponderada da expectativa de vida por continente (2007).\",\n    caption = \"Fonte: Gapminder\"\n    ) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(\n      color = \"#000000\",\n      size = 12,\n      hjust = 0.5,\n      vjust = 0.5\n      )\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#antes-e-depois-da-crise",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#antes-e-depois-da-crise",
    "title": "Indo Além: Lollipops",
    "section": "Antes e depois da Crise",
    "text": "Antes e depois da Crise\nPara esta visualização, primeiro, vamos selecionar um subconjunto de cidades a ser comparadas em 2006 e 2010. O código abaixo agrega as vendas mensais, ano a ano, em oito cidades do Texas em 2006 e 2010.\n\nsel_cities &lt;- c(\n  \"Houston\", \"Dallas\", \"Denton County\", \"Austin\",\"San Antonio\",\n  \"Collin County\", \"Fort Worth\", \"Fort Bend\"\n  )\n\n#&gt; Seleciona um subconjunto de cidades e soma as vendas a cada ano\nsub &lt;- txhousing |&gt;\n  filter(city %in% sel_cities, year %in% c(2006, 2010)) |&gt; \n  group_by(city, year) |&gt; \n  summarise(year_sales = sum(sales))\n\nPara definir a ordem das cidades no gráfico ordeno as cidades por número de vendas, em 2006, em ordem ascendente (da menor para a maior).\n\n# Pega o nome das cidades ordenadas por vendas em 2006\nlvls &lt;- sub |&gt; \n  filter(year == 2006) |&gt; \n  arrange(year_sales) |&gt; \n  pull(city)\n\nsub &lt;- mutate(sub, city = factor(city, levels = lvls))\n\nO gráfico abaixo mostra a queda na venda de imóveis nestas cidades entre os anos de 2006 a 2010. No caso de Houston, a queda foi de mais de 20 mil unidades. Note que a diferença no porte das cidades acaba dificultando a comparação da queda nas vendas nas regiões menores como Fort Bend e Denton County.\n\n\nCode\nggplot(sub, aes(x = city, y = year_sales)) +\n  geom_line(aes(group = city), color = \"gray30\") +\n  geom_point(aes(color = as.factor(year)), size = 3) +\n  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 8)) +\n  scale_y_continuous(\n    breaks = seq(0, 80000, 10000),\n    labels = scales::label_number(big.mark = \".\"),\n    limits = c(0, NA)\n    ) +\n  scale_color_manual(\n    name = \"Ano\",\n    values = c(\"#2A9D8F\", \"#E9C46A\"),\n    labels = c(\"2006\", \"2010\")\n  ) +\n  coord_flip() +\n  labs(\n    title = \"Queda na venda de imóveis nos principais mercados\", \n    x = NULL,\n    y = \"Vendas\") +\n  theme_light() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_text(hjust = 0.5),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank()\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#histórico-de-vendas-nas-cidades-pequenas",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#histórico-de-vendas-nas-cidades-pequenas",
    "title": "Indo Além: Lollipops",
    "section": "Histórico de vendas nas cidades pequenas",
    "text": "Histórico de vendas nas cidades pequenas\nGráficos de lollipop também podem ser utilizados para mostrar a variância num conjunto de dados entre diferentes grupos. Podemos, por exemplo, mostrar o número mínimo, máximo e médio de vendas nas cidades ao longo do período da amostra (2000 a 2015).\nO código abaixo encontra os valores de venda máximo, mínimo e médio em cada uma das cidades. Além disso, também encontro em qual ano que os valores máximo e mínimo ocorreram. Note que se exclui o ano de 2015, pois as observações neste ano não estão completas.\n\n\nCode\ntab_summary_sales &lt;- txhousing |&gt; \n  #&gt; Remove 2015\n  filter(year &lt; 2015) |&gt; \n  #&gt; Total de vendas anuais por cidade\n  group_by(city, year) |&gt; \n  summarise(total = sum(sales)) |&gt; \n  #&gt; Máximos, mínimos e média por cidade\n  group_by(city) |&gt; \n  summarise(\n    sales_max = max(total),\n    sales_min = min(total),\n    sales_avg = mean(total),\n    year_max = year[which.max(total)],\n    year_min = year[which.min(total)]\n    )\n\n\nPara melhorar a visualização, escolho somente as cidades pequenas, que registraram número médio de vendas entre 1000 e 5000 unidades.\n\n\nCode\n#&gt; Converte os dados para long e remove valores ausentes\ntab_sales &lt;- tab_summary_sales |&gt; \n  pivot_longer(-city, names_to = \"stat\") |&gt; \n  filter(!is.na(value))\n\n#&gt; Pega o nome das cidades com número médio de vendas entre 1000 e 5000\n#&gt; ordenadas pelo número de vendas\nsmall_cities &lt;- tab_sales |&gt; \n  filter(stat == \"sales_avg\", value &gt; 1000, value &lt; 5000) |&gt; \n  arrange(value) |&gt; \n  pull(city) |&gt; \n  unique()\n\n#&gt; Filtra a tabela para conter apenas cidades pequenas\ntab_small_sales &lt;- tab_sales |&gt; \n  filter(city %in% small_cities) |&gt; \n  mutate(city = factor(city, levels = small_cities))\n\n\nSeparo os valores com os números de vendas num tibble e os valores com os anos num outro tibble. O primeiro conjunto de dados será utilizado para plotar os pontos e a linha (o lollipop), enquanto o segundo será utilizado para plotar os labels de texto (com os números).\n\n\nCode\n#&gt; Número de vendas (pontos e linha)\nsub_sales &lt;- tab_small_sales |&gt; \n  filter(stringr::str_detect(stat, \"sales\")) |&gt; \n  mutate(stat = factor(stat, levels = c(\"sales_min\", \"sales_avg\", \"sales_max\")))\n\n#&gt; Anos recordes (texto)\nsub_years &lt;- tab_small_sales |&gt; \n  filter(stringr::str_detect(stat, \"max|min\")) |&gt; \n  pivot_wider(\n    id_cols = \"city\",\n    names_from = \"stat\",\n    names_sep = \"_\",\n    values_from = \"value\"\n  )\n\n\nO resultado final é bastante interessante. Em muitas cidades, o menor valor de vendas foi registrado em 2000 e o maior, em 2013 ou 2014. Nestes casos, como em Irving e San Angelo, parece que houve uma tendência geral de crescimento nas vendas.\nJá em Amarillo e Killeen-Fort Hood o pico de vendas ocorreu antes da Crise em 2005/2006, indicando que estas regiões não voltaram a registrar vendas no mesmo patamar ao pré-crise. No caso da segunda região, o número médio histórico de vendas está bem abaixo do pico, cerca de 50%.\nWichita Falls e Corpus Christi também apresentam pico em 2006, mas seus piores anos foram em 2011, sugerindo que o período pós-crise foi particularmente severo nestas regiões.\n\n\nCode\nggplot(sub_sales, aes(x = city, y = value)) +\n  geom_line(aes(group = city)) +\n  geom_point(aes(color = stat), size = 3) +\n  geom_text(\n    data = sub_years,\n    aes(x = city, y = sales_min - 200, label = year_min),\n    size = 3\n  ) +\n  geom_text(\n    data = sub_years,\n    aes(x = city, y = sales_max + 200, label = year_max),\n    size = 3\n  ) +\n  scale_x_discrete(labels = function(x) stringr::str_wrap(x, width = 8)) +\n  scale_y_continuous(\n    breaks = seq(0, 5500, 500),\n    labels = scales::label_number(big.mark = \".\"),\n    limits = c(0, NA)\n    ) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#e9c46a\", \"#f4a261\", \"#e76f51\"),\n    labels = c(\"Mínimo\", \"Média\", \"Máximo\")\n  ) +\n  labs(\n    x = NULL,\n    y = \"Vendas\",\n    title = \"Variação de vendas (2000-2014)\",\n    subtitle = \"Número mínimo, máximo e médio de vendas anuais. Números indicam quais foram os anos de recordes.\"\n  ) +\n  coord_flip() +\n  theme_light() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_text(hjust = 0.5),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank()\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#outros-posts-citados",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#outros-posts-citados",
    "title": "Indo Além: Lollipops",
    "section": "Outros posts citados",
    "text": "Outros posts citados\n\nFundamentos: gráfico de coluna\nFundamentos: gráfico de linha\nFundamentos: gráfico de dispersão\nEstético: Destacando informação\nEstético: Tipografia e temas"
  },
  {
    "objectID": "posts/ggplot2-tutorial/8-lollipop.html#footnotes",
    "href": "posts/ggplot2-tutorial/8-lollipop.html#footnotes",
    "title": "Indo Além: Lollipops",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEvidentemente, as funções geom_path e geom_step também funcionariam no lugar de geom_line.↩︎\nPara mais sobre como usar geom_text e outras maneiras de usar elementos textuais no gráfico consulte Estético: Destancado informação.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-brasilia/index 2.html",
    "href": "posts/general-posts/2023-09-brasilia/index 2.html",
    "title": "Mapa de altitude de ruas de Brasília",
    "section": "",
    "text": "Inspirado num antigo post de BlakeRMills, criador do pacote {MetBrewer}, criei um mapa com a altitude das ruas em Brasília.\nEm breve farei um post com tutorial detalhado e também pretendo replicar este tipo de mapa para outras cidades interessantes. O código para replicar o gráfico está abaixo.\n\n\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(osmdata)\nlibrary(purrr)\nsf::sf_use_s2(FALSE)\nsysfonts::font_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nshowtext::showtext_auto()\n\nurl = \"https://pt.wikipedia.org/wiki/Lista_de_municípios_do_Brasil_por_população\"\n\ntab = xml2::read_html(url) |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(2)\n\nas_numeric_char = Vectorize(function(x) {\n  ls = stringr::str_extract_all(x, \"[[:digit:]]\")\n  y = paste(ls[[1]], collapse = \"\")\n  as.numeric(y)\n})\n\nclean_tab = tab |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    code_muni = codigo_ibge,\n    name_muni = municipio,\n    rank = posicao,\n    name_state = unidade_federativa,\n    pop = populacao\n  ) |&gt; \n  filter(name_muni != \"Brasil\") |&gt; \n  mutate(\n    code_muni = as.numeric(code_muni),\n    pop = as_numeric_char(pop),\n    rank = rank(-pop)\n  )\n\ntop20 = slice_max(clean_tab, pop, n = 20)\n\nget_border = function(city) {\n  \n  #&gt; Encontra o código do município\n  code_muni = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(code_muni) |&gt; \n    unique()\n  \n  stopifnot(length(code_muni) == 1)\n  \n  #&gt; Baixa o shapefile do município\n  border = geobr::read_municipality(code_muni, showProgress = FALSE)\n  \n  return(border)\n}\n\nget_streets = function(city, border) {\n  \n  #&gt; Encontra o nome da Unidade Federativa\n  nome_uf = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(name_state)\n  #&gt; Monta o nome do local\n  name_place = stringr::str_glue(\"{city}, {nome_uf}, Brazil\")\n  #&gt; Monta a query\n  place = opq(bbox = getbb(name_place))\n  \n  #&gt; Importa todas as principais vias da cidade\n  # streets = add_osm_feature(\n  #   place,\n  #   key = \"highway\",\n  #   value = c(\n  #     \"motorway\", \"primary\", \"motorway_link\", \"primary_link\",\n  #     \"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\",\n  #     \"residential\"\n  #     )\n  # )\n  \n  streets = add_osm_feature(place, key = \"highway\")\n  \n  #&gt; Converte o dado\n  streets = streets %&gt;%\n    osmdata_sf() %&gt;%\n    .$osm_lines %&gt;%\n    select(osm_id, name) %&gt;%\n    st_transform(crs = 4674)\n  \n  #&gt; Enconrtra a intersecção entre as estradas e o limites do município\n  streets_border = st_intersection(streets, border)\n  \n  # out = list(streets = streets, streets_border = streets_border)\n  \n  return(streets_border)\n  \n}\n\nget_elevation = function(border, z = 8) {\n  \n  altitude = elevatr::get_elev_raster(border, z = z, clip = \"bbox\")\n  altitude = raster::rasterToPolygons(altitude)\n  altitude = st_as_sf(altitude)\n  names(altitude)[1] = \"elevation\"\n  \n  altitude = st_transform(altitude, crs = 4674)\n  altitude = suppressWarnings(st_intersection(altitude, border))\n  altitude = filter(altitude, st_is_valid(altitude))\n  \n  return(altitude)\n  \n}\n\nadd_jenks_breaks = function(shp, k = 7, round = TRUE, r = 0) {\n  #&gt; Classifica os dados de altitude em k grupos segundo o algo. de Jenks\n  jbreaks = BAMMtools::getJenksBreaks(shp$elevation, k = k)\n  #&gt; Arredonda os números para chegar numa legenda menos quebrada\n  if (round) {\n    jbreaks = round(jbreaks, r)\n  }\n  #&gt; Cria a coluna 'jenks_group' que classifica cada valor num grupo\n  shp = mutate(shp, jenks_group = cut(elevation, jbreaks))\n  \n  #&gt; Verifica se todas as observações tem um grupo\n  check = any(is.na(shp$jenks_group))\n  if (check) {\n    warning(\"Some observations have failed to be grouped\")\n  }\n  \n  #&gt; Transforma os groups em legendas\n  labels = get_jenks_labels(jbreaks)\n  \n  #&gt; Retorna o output numa lista\n  out = list(shp = shp, labels = labels)\n  return(out)\n  \n}\n\nget_jenks_labels = function(x) {\n  labels = paste(x, x[-1], sep = \"–\")\n  labels[1] = paste(\"&lt;\", x[2])\n  labels[length(labels)] = paste(\"&gt;\", max(x))\n  return(labels)\n}\n\nget_streets_altitude = function(altitude, streets) {\n  \n  stopifnot(any(colnames(altitude) %in% \"jenks_group\"))\n  \n  #&gt; Get all groups\n  groups = levels(altitude$jenks_group)\n  \n  #&gt; For each group get the full polygon and join with streets\n  join_streets = function(group) {\n    \n    poly = altitude %&gt;%\n      filter(jenks_group == group) %&gt;%\n      st_union(.) %&gt;%\n      st_as_sf() %&gt;%\n      st_make_valid()\n    \n    joined = suppressWarnings(st_intersection(streets, poly))\n    \n    return(joined)\n    \n  }\n  #&gt; Apply the function to all groups\n  street_levels = purrr::map(groups, join_streets)\n  #&gt; Bind all results together\n  out = bind_rows(street_levels, .id = \"level\")\n  \n  return(out)\n  \n}\n\nmap_plot = function(shp, labels, title, showtext = TRUE) {\n  \n  colors = viridis::plasma(n = length(labels) + 1)\n  colors = colors[-length(colors)]\n  \n  font = ifelse(showtext == TRUE, \"Roboto Condensed\", \"sans\")\n  \n  plot =\n    ggplot(data = shp) +\n    geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n    scale_color_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    scale_fill_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n    ggtitle(title) +\n    ggthemes::theme_map() +\n    coord_sf() +\n    theme(\n      plot.title = element_text(\n        size = 30,\n        hjust = 0.5,\n        family = font\n      ),\n      legend.title = element_text(\n        size = 20,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.text = element_text(\n        size = 14,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.position = \"top\",\n      legend.direction = \"horizontal\",\n      plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n    )\n  \n  return(plot)\n  \n}\n\nmap_altitude = function(city, k, z) {\n  \n  #&gt; Importa o shape do limite do município\n  message(\"Importando os limites do município: \", city)\n  city_border = get_border(city)\n  #&gt; Importa as principais vias da cidade e junta com o limite do muni\n  message(\"Importando as vias.\")\n  city_street = get_streets(city, city_border)\n  #&gt; Importa a altitude da cidade\n  message(\"Importando a altitude.\")\n  city_elevation = suppressMessages(get_elevation(city_border, z = z))\n  #&gt; Classifica a altitude em grupos\n  message(\"Classificando e juntando os shapefiles.\")\n  jenks = add_jenks_breaks(city_elevation, k = k)\n  city_elevation = jenks[[\"shp\"]]\n  labels = jenks[[\"labels\"]]\n  #&gt; Junta a altitude (agrupada) com as vias\n  city_street_elevation = get_streets_altitude(city_elevation, city_street)\n  \n  #&gt; Monta o mapa final\n  message(\"Gerando o mapa final.\")\n  plot = map_plot(city_street_elevation, labels = labels, title = city)\n  message(\"Feito.\")\n  #&gt; Retorna o output numa lista\n  out = list(\n    shp = city_street_elevation,\n    streets = city_street,\n    elevation = city_elevation,\n    plot = plot\n  )\n  \n  return(out)\n  \n}\n\nexport_citymap = function(city, w = 14, h = 16, ...) {\n  \n  plot = map_altitude(city, ...)$plot\n  \n  if (is.numeric(city)) {\n    name_city = cities_brasil |&gt; \n      filter(code_muni == city) |&gt; \n      pull(name_muni)\n  } else if (is.character(city)) {\n    name_city = city\n  }\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.svg\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.png\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n}\n\nsafe_export = purrr::safely(export_citymap)\n\nparams = tribble(\n  ~city,       ~z, ~k,\n  #----------------#---#---#\n  \"São Paulo\",       8,  7,\n  \"Rio de Janeiro\",  9,  7,\n  \"Brasília\",        8,  7,\n  \"Fortaleza\",      11,  7,\n  \"Salvador\",       10,  7,\n  \"Belo Horizonte\", 10,  7,\n  \"Manaus\",          6,  7,\n  \"Curitiba\",       10,  7,\n  \"Recife\",         10,  7,\n  \"Goiânia\",         8,  7,\n  \"Porto Alegre\",    8,  7,\n  \"Belém\",           9,  7,\n  \"Guarulhos\",       9,  7,\n  \"Campinas\",        9,  7,\n  \"São Luís\",        9,  7,\n  \"Maceió\",         11,  8,\n  \"Campo Grande\",    7,  7,\n  \"São Gonçalo\",     8,  7,\n  \"Teresina\",        10,  7,\n  \"João Pessoa\",     9,  7,\n  \"Joinville\",       9,  7\n  )\n\n#&gt; Exportar todas as cidades listadas acima\n#pmap(params, safe_export)\n#&gt; Exportar uma cidade em particular\n#safe_export(\"Brasília\", z = 8, k = 7)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html",
    "href": "posts/ggplot2-tutorial/9-facets.html",
    "title": "Indo além: facets",
    "section": "",
    "text": "A função facet_wrap() do pacote ggplot2 do R permite decompor uma visualização em vários gráficos menores, chamados de “facets”. Cada gráfico é criado como uma combinação de um ou mais grupos nos dados. Isto pode ser útil para comparar diferentes subconjuntos de dados ou para exibir muitas séries de dados no mesmo gráfico de uma maneira organizada.\n\n\n\n\n\n\n\n\n\nEste tutorial vai utilizar os seguintes geoms: geom_histogram(), geom_point(), geom_col() e geom_line(). Assim, se você não tiver familiaridade com estas funções consulte os posts abaixo:\n\nGráfico de histograma\nGráfico de linha\nScatterplot (gráfico de pontos)\nGráfico de coluna\n\nAlém disso, alguma manipulação de dados será necessária para remodelar os dados. Não é necessário ter conhecimento sobre estas funções adicionais, mas caso queira aprender mais sobre manipulação/limpeza de dados veja o post Manipular para enxergar: o básico da limpeza de dados.\nPara começar vamos importar os pacotes necessários:\n\n# Instala o pacote ggplot2 (se necessário)\ninstall.packages(c(\"ggplot2\", \"dplyr\", \"tidyr\", \"gapminder\", \"GetBCBData\"))\n\n# Carrega os pacotes\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gapminder)\nlibrary(GetBCBData)\n\n\n\nVamos começar importando algumas séries de tempo do site do Banco Central do Brasil utilizando o pacote GetBCBData. Vamos importar algumas séries do Índice de Produção Industrial (IPI). As séries são mensais, dessazonalizadas e indexadas com base nos valores médios de 2012.\n\n# Código das séries\ncodigos &lt;- c(28503, 28505, 28506, 28507, 28508, 28511)\n# Importar as séries\nseries &lt;- gbcbd_get_series(id = codigos, first.date = as.Date(\"2010-01-01\"))\n\nNossa base de dados series está no formato “long”. A coluna ref.date indica a data da observação, a coluna series.name identifica cada uma das séries pelo seu código numérico e, por fim, a coluna value retorna o valor de cada série em cada momento do tempo.\n\n\n\n\n\nref.date\nvalue\nid.num\nseries.name\n\n\n\n\n2010-01-01\n117\n28503\nid = 28503\n\n\n2010-02-01\n117\n28503\nid = 28503\n\n\n2010-03-01\n120\n28503\nid = 28503\n\n\n2010-04-01\n120\n28503\nid = 28503\n\n\n2010-05-01\n120\n28503\nid = 28503\n\n\n2010-06-01\n119\n28503\nid = 28503\n\n\n\n\n\n\n\nSabemos que é possível plotar múltiplas séries de tempo atribuindo, por exemplo, uma cor diferente para cada série. Note, contudo, que o resultado final fica muito confuso pois há sobreposição entre as séries.\n\n# Gráfico de linha com cores diferentes para cada série\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name))\n\n\n\n\n\n\n\n\nPara dividir estas séries em pequenos gráficos distintos usamos a função facet_wrap(). Esta função exige apenas um argumento chamado facets que indica qual variável deve ser utilizada para “separar” os gráficos.\nPodemos indicar a variável de duas formas: (1) usando a função vars(); ou (2) utilizando a sintaxe de fórmula que é precedida pelo “til” ~.\nO código abaixo exemplifica ambas as opções. Vamos separar as séries usando a variável series.name.\n\n# Gráfico usando vars()\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name))\n\n# Gráfico usando ~variavel\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(~ series.name)\n\n\n\n\n\n\n\n\n\n\nO gráfico de “facet” cria um gráfico separado para cada uma das séries. Note que os eixos são fixos para garantir que eles sejam comparáveis entre si. Assim, fica mais fácil de perceber como a série 28511 (insumos da construção civil) é mais curta que as demais e como a série 28506 (bens de capital) apresenta maior volatilidade que as outras séries.\nA sintaxe com vars() é a mais atual e é a que será utilizada neste post. Podemos controlar a disposição dos gráficos restringindo o comportamento dos eixos com o argumento scales e definindo o número de colunas/linhas atribuindo valores para nrow (número de linhas) ou para ncol (número de colunas).\nO argumento scales admite quatro valores:\n\nscales = \"free_x\" - o eixo-x de cada gráfico é individual\nscales = \"free_y\" - o eixo-y de cada gráfico é individual.\nscales = \"free\" - ambos os eixos x e y são individuais em cada gráfico.\nscales = \"fixed\" - todos os gráficos compartilham os mesmos eixos (opção padrão)\n\nO gráfico abaixo permite que cada gráfico tenha seu próprio eixo-y. Note como agora os gráficos individuais não são mais diretamente comparáveis entre si. Contudo, a variação individual de cada série fica mais evidente.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name), scales = \"free_y\")\n\n\n\n\n\n\n\n\nDeixando livre o eixo-x vemos como a série 28511 é alterada.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name), scales = \"free_x\")\n\n\n\n\n\n\n\n\nPor fim, podemos mudar a disposição dos gráficos alterando o número de linhas (nrow) ou de colunas (ncol). Note que a ordem os gráficos é definida pela ordem da variável series.name. Para modificar esta ordem é preciso definir os levels do factor que representa a variável categórica; e para modificar os pequenos títulos, que aparecem no topo de cada “facet”, é preciso definir os labels.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name), nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\nPara nosso segundo exemplo vamos explorar a relação entre a riqueza de um país e a sua expectativa de vida usando os dados do pacote gapminder. Inicialmente, vamos restringir nosso foco apenas aos dados de 2007.\n\n# Carrega o pacote gapminder\nlibrary(gapminder)\n# Seleciona apenas dados referentes ao ano de 2007\ngap07 &lt;- subset(gapminder, year == 2007)\n\nA tabela de dados contém informações de vários países. A coluna continent indica a qual continente o país pertence, a coluna gdpPercap é o PIB per capita do país, em dólares constantes, a coluna lifeExp é a expectativa de vida ao nascer do país em anos.\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n2007\n44\n31889923\n975\n\n\nAlbania\nEurope\n2007\n76\n3600523\n5937\n\n\nAlgeria\nAfrica\n2007\n72\n33333216\n6223\n\n\nAngola\nAfrica\n2007\n43\n12420476\n4797\n\n\nArgentina\nAmericas\n2007\n75\n40301927\n12779\n\n\nAustralia\nOceania\n2007\n81\n20434176\n34435\n\n\n\n\n\n\n\n\n\nVamos montar um gráfico simples que ilustra a relação entre o PIB per capita e a expectativa de vida entre os países, seperando-os por continente. Utilizamos a variável PIB per capita em logaritmo usando a função log(). Para incluir uma linha de tendência utilizamos a função geom_smooth() com method = \"lm\".\n\nggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +\n  # Desenha pontos com cores diferentes para cada continente\n  geom_point(aes(color = continent), alpha = 0.5) +\n  # Linha de regressão linear\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(vars(continent))\n\n\n\n\n\n\n\n\nNote que pode-se dispensar da legenda, já que facet_wrap() gera pequenos títulos para cada gráfico individual.\n\nggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(vars(continent)) +\n  # Omite a legenda de cores.\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nHá diferenças grandes na amplitude do PIB per capita entre os continentes. Podemos usar scales = \"free_x\" para dar um “zoom” em cada um deles.\n\nggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(vars(continent), scales = \"free_x\") +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nA função facet_wrap() permite também criar pequenos gráficos em dois grupos distintos. No exemplo abaixo, vamos comparar a evolução do PIB per capita e da expectativa de vida nos páises asiáticos e nos países americanos, nos anos de 1952, 1972 e 1992.\nNote que basta incluir a variável adicional dentro de vars().\n\n# Seleciona apenas as linhas dos continentes Asia e Americas nos anos de 52, 72, e 92\ngap_compare &lt;- subset(\n  gapminder,\n  continent %in% c(\"Asia\", \"Americas\") & year %in% c(1952, 1972, 1992)\n)\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_wrap(vars(year, continent), ncol = 2)\n\n\n\n\n\n\n\n\nA ordem das variáveis dentro de vars() importa. A primeira variável fica “por fora” e é mapeada nas linhas, enquanto a segunda variável é mapeada nas colunas. A escolha da ordem depende de finalidade da visualização. No caso do gráfico acima, cada linha é um mesmo ano para dois continentes diferentes.\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_wrap(vars(continent, year))\n\n\n\n\n\n\n\n\nNeste segundo gráfico invertemos a ordem das variáveis e agora cada linha apresenta a trajetória de um continente (da esquerda para a direita). Vemos como os países americanos gradualmente foram se aglomerando no canto superior-direito do gráfico (renda mais elevada e maior expectativa de vida). Já nos gráficos de baixo, vemos como os países asiáticos estavam bastante atrás dos países americanos em 1952 e 1972. Além disso, os países asiáticos estavam muito mais dispersos no eixo-x (PIB per capita). O salto veio entre 1972 e 1992, quando houve aumento tanto na renda como na expectativa de vida.\nPor fim, podemos também misturar gráficos. O código abaixo gera vários gráficos separados por continente como nos primeiros exemplos, mas agora todos os países estão plotados no fundo, em cor cinza transparente. Esta visualização ajuda a contextualizar a posição dos países relativamente ao mundo.\n\n# Base de dados auxiliar para plotar os pontos no fundo do gráfico\npontos_fundo &lt;- dplyr::select(gap07, -continent)\n\nggplot(data = gap07, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(data = pontos_fundo, color = \"gray50\", alpha = 0.5) +\n  geom_point(aes(color = continent), alpha = 0.8) +\n  facet_wrap(vars(continent)) +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos usar a mesma base de dados do gapminder para comparar a distribuição global da expectativa de vida e do PIB per capita no mundo. Como queremos comparar a evolução da distribuição no tempo é importante manter os eixos fixos.\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram(bins = 10, color = \"white\") +\n  facet_wrap(vars(year))\n\n\n\n\n\n\n\n\nVemos no gráfico que, de maneira geral, os países convergiram para a direita da distribuição. Isto indica não apenas que houve um aumento da expectativa de vida, mas também que boa parte dos países hoje tem expectativa de vida na casa de 70-80 anos.\nO código abaixo faz o mesmo tipo de gráfico para a distribuição do PIB per capita entre os países. Novamente usamos a transformação log. Vemos que houve uma transição para a direita da distribuição (mais países de renda-alta), mas ainda há bastante variância. Em alguns momentos, a distribuição parece quase seguir uma distruição uniforme com outliers - tanto acima como abaixo.\n\nggplot(gapminder, aes(x = log(gdpPercap))) +\n  geom_histogram(bins = 15, color = \"white\") +\n  facet_wrap(vars(year))\n\n\n\n\n\n\n\n\n\n\n\nVamos relembrar um dos exemplos do post de histograma. Usando uma base de preços imobiliários em cidades do Texas, EUA, fizemos um histograma que combinava informação de quatro cidades diferentes. Este foi um exercício para exemplificar como usar cores para representar diferentes grupos nos dados.\nPor conveniência, segue abaixo o código para gerar o exemplo citado. Vemos que é possível perceber algumas diferenças entre as cidades: Austin, por exemplo, têm imóveis com preços mais elevados (à direita no gráfico) do que San Angelo. Os histogramas empilhados, contudo, dificultam a comparação na faixa de 100-200 mil dólares.\n\n# Cria um vetor com as cidades selecionadas\ncities &lt;- c(\"Austin\", \"Dallas\", \"Houston\", \"San Angelo\")\n# Seleciona apenas as linhas que contêm informações sobre estas cidades\nsubtxhousing &lt;- subset(txhousing, city %in% cities)\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(aes(fill = city))\n\n\n\n\n\n\n\n\nPodemos refazer o mesmo exercício aplicando agora a função facet_wrap(). Como os eixos são constantes entre os gráficos, fica fácil comparar os preços medianos de venda de cada uma das cidades.\n\nggplot(data = subtxhousing, aes(x = median)) +\n  # Histograma\n  geom_histogram(\n    aes(fill = city),\n    # Define o número de intervalos\n    bins = 25,\n    color = \"white\") +\n  # Remove a legenda de cores\n  guides(fill = \"none\") +\n  facet_wrap(vars(city))\n\n\n\n\n\n\n\n\nNote que agora as cores perdem parte de seu propósito, já que os dados estão dividos em “facets”. Como cada “facet”, na verdade, apresenta a mesma variável (preço mediano de venda) pode-se fazer o argumento que o mais apropriado seria manter uma mesma cor para cada um dos gráficos.\n\n\n\nSabemos que um bom histograma depende do número correto de colunas/intervalos. No gráfico acima, selecionamos bins = 25, mas dificilmente este valor é o mais apropriado para cada uma das cidades.\nPara variar o número de intervalos, alteramos o número de bins ou definimos o tamanho de cada um dos intervalos via binwidth. Como temos múltiplos histogramas podemos inserir um vetor de números como argumento, como bins = c(12, 30, 25, 17), por exemplo. O mesmo vale para o argumento bindwidth.\nOutra opção é usar uma função customizada como no exemplo abaixo. Neste código usamos regra de Freedman–Diaconis para selecionar o tamanho ótimo dos intervalos em cada um dos histogramas usando as funções base ceiling(), IQR() e length(). Note que também seria possível utilizar a função nclass.FD().\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(\n    fill = \"#2a9d8f\",\n    color = \"white\",\n    # Escolhe o tamanho ótimo do intervalo\n    binwidth = function(x) ceiling(2 * IQR(x) / (length(x)^(1/3)))) +\n  facet_wrap(vars(city))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO exemplo abaixo mostra o número total de imóveis vendidos nas mesmas quatro cidades. Note que o eixo-y está livre, que enfatiza como a dinâmica das vendas seguiu trajetória similar nas quatro cidades, ainda que o volume de vendas seja bastante distinto.\n\nsales_city &lt;- subtxhousing |&gt; \n  filter(year &gt;= 2005, year &lt;= 2012) |&gt; \n  group_by(city, year) |&gt; \n  summarise(total_sales = sum(sales, na.rm = TRUE))\n\nggplot(data = sales_city, aes(x = year, y = total_sales)) +\n  geom_col(fill = \"#2a9d8f\") +\n  facet_wrap(vars(city), scales = \"free_y\") +\n  # Modifica o eixo-y para incluir separador de milhar\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\"))\n\n\n\n\n\n\n\n\n\n\n\nGráficos de coluna também podem servir bem quando temos grupos incompletos. O exemplo abaixo mostra o preço médio de venda de imóveis por números de dormitórios. A variável de “facet” é a coluna colonial, uma variável binária que indica se o estilo arquitetônico da casa é colonial (rústico).\nA base utilizada é a hprice do pacote wooldridge a mesma utilizada no post de gráficos de colunas.\n\n# Carrega a base de dados\nlibrary(wooldridge)\ndata(\"hprice1\")\n# Estima o preço médio de venda por número de dormitórios x colonial\nprice &lt;- hprice1 |&gt; \n  group_by(bdrms, colonial) |&gt; \n  summarise(avg = mean(price)) \n# Cria um grid com todas as combinações de dormitórios x colonial\ngrid &lt;- expand_grid(bdrms = 1:max(price$bdrms), colonial = 0:1)\n# Junta os dados de preços agrupados com as combinações\ndata &lt;- left_join(grid, price, by = c(\"bdrms\", \"colonial\"))\n\n# Gráfico\nggplot(data = data, aes(x = as.factor(bdrms), y = avg)) +\n  geom_col() +\n  facet_wrap(vars(colonial), ncol = 1) +\n  # Gira o gráfico\n  coord_flip()\n\n\n\n\n\n\n\n\nFica imediatamente óbvio que não houve vendas de imóveis de 1 e 2 dormitórios no estilo colonial/rústico, sugerindo que este estilo é mais presente entre imóveis maiores. No mesmo sentido, não há vendas de imóveis de 6 e 7 dormitórios que não sejam construídos no estilo colonial.\n\n\n\n\n\n\nAté agora focamos somente na função facet_wrap() que cria vários pequenos gráficos com base nos níveis de uma variável categórica. Vimos que podemos também combinar duas variáveis para comparar, por exemplo, a evolução do PIB per capita com a expectativa de vida entre dois continentes ao longo do tempo.\nQuando trabalhamos com duas variáveis categóricas vale a pena experimentar com a função facet_grid(). A sintaxe desta função é muito similar a da função facet_wrap(), mas a primeira é especificamente voltada para casos em que há duas variáveis categóricas de interesse.\nA função facet_grid() também permite controle mais direto e intuitivo sobre o layout final dos “facets” pois recebe os argumentos rows e cols.\n\n# Exemplo usando facet_wrap\nfacet_wrap(facet = vars(x, y))\n# Exemplo usando facet_grid\nfacet_grid(rows = vars(x), cols = vars(y))\n\nO código abaixo faz um exemplo comparativo. Note que há poucas diferenças entre os gráficos, mas que o resultado do facet_grid() é um pouco mais otimizado.\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_wrap(facets = vars(year, continent), ncol = 2) +\n  labs(title = \"facet_wrap\")\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_grid(rows = vars(year), cols = vars(continent)) +\n  labs(title = \"facet_grid\")\n\n\n\n\n\n\n\n\n\n\nUma importante vantagem da função facet_grid() é a de poder plotar a distribuição conjunta dos dados via o argumento margins = TRUE. O gráfico agora contém uma terceira coluna que mostra o gráfico de dispersão com todos os dados da amostra.\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_grid(rows = vars(year), cols = vars(continent), margins = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nPara melhor ilustrar algumas das capacidades da facet_grid() vamos importar uma versão aumentada da base txhousing do pacote ggplot2. O código abaixo importa uma versão desta base acrescida de colunas que identificam a quais condados (counties) cada cidade pertence, além de informações de população e crescimento populacional1.\n\ntxhousing_counties &lt;- read_csv(\"https://github.com/viniciusoike/restateinsight/raw/main/posts/ggplot2-tutorial/texas_cities_counties.csv\")\n\nVamos montar um gráfico que mostra o preço médio de venda em 2012 nas principais cidades do Texas (apenas cidades com mais de 100 mil habitantes), agrupando as cidades pelo condado principal as quais elas pertencem. Assim, temos uma variável categórica no eixo-x e uma variável categórica no “facet”.\nO código abaixo primeiro faz a manipulação dos dados e depois monta o gráfico. Note o uso dos argumentos scales = \"free\" e space = \"free\".\n\n# Estima o preço médio de venda em 2012 (ponderado pelas vendas mensais)\n# Agrupado por cidade x condado (principal)\nprice_2012 &lt;- txhousing_counties |&gt; \n  filter(year == 2012, population &gt; 100000) |&gt; \n  group_by(city, primary_county) |&gt; \n  summarise(price = weighted.mean(median, sales, na.rm = TRUE))\n\n# Gráfico\nggplot(data = price_2012, aes(x = city, y = price)) +\n  geom_col() +\n  facet_grid(\n    rows = vars(primary_county),\n    scales = \"free\",\n    space = \"free\") +\n  # Vira o gráfico\n  coord_flip() +\n  # Adiciona seperador de milhar no eixo-y\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  # Título dos facets na horizontal\n  theme(\n    strip.text.y = element_text(angle = 0)\n    )\n\n\n\n\n\n\n\n\n\n\n\nComo último exercício, podemos combinar vários conhecimentos adquiridos nos posts anteriores e mostrar a evolução dos preços em cada cidade, no período 2005-2011. Assim, temos um período de 3 anos antes e depois da crise imobiliária de 2008.\nO código abaixo estima o preço médio de venda anual, ponderado pelas vendas mensais, em cada cidade. O gráfico final inclui apenas cidades que contêm observações completas em todos os anos.\n\nprice_sales &lt;- txhousing_counties |&gt;\n  filter(year &gt;= 2005, year &lt;= 2011) |&gt; \n  group_by(year, city, primary_county) |&gt; \n  summarise(price = weighted.mean(median, sales)) |&gt; \n  group_by(city) |&gt; \n  mutate(check = n()) |&gt; \n  filter(check == 7)\n\nO código abaixo pode parecer muito extenso e confuso à primeira vista, mas lembre-se de focar em cada elemento individualmente. Como o ggplot funciona somando elementos basta focar em cada elo da cadeia individualmente. Por fim, vale notar que o elemento temático que controla o título de cada “facet” é o strip.text2.\n\nggplot(data = price_sales, aes(x = city, y = price)) +\n  # Desenha os círculos\n  geom_point(\n    # Cor de dentro do círculo representa o ano\n    aes(fill = as.factor(year)),\n    # Cor do contorno\n    color = \"gray50\",\n    shape = 21,\n    size = 2,\n    # Transparência das cores para evitar overplotting\n    alpha = 0.8) +\n  # Coloca os gráficos no grid\n  facet_grid(\n    rows = vars(primary_county),\n    scales = \"free\",\n    space = \"free\") +\n  # Adiciona seperador de milhar no eixo-y\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  # Controla as cores que preenchem os círculos\n  scale_fill_manual(\n    name = \"Year\",\n    values = c(\n      \"#006d77\", \"#42999B\", \"#83c5be\", \"#edf6f9\", \"#ffddd2\",\n      \"#F1B9A5\", \"#e29578\")) +\n  # Força a legenda de cores a ser lado-a-lado numa única linha\n  guides(fill = guide_legend(nrow = 1)) +\n  # Vira o gráfico de lado\n  coord_flip() +\n  # Título, subtítulo e nome dos eixos\n  labs(\n    title = \"Evolution of House Prices in Texas\",\n    subtitle = \"Sales-weighted average sale prices in main cities in Texas\",\n    x = NULL,\n    y = \"Average Sale Price (US$)\") +\n  # Tema minimalista com fundo branco\n  theme_light() +\n  # Ajusta a orientação dos títulos dos facets\n  theme(\n    legend.position = \"top\",\n    strip.text.y = element_text(angle = 0)\n    )\n\n\n\n\n\n\n\n\nO gráfico acima é muito rico em informação. Cada ponto indica o preço médio de venda dos imóveis na cidade; as cores em verde indicam pontos antes da crise imobiliária, enquanto as cores em laranja indicam pontos após a crise imobiliária; o ano da crise, 2008, está em branco.\nA crise imobiliária parece ter tido efeitos heterogêneos nas cidades.\nEm Dallas, por exemplo, os preços caíram e estagnaram: as observações estão quase todas sobrepostas. Em algumas cidades como Brownsville e Garland, os preços caíram e até 2011 ainda estavam abaixo dos valores médios pré-crise.\nEm algumas cidades centrais como Austin, Houston e San Antonio, a crise parece ter desacelerado o ritmo de crescimento. No período 2005-2007 há um crescimento forte nos preços, enquanto que o período 2008-2011 é de estagnação total.\nJá em cidades como Lubbock, Wacco e Midland, por exemplo, a crise parece ter tido efeito momentâneo: os preços voltam a crescer no período 2009-2011."
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#séries-de-tempo",
    "href": "posts/ggplot2-tutorial/9-facets.html#séries-de-tempo",
    "title": "Indo além: facets",
    "section": "",
    "text": "Vamos começar importando algumas séries de tempo do site do Banco Central do Brasil utilizando o pacote GetBCBData. Vamos importar algumas séries do Índice de Produção Industrial (IPI). As séries são mensais, dessazonalizadas e indexadas com base nos valores médios de 2012.\n\n# Código das séries\ncodigos &lt;- c(28503, 28505, 28506, 28507, 28508, 28511)\n# Importar as séries\nseries &lt;- gbcbd_get_series(id = codigos, first.date = as.Date(\"2010-01-01\"))\n\nNossa base de dados series está no formato “long”. A coluna ref.date indica a data da observação, a coluna series.name identifica cada uma das séries pelo seu código numérico e, por fim, a coluna value retorna o valor de cada série em cada momento do tempo.\n\n\n\n\n\nref.date\nvalue\nid.num\nseries.name\n\n\n\n\n2010-01-01\n117\n28503\nid = 28503\n\n\n2010-02-01\n117\n28503\nid = 28503\n\n\n2010-03-01\n120\n28503\nid = 28503\n\n\n2010-04-01\n120\n28503\nid = 28503\n\n\n2010-05-01\n120\n28503\nid = 28503\n\n\n2010-06-01\n119\n28503\nid = 28503\n\n\n\n\n\n\n\nSabemos que é possível plotar múltiplas séries de tempo atribuindo, por exemplo, uma cor diferente para cada série. Note, contudo, que o resultado final fica muito confuso pois há sobreposição entre as séries.\n\n# Gráfico de linha com cores diferentes para cada série\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name))\n\n\n\n\n\n\n\n\nPara dividir estas séries em pequenos gráficos distintos usamos a função facet_wrap(). Esta função exige apenas um argumento chamado facets que indica qual variável deve ser utilizada para “separar” os gráficos.\nPodemos indicar a variável de duas formas: (1) usando a função vars(); ou (2) utilizando a sintaxe de fórmula que é precedida pelo “til” ~.\nO código abaixo exemplifica ambas as opções. Vamos separar as séries usando a variável series.name.\n\n# Gráfico usando vars()\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name))\n\n# Gráfico usando ~variavel\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(~ series.name)\n\n\n\n\n\n\n\n\n\n\nO gráfico de “facet” cria um gráfico separado para cada uma das séries. Note que os eixos são fixos para garantir que eles sejam comparáveis entre si. Assim, fica mais fácil de perceber como a série 28511 (insumos da construção civil) é mais curta que as demais e como a série 28506 (bens de capital) apresenta maior volatilidade que as outras séries.\nA sintaxe com vars() é a mais atual e é a que será utilizada neste post. Podemos controlar a disposição dos gráficos restringindo o comportamento dos eixos com o argumento scales e definindo o número de colunas/linhas atribuindo valores para nrow (número de linhas) ou para ncol (número de colunas).\nO argumento scales admite quatro valores:\n\nscales = \"free_x\" - o eixo-x de cada gráfico é individual\nscales = \"free_y\" - o eixo-y de cada gráfico é individual.\nscales = \"free\" - ambos os eixos x e y são individuais em cada gráfico.\nscales = \"fixed\" - todos os gráficos compartilham os mesmos eixos (opção padrão)\n\nO gráfico abaixo permite que cada gráfico tenha seu próprio eixo-y. Note como agora os gráficos individuais não são mais diretamente comparáveis entre si. Contudo, a variação individual de cada série fica mais evidente.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name), scales = \"free_y\")\n\n\n\n\n\n\n\n\nDeixando livre o eixo-x vemos como a série 28511 é alterada.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name), scales = \"free_x\")\n\n\n\n\n\n\n\n\nPor fim, podemos mudar a disposição dos gráficos alterando o número de linhas (nrow) ou de colunas (ncol). Note que a ordem os gráficos é definida pela ordem da variável series.name. Para modificar esta ordem é preciso definir os levels do factor que representa a variável categórica; e para modificar os pequenos títulos, que aparecem no topo de cada “facet”, é preciso definir os labels.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line() +\n  facet_wrap(vars(series.name), nrow = 3)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#scatterplots",
    "href": "posts/ggplot2-tutorial/9-facets.html#scatterplots",
    "title": "Indo além: facets",
    "section": "",
    "text": "Para nosso segundo exemplo vamos explorar a relação entre a riqueza de um país e a sua expectativa de vida usando os dados do pacote gapminder. Inicialmente, vamos restringir nosso foco apenas aos dados de 2007.\n\n# Carrega o pacote gapminder\nlibrary(gapminder)\n# Seleciona apenas dados referentes ao ano de 2007\ngap07 &lt;- subset(gapminder, year == 2007)\n\nA tabela de dados contém informações de vários países. A coluna continent indica a qual continente o país pertence, a coluna gdpPercap é o PIB per capita do país, em dólares constantes, a coluna lifeExp é a expectativa de vida ao nascer do país em anos.\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n2007\n44\n31889923\n975\n\n\nAlbania\nEurope\n2007\n76\n3600523\n5937\n\n\nAlgeria\nAfrica\n2007\n72\n33333216\n6223\n\n\nAngola\nAfrica\n2007\n43\n12420476\n4797\n\n\nArgentina\nAmericas\n2007\n75\n40301927\n12779\n\n\nAustralia\nOceania\n2007\n81\n20434176\n34435\n\n\n\n\n\n\n\n\n\nVamos montar um gráfico simples que ilustra a relação entre o PIB per capita e a expectativa de vida entre os países, seperando-os por continente. Utilizamos a variável PIB per capita em logaritmo usando a função log(). Para incluir uma linha de tendência utilizamos a função geom_smooth() com method = \"lm\".\n\nggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +\n  # Desenha pontos com cores diferentes para cada continente\n  geom_point(aes(color = continent), alpha = 0.5) +\n  # Linha de regressão linear\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(vars(continent))\n\n\n\n\n\n\n\n\nNote que pode-se dispensar da legenda, já que facet_wrap() gera pequenos títulos para cada gráfico individual.\n\nggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(vars(continent)) +\n  # Omite a legenda de cores.\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nHá diferenças grandes na amplitude do PIB per capita entre os continentes. Podemos usar scales = \"free_x\" para dar um “zoom” em cada um deles.\n\nggplot(data = gapminder, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(vars(continent), scales = \"free_x\") +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nA função facet_wrap() permite também criar pequenos gráficos em dois grupos distintos. No exemplo abaixo, vamos comparar a evolução do PIB per capita e da expectativa de vida nos páises asiáticos e nos países americanos, nos anos de 1952, 1972 e 1992.\nNote que basta incluir a variável adicional dentro de vars().\n\n# Seleciona apenas as linhas dos continentes Asia e Americas nos anos de 52, 72, e 92\ngap_compare &lt;- subset(\n  gapminder,\n  continent %in% c(\"Asia\", \"Americas\") & year %in% c(1952, 1972, 1992)\n)\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_wrap(vars(year, continent), ncol = 2)\n\n\n\n\n\n\n\n\nA ordem das variáveis dentro de vars() importa. A primeira variável fica “por fora” e é mapeada nas linhas, enquanto a segunda variável é mapeada nas colunas. A escolha da ordem depende de finalidade da visualização. No caso do gráfico acima, cada linha é um mesmo ano para dois continentes diferentes.\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_wrap(vars(continent, year))\n\n\n\n\n\n\n\n\nNeste segundo gráfico invertemos a ordem das variáveis e agora cada linha apresenta a trajetória de um continente (da esquerda para a direita). Vemos como os países americanos gradualmente foram se aglomerando no canto superior-direito do gráfico (renda mais elevada e maior expectativa de vida). Já nos gráficos de baixo, vemos como os países asiáticos estavam bastante atrás dos países americanos em 1952 e 1972. Além disso, os países asiáticos estavam muito mais dispersos no eixo-x (PIB per capita). O salto veio entre 1972 e 1992, quando houve aumento tanto na renda como na expectativa de vida.\nPor fim, podemos também misturar gráficos. O código abaixo gera vários gráficos separados por continente como nos primeiros exemplos, mas agora todos os países estão plotados no fundo, em cor cinza transparente. Esta visualização ajuda a contextualizar a posição dos países relativamente ao mundo.\n\n# Base de dados auxiliar para plotar os pontos no fundo do gráfico\npontos_fundo &lt;- dplyr::select(gap07, -continent)\n\nggplot(data = gap07, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(data = pontos_fundo, color = \"gray50\", alpha = 0.5) +\n  geom_point(aes(color = continent), alpha = 0.8) +\n  facet_wrap(vars(continent)) +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  guides(color = \"none\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#histograma",
    "href": "posts/ggplot2-tutorial/9-facets.html#histograma",
    "title": "Indo além: facets",
    "section": "",
    "text": "Vamos usar a mesma base de dados do gapminder para comparar a distribuição global da expectativa de vida e do PIB per capita no mundo. Como queremos comparar a evolução da distribuição no tempo é importante manter os eixos fixos.\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram(bins = 10, color = \"white\") +\n  facet_wrap(vars(year))\n\n\n\n\n\n\n\n\nVemos no gráfico que, de maneira geral, os países convergiram para a direita da distribuição. Isto indica não apenas que houve um aumento da expectativa de vida, mas também que boa parte dos países hoje tem expectativa de vida na casa de 70-80 anos.\nO código abaixo faz o mesmo tipo de gráfico para a distribuição do PIB per capita entre os países. Novamente usamos a transformação log. Vemos que houve uma transição para a direita da distribuição (mais países de renda-alta), mas ainda há bastante variância. Em alguns momentos, a distribuição parece quase seguir uma distruição uniforme com outliers - tanto acima como abaixo.\n\nggplot(gapminder, aes(x = log(gdpPercap))) +\n  geom_histogram(bins = 15, color = \"white\") +\n  facet_wrap(vars(year))\n\n\n\n\n\n\n\n\n\n\n\nVamos relembrar um dos exemplos do post de histograma. Usando uma base de preços imobiliários em cidades do Texas, EUA, fizemos um histograma que combinava informação de quatro cidades diferentes. Este foi um exercício para exemplificar como usar cores para representar diferentes grupos nos dados.\nPor conveniência, segue abaixo o código para gerar o exemplo citado. Vemos que é possível perceber algumas diferenças entre as cidades: Austin, por exemplo, têm imóveis com preços mais elevados (à direita no gráfico) do que San Angelo. Os histogramas empilhados, contudo, dificultam a comparação na faixa de 100-200 mil dólares.\n\n# Cria um vetor com as cidades selecionadas\ncities &lt;- c(\"Austin\", \"Dallas\", \"Houston\", \"San Angelo\")\n# Seleciona apenas as linhas que contêm informações sobre estas cidades\nsubtxhousing &lt;- subset(txhousing, city %in% cities)\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(aes(fill = city))\n\n\n\n\n\n\n\n\nPodemos refazer o mesmo exercício aplicando agora a função facet_wrap(). Como os eixos são constantes entre os gráficos, fica fácil comparar os preços medianos de venda de cada uma das cidades.\n\nggplot(data = subtxhousing, aes(x = median)) +\n  # Histograma\n  geom_histogram(\n    aes(fill = city),\n    # Define o número de intervalos\n    bins = 25,\n    color = \"white\") +\n  # Remove a legenda de cores\n  guides(fill = \"none\") +\n  facet_wrap(vars(city))\n\n\n\n\n\n\n\n\nNote que agora as cores perdem parte de seu propósito, já que os dados estão dividos em “facets”. Como cada “facet”, na verdade, apresenta a mesma variável (preço mediano de venda) pode-se fazer o argumento que o mais apropriado seria manter uma mesma cor para cada um dos gráficos.\n\n\n\nSabemos que um bom histograma depende do número correto de colunas/intervalos. No gráfico acima, selecionamos bins = 25, mas dificilmente este valor é o mais apropriado para cada uma das cidades.\nPara variar o número de intervalos, alteramos o número de bins ou definimos o tamanho de cada um dos intervalos via binwidth. Como temos múltiplos histogramas podemos inserir um vetor de números como argumento, como bins = c(12, 30, 25, 17), por exemplo. O mesmo vale para o argumento bindwidth.\nOutra opção é usar uma função customizada como no exemplo abaixo. Neste código usamos regra de Freedman–Diaconis para selecionar o tamanho ótimo dos intervalos em cada um dos histogramas usando as funções base ceiling(), IQR() e length(). Note que também seria possível utilizar a função nclass.FD().\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(\n    fill = \"#2a9d8f\",\n    color = \"white\",\n    # Escolhe o tamanho ótimo do intervalo\n    binwidth = function(x) ceiling(2 * IQR(x) / (length(x)^(1/3)))) +\n  facet_wrap(vars(city))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#colunas",
    "href": "posts/ggplot2-tutorial/9-facets.html#colunas",
    "title": "Indo além: facets",
    "section": "",
    "text": "O exemplo abaixo mostra o número total de imóveis vendidos nas mesmas quatro cidades. Note que o eixo-y está livre, que enfatiza como a dinâmica das vendas seguiu trajetória similar nas quatro cidades, ainda que o volume de vendas seja bastante distinto.\n\nsales_city &lt;- subtxhousing |&gt; \n  filter(year &gt;= 2005, year &lt;= 2012) |&gt; \n  group_by(city, year) |&gt; \n  summarise(total_sales = sum(sales, na.rm = TRUE))\n\nggplot(data = sales_city, aes(x = year, y = total_sales)) +\n  geom_col(fill = \"#2a9d8f\") +\n  facet_wrap(vars(city), scales = \"free_y\") +\n  # Modifica o eixo-y para incluir separador de milhar\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\"))\n\n\n\n\n\n\n\n\n\n\n\nGráficos de coluna também podem servir bem quando temos grupos incompletos. O exemplo abaixo mostra o preço médio de venda de imóveis por números de dormitórios. A variável de “facet” é a coluna colonial, uma variável binária que indica se o estilo arquitetônico da casa é colonial (rústico).\nA base utilizada é a hprice do pacote wooldridge a mesma utilizada no post de gráficos de colunas.\n\n# Carrega a base de dados\nlibrary(wooldridge)\ndata(\"hprice1\")\n# Estima o preço médio de venda por número de dormitórios x colonial\nprice &lt;- hprice1 |&gt; \n  group_by(bdrms, colonial) |&gt; \n  summarise(avg = mean(price)) \n# Cria um grid com todas as combinações de dormitórios x colonial\ngrid &lt;- expand_grid(bdrms = 1:max(price$bdrms), colonial = 0:1)\n# Junta os dados de preços agrupados com as combinações\ndata &lt;- left_join(grid, price, by = c(\"bdrms\", \"colonial\"))\n\n# Gráfico\nggplot(data = data, aes(x = as.factor(bdrms), y = avg)) +\n  geom_col() +\n  facet_wrap(vars(colonial), ncol = 1) +\n  # Gira o gráfico\n  coord_flip()\n\n\n\n\n\n\n\n\nFica imediatamente óbvio que não houve vendas de imóveis de 1 e 2 dormitórios no estilo colonial/rústico, sugerindo que este estilo é mais presente entre imóveis maiores. No mesmo sentido, não há vendas de imóveis de 6 e 7 dormitórios que não sejam construídos no estilo colonial."
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#wraps-ou-grids",
    "href": "posts/ggplot2-tutorial/9-facets.html#wraps-ou-grids",
    "title": "Indo além: facets",
    "section": "",
    "text": "Até agora focamos somente na função facet_wrap() que cria vários pequenos gráficos com base nos níveis de uma variável categórica. Vimos que podemos também combinar duas variáveis para comparar, por exemplo, a evolução do PIB per capita com a expectativa de vida entre dois continentes ao longo do tempo.\nQuando trabalhamos com duas variáveis categóricas vale a pena experimentar com a função facet_grid(). A sintaxe desta função é muito similar a da função facet_wrap(), mas a primeira é especificamente voltada para casos em que há duas variáveis categóricas de interesse.\nA função facet_grid() também permite controle mais direto e intuitivo sobre o layout final dos “facets” pois recebe os argumentos rows e cols.\n\n# Exemplo usando facet_wrap\nfacet_wrap(facet = vars(x, y))\n# Exemplo usando facet_grid\nfacet_grid(rows = vars(x), cols = vars(y))\n\nO código abaixo faz um exemplo comparativo. Note que há poucas diferenças entre os gráficos, mas que o resultado do facet_grid() é um pouco mais otimizado.\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_wrap(facets = vars(year, continent), ncol = 2) +\n  labs(title = \"facet_wrap\")\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_grid(rows = vars(year), cols = vars(continent)) +\n  labs(title = \"facet_grid\")\n\n\n\n\n\n\n\n\n\n\nUma importante vantagem da função facet_grid() é a de poder plotar a distribuição conjunta dos dados via o argumento margins = TRUE. O gráfico agora contém uma terceira coluna que mostra o gráfico de dispersão com todos os dados da amostra.\n\nggplot(data = gap_compare, aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point() +\n  facet_grid(rows = vars(year), cols = vars(continent), margins = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nPara melhor ilustrar algumas das capacidades da facet_grid() vamos importar uma versão aumentada da base txhousing do pacote ggplot2. O código abaixo importa uma versão desta base acrescida de colunas que identificam a quais condados (counties) cada cidade pertence, além de informações de população e crescimento populacional1.\n\ntxhousing_counties &lt;- read_csv(\"https://github.com/viniciusoike/restateinsight/raw/main/posts/ggplot2-tutorial/texas_cities_counties.csv\")\n\nVamos montar um gráfico que mostra o preço médio de venda em 2012 nas principais cidades do Texas (apenas cidades com mais de 100 mil habitantes), agrupando as cidades pelo condado principal as quais elas pertencem. Assim, temos uma variável categórica no eixo-x e uma variável categórica no “facet”.\nO código abaixo primeiro faz a manipulação dos dados e depois monta o gráfico. Note o uso dos argumentos scales = \"free\" e space = \"free\".\n\n# Estima o preço médio de venda em 2012 (ponderado pelas vendas mensais)\n# Agrupado por cidade x condado (principal)\nprice_2012 &lt;- txhousing_counties |&gt; \n  filter(year == 2012, population &gt; 100000) |&gt; \n  group_by(city, primary_county) |&gt; \n  summarise(price = weighted.mean(median, sales, na.rm = TRUE))\n\n# Gráfico\nggplot(data = price_2012, aes(x = city, y = price)) +\n  geom_col() +\n  facet_grid(\n    rows = vars(primary_county),\n    scales = \"free\",\n    space = \"free\") +\n  # Vira o gráfico\n  coord_flip() +\n  # Adiciona seperador de milhar no eixo-y\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  # Título dos facets na horizontal\n  theme(\n    strip.text.y = element_text(angle = 0)\n    )\n\n\n\n\n\n\n\n\n\n\n\nComo último exercício, podemos combinar vários conhecimentos adquiridos nos posts anteriores e mostrar a evolução dos preços em cada cidade, no período 2005-2011. Assim, temos um período de 3 anos antes e depois da crise imobiliária de 2008.\nO código abaixo estima o preço médio de venda anual, ponderado pelas vendas mensais, em cada cidade. O gráfico final inclui apenas cidades que contêm observações completas em todos os anos.\n\nprice_sales &lt;- txhousing_counties |&gt;\n  filter(year &gt;= 2005, year &lt;= 2011) |&gt; \n  group_by(year, city, primary_county) |&gt; \n  summarise(price = weighted.mean(median, sales)) |&gt; \n  group_by(city) |&gt; \n  mutate(check = n()) |&gt; \n  filter(check == 7)\n\nO código abaixo pode parecer muito extenso e confuso à primeira vista, mas lembre-se de focar em cada elemento individualmente. Como o ggplot funciona somando elementos basta focar em cada elo da cadeia individualmente. Por fim, vale notar que o elemento temático que controla o título de cada “facet” é o strip.text2.\n\nggplot(data = price_sales, aes(x = city, y = price)) +\n  # Desenha os círculos\n  geom_point(\n    # Cor de dentro do círculo representa o ano\n    aes(fill = as.factor(year)),\n    # Cor do contorno\n    color = \"gray50\",\n    shape = 21,\n    size = 2,\n    # Transparência das cores para evitar overplotting\n    alpha = 0.8) +\n  # Coloca os gráficos no grid\n  facet_grid(\n    rows = vars(primary_county),\n    scales = \"free\",\n    space = \"free\") +\n  # Adiciona seperador de milhar no eixo-y\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  # Controla as cores que preenchem os círculos\n  scale_fill_manual(\n    name = \"Year\",\n    values = c(\n      \"#006d77\", \"#42999B\", \"#83c5be\", \"#edf6f9\", \"#ffddd2\",\n      \"#F1B9A5\", \"#e29578\")) +\n  # Força a legenda de cores a ser lado-a-lado numa única linha\n  guides(fill = guide_legend(nrow = 1)) +\n  # Vira o gráfico de lado\n  coord_flip() +\n  # Título, subtítulo e nome dos eixos\n  labs(\n    title = \"Evolution of House Prices in Texas\",\n    subtitle = \"Sales-weighted average sale prices in main cities in Texas\",\n    x = NULL,\n    y = \"Average Sale Price (US$)\") +\n  # Tema minimalista com fundo branco\n  theme_light() +\n  # Ajusta a orientação dos títulos dos facets\n  theme(\n    legend.position = \"top\",\n    strip.text.y = element_text(angle = 0)\n    )\n\n\n\n\n\n\n\n\nO gráfico acima é muito rico em informação. Cada ponto indica o preço médio de venda dos imóveis na cidade; as cores em verde indicam pontos antes da crise imobiliária, enquanto as cores em laranja indicam pontos após a crise imobiliária; o ano da crise, 2008, está em branco.\nA crise imobiliária parece ter tido efeitos heterogêneos nas cidades.\nEm Dallas, por exemplo, os preços caíram e estagnaram: as observações estão quase todas sobrepostas. Em algumas cidades como Brownsville e Garland, os preços caíram e até 2011 ainda estavam abaixo dos valores médios pré-crise.\nEm algumas cidades centrais como Austin, Houston e San Antonio, a crise parece ter desacelerado o ritmo de crescimento. No período 2005-2007 há um crescimento forte nos preços, enquanto que o período 2008-2011 é de estagnação total.\nJá em cidades como Lubbock, Wacco e Midland, por exemplo, a crise parece ter tido efeito momentâneo: os preços voltam a crescer no período 2009-2011."
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#outros-posts-citados",
    "href": "posts/ggplot2-tutorial/9-facets.html#outros-posts-citados",
    "title": "Indo além: facets",
    "section": "Outros posts citados",
    "text": "Outros posts citados\n\nFundamentos: gráfico de coluna\nFundamentos: gráfico de linha\nFundamentos: gráfico de dispersão\nFundamentos: histograma\nEstético: Tipografia e temas"
  },
  {
    "objectID": "posts/ggplot2-tutorial/9-facets.html#footnotes",
    "href": "posts/ggplot2-tutorial/9-facets.html#footnotes",
    "title": "Indo além: facets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO código utilizado para gerar esta base está disponível no link.↩︎\nOutros elementos temáticos deste título, como a cor do fundo ou a sua posição são controlados via outros elementos strip_*. Para mais detalhes consulte help(\"theme\"). Veja também o post Estético: tipografias e temas.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-wz-unemployment/index.html",
    "href": "posts/general-posts/2023-09-wz-unemployment/index.html",
    "title": "Brazil in Charts: Unemployment",
    "section": "",
    "text": "After a slow start in 2021, Brazilian unemployment registered a remarkable drop, boosted by soaring commodity prices and generous fiscal expansion. Throughout 2020 and up until the 2022 elections, the Federal Government sustained several subsidy programs that initially accounted for up to 10% of total GDP. Even after the election, government is still running at a significant deficit, as the promised budget cuts fail to be approved.\nThe job market numbers are even more impressive when one accounts for the very high real interest rates that prevailed through 2021 and 20221. Despite a global slowdown in the markets, Brazilian GDP surprised in the first quarter of 2023 due to an impressive performance of the agricultural sector. Agricultural related activities grew 18,8% YoY and led to a 4% increase in GDP YoY2.\n\n\n\n\n\n\n\n\n\nThe drop in unemployment has been seen across all Brazilian states. The largest drop happened in the northern state of Roraíma, which saw unemployment drop 10 percentage points. Santa Catarina had the lowest unemployment rate in 2019-Q2 and now has the third lowest rate at 3.5%.\n\n\n\n\n\n\n\n\n\nOverall, the Mid-Western states of Mato Grosso and Mato Grosso do Sul are at historically low unemployment rates. Both states have strong ties to the primary sector and have benefited from recent record-breaking soybean crops. The states in the South and Southeast all have below average unemployment rates with the exception of Rio de Janeiro. Rio’s economy is still struggling after repeated blows; the aftermath of the 2014 World Cup has been a continuing string of crisis: the 2015-17 recession, Lavajato, and the Covid-19 Pandemic have all aggravated a pre-existing economic stagnation."
  },
  {
    "objectID": "posts/general-posts/2023-09-wz-unemployment/index.html#unemployment-rate-dropped-sharply-in-the-post-pandemic-period-but-still-above-pre-recession-levels",
    "href": "posts/general-posts/2023-09-wz-unemployment/index.html#unemployment-rate-dropped-sharply-in-the-post-pandemic-period-but-still-above-pre-recession-levels",
    "title": "Brazil in Charts: Unemployment",
    "section": "",
    "text": "After a slow start in 2021, Brazilian unemployment registered a remarkable drop, boosted by soaring commodity prices and generous fiscal expansion. Throughout 2020 and up until the 2022 elections, the Federal Government sustained several subsidy programs that initially accounted for up to 10% of total GDP. Even after the election, government is still running at a significant deficit, as the promised budget cuts fail to be approved.\nThe job market numbers are even more impressive when one accounts for the very high real interest rates that prevailed through 2021 and 20221. Despite a global slowdown in the markets, Brazilian GDP surprised in the first quarter of 2023 due to an impressive performance of the agricultural sector. Agricultural related activities grew 18,8% YoY and led to a 4% increase in GDP YoY2.\n\n\n\n\n\n\n\n\n\nThe drop in unemployment has been seen across all Brazilian states. The largest drop happened in the northern state of Roraíma, which saw unemployment drop 10 percentage points. Santa Catarina had the lowest unemployment rate in 2019-Q2 and now has the third lowest rate at 3.5%.\n\n\n\n\n\n\n\n\n\nOverall, the Mid-Western states of Mato Grosso and Mato Grosso do Sul are at historically low unemployment rates. Both states have strong ties to the primary sector and have benefited from recent record-breaking soybean crops. The states in the South and Southeast all have below average unemployment rates with the exception of Rio de Janeiro. Rio’s economy is still struggling after repeated blows; the aftermath of the 2014 World Cup has been a continuing string of crisis: the 2015-17 recession, Lavajato, and the Covid-19 Pandemic have all aggravated a pre-existing economic stagnation."
  },
  {
    "objectID": "posts/general-posts/2023-09-wz-unemployment/index.html#footnotes",
    "href": "posts/general-posts/2023-09-wz-unemployment/index.html#footnotes",
    "title": "Brazil in Charts: Unemployment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.nytimes.com/interactive/2022/06/16/business/economy/global-interest-rate-increases.html↩︎\nhttps://agenciadenoticias.ibge.gov.br/agencia-sala-de-imprensa/2013-agencia-de-noticias/releases/37029-pib-cresce-1-9-no-1-trimestre-de-2023↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html",
    "href": "posts/general-posts/2023-10-realestatebr/index.html",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "Consumir os dados do mercado imobiliário brasileiro não é tarefa fácil. Pensando em simplificar este processo eu criei um pacote chamado realestatebr que importa e limpa diversas bases de dados relacionadas ao mercado imobiliário. Atualmente, o pacote está majoritariamente focado no mercado residencial.\nPara instalar o pacote é preciso ter o devtools ou remotes instalado.\n\n# Instala (se necessário)\ninstall.packages(\"remotes\")\n# Instala o pacote realestatebr\nremotes::install_github(\"viniciusoike/realestatebr\")\n\nO pacote é estruturado em torno de funções get_* que importam bases de dados. O pacote cobre:\n\nAbrainc: Indicadores\nAbrainc: IAMI\nAbrainc: Radar\nAbrainc: Relatórios MCMV e MAP\nAbecip: IGMI-R\nAbecip: Indicadores de crédito\nBIS: Residential Property Price Indices\nBanco Central do Brasil: Estatísticas do Mercado Imobiliário\nBanco Central do Brasil: Séries macroeconômicas\nB3: Ações de empresas relacionadas ao mercado imobiliário\nFipeZap: Índice FipeZap\nFGV-Ibre: Séries macroeconômicas, IVAR, INCC, etc.\nQuintoAndar: Índice QuintoAndar de Aluguel\nRegistro de Imóveis\nSECOVI-SP: Panorama do Mercado Imobiliário\n\nPara mais detalhes sobre o pacote consulte o repositório no GitHub.\n\n\nOs exemplos abaixo demonstram alguns dos usos do pacote.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RcppRoll)\nlibrary(realestatebr)\n\n\n\n\nO Brasil tem diversos índices de preços imobiliários. A função get_rppi permite baixar tanto os índices de aluguel como de compra/venda. O código abaixo importa as séries do FipeZap, IGMI-R (FGV e Abecip) e IVG-R (BCB).\n\nsale &lt;- get_rppi(\"sale\", stack = TRUE)\n\nOs dados são importados em formato longitudinal identificados pela coluna source. A coluna chg traz a variação mensal do índice enquanto a coluna acum12m traz a variação acumulada em 12 meses.\n\n\n\n\n\n\n\nO código abaixo mostra os três índices desde 2018. Note que há um descolamento interessante entre o IGMI-R e o IVG-R. O Índice FipeZap e o IVG-R, que historicamente costumam convergir, também apresentam comportamento divergente nesta janela de tempo.\n\n# Filter only Brazil\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2018-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra as séries num horizonte maior de tempo.\n\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2009-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nPode-se importar facilmente os dados da Abecip sobre financiamentos imobiliários. A função get_abecip_indicators é uma lista que retorna dados sobre o SBPE e o CGI.\n\n# Import data from Abecip\nabecip &lt;- get_abecip_indicators()\n\nnames(abecip)\n\n[1] \"sbpe\"  \"units\" \"cgi\"  \n\n\nO gráfico abaixo mostra o total de unidades financiadas a cada ano.\n\nunidades &lt;- abecip$units\n\ntbl_unidades_ano &lt;- unidades |&gt; \n  mutate(ano = lubridate::year(date)) |&gt; \n  summarise(total_ano = sum(units_total), .by = \"ano\")\n\nggplot(tbl_unidades_ano, aes(x = ano, y = total_ano)) +\n  geom_col(fill = \"#264653\") +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = 2001:2023) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  labs(\n    title = \"Unidades Financiadas SBPE\",\n    x = NULL,\n    y = \"Unidades\",\n    caption = \"Unidades de 2023 acumuladas até agosto.\"\n    ) +\n  theme_light() +\n  theme(\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\n\n\n\n\n\n\n\nO Banco Central disponibiliza uma enorme variedade de séries relacionadas ao mercado imobiliário. A função get_bcb_realestate retorna mais de 3500 séries. Para agilizar o download destas bases, todas as funções oferecem a opção cached = TRUE. Usando esta opção o download é feito via GitHub e tende a ser mais rápido; o dado, contudo, corre o risco de estar levemente desatualizado.\n\nbcb &lt;- get_bcb_realestate(category = \"all\", cached = TRUE)\n\nlength(unique(bcb$series_info))\n\n[1] 3656\n\n\nInfelizmente, não é trivial usar esta base de dados. As colunas category, type e v1 a v5 tentam facilitar o trabalho de filtrar as linhas. Há seis grandes categorias.\n\ncount(bcb, category)\n\n# A tibble: 6 × 2\n  category            n\n  &lt;fct&gt;           &lt;int&gt;\n1 contabil          303\n2 credito        285853\n3 direcionamento   5223\n4 fontes            604\n5 indices           555\n6 imoveis         25353\n\n\nCada série é identificada pela coluna series_info. Quebrando esta coluna em outras, fica um pouco mais fácil de encontrar as séries desejadas.\n\nbcb |&gt; \n  filter(category == \"contabil\") |&gt; \n  count(series_info, type, v1)\n\n# A tibble: 3 × 4\n  series_info                           type          v1              n\n  &lt;chr&gt;                                 &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt;\n1 contabil_bndu_imobiliario_br          bndu          imobiliario    81\n2 contabil_financiamento_comercial_br   financiamento comercial     111\n3 contabil_financiamento_residencial_br financiamento residencial   111\n\n\nO gráfico abaixo mostra o preço mediano do imóvel financiado nos estados da região Sul. Note que estas séries possuem um comportamento irregular em 2019.\n\ntbl_imoveis_sul &lt;- bcb |&gt; \n  filter(\n    category == \"imoveis\",\n    type == \"valor\",\n    v1 == \"compra\",\n    abbrev_state %in% c(\"RS\", \"SC\", \"PR\")\n    )\n\nggplot(tbl_imoveis_sul, aes(x = date, y = value, color = abbrev_state)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Preço de financiamento\",\n    x = NULL,\n    y = \"R$\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nA função get_bcb_series facilita a importação de séries macroeconômicas potencialmente relacionadas com o mercado imobiliário. Como há muitas séries, o argumento category facilita a seleção de temas específicos. A categoria de preços inlcui os principais índices de preço relacionados ao mercado, como o INCC, IPCA e IGPM.\n\nmacro &lt;- get_bcb_series(category = \"price\")\n\nunique(macro$name_pt)\n\n[1] \"Índice geral de preços do mercado (IGP-M)\"                                         \n[2] \"Índice geral de preços-disponibilidade interna (IGP-DI)\"                           \n[3] \"Índice nacional de custo da construção (INCC)\"                                     \n[4] \"Índice nacional de preços ao consumidor-amplo (IPCA)\"                              \n[5] \"Índice nacional de preços ao consumidor-Amplo (IPCA) - Habitação\"                  \n[6] \"Índice nacional de preços ao consumidor (INPC) - Habitação\"                        \n[7] \"Meios de pagamento - M1 (saldo em final de período) - Novo - sazonalmente ajustado\"\n\n\nO gráfico abaixo combina a série de preços do IGMI-R, importada anteriormente, com o IPCA. Os valores são acumulados em 12 meses e apresentados desde 2018. Nota-se como o preço dos imóveis cresceu acima da inflação a partir de 2020.\n\nseries_ipca &lt;- macro |&gt; \n  filter(name_simplified == \"ipca\") |&gt; \n  mutate(\n    acum12m = roll_prodr(1 + value / 100, n = 12) - 1,\n    series = \"ipca\"\n    ) |&gt; \n  select(date, series, acum12m)\n\nseries_igmi &lt;- sale |&gt; \n  filter(\n    source == \"IGMI-R\",\n    name_muni %in% c(\"Porto Alegre\", \"Brazil\")\n    ) |&gt; \n  select(date, series = name_muni, acum12m)\n\nseries_macro &lt;- rbind(series_ipca, series_igmi)\n\nseries_macro &lt;- series_macro |&gt; \n  filter(date &gt;= as.Date(\"2018-01-01\"), date &lt;= as.Date(\"2023-07-01\")) |&gt; \n  mutate(\n    series = factor(series, levels = c(\"Brazil\", \"Porto Alegre\", \"ipca\"))\n  )\n\nggplot(series_macro, aes(x = date, y = acum12m, color = series)) +\n  geom_line(linewidth = 0.8) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\"),\n    labels = c(\"Brasil (geral)\", \"Porto Alegre\", \"IPCA\")\n    ) +\n  labs(\n    title = \"Preços de imóveis crescem acima da inflação\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nO desenvolvimento futuro do pacote será guiado pela incorporação de mais bases de dados e relatórios de mercado."
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#usando-o-pacote",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#usando-o-pacote",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "Os exemplos abaixo demonstram alguns dos usos do pacote.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RcppRoll)\nlibrary(realestatebr)"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#índices-de-preços",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#índices-de-preços",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O Brasil tem diversos índices de preços imobiliários. A função get_rppi permite baixar tanto os índices de aluguel como de compra/venda. O código abaixo importa as séries do FipeZap, IGMI-R (FGV e Abecip) e IVG-R (BCB).\n\nsale &lt;- get_rppi(\"sale\", stack = TRUE)\n\nOs dados são importados em formato longitudinal identificados pela coluna source. A coluna chg traz a variação mensal do índice enquanto a coluna acum12m traz a variação acumulada em 12 meses.\n\n\n\n\n\n\n\nO código abaixo mostra os três índices desde 2018. Note que há um descolamento interessante entre o IGMI-R e o IVG-R. O Índice FipeZap e o IVG-R, que historicamente costumam convergir, também apresentam comportamento divergente nesta janela de tempo.\n\n# Filter only Brazil\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2018-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra as séries num horizonte maior de tempo.\n\nrppi_brasil &lt;- sale |&gt; \n  filter(\n    name_muni == \"Brazil\" | name_muni == \"Índice Fipezap\",\n    date &gt;= as.Date(\"2009-01-01\"),\n    date &lt;= as.Date(\"2023-06-01\")\n  )\n\nggplot(rppi_brasil, aes(date, acum12m)) +\n  geom_line(aes(color = source)) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Índices de Preço\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#financiamento-de-imóveis",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#financiamento-de-imóveis",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "Pode-se importar facilmente os dados da Abecip sobre financiamentos imobiliários. A função get_abecip_indicators é uma lista que retorna dados sobre o SBPE e o CGI.\n\n# Import data from Abecip\nabecip &lt;- get_abecip_indicators()\n\nnames(abecip)\n\n[1] \"sbpe\"  \"units\" \"cgi\"  \n\n\nO gráfico abaixo mostra o total de unidades financiadas a cada ano.\n\nunidades &lt;- abecip$units\n\ntbl_unidades_ano &lt;- unidades |&gt; \n  mutate(ano = lubridate::year(date)) |&gt; \n  summarise(total_ano = sum(units_total), .by = \"ano\")\n\nggplot(tbl_unidades_ano, aes(x = ano, y = total_ano)) +\n  geom_col(fill = \"#264653\") +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = 2001:2023) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \".\")) +\n  labs(\n    title = \"Unidades Financiadas SBPE\",\n    x = NULL,\n    y = \"Unidades\",\n    caption = \"Unidades de 2023 acumuladas até agosto.\"\n    ) +\n  theme_light() +\n  theme(\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#crédito-imobiliário",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#crédito-imobiliário",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O Banco Central disponibiliza uma enorme variedade de séries relacionadas ao mercado imobiliário. A função get_bcb_realestate retorna mais de 3500 séries. Para agilizar o download destas bases, todas as funções oferecem a opção cached = TRUE. Usando esta opção o download é feito via GitHub e tende a ser mais rápido; o dado, contudo, corre o risco de estar levemente desatualizado.\n\nbcb &lt;- get_bcb_realestate(category = \"all\", cached = TRUE)\n\nlength(unique(bcb$series_info))\n\n[1] 3656\n\n\nInfelizmente, não é trivial usar esta base de dados. As colunas category, type e v1 a v5 tentam facilitar o trabalho de filtrar as linhas. Há seis grandes categorias.\n\ncount(bcb, category)\n\n# A tibble: 6 × 2\n  category            n\n  &lt;fct&gt;           &lt;int&gt;\n1 contabil          303\n2 credito        285853\n3 direcionamento   5223\n4 fontes            604\n5 indices           555\n6 imoveis         25353\n\n\nCada série é identificada pela coluna series_info. Quebrando esta coluna em outras, fica um pouco mais fácil de encontrar as séries desejadas.\n\nbcb |&gt; \n  filter(category == \"contabil\") |&gt; \n  count(series_info, type, v1)\n\n# A tibble: 3 × 4\n  series_info                           type          v1              n\n  &lt;chr&gt;                                 &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt;\n1 contabil_bndu_imobiliario_br          bndu          imobiliario    81\n2 contabil_financiamento_comercial_br   financiamento comercial     111\n3 contabil_financiamento_residencial_br financiamento residencial   111\n\n\nO gráfico abaixo mostra o preço mediano do imóvel financiado nos estados da região Sul. Note que estas séries possuem um comportamento irregular em 2019.\n\ntbl_imoveis_sul &lt;- bcb |&gt; \n  filter(\n    category == \"imoveis\",\n    type == \"valor\",\n    v1 == \"compra\",\n    abbrev_state %in% c(\"RS\", \"SC\", \"PR\")\n    )\n\nggplot(tbl_imoveis_sul, aes(x = date, y = value, color = abbrev_state)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Preço de financiamento\",\n    x = NULL,\n    y = \"R$\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#séries-macroeconômicas",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#séries-macroeconômicas",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "A função get_bcb_series facilita a importação de séries macroeconômicas potencialmente relacionadas com o mercado imobiliário. Como há muitas séries, o argumento category facilita a seleção de temas específicos. A categoria de preços inlcui os principais índices de preço relacionados ao mercado, como o INCC, IPCA e IGPM.\n\nmacro &lt;- get_bcb_series(category = \"price\")\n\nunique(macro$name_pt)\n\n[1] \"Índice geral de preços do mercado (IGP-M)\"                                         \n[2] \"Índice geral de preços-disponibilidade interna (IGP-DI)\"                           \n[3] \"Índice nacional de custo da construção (INCC)\"                                     \n[4] \"Índice nacional de preços ao consumidor-amplo (IPCA)\"                              \n[5] \"Índice nacional de preços ao consumidor-Amplo (IPCA) - Habitação\"                  \n[6] \"Índice nacional de preços ao consumidor (INPC) - Habitação\"                        \n[7] \"Meios de pagamento - M1 (saldo em final de período) - Novo - sazonalmente ajustado\"\n\n\nO gráfico abaixo combina a série de preços do IGMI-R, importada anteriormente, com o IPCA. Os valores são acumulados em 12 meses e apresentados desde 2018. Nota-se como o preço dos imóveis cresceu acima da inflação a partir de 2020.\n\nseries_ipca &lt;- macro |&gt; \n  filter(name_simplified == \"ipca\") |&gt; \n  mutate(\n    acum12m = roll_prodr(1 + value / 100, n = 12) - 1,\n    series = \"ipca\"\n    ) |&gt; \n  select(date, series, acum12m)\n\nseries_igmi &lt;- sale |&gt; \n  filter(\n    source == \"IGMI-R\",\n    name_muni %in% c(\"Porto Alegre\", \"Brazil\")\n    ) |&gt; \n  select(date, series = name_muni, acum12m)\n\nseries_macro &lt;- rbind(series_ipca, series_igmi)\n\nseries_macro &lt;- series_macro |&gt; \n  filter(date &gt;= as.Date(\"2018-01-01\"), date &lt;= as.Date(\"2023-07-01\")) |&gt; \n  mutate(\n    series = factor(series, levels = c(\"Brazil\", \"Porto Alegre\", \"ipca\"))\n  )\n\nggplot(series_macro, aes(x = date, y = acum12m, color = series)) +\n  geom_line(linewidth = 0.8) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\"),\n    labels = c(\"Brasil (geral)\", \"Porto Alegre\", \"IPCA\")\n    ) +\n  labs(\n    title = \"Preços de imóveis crescem acima da inflação\",\n    x = NULL,\n    y = \"Acumulado 12 meses\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#preços-de-imóveis",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#preços-de-imóveis",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O Banco Central disponibiliza uma enorme variedade de séries relacionadas ao mercado imobiliário. A função get_bcb_realestate retorna mais de 3500 séries. Para agilizar o download destas bases, todas as funções oferecem a opção cached = TRUE. Usando esta opção o download é feito via GitHub e tende a ser mais rápido; o dado, contudo, corre o risco de estar levemente desatualizado.\n\nbcb &lt;- get_bcb_realestate(category = \"all\", cached = TRUE)\n\nlength(unique(bcb$series_info))\n\n[1] 3656\n\n\nInfelizmente, não é trivial usar esta base de dados. As colunas category, type e v1 a v5 tentam facilitar o trabalho de filtrar as linhas. Há seis grandes categorias.\n\ncount(bcb, category)\n\n# A tibble: 6 × 2\n  category            n\n  &lt;fct&gt;           &lt;int&gt;\n1 contabil          303\n2 credito        285853\n3 direcionamento   5223\n4 fontes            604\n5 indices           555\n6 imoveis         25353\n\n\nCada série é identificada pela coluna series_info. Quebrando esta coluna em outras, fica um pouco mais fácil de encontrar as séries desejadas.\n\nbcb |&gt; \n  filter(category == \"contabil\") |&gt; \n  count(series_info, type, v1)\n\n# A tibble: 3 × 4\n  series_info                           type          v1              n\n  &lt;chr&gt;                                 &lt;chr&gt;         &lt;chr&gt;       &lt;int&gt;\n1 contabil_bndu_imobiliario_br          bndu          imobiliario    81\n2 contabil_financiamento_comercial_br   financiamento comercial     111\n3 contabil_financiamento_residencial_br financiamento residencial   111\n\n\nO gráfico abaixo mostra o preço mediano do imóvel financiado nos estados da região Sul. Note que estas séries possuem um comportamento irregular em 2019.\n\ntbl_imoveis_sul &lt;- bcb |&gt; \n  filter(\n    category == \"imoveis\",\n    type == \"valor\",\n    v1 == \"compra\",\n    abbrev_state %in% c(\"RS\", \"SC\", \"PR\")\n    )\n\nggplot(tbl_imoveis_sul, aes(x = date, y = value, color = abbrev_state)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(\n    name = \"\", values = c(\"#264653\", \"#2a9d8f\", \"#e9c46a\")\n    ) +\n  labs(\n    title = \"Preço de financiamento\",\n    x = NULL,\n    y = \"R$\"\n  ) +\n  theme_light() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/general-posts/2023-10-realestatebr/index.html#caminhos-futuros",
    "href": "posts/general-posts/2023-10-realestatebr/index.html#caminhos-futuros",
    "title": "Um pacote para dados do mercado imobiliário",
    "section": "",
    "text": "O desenvolvimento futuro do pacote será guiado pela incorporação de mais bases de dados e relatórios de mercado."
  },
  {
    "objectID": "posts/general-posts/2023-10-wz-census/index.html",
    "href": "posts/general-posts/2023-10-wz-census/index.html",
    "title": "Weekly Viz - Brazilian Census",
    "section": "",
    "text": "This week I show the population growth rate of cities in Brazil using the recent 2022 Census data. Overall, a large share of cities registered negative growth rates in the past 12 years: 43.28% of cities experienced a decrease in population. The spatial patterns of population growth varied by each region. The southern region recorded a large share of cities with declining population (44.8%), mainly due to demographic factors: the region has the lowest fertility rate and the highest elder to youth ratio. The Southeast and the Midwest had the largest share of cities with growth: 61.1% and 64.9% respectively.\n\n\n\n\nBoth the north and northeastern cities benefit from relatively younger demographics. Adult population is still - by far - the largest demographic group and fertility rates are usually above the national average. These cities usually have the lowest share of above 65 years population. Some regions, and the state of Roraíma in particular, have also benefited from immigration from neighbor Venezuela.\n\n\n\n\n\n\n\n\nDespite favorable demographics, northeastern cities are aging rapidly and shrinking in size. Alagoas and Bahia registered some of the lowest growth rates at 0.02% and 0.07% respectively. The region, as a whole, struggles with safety issues, high unemployment, and stagnant income growth.\n\n\n\n\n\n\n\n\nThe southeast region concentrates the largest share of GDP and population in Brazil. Demographic indicators show a large elder population and very low fertility rates. Growth in São Paulo and Espírito Santo has been close to the national average while Rio de Janeiro and Minas Gerais lag behind. The latter state has the second lowest growth rate, 0.03%. As shown in the previous Brazil in Charts post, Rio de Janeiro has the highest unemployment rate among the southeastern states.\n\n\n\n\n\n\n\n\nThe southern region cities registered significant population losses. The southernmost state, Rio Grande do Sul, had a growth rate of only 0.14%. Economic stagnation, fertility rate decreases, and the rise of safety problems have also increased emigrations flows. Santa Catarina had the second highest growth of all states, 1.66%, falling only behind Roraíma, which received a huge influx of Venezuelan migrants. Santa Catarina boasts low unemployment, high income, and solid economic growth. The northeastern cities and the Chapecó metropolitan region, in particular, had some of the highest growth rates among large cities (over 100 thousand inhabitants).\n\n\n\n\n\n\n\n\nThe midwest cities exhibited significant population growth. Agriculture is still the most productive Brazilian sector and the Midwest accounts for the majority of the countries agricultural output. High income per capita and job opportunities make these cities attractive destinations for migrants.\nThe map can be deceptive, since some of the bigger municipalities actually have medium or small populations.\n\n\n\n\n\n\n\n\n\nThe geometric growth rate expresses the change in a population, assuming that it increases or decreases at a fixed rate.\nLets assume that population at a given time, \\(P_{t}\\), increases at a fixed rate \\(R\\). Then, the relative population change/growth \\(\\frac{\\Delta P_{t}}{P_{t}}\\) is given by\n\\[\n\\frac{\\Delta P_{t}}{P_{t}} = \\frac{P_{t+1} - P_{t}}{P_{t}} = R\n\\]\nRearranging terms,\n\\[\nP_{t+1} = P_{t} + RP_{t} = (1 + R)P_{t}\n\\]\nwhich states that the population in a future period is simply the current population increased by \\(RP_{t}\\) where \\(R\\) is the geometric growth rate. Since this growth rate is constant, we can generalize that\n\\[\nP_{t+n} = (1+R)P_{t+n-1} = (1+R)^2P_{t+n-2} = \\dots = (1+R)^nP_{t}\n\\]\nThis means that the population in any given time in the future is equal to the current population increased by the compounded geometric rate of growth. This is likely an unrealistic assumption for long periods of time but is a decent approximation in short periods. This last equation also provides the definition for the geometric growth rate:\n\\[\nR = (\\frac{P_{t+n}}{P_{t}})^\\frac{1}{n} - 1\n\\]"
  },
  {
    "objectID": "posts/general-posts/2023-10-wz-census/index.html#brazilian-census-in-maps",
    "href": "posts/general-posts/2023-10-wz-census/index.html#brazilian-census-in-maps",
    "title": "Weekly Viz - Brazilian Census",
    "section": "",
    "text": "Both the north and northeastern cities benefit from relatively younger demographics. Adult population is still - by far - the largest demographic group and fertility rates are usually above the national average. These cities usually have the lowest share of above 65 years population. Some regions, and the state of Roraíma in particular, have also benefited from immigration from neighbor Venezuela.\n\n\n\n\n\n\n\n\nDespite favorable demographics, northeastern cities are aging rapidly and shrinking in size. Alagoas and Bahia registered some of the lowest growth rates at 0.02% and 0.07% respectively. The region, as a whole, struggles with safety issues, high unemployment, and stagnant income growth.\n\n\n\n\n\n\n\n\nThe southeast region concentrates the largest share of GDP and population in Brazil. Demographic indicators show a large elder population and very low fertility rates. Growth in São Paulo and Espírito Santo has been close to the national average while Rio de Janeiro and Minas Gerais lag behind. The latter state has the second lowest growth rate, 0.03%. As shown in the previous Brazil in Charts post, Rio de Janeiro has the highest unemployment rate among the southeastern states.\n\n\n\n\n\n\n\n\nThe southern region cities registered significant population losses. The southernmost state, Rio Grande do Sul, had a growth rate of only 0.14%. Economic stagnation, fertility rate decreases, and the rise of safety problems have also increased emigrations flows. Santa Catarina had the second highest growth of all states, 1.66%, falling only behind Roraíma, which received a huge influx of Venezuelan migrants. Santa Catarina boasts low unemployment, high income, and solid economic growth. The northeastern cities and the Chapecó metropolitan region, in particular, had some of the highest growth rates among large cities (over 100 thousand inhabitants).\n\n\n\n\n\n\n\n\nThe midwest cities exhibited significant population growth. Agriculture is still the most productive Brazilian sector and the Midwest accounts for the majority of the countries agricultural output. High income per capita and job opportunities make these cities attractive destinations for migrants.\nThe map can be deceptive, since some of the bigger municipalities actually have medium or small populations."
  },
  {
    "objectID": "posts/general-posts/2023-10-wz-census/index.html#what-is-the-geometric-growth-rate",
    "href": "posts/general-posts/2023-10-wz-census/index.html#what-is-the-geometric-growth-rate",
    "title": "Weekly Viz - Brazilian Census",
    "section": "",
    "text": "The geometric growth rate expresses the change in a population, assuming that it increases or decreases at a fixed rate.\nLets assume that population at a given time, \\(P_{t}\\), increases at a fixed rate \\(R\\). Then, the relative population change/growth \\(\\frac{\\Delta P_{t}}{P_{t}}\\) is given by\n\\[\n\\frac{\\Delta P_{t}}{P_{t}} = \\frac{P_{t+1} - P_{t}}{P_{t}} = R\n\\]\nRearranging terms,\n\\[\nP_{t+1} = P_{t} + RP_{t} = (1 + R)P_{t}\n\\]\nwhich states that the population in a future period is simply the current population increased by \\(RP_{t}\\) where \\(R\\) is the geometric growth rate. Since this growth rate is constant, we can generalize that\n\\[\nP_{t+n} = (1+R)P_{t+n-1} = (1+R)^2P_{t+n-2} = \\dots = (1+R)^nP_{t}\n\\]\nThis means that the population in any given time in the future is equal to the current population increased by the compounded geometric rate of growth. This is likely an unrealistic assumption for long periods of time but is a decent approximation in short periods. This last equation also provides the definition for the geometric growth rate:\n\\[\nR = (\\frac{P_{t+n}}{P_{t}})^\\frac{1}{n} - 1\n\\]"
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html",
    "href": "posts/general-posts/2023-10-censo-erros/index.html",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Houve certo rebuliço nas redes sociais, quando do lançamento dos dados mais recentes do Censo Demográfico de 2022. O fato carregado na maior parte das manchetes do Brasil foi a queda no número projetado da população brasileira. Até 2021, projetava-se que a população brasileira estivesse em torno de 213 milhões de habitantes, segundo a pesquisa Estimativas da População do IBGE. O número que o Censo trouxe foi de 203 milhões, ou seja, houve uma queda de 10 milhões de habitantes em relação ao previsto. Ainda que pareça grande, o “erro de projeção” foi de menos de 5%. Contudo, é importante entender por que este número foi subestimado.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMesmo antes da divulgação dos primeiros dados de população, a mais recente edição do Censo enfrentou diversos problemas. A eclosão da Pandemia Covid-19, em 2020, forçou o adiamento da pesquisa para o ano seguinte; em 2021, contudo, o orçamento do Censo, originalmente de R$2 bilhão foi reduzido em mais de 90%, efetivamente impossibilitando a sua execução. Inicialmente, previa-se que haveria cerca de 200 mil recenseadores em operação: na prática, houve menos da metade, em torno de 91 mil; além disso, a redução do orçamento comprometeu o treinamento destes funcionários. A situação chegou a tal ponto que se decretou uma medida provisória permitindo a contratação de recenseadores sem processo seletivo algum.\nO processo de coleta dos dados enfrentou diversos atrasos, sobretudo em função do número insuficiente de recenseadores em campo. A falta de verba também comprometeu o pagamento destes funcionários, que chegaram a organizar uma greve em setembro de 2022, em resposta às más condições de trabalho.\n\n\n\nA fraca divulgação do Censo na imprensa, junto com a politização da pesquisa e a propagação de fake news, reduziu a adesão da população à pesquisa. A taxa geral de não-resposta foi de 4,23%, mas chegou a 8,11% no estado de São Paulo; em alguns municípios como Santana de Parnaíba, na Região Metropolitana de São Paulo, esta taxa chegou a 16,8%1. O problema foi particularmente severo nos domicílios de alta renda e condomínios fechados; também há evidência de que cidades, com maior proporção de votos para Bolsonaro no 2o turno , tiveram menor adesão ao Censo.\n\n\n\nPor fim, como os dados de população de municípios têm um impacto direto sobre o repasse do Fundo de Participação de Municípios as prefeitura tem um incentivo perverso para distorcer os dados populacionais. De fato, o economista Leonardo Monastério verificou que isto já aconteceu em Censos anteriores e que o problema vem se agravando com o tempo. No histograma abaixo, retirado do trabalho original, vê-se que há quebras suspeitas nos valores da população que coincidem as faixas do FPM. Será interessante replicar o experimento para os dados atuais do Censo.\n\n\n\nReprodução de gráfico de Monasterio (2013)\n\n\n\n\n\nO Censo enfrentou diversos problemas:\n\nPandemia do Covid-19, cortes no orçamento e mudanças na presidência. Além de gerar atrasos na pesquisa, os cortes comprometeram a capacidade de contratação e treinamento de funcionários.\nFraca divulgação da pesquisa e menor adesão da população. Autoridades importantes do governo, como o próprio presidente da república à época minaram a credibilidade do IBGE publicamente.\n\nA ex-presidente do IBGE, Wasmália Bivar, avalia que o corte de orçamento comprometeu a pesquisa e que a subestimação da população era esperada. Já Roberto Olinto, também ex-presidente do instituto, pediu por uma auditoria do Censo, chegando a sugerir a possibilidade de ser necessário refazer a pesquisa. Apesar de todas as dificuldades, os representantes do IBGE defendem que o número divulgado é confiável e avaliam que o Censo foi um sucesso, quando se considera todos os desafios que foram enfrentados.\n\n\n\n\n\n\nA fórmula da dinâmica populacional é bastante simples. A população num determinado ano é igual à população do ano anterior somada da variação populacional. Esta variação populacional é o (1) número total de nascimentos, (2) óbitos, (3) imigrantes e emigrantes.\nFormalmente, seja a população no ano seguinte denotada por \\(P_{t+1}\\) e população no ano corrente, \\(P_{t}\\). A partir deste número, soma-se o total de nascimentos \\(B_{t, t+1}\\), subtrai-se o total de óbitos \\(D_{t, t+1}\\) e soma-se o fluxo líquido de migrantes \\(NM_{t, t+1}\\) . A equação abaixo resume estes fatos:\n\\[\nP_{t+1} = P_{t} + B_{t,t+1} - D_{t,t+1} + NM_{t,t+1}\n\\]\nO IBGE tem boas maneiras de estimar estes números:\n\nO número de nascimentos, \\(B_{t, t+1}\\), é uma função da taxa de fertilidade e do número de mulheres em idade fértil;\nO número de óbitos, \\(D_{t,t+1}\\), similarmente, pode ser estimado a partir de tábuas atuariais de mortalidade, discriminadas por grupos de idade e sexo;\nPor fim, ainda que seja difícil quantificar o fluxo migratório, \\(NM_{t,t+1}\\), ele tem pouco impacto no número final, no caso do Brasil2.\n\nA projeção da população, segundo a equação acima, é chamada de Método das Componentes Demográficas. Esta metodologia tem amplo respaldo teórico3 e é utilizada na pesquisa Projeções da População do IBGE. Na mais recente edição divulgada, de 2018, a projeção populacional do Brasil para 2022 é de 214,8 milhões de habitantes.\nO Método das Componentes Demográficas também é utilizado pela Organização das Nações Unidas (ONU). Na edição de 2019 do World Population Prospects, projetava-se que a população do Brasil deveria chegar a 219 milhões de habitantes em 2025. Este número é muito próximo ao projetado pelo Projeções da População, citado acima. Segundo os dados do Censo, contudo, seria necessário um crescimento de 16 milhões de habitantes para alcançar este resultado.\n\n\n\nUma maneira ainda mais simples de modelar a dinâmica da população é via uma equação diferencial que expressa o crescimento exponencial da população. Na prática, populações frequentemente exibem comportamentos deste tipo, crescendo ou decaindo exponencialmente a uma taxa fixa. Esta taxa fixa \\(r\\) pode ser interpretada como a taxa geométrica de crescimento (TGC) da população.\n\\[\nP_{t} = P_{0}\\times(1 + r)^{t}\n\\]\nSupondo, que a TGC observada entre os censos de 2000 e 2010, de 1,18%, permancesse constante durante os próximos doze anos, a população em 2022 deveria ser de 220 milhões. Usando uma estimativa mais conservadora de 0,97% (TGC entre 2009 e 2010) a população seria de 214 milhões - valor muito próximo ao previsto pelo mais complexo Método de Componentes Demográficas acima. Por fim, utilizando a estimativa da TGC do IBGE em 2010, de 0,88%, a população brasileira deveria crescer para 212 milhões. A TGC verifica pelo Censo foi de 0,52%, que resulta na população de 203 milhões verificada.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparando os dados da Projeção da População, de 2018, e do Censo Demográfico de 2022 podemos mensurar a discrepância entre os valores projetados e os efetivamente verificados. De maneira geral, as projeções superestimaram o crescimento populacional em todas as Unidades Federativas com exceção de Mato Grosso e Santa Catarina. Em alguns casos, como no Amapá a diferença foi de quase 18%: projetou-se uma população de 886 mil, mas verificou apenas 811 mil. Em termos absolutos, a maior diferença aconteceu no estado de São Paulo, onde o Censo encontrou um valor 2,5 milhões inferior ao projetado.\n\n\n\n\n\n\n\nNos termos da equação populacional acima, a divergência do Censo deve ser explicada por:\n\nSuperestimação do número de nascimentos. Isto pode ter ocorrido em função da queda de renda e aumento de desemprego causado pela recessão de 2015-174 e também pela Pandemia5. Também pode ser o caso de que a taxa de fecundidade do Brasil diminuiu mais rápido do que o esperado.\nSubestimação do número de óbitos. A Pandemia trouxe muito mais óbitos do que qualquer modelo demográfico razoável poderia prever.\nSubestimação do fluxo migratório para fora do país. Desde a crise econômica, aumentou o número de emigrantes no Brasil e taxa de brasileiros que não voltaram ao país também subiu.\n\nNo Brasil, os fluxos migratórios são pequenos, relativamente ao tamanho da população. Em 2021, por exemplo, o saldo líquido migratório brasileiro foi negativo em cerca de 300 mil pessoas6, equivalente a cerca de 0,1% da população. Assim, o número de nascimentos e óbitos deve explicar a maior parte da diferença entre o valor estimado pelo Censo e o valor projetado anteriormente.\n\n\n\nAs séries de nascimentos e óbitos, do Registro Civil, apontam que houve uma queda na tendência de nascimentos durante a última década7. Olhando para a tendência da série, vê-se que a crise econômica de 2015-17 parece ter tido impacto negativo no número de nascidos vivos. Mesmo depois da recuperação da crise, a série segue numa tendência de queda, que se acentua a partir da Pandemia.\nJá a série de óbitos segue uma tendência estável de crescimento desde 2003. Houve um ligeiro aumento dos óbitos em 2015, mas rapidamente viu-se um retorno da série à sua tendência de longo prazo. Por fim, é notável como a Pandemia teve um efeito severo sobre o número de óbitos no país.\n\n\n\n\n\n\n\n\n\nModelando o comportamento da série de óbitos, pode-se elaborar uma espécie de contrafactual: isto é, pode-se ter uma estimativa de como teria sido o número de óbitos caso na ausência da pandemia.\nNo gráfico abaixo, a curva amarela mostra os valores preditos da série junto com intervalo de confiança de 95%. O modelo é treinado com os dados de janeiro de 2003 a dezembro de 2019. Nota-se como os valores previstos divergem dos valores observados a partir de abril de 2020. Ao longo do período, o número acumulado de óbitos, acima do previsto pelo modelo, supera 600 mil.\n\n\n\n\n\n\n\n\n\nComparando as projeções de curto prazo do IBGE para nascimentos e óbitos, divulgadas em 2018, com os dados mais recentes do Registro Civil, divulgados em 2022, pode-se ver como o número de nascimentos foi superestimado e o número de óbitos, subestimado. Ainda assim, a divergência entre os números não é grande o suficiente para explicar a diferença de 10 milhões de habitantes verificada pelo Censo. Grosso modo, houve cerca de 400 mil mortes a mais do que o projetado e 800 mil nascimentos a menos.\n\n\n\n\n\n\n\n\n\nPode-se também usar os dados do Censo de 2010 e atualizar os valores populacionais, ano a ano, usando os nascimentos e óbitos do Registro Civil. O economista Francisco Faria, no blog do IBRE, simulou este cenário e encontrou populações de 208,5 ou 212,9 milhões, a depender se usa-se o valor original ou revisado do Censo de 2010. De qualquer modo, ambos os valores estão acima do valor de 203 milhões.\nRecentemente, o IBGE divulgou os resultados de Censo por grupos de idade o que nos permite fazer ainda mais uma comparação. Os valores projetados estão disponíveis em grupos quinquenais de idade-sexo até 80 anos. O primeiro gráfico abaixo mostra a pirâmide populacional observada no Censo de 2022; as colunas transparentes mostram o valor projetado em cada grupo: vê-se como o valor observado foi menor em praticamente todos os grupos.\nO gráfico da direita mostra a diferença percentual entre o valor projetado e o valor observado em cada grupo de idade. É interessante notar que as projeções, aparentemente, superestimaram o número de novos nascidos vivos. Projetou-se quase 15 milhões de recém-nascidos, de 0 a 4 anos, mas verificou-se algo em torno de 12.7 milhões. O número de jovens e de adultos também foi superestimado, enquanto a população de 55 a 74 anos observada ficou muito próxima da projetada.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNão há consenso ainda que explique a discrepância verificada entre as projeções do IBGE e o valor auferido no Censo. Certamente, a causa será um misto dos três motivos apresentados acima: nascimentos, óbitos e fluxos migratórios. Somente com o tempo será possível entender melhor o que houve com o Censo de 2022. Ainda que os dados do Censo possam apresentar algum erro, a tendência geral demográfica é certa, o Brasil:\n\nTem uma proporção cada vez maior de idosos em relação a jovens.\nApresenta taxa de crescimento populacional decrescente.\nEm alguns anos vai começar a observar quedas na sua população.\n\nEstes três fatos já são a realidade de países desenvolvidos como Itália, Coreia do Sul, Japão, Alemanha e tantos outros. Até agora nenhum país conseguiu reverter a queda da taxa de fecundidade, mesmo com generosos benefícios fiscais. Os poucos países ricos que conseguem manter algum crescimento populacional se beneficiam de grandes influxos de migrantes que, por sua vez, trazem desafios adicionais. O Brasil encontra-se numa posição delicada, pois ele chega ao fim do seu crescimento populacional com um PIB per capita ainda relativamente baixo; além disso, o Brasil, há muitos anos, é um país com fluxo migratório negativo, isto é, mais brasileiros emigram (saem) do país do que estrangeiros imigram para cá.\nNo próximo texto desta série vou explorar mais a fundo os dados por cidades e metrópoles. O envelhecimento da população e acúmulo de problemas urbanos em grandes cidades parece estar reduzindo o puxo destas metrópoles. Rio de Janeiro e Salvador, por exemplo, registraram perdas populacionais que, muito provavelmente, devem-se a fluxos migratórios internos.\nPor fim, vale terminar o texto relembrando que os funcionários do IBGE são extremamente competentes e plenamente capacitados. O Censo sofreu, infortuitamente, cortes de orçamento severos e teve de ser atrasado devido à pandemia. Os dados do Censo de 2022, assim como dos Censos anteriores, serão revisados futuramente e, muito provavelmente, as discrepâncias mencionadas neste texto serão dirmidas."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#o-contexto-da-pesquisa",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#o-contexto-da-pesquisa",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Mesmo antes da divulgação dos primeiros dados de população, a mais recente edição do Censo enfrentou diversos problemas. A eclosão da Pandemia Covid-19, em 2020, forçou o adiamento da pesquisa para o ano seguinte; em 2021, contudo, o orçamento do Censo, originalmente de R$2 bilhão foi reduzido em mais de 90%, efetivamente impossibilitando a sua execução. Inicialmente, previa-se que haveria cerca de 200 mil recenseadores em operação: na prática, houve menos da metade, em torno de 91 mil; além disso, a redução do orçamento comprometeu o treinamento destes funcionários. A situação chegou a tal ponto que se decretou uma medida provisória permitindo a contratação de recenseadores sem processo seletivo algum.\nO processo de coleta dos dados enfrentou diversos atrasos, sobretudo em função do número insuficiente de recenseadores em campo. A falta de verba também comprometeu o pagamento destes funcionários, que chegaram a organizar uma greve em setembro de 2022, em resposta às más condições de trabalho.\n\n\n\nA fraca divulgação do Censo na imprensa, junto com a politização da pesquisa e a propagação de fake news, reduziu a adesão da população à pesquisa. A taxa geral de não-resposta foi de 4,23%, mas chegou a 8,11% no estado de São Paulo; em alguns municípios como Santana de Parnaíba, na Região Metropolitana de São Paulo, esta taxa chegou a 16,8%1. O problema foi particularmente severo nos domicílios de alta renda e condomínios fechados; também há evidência de que cidades, com maior proporção de votos para Bolsonaro no 2o turno , tiveram menor adesão ao Censo.\n\n\n\nPor fim, como os dados de população de municípios têm um impacto direto sobre o repasse do Fundo de Participação de Municípios as prefeitura tem um incentivo perverso para distorcer os dados populacionais. De fato, o economista Leonardo Monastério verificou que isto já aconteceu em Censos anteriores e que o problema vem se agravando com o tempo. No histograma abaixo, retirado do trabalho original, vê-se que há quebras suspeitas nos valores da população que coincidem as faixas do FPM. Será interessante replicar o experimento para os dados atuais do Censo.\n\n\n\nReprodução de gráfico de Monasterio (2013)\n\n\n\n\n\nO Censo enfrentou diversos problemas:\n\nPandemia do Covid-19, cortes no orçamento e mudanças na presidência. Além de gerar atrasos na pesquisa, os cortes comprometeram a capacidade de contratação e treinamento de funcionários.\nFraca divulgação da pesquisa e menor adesão da população. Autoridades importantes do governo, como o próprio presidente da república à época minaram a credibilidade do IBGE publicamente.\n\nA ex-presidente do IBGE, Wasmália Bivar, avalia que o corte de orçamento comprometeu a pesquisa e que a subestimação da população era esperada. Já Roberto Olinto, também ex-presidente do instituto, pediu por uma auditoria do Censo, chegando a sugerir a possibilidade de ser necessário refazer a pesquisa. Apesar de todas as dificuldades, os representantes do IBGE defendem que o número divulgado é confiável e avaliam que o Censo foi um sucesso, quando se considera todos os desafios que foram enfrentados."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#a-matemática-da-população",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#a-matemática-da-população",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "A fórmula da dinâmica populacional é bastante simples. A população num determinado ano é igual à população do ano anterior somada da variação populacional. Esta variação populacional é o (1) número total de nascimentos, (2) óbitos, (3) imigrantes e emigrantes.\nFormalmente, seja a população no ano seguinte denotada por \\(P_{t+1}\\) e população no ano corrente, \\(P_{t}\\). A partir deste número, soma-se o total de nascimentos \\(B_{t, t+1}\\), subtrai-se o total de óbitos \\(D_{t, t+1}\\) e soma-se o fluxo líquido de migrantes \\(NM_{t, t+1}\\) . A equação abaixo resume estes fatos:\n\\[\nP_{t+1} = P_{t} + B_{t,t+1} - D_{t,t+1} + NM_{t,t+1}\n\\]\nO IBGE tem boas maneiras de estimar estes números:\n\nO número de nascimentos, \\(B_{t, t+1}\\), é uma função da taxa de fertilidade e do número de mulheres em idade fértil;\nO número de óbitos, \\(D_{t,t+1}\\), similarmente, pode ser estimado a partir de tábuas atuariais de mortalidade, discriminadas por grupos de idade e sexo;\nPor fim, ainda que seja difícil quantificar o fluxo migratório, \\(NM_{t,t+1}\\), ele tem pouco impacto no número final, no caso do Brasil2.\n\nA projeção da população, segundo a equação acima, é chamada de Método das Componentes Demográficas. Esta metodologia tem amplo respaldo teórico3 e é utilizada na pesquisa Projeções da População do IBGE. Na mais recente edição divulgada, de 2018, a projeção populacional do Brasil para 2022 é de 214,8 milhões de habitantes.\nO Método das Componentes Demográficas também é utilizado pela Organização das Nações Unidas (ONU). Na edição de 2019 do World Population Prospects, projetava-se que a população do Brasil deveria chegar a 219 milhões de habitantes em 2025. Este número é muito próximo ao projetado pelo Projeções da População, citado acima. Segundo os dados do Censo, contudo, seria necessário um crescimento de 16 milhões de habitantes para alcançar este resultado.\n\n\n\nUma maneira ainda mais simples de modelar a dinâmica da população é via uma equação diferencial que expressa o crescimento exponencial da população. Na prática, populações frequentemente exibem comportamentos deste tipo, crescendo ou decaindo exponencialmente a uma taxa fixa. Esta taxa fixa \\(r\\) pode ser interpretada como a taxa geométrica de crescimento (TGC) da população.\n\\[\nP_{t} = P_{0}\\times(1 + r)^{t}\n\\]\nSupondo, que a TGC observada entre os censos de 2000 e 2010, de 1,18%, permancesse constante durante os próximos doze anos, a população em 2022 deveria ser de 220 milhões. Usando uma estimativa mais conservadora de 0,97% (TGC entre 2009 e 2010) a população seria de 214 milhões - valor muito próximo ao previsto pelo mais complexo Método de Componentes Demográficas acima. Por fim, utilizando a estimativa da TGC do IBGE em 2010, de 0,88%, a população brasileira deveria crescer para 212 milhões. A TGC verifica pelo Censo foi de 0,52%, que resulta na população de 203 milhões verificada."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#a-discrepância-do-censo",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#a-discrepância-do-censo",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Comparando os dados da Projeção da População, de 2018, e do Censo Demográfico de 2022 podemos mensurar a discrepância entre os valores projetados e os efetivamente verificados. De maneira geral, as projeções superestimaram o crescimento populacional em todas as Unidades Federativas com exceção de Mato Grosso e Santa Catarina. Em alguns casos, como no Amapá a diferença foi de quase 18%: projetou-se uma população de 886 mil, mas verificou apenas 811 mil. Em termos absolutos, a maior diferença aconteceu no estado de São Paulo, onde o Censo encontrou um valor 2,5 milhões inferior ao projetado.\n\n\n\n\n\n\n\nNos termos da equação populacional acima, a divergência do Censo deve ser explicada por:\n\nSuperestimação do número de nascimentos. Isto pode ter ocorrido em função da queda de renda e aumento de desemprego causado pela recessão de 2015-174 e também pela Pandemia5. Também pode ser o caso de que a taxa de fecundidade do Brasil diminuiu mais rápido do que o esperado.\nSubestimação do número de óbitos. A Pandemia trouxe muito mais óbitos do que qualquer modelo demográfico razoável poderia prever.\nSubestimação do fluxo migratório para fora do país. Desde a crise econômica, aumentou o número de emigrantes no Brasil e taxa de brasileiros que não voltaram ao país também subiu.\n\nNo Brasil, os fluxos migratórios são pequenos, relativamente ao tamanho da população. Em 2021, por exemplo, o saldo líquido migratório brasileiro foi negativo em cerca de 300 mil pessoas6, equivalente a cerca de 0,1% da população. Assim, o número de nascimentos e óbitos deve explicar a maior parte da diferença entre o valor estimado pelo Censo e o valor projetado anteriormente.\n\n\n\nAs séries de nascimentos e óbitos, do Registro Civil, apontam que houve uma queda na tendência de nascimentos durante a última década7. Olhando para a tendência da série, vê-se que a crise econômica de 2015-17 parece ter tido impacto negativo no número de nascidos vivos. Mesmo depois da recuperação da crise, a série segue numa tendência de queda, que se acentua a partir da Pandemia.\nJá a série de óbitos segue uma tendência estável de crescimento desde 2003. Houve um ligeiro aumento dos óbitos em 2015, mas rapidamente viu-se um retorno da série à sua tendência de longo prazo. Por fim, é notável como a Pandemia teve um efeito severo sobre o número de óbitos no país.\n\n\n\n\n\n\n\n\n\nModelando o comportamento da série de óbitos, pode-se elaborar uma espécie de contrafactual: isto é, pode-se ter uma estimativa de como teria sido o número de óbitos caso na ausência da pandemia.\nNo gráfico abaixo, a curva amarela mostra os valores preditos da série junto com intervalo de confiança de 95%. O modelo é treinado com os dados de janeiro de 2003 a dezembro de 2019. Nota-se como os valores previstos divergem dos valores observados a partir de abril de 2020. Ao longo do período, o número acumulado de óbitos, acima do previsto pelo modelo, supera 600 mil.\n\n\n\n\n\n\n\n\n\nComparando as projeções de curto prazo do IBGE para nascimentos e óbitos, divulgadas em 2018, com os dados mais recentes do Registro Civil, divulgados em 2022, pode-se ver como o número de nascimentos foi superestimado e o número de óbitos, subestimado. Ainda assim, a divergência entre os números não é grande o suficiente para explicar a diferença de 10 milhões de habitantes verificada pelo Censo. Grosso modo, houve cerca de 400 mil mortes a mais do que o projetado e 800 mil nascimentos a menos.\n\n\n\n\n\n\n\n\n\nPode-se também usar os dados do Censo de 2010 e atualizar os valores populacionais, ano a ano, usando os nascimentos e óbitos do Registro Civil. O economista Francisco Faria, no blog do IBRE, simulou este cenário e encontrou populações de 208,5 ou 212,9 milhões, a depender se usa-se o valor original ou revisado do Censo de 2010. De qualquer modo, ambos os valores estão acima do valor de 203 milhões.\nRecentemente, o IBGE divulgou os resultados de Censo por grupos de idade o que nos permite fazer ainda mais uma comparação. Os valores projetados estão disponíveis em grupos quinquenais de idade-sexo até 80 anos. O primeiro gráfico abaixo mostra a pirâmide populacional observada no Censo de 2022; as colunas transparentes mostram o valor projetado em cada grupo: vê-se como o valor observado foi menor em praticamente todos os grupos.\nO gráfico da direita mostra a diferença percentual entre o valor projetado e o valor observado em cada grupo de idade. É interessante notar que as projeções, aparentemente, superestimaram o número de novos nascidos vivos. Projetou-se quase 15 milhões de recém-nascidos, de 0 a 4 anos, mas verificou-se algo em torno de 12.7 milhões. O número de jovens e de adultos também foi superestimado, enquanto a população de 55 a 74 anos observada ficou muito próxima da projetada."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#o-futuro-do-brasil",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#o-futuro-do-brasil",
    "title": "Censo 2022: O que houve de errado?",
    "section": "",
    "text": "Não há consenso ainda que explique a discrepância verificada entre as projeções do IBGE e o valor auferido no Censo. Certamente, a causa será um misto dos três motivos apresentados acima: nascimentos, óbitos e fluxos migratórios. Somente com o tempo será possível entender melhor o que houve com o Censo de 2022. Ainda que os dados do Censo possam apresentar algum erro, a tendência geral demográfica é certa, o Brasil:\n\nTem uma proporção cada vez maior de idosos em relação a jovens.\nApresenta taxa de crescimento populacional decrescente.\nEm alguns anos vai começar a observar quedas na sua população.\n\nEstes três fatos já são a realidade de países desenvolvidos como Itália, Coreia do Sul, Japão, Alemanha e tantos outros. Até agora nenhum país conseguiu reverter a queda da taxa de fecundidade, mesmo com generosos benefícios fiscais. Os poucos países ricos que conseguem manter algum crescimento populacional se beneficiam de grandes influxos de migrantes que, por sua vez, trazem desafios adicionais. O Brasil encontra-se numa posição delicada, pois ele chega ao fim do seu crescimento populacional com um PIB per capita ainda relativamente baixo; além disso, o Brasil, há muitos anos, é um país com fluxo migratório negativo, isto é, mais brasileiros emigram (saem) do país do que estrangeiros imigram para cá.\nNo próximo texto desta série vou explorar mais a fundo os dados por cidades e metrópoles. O envelhecimento da população e acúmulo de problemas urbanos em grandes cidades parece estar reduzindo o puxo destas metrópoles. Rio de Janeiro e Salvador, por exemplo, registraram perdas populacionais que, muito provavelmente, devem-se a fluxos migratórios internos.\nPor fim, vale terminar o texto relembrando que os funcionários do IBGE são extremamente competentes e plenamente capacitados. O Censo sofreu, infortuitamente, cortes de orçamento severos e teve de ser atrasado devido à pandemia. Os dados do Censo de 2022, assim como dos Censos anteriores, serão revisados futuramente e, muito provavelmente, as discrepâncias mencionadas neste texto serão dirmidas."
  },
  {
    "objectID": "posts/general-posts/2023-10-censo-erros/index.html#footnotes",
    "href": "posts/general-posts/2023-10-censo-erros/index.html#footnotes",
    "title": "Censo 2022: O que houve de errado?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecusas para entrevista não prejudicaram resultado final do Censo, diz IBGE (Folha SP)↩︎\nDe 2010 a 2020 houve um aumento de 24,4% no número de imigrantes no Brasil, resultando numa população de 1,3 milhão. Isto é equivalent a 0,6% da população total do Brasil. Apesar do Brasil ter hospedado imigrantes de diversos países ao longo do século XX, atualmente o país tem um fluxo migratório negativo (mais brasileiros saem do país do que estrangeiros entram) e uma população de imigrantes bastante reduzida.↩︎\nPara uma referência simples veja Census dos EUA.)↩︎\nExiste alguma evidência de que recessões econômicas diminuem as taxas de fecundidade, sobretudo em países em desenvolvimento. Veja, por exemplo, este artigo sobre a Colômbia e este sobre a Grécia.↩︎\nDe maneira geral, a Pandemia do Covid-19 teve um efeito negativo sobre as taxas de fecundidade e natalidade mundo afora. Em alguns países houve uma forte recuperação, mas em outros estas taxas seguem em níveis menores que os pré-pandêmicos. Link↩︎\nTaxa de brasileiros que saem do país e não voltam é a maior em 11 anos (Valor)↩︎\nA tendência das séries é estimada com uma média móvel centrada de 12 meses.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/10-grafico-de-area.html",
    "href": "posts/ggplot2-tutorial/10-grafico-de-area.html",
    "title": "Indo além: empilhando áreas",
    "section": "",
    "text": "Gráficos de área ajudam a visualizar a dinâmica de um conjunto de valores ao longo do tempo. Um gráfico de área é, essencialmente, um gráfico de linha, que passa por cima de uma região sombreada. Pode-se empilhar pequenos gráficos de linha uns sobre os outros de tal maneira que a área entre as linhas fica preenchida com cores. Isto permite que se veja a tendência geral dos dados, assim como a contribuição de cada grupo para o resultado total.\n\n\n\n\n\n\n\n\n\nEste post ensina como fazer gráficos de área usando o ggplot2 no R. Primeiro, vamos ganhar intuição construindo um gráfico simples a partir de dados simluados. Depois, vamos revisar os principais elementos estéticos de um gráfico de área.\nVisto o básico, partiremos para um caso aplicado, analisando as concessões de crédito direcionadas no Brasil. Neste exemplo, vamos usar o conhecimento de escalas e temas para aprimorar o gráfico de área. Por fim, vamos explorar uma base de projeções demográficas do Brasil usando gráficos de área.\n\n\nAntes de mais nada, precisamos instalar e carregar alguns pacotes. Assim como em posts anteriores, além do pacote ggplot2 vamos utilizar alguns pacotes auxiliares para facilitar a manipulação dos dados.\n\n#&gt; Instala o pacote ggplot2 (se necessário)\ninstall.packages(c(\"dplyr\", \"tidyr\", \"ggplot2\", \"RcppRoll\", \"GetBCBData\"))\n\n#&gt; Carrega os pacotes\nlibrary(\"ggplot2\")\n#&gt; Manipulação de dados\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"RcppRoll\")\n#&gt; Importar dados da API do Banco Central do Brasil\nlibrary(\"GetBCBData\")\n\n\n\nPrimeiro, vamos simular alguns dados para o nosso gráfico de área. No primeiro exemplo vamos montar uma base de dados (tibble) com os valores de uma série de vendas anuais de 2000 a 2005.\nA coluna ano é uma variável contínua de 2000 a 2005. As colunas venda_a, venda_b e venda_c representam números hipotéticos de venda três lojas distintas (“1”, “2” e “3”).\n\ntbl &lt;- tibble(\n  ano = 2000:2005,\n  venda_1 = c(20, 24, 23, 27, 25, 26),\n  venda_2 = c(30, 22, 17, 23, 21, 18),\n  venda_3 = c(18, 19, 17, 17, 18, 19)\n)\n\nA tabela tem a forma abaixo.\n\n\n\n\n\n\nano\nvenda_1\nvenda_2\nvenda_3\n\n\n\n\n2000\n20\n30\n18\n\n\n2001\n24\n22\n19\n\n\n2002\n23\n17\n17\n\n\n2003\n27\n23\n17\n\n\n2004\n25\n21\n18\n\n\n2005\n26\n18\n19\n\n\n\n\n\n\n\n\nUsamos a função geom_area() para criar um gráfico de área. Esta função recebe os argumentos x e y, que mapeiam as variáveis de dados no eixo x e no eixo y do gráfico, respectivamente.\nO código abaixo monta um gráfico de área que mostra o número de vendas na loja 1.\n\nggplot(data = tbl) +\n  geom_area(aes(x = ano, y = venda_1))\n\n\n\n\n\n\n\n\nPara adicionar as vendas das outras lojas precisamos remodelar o formato dos nossos dados. Nossos dados estão em formato “wide”, no qual cada variável (vendas) é uma coluna distinta e cada observação é uma linha. O ggplot2 segue os princípios de tidy data e funciona melhor com dados em formato “long”, onde cada linha é uma observação única.\nConvertemos os nossos dados para o formato “long” utilizando a função pivot_longer() da seguinte maneira:\n\nlong &lt;- pivot_longer(\n  tbl,\n  venda_1:venda_3,\n  names_to = \"loja\",\n  values_to = \"total\"\n  )\n\nAgora cada linha representa o número de vendas em uma loja específica e num ano específico. Além disso, cada coluna é uma variável diferente: a coluna ano indica que o ano da observação, a coluna loja indica a loja e a coluna total indica o total de vendas.\n\n\n\n\n\n\nano\nloja\ntotal\n\n\n\n\n2000\nvenda_1\n20\n\n\n2000\nvenda_2\n30\n\n\n2000\nvenda_3\n18\n\n\n2001\nvenda_1\n24\n\n\n2001\nvenda_2\n22\n\n\n2001\nvenda_3\n19\n\n\n2002\nvenda_1\n23\n\n\n2002\nvenda_2\n17\n\n\n2002\nvenda_3\n17\n\n\n2003\nvenda_1\n27\n\n\n2003\nvenda_2\n23\n\n\n2003\nvenda_3\n17\n\n\n2004\nvenda_1\n25\n\n\n2004\nvenda_2\n21\n\n\n2004\nvenda_3\n18\n\n\n2005\nvenda_1\n26\n\n\n2005\nvenda_2\n18\n\n\n2005\nvenda_3\n19\n\n\n\n\n\n\n\n\nAgora que temos nossos dados preparados, podemos montar nosso gráfico de área. Além dos argumentos x e y, também vamos especificar o argumento fill para indicar qual variável deve ser utilizada para preencher as cores entre as linhas.\n\nggplot(data = long) +\n  geom_area(aes(x = ano, y = total, fill = loja))\n\n\n\n\n\n\n\n\n\n\n\nNo gráfico acima, as áreas são empilhadas umas sobre as outras. Tipicamente, os grupos com os valores mais altos devem estar no topo da pilha e os grupos com valores mais baixos, na parte inferior. Na prática, a ordem é determinada pela ordem da variável fill. Neste caso, a ordem é dada pela variável categória loja1. Variáveis categóricas no R sempre devem ser do tipo factor. Já se apresentou factors anteriormente, mas vale a pena revisar esta importante classe.\nUm factor é um tipo especial de string que possui um ordenamento (levels) que define a relação hierárquica entre os grupos. Imagine que você tem uma série de avaliações que podem ser “Bom”, “Médio” ou “Ruim” armazenadas num vetor chamado feedback. Para estruturar esta variável como factor é preciso definir qual a ordem destes valores. No exemplo abaixo define-se uma relação crescente: de “Ruim” até “Bom”.\n\nfeedback &lt;- c(\"Bom\", \"Bom\", \"Médio\", \"Ruim\", \"Médio\", \"Médio\")\nsatisfacao &lt;- factor(feedback, levels = c(\"Ruim\", \"Médio\", \"Bom\"))\n\nNo caso do gráfico acima, como não definimos a ordem da variável categórica, o ggplot2 tenta adivinha-la. De maneira geral, o ggplot respeita a ordem alfabética e a ordem de grandeza numérica. Por isso, no gráfico acima a ordem foi venda_1, venda_2 e venda_3.\nPara definir a ordem dos lojass é preciso criar uma variável do tipo factor e especificar o argumento levels. O exemplo abaixo define uma nova ordem para os grupos o que resulta num gráfico diferente.\n\nlong_reordenado &lt;- long |&gt; \n  mutate(\n    loja = factor(loja, levels = c(\"venda_3\", \"venda_1\", \"venda_2\"))\n  )\n\nggplot(data = long_reordenado) +\n  geom_area(aes(x = ano, y = total, fill = loja))\n\n\n\n\n\n\n\n\n\n\n\n\nOs principais elementos estéticos da função geom_area() são\n\nfill - A cor que preenche a área abaixo da linha.\ncolor - A cor da linha.\nalpha - O nível de transparência das cores.\nlinetype - O tipo de linha (tracejado).\nlinewidth - A espessura da linha.\n\nOs elementos estéticos podem receber dois tipos de valores: constantes ou variáveis. Uma constante é simplesmente um valor (número, texto, etc.) enquanto uma variável é o nome de alguma coluna da base de dados. Mapear variáveis em elementos estéticos permite fazer um gráfico de área em que cada cor representa um grupo distinto, como se viu no exemplo anterior.\nO gráfico abaixo mostra o volume de inventário de casas à venda em três cidades no Texas, usando a base txhousing. Note que o elemento fill é variável e definido como fill = city dentro da função aes(). Quando se mapeia uma coluna/variável para um elemento estético sempre se usa a função aes() da mesma maneira como se faz para mapear as variáveis x e y. Como a variável city é mapeada usa-se scale_fill_manual() para definir as suas cores e para controlar a legenda.\n\nsub &lt;- txhousing |&gt; \n  filter(\n    city %in% c(\"Austin\", \"Houston\", \"Dallas\"),\n    year &gt;= 2010\n    )\n\nggplot(data = sub) +\n  geom_area(aes(x = date, y = inventory, fill = city)) +\n  #&gt; Define as cores e controla a legenda\n  scale_fill_manual(\n    #&gt; Título da legenda\n    name = \"Cidades\",\n    #&gt; Cores\n    values = c(\"#0a9396\", \"#ee9b00\", \"#ae2012\"))\n\n\n\n\n\n\n\n\nPara seguir os exemplos abaixo será útil definir este gráfico como um template padrão. Vamos chamá-lo de base_plot.\n\nbase_plot &lt;- ggplot(sub, aes(x = date, y = inventory, fill = city)) +\n  #&gt; Define as cores e controla a legenda\n  scale_fill_manual(\n    #&gt; Título da legenda\n    name = \"Cidades\",\n    #&gt; Cores\n    values = c(\"#0a9396\", \"#ee9b00\", \"#ae2012\")\n    ) +\n  guides(fill = \"none\")\n\n\n\nO argumento alpha controla o nível de transparência do objeto, isto é, da linha e da área abaixo da linha. O valor escolhido deve variar entre 0 e 1, onde 0 indica transparência máxima. Também é possível mapear uma coluna usando alpha, de maneira que cada grupo será representado por um nível de transparência distinto, mas há pouca utilidade prática nisto. O painel de gráficos abaixo mostra alguns valores de alpha.\n\nbase_plot + geom_area(alpha = 0.2)\nbase_plot + geom_area(alpha = 0.4)\nbase_plot + geom_area(alpha = 0.6)\nbase_plot + geom_area(alpha = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nO padrão da função geom_area é de sempre manter os argumentos fill e color iguais. Isto garante uma visualização simples e fluída. Contudo, é possível definir especificamente uma cor para cada elemento. O argumento color define a cor da linha no topo da área, enquanto fill define a cor que preenche a área. Para mostrar isto, usa-se novamente o exemplo com dados simulados.\nNote que para tornar a cor da linha mais aparente uso linewidth = 3. Isto torna a linha mais espessa.\n\n#&gt; Linha e área tem a mesma cor\nggplot(tbl, aes(x = ano, y = venda_1)) +\n  geom_area(fill = \"#0a9396\")\n\n#&gt; Linha amarela e área verde\nggplot(tbl, aes(x = ano, y = venda_1)) +\n  geom_area(fill = \"#0a9396\", color = \"#ee9b00\", linewidth = 3)\n\n#&gt; Linha e área tem a mesma cor\nggplot(long, aes(x = ano, y = total, fill = loja)) +\n  geom_area()\n\n#&gt; Linha amarela e área variando por grupo\nggplot(long, aes(x = ano, y = total, fill = loja)) +\n  geom_area(color = \"#ee9b00\", linewidth = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstes argumentos se comportam da mesma maneira como no caso de gráficos de linha, com a função geom_line. O argumento linetype controla o tipo da linha (e.g. sólida, tracejada, etc.) enquanto linewidth controla a espessura da linha. Note que os argumento serão aplicados apenas se houver uma diferença entre color e fill.\nO exemplo abaixo mostra como mudar o tipo da linha que fica sobre a área colorida. Para ver os demais tipos de linha consulte o post inicial sobre gráficos de linha.\n\nbase_plot + geom_area(linetype = 5) \nbase_plot + geom_area(color = \"white\", linetype = 1)\nbase_plot + geom_area(color = \"white\", linetype = 5)\nbase_plot + geom_area(color = \"white\", linetype = 6)\n\n\n\n\n\n\n\n\n\n\nO painel de gráficos abaixo mostra como variar a espessura da linha. Novamente, este argumento será aplicado apenas se o argumento color for diferente do argumento fill.\n\nbase_plot + geom_area(color = \"white\", linewidth = 0.5) \nbase_plot + geom_area(color = \"white\", linewidth = 1) \nbase_plot + geom_area(color = \"white\", linewidth = 2) \nbase_plot + geom_area(color = \"white\", linewidth = 5)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/10-grafico-de-area.html#ggplot2",
    "href": "posts/ggplot2-tutorial/10-grafico-de-area.html#ggplot2",
    "title": "Indo além: empilhando áreas",
    "section": "",
    "text": "Antes de mais nada, precisamos instalar e carregar alguns pacotes. Assim como em posts anteriores, além do pacote ggplot2 vamos utilizar alguns pacotes auxiliares para facilitar a manipulação dos dados.\n\n#&gt; Instala o pacote ggplot2 (se necessário)\ninstall.packages(c(\"dplyr\", \"tidyr\", \"ggplot2\", \"RcppRoll\", \"GetBCBData\"))\n\n#&gt; Carrega os pacotes\nlibrary(\"ggplot2\")\n#&gt; Manipulação de dados\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"RcppRoll\")\n#&gt; Importar dados da API do Banco Central do Brasil\nlibrary(\"GetBCBData\")\n\n\n\nPrimeiro, vamos simular alguns dados para o nosso gráfico de área. No primeiro exemplo vamos montar uma base de dados (tibble) com os valores de uma série de vendas anuais de 2000 a 2005.\nA coluna ano é uma variável contínua de 2000 a 2005. As colunas venda_a, venda_b e venda_c representam números hipotéticos de venda três lojas distintas (“1”, “2” e “3”).\n\ntbl &lt;- tibble(\n  ano = 2000:2005,\n  venda_1 = c(20, 24, 23, 27, 25, 26),\n  venda_2 = c(30, 22, 17, 23, 21, 18),\n  venda_3 = c(18, 19, 17, 17, 18, 19)\n)\n\nA tabela tem a forma abaixo.\n\n\n\n\n\n\nano\nvenda_1\nvenda_2\nvenda_3\n\n\n\n\n2000\n20\n30\n18\n\n\n2001\n24\n22\n19\n\n\n2002\n23\n17\n17\n\n\n2003\n27\n23\n17\n\n\n2004\n25\n21\n18\n\n\n2005\n26\n18\n19\n\n\n\n\n\n\n\n\nUsamos a função geom_area() para criar um gráfico de área. Esta função recebe os argumentos x e y, que mapeiam as variáveis de dados no eixo x e no eixo y do gráfico, respectivamente.\nO código abaixo monta um gráfico de área que mostra o número de vendas na loja 1.\n\nggplot(data = tbl) +\n  geom_area(aes(x = ano, y = venda_1))\n\n\n\n\n\n\n\n\nPara adicionar as vendas das outras lojas precisamos remodelar o formato dos nossos dados. Nossos dados estão em formato “wide”, no qual cada variável (vendas) é uma coluna distinta e cada observação é uma linha. O ggplot2 segue os princípios de tidy data e funciona melhor com dados em formato “long”, onde cada linha é uma observação única.\nConvertemos os nossos dados para o formato “long” utilizando a função pivot_longer() da seguinte maneira:\n\nlong &lt;- pivot_longer(\n  tbl,\n  venda_1:venda_3,\n  names_to = \"loja\",\n  values_to = \"total\"\n  )\n\nAgora cada linha representa o número de vendas em uma loja específica e num ano específico. Além disso, cada coluna é uma variável diferente: a coluna ano indica que o ano da observação, a coluna loja indica a loja e a coluna total indica o total de vendas.\n\n\n\n\n\n\nano\nloja\ntotal\n\n\n\n\n2000\nvenda_1\n20\n\n\n2000\nvenda_2\n30\n\n\n2000\nvenda_3\n18\n\n\n2001\nvenda_1\n24\n\n\n2001\nvenda_2\n22\n\n\n2001\nvenda_3\n19\n\n\n2002\nvenda_1\n23\n\n\n2002\nvenda_2\n17\n\n\n2002\nvenda_3\n17\n\n\n2003\nvenda_1\n27\n\n\n2003\nvenda_2\n23\n\n\n2003\nvenda_3\n17\n\n\n2004\nvenda_1\n25\n\n\n2004\nvenda_2\n21\n\n\n2004\nvenda_3\n18\n\n\n2005\nvenda_1\n26\n\n\n2005\nvenda_2\n18\n\n\n2005\nvenda_3\n19\n\n\n\n\n\n\n\n\nAgora que temos nossos dados preparados, podemos montar nosso gráfico de área. Além dos argumentos x e y, também vamos especificar o argumento fill para indicar qual variável deve ser utilizada para preencher as cores entre as linhas.\n\nggplot(data = long) +\n  geom_area(aes(x = ano, y = total, fill = loja))\n\n\n\n\n\n\n\n\n\n\n\nNo gráfico acima, as áreas são empilhadas umas sobre as outras. Tipicamente, os grupos com os valores mais altos devem estar no topo da pilha e os grupos com valores mais baixos, na parte inferior. Na prática, a ordem é determinada pela ordem da variável fill. Neste caso, a ordem é dada pela variável categória loja1. Variáveis categóricas no R sempre devem ser do tipo factor. Já se apresentou factors anteriormente, mas vale a pena revisar esta importante classe.\nUm factor é um tipo especial de string que possui um ordenamento (levels) que define a relação hierárquica entre os grupos. Imagine que você tem uma série de avaliações que podem ser “Bom”, “Médio” ou “Ruim” armazenadas num vetor chamado feedback. Para estruturar esta variável como factor é preciso definir qual a ordem destes valores. No exemplo abaixo define-se uma relação crescente: de “Ruim” até “Bom”.\n\nfeedback &lt;- c(\"Bom\", \"Bom\", \"Médio\", \"Ruim\", \"Médio\", \"Médio\")\nsatisfacao &lt;- factor(feedback, levels = c(\"Ruim\", \"Médio\", \"Bom\"))\n\nNo caso do gráfico acima, como não definimos a ordem da variável categórica, o ggplot2 tenta adivinha-la. De maneira geral, o ggplot respeita a ordem alfabética e a ordem de grandeza numérica. Por isso, no gráfico acima a ordem foi venda_1, venda_2 e venda_3.\nPara definir a ordem dos lojass é preciso criar uma variável do tipo factor e especificar o argumento levels. O exemplo abaixo define uma nova ordem para os grupos o que resulta num gráfico diferente.\n\nlong_reordenado &lt;- long |&gt; \n  mutate(\n    loja = factor(loja, levels = c(\"venda_3\", \"venda_1\", \"venda_2\"))\n  )\n\nggplot(data = long_reordenado) +\n  geom_area(aes(x = ano, y = total, fill = loja))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/10-grafico-de-area.html#elementos-estéticos",
    "href": "posts/ggplot2-tutorial/10-grafico-de-area.html#elementos-estéticos",
    "title": "Indo além: empilhando áreas",
    "section": "",
    "text": "Os principais elementos estéticos da função geom_area() são\n\nfill - A cor que preenche a área abaixo da linha.\ncolor - A cor da linha.\nalpha - O nível de transparência das cores.\nlinetype - O tipo de linha (tracejado).\nlinewidth - A espessura da linha.\n\nOs elementos estéticos podem receber dois tipos de valores: constantes ou variáveis. Uma constante é simplesmente um valor (número, texto, etc.) enquanto uma variável é o nome de alguma coluna da base de dados. Mapear variáveis em elementos estéticos permite fazer um gráfico de área em que cada cor representa um grupo distinto, como se viu no exemplo anterior.\nO gráfico abaixo mostra o volume de inventário de casas à venda em três cidades no Texas, usando a base txhousing. Note que o elemento fill é variável e definido como fill = city dentro da função aes(). Quando se mapeia uma coluna/variável para um elemento estético sempre se usa a função aes() da mesma maneira como se faz para mapear as variáveis x e y. Como a variável city é mapeada usa-se scale_fill_manual() para definir as suas cores e para controlar a legenda.\n\nsub &lt;- txhousing |&gt; \n  filter(\n    city %in% c(\"Austin\", \"Houston\", \"Dallas\"),\n    year &gt;= 2010\n    )\n\nggplot(data = sub) +\n  geom_area(aes(x = date, y = inventory, fill = city)) +\n  #&gt; Define as cores e controla a legenda\n  scale_fill_manual(\n    #&gt; Título da legenda\n    name = \"Cidades\",\n    #&gt; Cores\n    values = c(\"#0a9396\", \"#ee9b00\", \"#ae2012\"))\n\n\n\n\n\n\n\n\nPara seguir os exemplos abaixo será útil definir este gráfico como um template padrão. Vamos chamá-lo de base_plot.\n\nbase_plot &lt;- ggplot(sub, aes(x = date, y = inventory, fill = city)) +\n  #&gt; Define as cores e controla a legenda\n  scale_fill_manual(\n    #&gt; Título da legenda\n    name = \"Cidades\",\n    #&gt; Cores\n    values = c(\"#0a9396\", \"#ee9b00\", \"#ae2012\")\n    ) +\n  guides(fill = \"none\")\n\n\n\nO argumento alpha controla o nível de transparência do objeto, isto é, da linha e da área abaixo da linha. O valor escolhido deve variar entre 0 e 1, onde 0 indica transparência máxima. Também é possível mapear uma coluna usando alpha, de maneira que cada grupo será representado por um nível de transparência distinto, mas há pouca utilidade prática nisto. O painel de gráficos abaixo mostra alguns valores de alpha.\n\nbase_plot + geom_area(alpha = 0.2)\nbase_plot + geom_area(alpha = 0.4)\nbase_plot + geom_area(alpha = 0.6)\nbase_plot + geom_area(alpha = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nO padrão da função geom_area é de sempre manter os argumentos fill e color iguais. Isto garante uma visualização simples e fluída. Contudo, é possível definir especificamente uma cor para cada elemento. O argumento color define a cor da linha no topo da área, enquanto fill define a cor que preenche a área. Para mostrar isto, usa-se novamente o exemplo com dados simulados.\nNote que para tornar a cor da linha mais aparente uso linewidth = 3. Isto torna a linha mais espessa.\n\n#&gt; Linha e área tem a mesma cor\nggplot(tbl, aes(x = ano, y = venda_1)) +\n  geom_area(fill = \"#0a9396\")\n\n#&gt; Linha amarela e área verde\nggplot(tbl, aes(x = ano, y = venda_1)) +\n  geom_area(fill = \"#0a9396\", color = \"#ee9b00\", linewidth = 3)\n\n#&gt; Linha e área tem a mesma cor\nggplot(long, aes(x = ano, y = total, fill = loja)) +\n  geom_area()\n\n#&gt; Linha amarela e área variando por grupo\nggplot(long, aes(x = ano, y = total, fill = loja)) +\n  geom_area(color = \"#ee9b00\", linewidth = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstes argumentos se comportam da mesma maneira como no caso de gráficos de linha, com a função geom_line. O argumento linetype controla o tipo da linha (e.g. sólida, tracejada, etc.) enquanto linewidth controla a espessura da linha. Note que os argumento serão aplicados apenas se houver uma diferença entre color e fill.\nO exemplo abaixo mostra como mudar o tipo da linha que fica sobre a área colorida. Para ver os demais tipos de linha consulte o post inicial sobre gráficos de linha.\n\nbase_plot + geom_area(linetype = 5) \nbase_plot + geom_area(color = \"white\", linetype = 1)\nbase_plot + geom_area(color = \"white\", linetype = 5)\nbase_plot + geom_area(color = \"white\", linetype = 6)\n\n\n\n\n\n\n\n\n\n\nO painel de gráficos abaixo mostra como variar a espessura da linha. Novamente, este argumento será aplicado apenas se o argumento color for diferente do argumento fill.\n\nbase_plot + geom_area(color = \"white\", linewidth = 0.5) \nbase_plot + geom_area(color = \"white\", linewidth = 1) \nbase_plot + geom_area(color = \"white\", linewidth = 2) \nbase_plot + geom_area(color = \"white\", linewidth = 5)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/10-grafico-de-area.html#outros-posts-citados",
    "href": "posts/ggplot2-tutorial/10-grafico-de-area.html#outros-posts-citados",
    "title": "Indo além: empilhando áreas",
    "section": "Outros posts citados",
    "text": "Outros posts citados\n\nFundamentos: gráfico de coluna\nFundamentos: gráfico de linha\nEstético: Destacando informação\nEstético: Escalas e cores\nEstético: Tipografia e temas"
  },
  {
    "objectID": "posts/ggplot2-tutorial/10-grafico-de-area.html#footnotes",
    "href": "posts/ggplot2-tutorial/10-grafico-de-area.html#footnotes",
    "title": "Indo além: empilhando áreas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara uma breve revisão sobre variáveis categóricas ou discretas, consulte o post.↩︎\nNa prática ela faz o mesmo que a função base `format()`.↩︎\nPara uma revisão sobre manipulação de dados consulte o post da série sobre o asssunto.↩︎\nPara mais detalhes sobre a função facet_wrap consulte o post da série sobre o assunto.↩︎\nPara mais detalhes sobre como destacar informação com texto consulte o post da série sobre o assunto.↩︎\nVale notar que o mesmo ajuste poderia ter sido realizado com nudge_x dentro de geom_label.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html",
    "title": "Indo além: mapas de calor",
    "section": "",
    "text": "Mapas de calor apresentam a variação de uma variável num plano bidimensional. Há dois tipos principais de mapas de calor: o mapa de calor espacial e o mapa de calor de clusters. O primeiro tipicamente é superimposto sobre um mapa e representa a intensidade de alguma variável, como a pluviosidade, temperatura, concentração de C02 e assim por diante. Já o mapa de calor de clusters é disposto numa matriz bidimensional para sugerir padrões, tendências, ou mesmo para visualizar a evolução de uma variável num grupo de classes.\n\n\n\n\n\n\n\n\n\nO pacote ggplot2 não tem uma função única para montar mapas de calor, então vamos explorar uma abordagem simples focada na função geom_tile(). Contudo, é possível montar mapas de calor também com as funções geom_bin_2d(), geom_density_2d(), geom_raster() ou geom_hex().\nNeste post vamos nos focar em mapas de clusters. A discussão de mapas de calor será apresentada no post subsequente que trata sobre mapas.\nO código abaixo carrega os pacotes necessários. Para a paleta de cores vou utilizar o pacote MetBrewer, que oferece algumas paletas de cores interessantes, inspiradas em artistas famosos do Metropolitan Museum de NY. O pacote pwt10 carrega a base de dados da Penn World Table que traz uma série de informações socioeconômicas dos países ao longo dos anos. Por fim, os pacotes astsa, wooldridge e GetBCBData são utilizados para conseguir acesso a base de dados para os exemplos.\n\n# Para instalar um pacote do GitHub\nremotes::install_github(\"BlakeRMills/MetBrewer\")\n\n\n# Carrega os pacotes\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(ggplot2)\n\n# Paletas de cores (ver acima como instalar)\nlibrary(MetBrewer)\n\n# Para carregar bases de dados para os exemplos\n# Dados da Penn World Table\nlibrary(pwt10)\n# Aquecimento global\nlibrary(astsa)\n# Séries do Banco Central do Brasil\nlibrary(GetBCBData)\n# Preços de imóveis\nlibrary(wooldridge)\n\n# Colors\ncolors_hiroshige &lt;- met.brewer(name = \"Hiroshige\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html#exemplo-crescimento-econômico",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html#exemplo-crescimento-econômico",
    "title": "Indo além: mapas de calor",
    "section": "Exemplo: Crescimento Econômico",
    "text": "Exemplo: Crescimento Econômico\nPrimeiro vamos importar a base de dados da Penn World Table usando o pacote pwt10.\n\n# A base de dados\npwt &lt;- as_tibble(pwt10::pwt10.0)\n\nA base tem várias colunas mas vamos nos focar somente nas colunas country, pop (população do país em milhões de habitante) e rgdpe, que o é o PIB (pela ótica da despesa) em milhões de US$ constantes (2017).\n\n\n\n\n\n\n\ncountry\nisocode\nyear\ncurrency\nrgdpe\nrgdpo\npop\nemp\navh\nhc\nccon\ncda\ncgdpe\ncgdpo\ncn\nck\nctfp\ncwtfp\nrgdpna\nrconna\nrdana\nrnna\nrkna\nrtfpna\nrwtfpna\nlabsh\nirr\ndelta\nxr\npl_con\npl_da\npl_gdpo\ni_cig\ni_xm\ni_xr\ni_outlier\ni_irr\ncor_exp\nstatcap\ncsh_c\ncsh_i\ncsh_g\ncsh_x\ncsh_m\ncsh_r\npl_c\npl_i\npl_g\npl_x\npl_m\npl_n\npl_k\n\n\n\n\nBrazil\nBRA\n1950\nBrazilian Real\n92711\n89223\n53\n17\n2042\n1\n67102\n83933\n87798\n85324\n227373\nNA\nNA\nNA\n145828\n108970\n144486\n608567\nNA\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNA\n\n\nBrazil\nBRA\n1951\nBrazilian Real\n96889\n91904\n55\n17\n2051\n1\n70038\n91417\n92312\n87631\n242814\nNA\nNA\nNA\n152969\n113887\n155273\n654280\nNA\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNA\n\n\nBrazil\nBRA\n1952\nBrazilian Real\n105943\n101667\n56\n18\n2060\n1\n76213\n100796\n101407\n97895\n261085\nNA\nNA\nNA\n167515\n123881\n170169\n705852\nNA\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNA\n\n\nBrazil\nBRA\n1953\nBrazilian Real\n108931\n105003\n58\n18\n2069\n1\n80936\n100441\n104321\n101629\n272078\nNA\nNA\nNA\n176028\n130481\n171026\n737951\nNA\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNA\n\n\nBrazil\nBRA\n1954\nBrazilian Real\n118686\n113983\n60\n19\n2078\n1\n88946\n111934\n113384\n110080\n287037\n0\n1\n1\n190468\n143957\n191257\n780613\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n\n\nBrazil\nBRA\n1955\nBrazilian Real\n127384\n122678\n61\n20\n2087\n1\n94400\n117254\n121279\n118126\n301169\n0\n1\n1\n202763\n153006\n201258\n820627\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\nBrazil\nBRA\n1956\nBrazilian Real\n131865\n127205\n63\n20\n2097\n1\n97827\n119971\n125158\n122067\n313178\n0\n1\n1\n210124\n159465\n206719\n854692\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\nBrazil\nBRA\n1957\nBrazilian Real\n143384\n138373\n65\n21\n2106\n1\n104123\n133653\n136122\n132684\n332711\n0\n1\n1\n231221\n170139\n228611\n911217\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\nBrazil\nBRA\n1958\nBrazilian Real\n151546\n146430\n67\n21\n2115\n1\n113085\n142400\n144065\n141124\n350683\n0\n1\n1\n245978\n184877\n244075\n963159\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\nBrazil\nBRA\n1959\nBrazilian Real\n158732\n153143\n69\n22\n2125\n1\n116669\n152878\n150332\n147218\n376789\n0\n1\n1\n264945\n191877\n262297\n1033868\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\nextrapolated\nextrapolated\nmarket\nno\nregular\nNA\nNA\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n\n\n\n\n\n\nVamos selecionar apenas os principais países da América Latina para montar nossa visualização.\nO primeiro painel que iremos explorar vai exibir os ciclos de expansão e de recessão nos países latino-americanos ao longo dos anos, desde 1950. Para isto vamos primeiro calcular o PIB per capital real de cada país e depois criar uma variável dummy que indica com o valor 1 se o país teve crescimento do PIB per capita real e 0 caso contrário.\n\n\nCode\n# Vetor com países\nlatam_countries &lt;- c(\n  \"ARG\", \"BOL\", \"BRA\", \"CHL\", \"COL\", \"CUB\", \"DOM\", \"ECU\", \"GTM\", \"HND\", \"HTI\",\n  \"MEX\", \"PER\", \"VEN\")\n\nlatam &lt;- pwt |&gt; \n  # Filtra apenas as linhas dos países selecionados\n  filter(isocode %in% latam_countries) |&gt; \n  # Agrupa por país\n  group_by(country) |&gt; \n  mutate(\n    # Calcula o PIB per capita de cada país\n    gdppc = rgdpe / pop,\n    # Calcula a variação do PIB per capita de cada país\n    d_gdppc = gdppc / lag(gdppc) - 1,\n    # Cria uma variável binária para indicar se houve crescimento \n    growth = factor(if_else(d_gdppc &gt; 0, 1L, 0L)),\n    # Remove parêntesis e o texto dentro dele\n    country = str_remove(country, \" \\\\(.+\\\\)\"),\n    # Abrevia o nome da República Dominicana\n    country = str_replace(country, \"Dominican Republic\", \"Dominican Rep.\")\n  ) |&gt; \n  ungroup()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html#gráfico-de-calor",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html#gráfico-de-calor",
    "title": "Indo além: mapas de calor",
    "section": "Gráfico de Calor",
    "text": "Gráfico de Calor\nApesar de bidimensional, um heatmap serve para representar dados em três dimensões. No nosso caso, a função geom_tile() mapeia as variáveis x e y nas coordenadas enquanto a variável fill representa uma variável numérica adicional, seja ela categórica ou contínua.\nO código abaixo monta um heatmap simples. No gráfico, os anos de crescimento e de recessão são destacados em cada país.\n\nggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = growth))\n\n\n\n\n\n\n\n\nO gráfico acima é bastante interessante, mas pode ser melhorado em diversos aspectos. Vamos implementar as melhorias sequencialmente.\n\nReordenar as colunas\nNote que os países no gráfico acima estão organizados de forma alfabética, mas a ordem está invertida. Para alterar a ordem de uma variável categórica temos que alterar os seus níveis. No R variáveis categóricas tem uma classe especial factor e seus níveis são definidos pelo argumento levels.\nTipicamente, isto envolve definir um vetor com a ordem dos níveis e criar uma variável nova como no exemplo abaixo.\n\n# Vetor com o nome de todos os países latino-americanos\nlvls &lt;- unique(latam[[\"country\"]])\n# Inverte a ordem do vetor\nlvls &lt;- rev(lvls)\n# Redefine os níveis da variável country\nlatam &lt;- latam |&gt; \n  mutate(country = factor(country, levels = lvls))\n\nHá, contudo, vários atalhos para melhor lidar com factors. Em particular, o pacote forcats carregado junto no tidyverse tem uma série de funções que facilitam o trato de variáveis categóricas. A função fct_rev(), por exemplo, serve para inverter a ordem de uma variável categórica.\nAssim, o código abaixo chega no mesmo resultado.\n\n# Inverte a ordem da variável country\nlatam &lt;- latam |&gt; \n  mutate(country = fct_rev(country))\n\nNote que o gráfico está em ordem alfabética, começando na Argentina e terminando na Venezuela.\n\n# Monta o gráfico\nggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = growth))\n\n\n\n\n\n\n\n\n\n\nDimensionar o gráfico\nPodemos inserir um pouco de espaço entre as séries de cada país alterando o argumento height da função geom_tile().\n\nggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = growth), height = 0.9)\n\n\n\n\n\n\n\n\n\n\nAlterar as cores\nA escolha do esquema de cores é talvez o aspecto mais importante de um heat map. Não é fácil encontrar boas cores que sejam intuitivas e que também sejam inclusivas. Aqui utilizamos uma paleta inspirada no artista plástico Hiroshige Utagawa para conseguir alguns tons de azul/vermelho para representar os anos de expansão/recessão.\n\n# Colors\ncolors_hiroshige &lt;- met.brewer(name = \"Hiroshige\")\ncolors_binary &lt;- colors_hiroshige[c(1, 8)]\n\np &lt;- ggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = growth), height = 0.9) +\n  # Altera a paleta de cores e controla a legenda  \n  scale_fill_manual(\n    # Título da legenda (opcional)\n    name = \"\",\n    # Paleta de cores\n    values = colors_binary,\n    # Nome das classes na legenda\n    labels = c(\"Recession\", \"Growth\"),\n    # Omite o grupo NA\n    na.translate = FALSE\n  )\n\np\n\n\n\n\n\n\n\n\n\n\nAjustar e duplicar o eixo\nA leitura dos anos no gráfico acima pode ser melhorada. Primeiro, vamos inserir mais quebras no eixo-x e sinalizar que a série tem início em 1951 e tem fim em 2019. Para ajustar o eixo usamos a função scale_x_continuous() variando o argumento breaks.\nQuando temos um gráfico grande, pode ser útil duplicar os eixos para facilitar a sua leitura. Vamos duplicar o eixo-x (anos) usando sec.axis = dup_axis().\nPor fim, note que há espaço ocioso tanto no lado esquerdo como no lado direito do gráfico (espaço entre os “tiles” e a borda do painel do gráfico). O argumento expand = c(0, 0) remove o espaço entre as barras e o limite do painel.\n\nlatam_na &lt;- latam |&gt; \n  filter(!is.na(growth))\n\np &lt;- p +\n  # Controla o eixo-x (anos)\n  scale_x_continuous(\n    # Define as quebras do gráfico \n    breaks = c(1951, seq(1960, 2010, 10), 2019),\n    # Preenche 100% do painel\n    expand = c(0, 0),\n    # Duplica o eixo\n    sec.axis = dup_axis()\n    )\n\np\n\n\n\n\n\n\n\n\n\n\nAjustar o tema\nOs ajustes finos de um gráfico são feitos com a função theme(). Neste caso, queremos um gráfico com poucos elementos. Uma abordagem simples para modificar os detalhes do gráfico é inicar com um template, um tema padrão. Neste caso, começamos como um tema chamado theme_minimal().\n\np &lt;- p +\n  # Tema minimalista que serve de template\n  theme_minimal() +\n  theme(\n    # Remove o título dos eixos x e y\n    axis.title = element_blank(),\n    # Remove as linhas horizontais e verticais no fundo do gráfico\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    # Coloca a legenda acima do gráfico\n    legend.position = \"top\"\n  )\n\np\n\n\n\n\n\n\n\n\n\n\nAdicionar linhas de grade\nAs linhas de grade de um gráfico facilitam a interpretação dos dados, mas a função geom_tile() fica por cima delas. Podemos sobrepor linhas verticais no gráfico para sinalizar o início de cada uma das décadas.\nO código abaixo faz isto com a função geom_vline() que serve para desenhar linhas verticais num gráfico com o argumento xintercept. O argumento linetype = 2 indica que a linha deve ser tracejada.\n\np &lt;- p +\n  # Desenha linhas verticais no gráfico\n  geom_vline(\n    # Define a posição das linhas verticais\n    xintercept = c(1951, seq(1960, 2010, 10), 2019),\n    # Tipo de linha (2 = tracejado)\n    linetype = 2,\n    # Cor da linha\n    colour = \"gray75\"\n    )\n\np\n\n\n\n\n\n\n\n\n\n\nAjustar o nome dos eixos\n\np &lt;- p + \n  labs(\n    title = \"Growth/Recession across Latin America\",\n    subtitle = \"Cycles are synchronized across countries. The 1980s was a difficult period for most countries, while the mid 2000s\\nonwards was more positive overall due to the commodtiy boom.\",\n    caption = \"Source: Penn World Table (pwt10)\"\n  )\n\np\n\n\n\n\n\n\n\n\n\n\nO gráfico final\nO código abaixo gera o gráfico finalizado.\n\nggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = growth), height = 0.9) +\n  geom_vline(\n    xintercept = c(1951, seq(1960, 2010, 10), 2019),\n    linetype = 2,\n    colour = \"gray75\") +\n  scale_fill_manual(\n    name = \"\",\n    values = colors_binary,\n    labels = c(\"Recession\", \"Growth\"),\n    na.translate = FALSE\n    ) +\n  scale_x_continuous(\n    breaks = c(1951, seq(1960, 2010, 10), 2019),\n    expand = c(0, 0),\n    sec.axis = dup_axis()\n    ) +\n  labs(\n    title = \"Growth/Recession across Latin America\",\n    subtitle = \"Cycles are synchronized across countries. The 1980s was a difficult period for most countries, while the mid 2000s\\nonwards was more positive due to the commodtiy boom.\",\n    caption = \"Source: Penn World Table (pwt10)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html#usando-variáveis-contínuas",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html#usando-variáveis-contínuas",
    "title": "Indo além: mapas de calor",
    "section": "Usando variáveis contínuas",
    "text": "Usando variáveis contínuas\nO exemplo acima mostra os ciclos de expansão e recessão usando uma simples variável binária. Este tipo de variável é também chamada de “discreta” (ou, às vezes, “contável”).\nPode-se adaptar o código acima para montar gráficos de calor para variáveis contínuas. Para evitar a repetição excessiva de código podemos criar uma função. Funções permitem que você automatize tarefas repetitivas; no fundo, evita que você tenha que ficar copiando e colando código.\nVou criar uma função simples que: (1) adiciona as linhas verticais tracejadas sempre nos mesmos anos; (2) ajusta e duplica o eixo-x do gráfico; e (3) aplica um tema padrão sobre o gráfico. O nome desta função será gg_heatmap() e o argumento da função será um gráfico “base” de ggplot2. Isto deve ficar mais compreensível após os exemplos.\nO código abaixo mostra a implementação da função.\n\nFunções\n\n\nCode\n# Cria um tema para os gráficos\ntheme_heatmap &lt;- theme_minimal() +\n  theme(\n    axis.title = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"top\",\n    legend.key.size = unit(1, \"cm\"),\n    legend.title = element_text(vjust = 0.75, hjust = 0.5)\n  )\n\ngg_heatmap &lt;- function(plot) {\n  \n  plot &lt;- plot +\n    # Desenha linhas verticais no gráfico\n    geom_vline(\n      # Define a posição das linhas verticais\n      xintercept = c(1951, seq(1960, 2010, 10), 2019),\n      # Tipo de linha (2 = tracejado)\n      linetype = 2,\n      # Cor da linha\n      colour = \"gray75\"\n      ) +\n    # Controla o eixo-x (anos)\n    scale_x_continuous(\n      # Define as quebras do gráfico \n      breaks = c(seq(1950, 2010, 10), 2019),\n      # Preenche 100% do painel\n      expand = c(0, 0),\n      # Duplica o eixo\n      sec.axis = dup_axis()) +\n    # Adiciona o tema\n    theme_heatmap\n  \n  return(plot)\n  \n}\n\n\n\n\nPIB per capita\nComo exemplo vamos olhar para o PIB per capita dos países da América Latina. Para melhorar a visualização uso os dados em escala logarítmica.\nNo código abaixo eu utilizo a função gg_heatmap() criada acima. Primeiro eu crio um gráfico de calor “base” usando apenas geom_tile() e modificando a escala de cor. Depois, aplico a função neste objeto e o resultado é o gráfico final.\n\n# Monta o mapa\np &lt;- ggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = log(gdppc)), height = 0.9) +\n  scale_fill_gradientn(\n    name = \"PIB per capita (log)\",\n    colours = met.brewer(\"Hokusai1\")\n    )\n\n# Aplica a função\ngg_heatmap(p)\n\n\n\n\n\n\n\n\nO gráfico de calor abaixo apresenta o PIB per capita em nível e com uma paleta de cores diferente. Note como o uso da função gg_heatmap() economiza muitas linhas de código.\n\np &lt;- ggplot(data = latam, aes(x = year, y = country)) +\n  geom_tile(aes(fill = gdppc / 1000), height = 0.9) +\n  scale_fill_viridis_c(name = \"PIB per capita (mil)\")\n\ngg_heatmap(p)\n\n\n\n\n\n\n\n\n\n\nInflação\nMapas de calor também podem servir para construir visualizações no formato de um “calendário de observações”. A visualização abaixo é adaptada do meu post Visualizando o IPCA e mostra o valor acumulado do IPCA em 12 meses a cada mês, desde janeiro de 2010. Os dados são importados do Banco Central do Brasil usando o pacote GetBCBData e uso o pacote lubridate para transformar os dados.\n\n\nCode\n#&gt; Importa a série do IPCA a partir da API do Banco Central\nipca &lt;- GetBCBData::gbcbd_get_series(\n  id = 433,\n  first.date = as.Date(\"2009-01-01\")\n  )\n\n#&gt; Limpeza de dados\nipca &lt;- ipca |&gt; \n  mutate(\n    #&gt; Cria uma coluna com o ano da observação\n    year = lubridate::year(ref.date),\n    #&gt; Cria uma coluna com o mês da obervação\n    month = lubridate::month(ref.date, label = TRUE, locale = \"pt_BR\"),\n    #&gt; Acumula os valores do IPCA em 12 meses\n    acum12m = RcppRoll::roll_prodr(1 + value / 100, n = 12) - 1,\n    acum12m = acum12m * 100\n    ) |&gt; \n  filter(\n    ref.date &gt;= as.Date(\"2010-01-01\"),\n    ref.date &lt;= as.Date(\"2023-12-01\")\n    )\n\n\n\nggplot(ipca, aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_x_discrete(position = \"top\") +\n  scale_y_reverse(breaks = 2010:2023, expand = c(0, 0)) +\n  scale_fill_viridis_c(\n    name = \"Var. percentual\\ndo IPCA acumulada\\nem 12 meses\",\n    option = \"magma\",\n    breaks = seq(2, 12, 2),\n    labels = paste0(seq(2, 12, 2), \"%\")) +\n  labs(title = \"Inflação no Brasil\") +\n  theme_heatmap +\n  theme(\n    axis.text = element_text(size = 11, color = \"gray15\")\n    )\n\n\n\n\n\n\n\n\nNa visualização vê-se como os anos iniciais de 2010-14 foram marcados por uma inflação moderadamente alta, oscilando entre 4-6%. Em 2015, libera-se os preços administrados, que ficaram artificialmente represados durante o período eleitoral, e a inflação estoura, chegando e depois superando a casa de 10%. Nos anos seguintes, a inflação é mais baixa, caindo até a casa de 2-3%. A quebra da série acontece durante a pandemia, quando a inflação chega a 12% e passa vários meses acima de 10%.\nTecnicamente, vale notar o uso da função scale_x_discrete já que a variável de mês é armazenada como factor. Inverte-se a escala dos anos usando scale_y_reverse, assim os anos vão “de cima para baixo”, na direção mais natural de leitura. Por fim, especifico manualmente os valores de quebras na legenda de cores, dentro da função scale_fill_viridis_c.\n\n\nEnergia elétrica\nPode-se usar mapas de calor para explorar a presença de padrões sazonais nos dados. A visualização abaixo é adaptada do meu post sobre nascimentos no Brasil mas olha para o consumo mensal de energia elétrica. Mais especificamente, a série apresenta o consumo residencial de energia elétrica desde 2002: escolhe-se este ano para evitar o apagão de 2021, que distorceria muito a visualização. Cada quadrado apresenta o consumo do mês, relativamente ao consumo total naquele ano.\nComo podia-se imaginar, o consumo de energia é mais elevado nos meses quentes: dezembro, janeiro, fevereiro e março apresentam os maiores valores. Já nos meses de inverno, em contraste, o consumo costuma ser menor. O pico da série aparece em janeiro de 2015, quande houve um verão particularmente quente1.\n\n\nCode\n#&gt; Importa a série de consumo residencial de energia elétrica a partir da\n#&gt; API do Banco Central\nenergia &lt;- GetBCBData::gbcbd_get_series(\n  id = 1403,\n  first.date = as.Date(\"1980-01-01\")\n  )\n\n#&gt; Limpeza de dados\nenergia &lt;- energia |&gt; \n  mutate(\n    mes = lubridate::month(ref.date, label = TRUE, locale = \"pt_BR\"),\n    mes = fct_rev(mes),\n    ano = lubridate::year(ref.date)\n  ) |&gt; \n  #&gt; Agrupa por ano e calcula o consumo percentual em cada mês relativamente ao ano\n  group_by(ano) |&gt; \n  mutate(cons_rel = value / sum(value) * 100) |&gt; \n  ungroup() |&gt; \n  #&gt; Começa a série em 2002 para evitar o \"apagão\" em 2001.\n  filter(ano &gt; 2002, ano &lt; 2023)\n\n\n\nggplot(energia, aes(x = ano, y = mes, fill = cons_rel)) +\n  geom_tile(color = \"gray90\") +\n  scale_x_continuous(breaks = 2001:2022, expand = c(0, 0), position = \"top\") +\n  scale_fill_viridis_c(name = \"Consumo de Energia\\nem relação ao ano (%)\") +\n  theme_heatmap +\n  theme(\n    legend.title = element_text(vjust = 0.8),\n    axis.text = element_text(size = 11, color = \"gray15\")\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html#escala-divergente",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html#escala-divergente",
    "title": "Indo além: mapas de calor",
    "section": "Escala divergente",
    "text": "Escala divergente\nOutra aplicação de gráficos de calor é de mostrar uma variável num gradiente de cores “divergente”. Um caso comum é quando quer-se mostrar uma variável como um desvio em relação a alguma média. Em geral, há duas possibilidades para visualizar os dados:\n\nO desvio dos valores em relação à média na “coluna”, isto é, entre os grupos. No caso do mapa de calor do PIB per capita isto seria equivalente a comparar o crescimento dos países (grupo) no mesmo ano.\nO desvio dos valores em relação à média na “linha”, isto é, dentro de um mesmo grupo. Novamente, no caso do mapa de calor do PIB per capita, isto seria equivalente a comparar o crescimento do país num ano em relação ao histórico de crescimento do país.\n\nNão há uma escolha certa ou errada com este tipo de visualização. É preciso saber somente qual tipo de relação que se quer visualizar. Em casos mais simples, a própria variável de interesse pode estar expressa na forma de desvios em relação a alguma média. Por fim, há também casos de variáveis que tem valores padronizados, ou indexados, que funcionam bem com escalas divergentes.\n\nAquecimento Global\nO exemplo abaixo usa a base gtemp_ocean que mensura as “anomalias de temperatura” nos últimos anos. Por anomalia de temperatura, entende-se, o desvio da temperatura média anual em relação à média histórica (1951-1980). Esta visualização é adaptada do meu post Aquecimento Global.\nNa visualização, fica evidente que os anos recentes concentram um grande número de “anomalias positivas”, isto é, anos em que a temperatura média esteve acima da média histórica. Este exemplo é bastante simples pois existe apenas um “grupo”, logo, os valores estão representados como desvios em relação à média na “linha”.\n\n# Carrega a base de dados 'gtemp_ocean'\ndata(\"gtemp_ocean\")\n# Converte o objeto para data.frame\ndf &lt;- data.frame(ano = as.numeric(time(gtemp_ocean)),\n temp = as.numeric(gtemp_ocean))\n\n# Monta o gráfico\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n  scale_fill_gradientn(\n    name = \"Percent Deviation\\nin relation to\\nWorld average\",\n    breaks = seq(-1, 1, 0.25),\n    colors = rev(met.brewer(\"Hiroshige\")), \n  ) +\n  theme_heatmap\n\n\n\n\n\n\n\n\n\n\nCorrelação\nA correlação é uma medida da relação linear entre um par de variáveis. Na construção de um modelo linear, ou modelo de regressão de forma geral, é interessante visualizar a relação de correlação entre as variáveis. Por construção, a correlação (de Pearson) é padronizada pelo desvio-padrão das variáveis, logo, varia entre -1 e 1. Uma correlação igual a -1 indica uma correlação negativa “perfeita” e uma correlação igual a 1 indica uma correlação positiva “perfeita”. Se a correlação entre duas variáveis for próxima de 0, não há dependência linear entre as variáveis.\nO código abaixo seleciona algumas das colunas da base de dados hprice2 exportada junto com o pacote wooldridge. Esta base contém o preço de alguns imóveis junto de algumas outras variáveis. Para consultar a definição das variáveis use ?hprice2.\n\n\nCode\ndat &lt;- wooldridge::hprice2\n\n#&gt; Seleciona colunas\nnum_cols &lt;- c(\"stratio\", \"dist\", \"nox\", \"crime\", \"rooms\", \"lprice\")\nsub &lt;- select(dat, all_of(num_cols))\n\ntbl_cor &lt;- sub |&gt; \n  #&gt; Calcula matriz de correlação\n  cor(method = \"pearson\") |&gt; \n  as_tibble() |&gt; \n  #&gt; Converte para longitudinal e adapta variáveis\n  mutate(name_1 = num_cols) |&gt; \n  pivot_longer(cols = -\"name_1\", names_to = \"name_2\", values_to = \"correl\") |&gt; \n  mutate(\n    name_1 = factor(name_1, levels = num_cols, labels = str_to_title(num_cols)),\n    name_2 = factor(name_2, levels = num_cols, labels = str_to_title(num_cols)),\n    correl = if_else(correl == 1, NA_real_, correl)\n  )\n\n\nO mapa de calor abaixo mostra a correlação entre algumas variáveis. A variável lprice é o logaritmo natural do preço do imóvel. Nota-se que esta variável tem uma correlação positiva com rooms (número de quartos) e dist (distância até polo de emprego). Por outro lado, há uma correlação negativa entre o preço e nox (medida de poluição do ar) e crime (proporção de crimes).\n\nggplot(tbl_cor, aes(x = name_1, y = name_2, fill = correl)) +\n  geom_tile() +\n  geom_text(aes(label = round(correl, 2)), size = 4) +\n  scale_fill_gradient2(\n    name = \"\",\n    low = colors_hiroshige[1],\n    high = colors_hiroshige[10],\n    limits = c(-1, 1),\n    na.value = \"gray90\"\n    ) +\n  guides(fill = \"none\") +\n  labs(title = \"Correlação entre variáveis\") +\n  theme_heatmap +\n  theme(axis.text = element_text(size = 12))\n\n\n\n\n\n\n\n\nO valor da correlação foi plotado usando geom_text. Como a escala da correlação varia sempre entre -1 e 1, suprime-se a legenda de cores. Para construir a escala divergente utiliza-se scale_fill_gradient2 com a mesma paleta de cores do Met Brewer (Hiroshige). Vale notar que existe um pacote dedicado a este tipo de visualização chamado ggcorplot - para mais detalhes veja o GitHub do pacote.\n\n\nPIB per capita\nVamos retornar, novamente, ao exemplo do PIB per capita. Restringindo a análise somente à América Latina, há duas maneiras intuitivas de olhar os dados: (1) relativamente ao ano (coluna); (2) relativamente ao país (linha).\nNo primeiro caso, compara-se a performance entre os países em cada ano: vê-se quais foram os países que cresceram mais ou menos em determinado ano. No segundo caso, compara-se a trajetória de cada país individualmente: fica mais evidente quais foram os anos de maior ou de menor crescimento.\nO código abaixo usa a função scale para “normalizar” os dados2.\n\n# Escala os dados em relação ao país\nscale_country &lt;- latam |&gt; \n  group_by(country) |&gt; \n  mutate(scaled = as.numeric(scale(d_gdppc)))\n\n# Escala os dados em relação ao ano\nscale_year &lt;- latam |&gt; \n  group_by(year) |&gt; \n  mutate(scaled = as.numeric(scale(d_gdppc)))\n\nO primeiro gráfico mostra o crescimento do PIB per capita anual relativamente ao crescimento médio do país. Note como oscila o crescimento da Argentina: anos de crescimento acima da média são imediatamente seguidos por anos de crescimento abaixo da média durante quase toda a sua história.\nJá o Brasil parece ter um crescimento mais ao estilo “voos de galinha”, com alguns poucos anos de crescimento excepcionais aglomerados. Vale notar, também, como a presença da Venezuela entre os países acaba distorcendo a escala.\n\np &lt;- ggplot(data = scale_country, aes(x = year, y = country)) +\n  geom_tile(aes(fill = scaled)) +\n  scale_fill_gradientn(\n    name = \"Desvio em relação\\nao crescimento\\nmédio do país\",\n    breaks = seq(-4, 4, 1),\n    colours = colors_hiroshige\n    )\n\ngg_heatmap(p) +\n  ggtitle(\"Crescimento relativo ao país\")\n\n\n\n\n\n\n\n\nO segundo gráfico mostra o crescimento anual de cada país em comparação com os demais países latinoamericanos. O padrão revela quais países cresceram mais ou menos em cada ano.\nDe maneira geral, o Brasil parece crescer ligeiramente acima da média da região. A Venezuela, que no gráfico anterior parecia exibir um crescimento relativamente estável ao longo dos anos, agora apresenta um padrão com maior oscilação. Dos anos 2000 em diante, Bolivia, Chile, Colombia e Peru são os países que consistentemente crescem acima da média da região. Novamente o colapso econômico da Venezuela acaba distorcendo a escala de cores.\n\np &lt;- ggplot(data = scale_year, aes(x = year, y = country)) +\n  geom_tile(aes(fill = scaled)) +\n  scale_fill_gradientn(\n    name = \"Desvio em relação\\nao crescimento\\nmédio de cada ano\",\n    breaks = seq(-4, 4, 1),\n    colours = colors_hiroshige\n    )\n\ngg_heatmap(p) +\n  ggtitle(\"Crescimento relativo ao ano\")\n\n\n\n\n\n\n\n\nSuponha que seja interessante descobrir quais países estão crescendo acima ou abaixo da média mundial. Para chegar nesta resposta precisa-se primeiro fazer um pouco de manipulação nos dados.\n\n\nCode\n# Calcula o PIB per capita mundial e o seu crescimento \nworld &lt;- pwt |&gt; \n  group_by(year) |&gt; \n  summarise(\n    gdp = sum(rgdpe, na.rm = TRUE),\n    n = sum(pop, na.rm = TRUE)) |&gt; \n  mutate(\n    gdppc_world = gdp / n,\n    d_gdppc_world = gdppc_world / lag(gdppc_world) - 1) |&gt; \n  select(year, gdppc_world, d_gdppc_world)\n\n# Calcula a razão do PIB per capita local x mundial e o desvio do \n# crescimento do PIB per capita em relação à média mundial\n# \nlatam_scaled &lt;- latam |&gt; \n  left_join(world, by = \"year\") |&gt; \n  mutate(\n    scaled = gdppc / gdppc_world * 100,\n    deviation = (d_gdppc - d_gdppc_world) / d_gdppc_world)\n\n# Trunca os outliers para melhorar a visualização\noutlier &lt;- boxplot.stats(latam_scaled$deviation)$stats[c(1, 5)]\n\nlatam_scaled &lt;- latam_scaled %&gt;%\n  mutate(\n    deviation_trunc = if_else(deviation &gt; 5, 5, deviation),\n    deviation_trunc = if_else(deviation &lt; -5, -5, deviation_trunc)\n  )\n\n\nApós a manipulação de dados acima, pode-se visualizar o crescimento da América Latina em relação ao resto do mundo. O primeiro gráfico abaixo mostra o desvio percentual do crescimento doméstico em relação à média mundial naquele ano.\nVê-se como, de maneira geral, os países latino-americanos tiveram dificuldades para acompanhar o resto do mundo nos anos 1980. Além disso, a maior parte dos países parece ficar próxima ou abaixo da média. A exceção é durante o começo dos anos 1990 e os anos do boom de commodities (2000-2012). Além disso, vê-se como a recessão brasileira de 2015-17, apesar de ter sido influenciada por uma leve desaceleração global, foi majoritariamente uma crise interna.\n\np &lt;- ggplot(data = latam_scaled, aes(x = year, y = country)) +\n  geom_tile(aes(fill = deviation_trunc), height = 0.8) +\n  scale_fill_gradientn(\n    name = \"Percent Deviation\\nin relation to\\nWorld average\",\n    breaks = seq(-5, 5, 1),\n    colors = met.brewer(\"Hiroshige\")\n  )\n\ngg_heatmap(p)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html#posts-relacionados",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html#posts-relacionados",
    "title": "Indo além: mapas de calor",
    "section": "Posts relacionados",
    "text": "Posts relacionados\n\nNascimentos no Brasil\nCrescimento do PIB per capita no mundo\nVisualizando o IPCA\nEstético: destacando informação\nEstético: tipografia e temas"
  },
  {
    "objectID": "posts/ggplot2-tutorial/11-grafico-calor.html#footnotes",
    "href": "posts/ggplot2-tutorial/11-grafico-calor.html#footnotes",
    "title": "Indo além: mapas de calor",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale reforçar que este não foi o pico absoluto de consumo de energia no país: os dados representam o consumo percentual de energia em relação ao total de energia consumida no ano.↩︎\nO termo “normalizar” é um tanto abusado hoje em dia, em contextos estatísticos. A função scale subtrai cada valor pela sua média e divide o resultado pelo desvio padrão.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-transport/index.html",
    "href": "posts/general-posts/2023-11-wz-transport/index.html",
    "title": "Weekly Viz: Transportation in São Paulo",
    "section": "",
    "text": "São Paulo Transportation\nThe Metropolitan Region of São Paulo (MRSP) accommodates over 20 million residents and facilitates over 40 million trips on a daily basis. The city of São Paulo recently announced that it aims to drastically reduce greenhouse gas emissions, achieving net-zero emissions by 2050. To attain this objective, the city must focus on improving transit options and reducing car-dependency.\nAccording to the most recent Metro Report, published in 2018, travel patterns in the Metropolitan Region of São Paulo are categorized as follows: one-third of journeys involve non-motorized modes (such as walking or cycling), while two-thirds are conducted through motorized means. Within the latter category, 55% of trips utilize collective modes of transportation (bus, subway, train), and 45% rely on individual modes (car, taxi, motorcycle).\nNotably, trips undertaken in private automobiles (excluding taxis) constitute around 27% of the total daily trips. In other words, roughly one in every four journeys in the metro region involves private car usage.\nThe diagram below delineates the entirety of daily trips within the MRSP. It’s important to acknowledge that the data pertains to the year 2017, during which many stations along the 4-Yellow and 5-Coral lines were still under construction. Similarly, the 13-Jade and 15-Silver lines (monorail) were not yet fully operational. For the sake of simplicity, the analysis encompasses all types of travel, although commuting between home and work constitutes the predominant share of this set."
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html",
    "title": "Replicando gráficos",
    "section": "",
    "text": "Abaixo segue a lista com os principais pacotes que utilizo neste post. Eventualmente, outros pacotes como o ggtext ou sidrar são utilizados, mas deixo isto sinalizado.\n\n# This post uses renv to ensure replicability\n#| results: 'hide'\n#| include: false\nrenv::use(lockfile = \"renv.lock\")\n\nThe following package(s) will be updated:\n\n# CRAN -----------------------------------------------------------------------\n- askpass        [* -&gt; 1.2.0]\n- astsa          [* -&gt; 2.1]\n- base64enc      [* -&gt; 0.1-3]\n- bit            [* -&gt; 4.0.5]\n- bit64          [* -&gt; 4.0.5]\n- bslib          [* -&gt; 0.7.0]\n- cachem         [* -&gt; 1.0.8]\n- cellranger     [* -&gt; 1.1.0]\n- cli            [* -&gt; 3.6.2]\n- clipr          [* -&gt; 0.8.0]\n- colorspace     [* -&gt; 2.1-0]\n- commonmark     [* -&gt; 1.9.1]\n- cpp11          [* -&gt; 0.4.7]\n- crayon         [* -&gt; 1.5.2]\n- curl           [* -&gt; 5.2.1]\n- digest         [* -&gt; 0.6.35]\n- dplyr          [* -&gt; 1.1.4]\n- evaluate       [* -&gt; 0.23]\n- fansi          [* -&gt; 1.0.6]\n- farver         [* -&gt; 2.1.1]\n- fastmap        [* -&gt; 1.1.1]\n- fontawesome    [* -&gt; 0.5.2]\n- forcats        [* -&gt; 1.0.0]\n- fs             [* -&gt; 1.6.3]\n- generics       [* -&gt; 0.1.3]\n- ggbump         [* -&gt; 0.1.0]\n- ggplot2        [* -&gt; 3.5.0]\n- ggrepel        [* -&gt; 0.9.5]\n- ggtext         [* -&gt; 0.1.2]\n- glue           [* -&gt; 1.7.0]\n- gridtext       [* -&gt; 0.1.5]\n- gtable         [* -&gt; 0.3.4]\n- highr          [* -&gt; 0.10]\n- hms            [* -&gt; 1.1.3]\n- htmltools      [* -&gt; 0.5.8.1]\n- httr           [* -&gt; 1.4.7]\n- isoband        [* -&gt; 0.2.7]\n- janitor        [* -&gt; 2.2.0]\n- jpeg           [* -&gt; 0.1-10]\n- jquerylib      [* -&gt; 0.1.4]\n- jsonlite       [* -&gt; 1.8.8]\n- knitr          [* -&gt; 1.46]\n- labeling       [* -&gt; 0.4.3]\n- lifecycle      [* -&gt; 1.0.4]\n- lubridate      [* -&gt; 1.9.3]\n- maddison       [* -&gt; 0.2]\n- magrittr       [* -&gt; 2.0.3]\n- markdown       [* -&gt; 1.12]\n- memoise        [* -&gt; 2.0.1]\n- mime           [* -&gt; 0.12]\n- munsell        [* -&gt; 0.5.1]\n- openssl        [* -&gt; 2.1.1]\n- patchwork      [* -&gt; 1.2.0]\n- pillar         [* -&gt; 1.9.0]\n- pkgconfig      [* -&gt; 2.0.3]\n- png            [* -&gt; 0.1-8]\n- prettyunits    [* -&gt; 1.2.0]\n- progress       [* -&gt; 1.2.3]\n- purrr          [* -&gt; 1.0.2]\n- R6             [* -&gt; 2.5.1]\n- rappdirs       [* -&gt; 0.3.3]\n- RColorBrewer   [* -&gt; 1.1-3]\n- Rcpp           [* -&gt; 1.0.12]\n- readr          [* -&gt; 2.1.5]\n- readxl         [* -&gt; 1.4.3]\n- rematch        [* -&gt; 2.0.0]\n- renv           [* -&gt; 1.0.5]\n- rjson          [* -&gt; 0.2.21]\n- rlang          [* -&gt; 1.1.3]\n- rmarkdown      [* -&gt; 2.26]\n- rvest          [* -&gt; 1.0.4]\n- sass           [* -&gt; 0.4.9]\n- scales         [* -&gt; 1.3.0]\n- selectr        [* -&gt; 0.4-2]\n- showtext       [* -&gt; 0.9-7]\n- showtextdb     [* -&gt; 3.0]\n- sidrar         [* -&gt; 0.2.9]\n- snakecase      [* -&gt; 0.11.1]\n- stringi        [* -&gt; 1.8.3]\n- stringr        [* -&gt; 1.5.1]\n- sys            [* -&gt; 3.4.2]\n- sysfonts       [* -&gt; 0.8.9]\n- tibble         [* -&gt; 3.2.1]\n- tidyr          [* -&gt; 1.3.1]\n- tidyselect     [* -&gt; 1.2.1]\n- timechange     [* -&gt; 0.3.0]\n- tinytex        [* -&gt; 0.50]\n- tzdb           [* -&gt; 0.4.0]\n- utf8           [* -&gt; 1.2.4]\n- vctrs          [* -&gt; 0.6.5]\n- viridisLite    [* -&gt; 0.4.2]\n- vroom          [* -&gt; 1.6.5]\n- withr          [* -&gt; 3.0.0]\n- xfun           [* -&gt; 0.43]\n- xml2           [* -&gt; 1.3.6]\n- yaml           [* -&gt; 2.3.8]\n\n# Installing packages --------------------------------------------------------\n- Installing R6 ...                             OK [linked from cache]\n- Installing RColorBrewer ...                   OK [linked from cache]\n- Installing Rcpp ...                           OK [linked from cache]\n- Installing sys ...                            OK [linked from cache]\n- Installing askpass ...                        OK [linked from cache]\n- Installing astsa ...                          OK [linked from cache]\n- Installing base64enc ...                      OK [linked from cache]\n- Installing bit ...                            OK [linked from cache]\n- Installing bit64 ...                          OK [linked from cache]\n- Installing rlang ...                          OK [linked from cache]\n- Installing fastmap ...                        OK [linked from cache]\n- Installing cachem ...                         OK [linked from cache]\n- Installing digest ...                         OK [linked from cache]\n- Installing htmltools ...                      OK [linked from cache]\n- Installing jquerylib ...                      OK [linked from cache]\n- Installing jsonlite ...                       OK [linked from cache]\n- Installing cli ...                            OK [linked from cache]\n- Installing glue ...                           OK [linked from cache]\n- Installing lifecycle ...                      OK [linked from cache]\n- Installing memoise ...                        OK [linked from cache]\n- Installing mime ...                           OK [linked from cache]\n- Installing fs ...                             OK [linked from cache]\n- Installing rappdirs ...                       OK [linked from cache]\n- Installing sass ...                           OK [linked from cache]\n- Installing bslib ...                          OK [linked from cache]\n- Installing rematch ...                        OK [linked from cache]\n- Installing fansi ...                          OK [linked from cache]\n- Installing magrittr ...                       OK [linked from cache]\n- Installing utf8 ...                           OK [linked from cache]\n- Installing vctrs ...                          OK [linked from cache]\n- Installing pillar ...                         OK [linked from cache]\n- Installing pkgconfig ...                      OK [linked from cache]\n- Installing tibble ...                         OK [linked from cache]\n- Installing cellranger ...                     OK [linked from cache]\n- Installing clipr ...                          OK [linked from cache]\n- Installing colorspace ...                     OK [linked from cache]\n- Installing commonmark ...                     OK [linked from cache]\n- Installing cpp11 ...                          OK [linked from cache]\n- Installing crayon ...                         OK [linked from cache]\n- Installing curl ...                           OK [linked from cache]\n- Installing generics ...                       OK [linked from cache]\n- Installing withr ...                          OK [linked from cache]\n- Installing tidyselect ...                     OK [linked from cache]\n- Installing dplyr ...                          OK [linked from cache]\n- Installing evaluate ...                       OK [linked from cache]\n- Installing farver ...                         OK [linked from cache]\n- Installing fontawesome ...                    OK [linked from cache]\n- Installing forcats ...                        OK [linked from cache]\n- Installing gtable ...                         OK [linked from cache]\n- Installing isoband ...                        OK [linked from cache]\n- Installing labeling ...                       OK [linked from cache]\n- Installing munsell ...                        OK [linked from cache]\n- Installing viridisLite ...                    OK [linked from cache]\n- Installing scales ...                         OK [linked from cache]\n- Installing ggplot2 ...                        OK [linked from cache]\n- Installing purrr ...                          OK [linked from cache]\n- Installing stringi ...                        OK [linked from cache]\n- Installing stringr ...                        OK [linked from cache]\n- Installing tidyr ...                          OK [linked from cache]\n- Installing ggbump ...                         OK [linked from cache]\n- Installing ggrepel ...                        OK [linked from cache]\n- Installing xfun ...                           OK [linked from cache]\n- Installing markdown ...                       OK [linked from cache]\n- Installing png ...                            OK [linked from cache]\n- Installing jpeg ...                           OK [linked from cache]\n- Installing xml2 ...                           OK [linked from cache]\n- Installing gridtext ...                       OK [linked from cache]\n- Installing ggtext ...                         OK [linked from cache]\n- Installing highr ...                          OK [linked from cache]\n- Installing hms ...                            OK [linked from cache]\n- Installing openssl ...                        OK [linked from cache]\n- Installing httr ...                           OK [linked from cache]\n- Installing timechange ...                     OK [linked from cache]\n- Installing lubridate ...                      OK [linked from cache]\n- Installing snakecase ...                      OK [linked from cache]\n- Installing janitor ...                        OK [linked from cache]\n- Installing yaml ...                           OK [linked from cache]\n- Installing knitr ...                          OK [linked from cache]\n- Installing maddison ...                       OK [linked from cache]\n- Installing patchwork ...                      OK [linked from cache]\n- Installing prettyunits ...                    OK [linked from cache]\n- Installing progress ...                       OK [linked from cache]\n- Installing tzdb ...                           OK [linked from cache]\n- Installing vroom ...                          OK [linked from cache]\n- Installing readr ...                          OK [linked from cache]\n- Installing readxl ...                         OK [linked from cache]\n- Installing renv ...                           OK [linked from cache]\n- Installing rjson ...                          OK [linked from cache]\n- Installing tinytex ...                        OK [linked from cache]\n- Installing rmarkdown ...                      OK [linked from cache]\n- Installing selectr ...                        OK [linked from cache]\n- Installing rvest ...                          OK [linked from cache]\n- Installing sysfonts ...                       OK [linked from cache]\n- Installing showtextdb ...                     OK [linked from cache]\n- Installing showtext ...                       OK [linked from cache]\n- Installing sidrar ...                         OK [linked from cache]\n\n\n\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(showtext)\nlibrary(patchwork)\n\nshowtext_auto()\n\n\n# Para fins de replicabilidade\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.1.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Sao_Paulo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] patchwork_1.2.0 showtext_0.9-7  showtextdb_3.0  sysfonts_0.8.9 \n[5] tidyr_1.3.1     dplyr_1.1.4     forcats_1.0.0   stringr_1.5.1  \n[9] ggplot2_3.5.0  \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      jsonlite_1.8.8    compiler_4.3.3    renv_1.0.5       \n [5] tidyselect_1.2.1  scales_1.3.0      yaml_2.3.8        fastmap_1.1.1    \n [9] R6_2.5.1          generics_0.1.3    knitr_1.46        tibble_3.2.1     \n[13] munsell_0.5.1     pillar_1.9.0      rlang_1.1.3       utf8_1.2.4       \n[17] stringi_1.8.3     xfun_0.43         cli_3.6.2         withr_3.0.0      \n[21] magrittr_2.0.3    digest_0.6.35     grid_4.3.3        lifecycle_1.0.4  \n[25] vctrs_0.6.5       evaluate_0.23     glue_1.7.0        fansi_1.0.6      \n[29] colorspace_2.1-0  rmarkdown_2.26    purrr_1.0.2       tools_4.3.3      \n[33] pkgconfig_2.0.3   htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#dados",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#dados",
    "title": "Replicando gráficos",
    "section": "Dados",
    "text": "Dados\n\n\nCode\n#&gt; Dados do HAI\ndados &lt;- tibble::tribble(\n  ~nome,             ~hai, ~type,\n  \"República\",       80.1,    1L,\n  \"Tatuapé\",         70.4,    1L,\n  \"Jabaquara\",       68.3,    1L,\n  \"Vila Mazzei\",     66.4,    1L,\n  \"Santana\",         62.8,    1L,\n  \"Jardim Brasil\",   51.9,    0L,\n  \"Belém\",           48.5,    0L,\n  \"Jardim Umarizal\", 42.1,    0L,\n  \"Parque Arariba\",  42.1,    0L,\n  \"Brasilândia\",     42.1,    0L\n)\n\ndados &lt;- dados |&gt; \n  mutate(nome = factor(nome), nome = fct_reorder(nome, hai))\n\n#&gt; Cores dos grupos\ncores &lt;- c(\"#B9D4EE\", \"#348ACA\")\n#&gt; Adiciona a fonte Roboto\nfont_add_google(\"Roboto\", \"Roboto\")"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico",
    "title": "Replicando gráficos",
    "section": "Replicando o gráfico",
    "text": "Replicando o gráfico\n\nBásico\nA primeira versão do gráfico contém apenas o essencial da imagem. Temos um gráfico de colunas, virado na horizontal, com labels de texto. Além disso, as cores estão variando por grupo e temos uma legenda de cores. Sem utilizar a função theme o resultado do gráfico fica próximo, mas ainda muito distante do original.\n\n\nCode\nggplot(dados) +\n  geom_col(aes(x = nome, y = hai, fill = as.factor(type)), width = 0.5) +\n  geom_text(aes(x = nome, y = hai + 5, label = hai), color = \"#000000\") +\n  coord_flip() +\n  labs(x = NULL, y = NULL) +\n  scale_fill_manual(\n    name = \"\",\n    values = c(\"#B9D4EE\", \"#348ACA\"),\n    labels = c(\n      \"Para um casal com dois\\nfilhos e renda mediana**\",\n      \"Para quem mora sozinho\\ne tem renda mediana**\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCompleto\nO código abaixo tenta chegar num resultado próximo ao da imagem original. Essencialmente, precisamos:\n\nAjustar a cor do fundo.\nRemover as linhas de grade.\nRemover todas as informações dos eixos.\nAjustar a posição da legenda.\nMudar a fonte e a cor do texto.\n\nAlém destas mudanças, também deixo os números em negrito e uso a vírgula como separador de decimal. O resultado final segue abaixo. Eu utilizo theme_minimal como um template inicial.\n\n\nCode\nggplot(dados) +\n  geom_col(\n    aes(x = nome, y = hai, fill = as.factor(type)),\n    width = 0.5\n    ) +\n  geom_text(\n    aes(x = nome, y = hai + 5, label = format(hai, decimal.mark = \",\")),\n    size = 4,\n    vjust = 0.5,\n    family = \"Roboto\",\n    color = \"#000000\",\n    fontface = \"bold\") +\n  scale_y_continuous(expand = c(0, 0), limits = c(NA, 95)) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, title = \"\") +\n  scale_fill_manual(\n    name = \"\",\n    values = c(\"#B9D4EE\", \"#348ACA\"),\n    labels = c(\n      \"Para um casal com dois\\nfilhos e renda mediana**\",\n      \"Para quem mora sozinho\\ne tem renda mediana**\")\n  ) +\n  theme_minimal() +\n  theme(\n    #&gt; Fundo branco\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    #&gt; Remove as linhas de grade\n    panel.grid = element_blank(),\n    #&gt; Aplica a fonte Roboto\n    legend.text = element_text(family = \"Roboto\", color = \"#000000\"),\n    #&gt; Ajusta o texto no eixo-y\n    axis.text.y = element_text(\n      family = \"Roboto\",\n      color = \"#000000\",\n      size = 12,\n      vjust = 0.4),\n    #&gt; Remove o texto no eixo-x\n    axis.text.x = element_blank(),\n    #&gt; Aumenta. margem superior para dar espaço para a legenda\n    plot.margin = margin(t = 40, r = 5, b = 5, l = 5),\n    #&gt; Ajusta a posição e direção da legenda\n    legend.position = c(0.15, 1.1),\n    legend.direction = \"horizontal\"\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#dados-1",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#dados-1",
    "title": "Replicando gráficos",
    "section": "Dados",
    "text": "Dados\nPara reproduzir este gráfico, primeiro importo as séries via o pacote OECD. É preciso filtrar os países, limpar as datas e indexar os valores.\n\n\nCode\nlibrary(OECD)\n\ndataset &lt;- \"HOUSE_PRICES\"\nhp &lt;- get_dataset(dataset)\ncountries &lt;- c(\"CAN\", \"DEU\", \"USA\", \"ITA\", \"GBR\", \"FRA\", \"ESP\")\n\ndat &lt;- hp |&gt; \n  janitor::clean_names() |&gt; \n  filter(ind == \"RHP\", cou %in% countries, stringr::str_length(time) &gt; 4) |&gt; \n  mutate(\n    date = zoo::as.Date(zoo::as.yearqtr(time, format = \"%Y-Q%q\")),\n    obs_value = as.numeric(obs_value)\n    ) |&gt; \n  filter(date &gt;= as.Date(\"2000-01-01\"), date &lt;= as.Date(\"2022-10-01\")) |&gt; \n  select(country = cou, date, index = obs_value)\n\ndat &lt;- dat |&gt; \n  mutate(reindex = index / first(index) * 100, .by = \"country\") \n\ncountry_order &lt;- dat |&gt; \n  filter(date == max(date)) |&gt; \n  arrange(desc(reindex)) |&gt; \n  pull(country)\n\ndat &lt;- dat |&gt; \n  mutate(country = factor(country, levels = country_order))"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-1",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-1",
    "title": "Replicando gráficos",
    "section": "Replicando o gráfico",
    "text": "Replicando o gráfico\n\nBásico\nNa sua essência, as duas linhas de código abaixo reproduzem o gráfico do Financial Times.\n\nggplot(dat, aes(x = date, y = reindex, color = country)) +\n  geom_line()\n\n\n\n\n\n\n\n\nPara recriar o gráfico vou precisar das cores das linhas. Tentei encontrar cores parecidas, mas os códigos abaixo não devem ser idênticos aos do gráfico original. Além disso, também preciso do nome - por extenso - dos países.\n\n#&gt; Cores\ncores &lt;- c(\n  \"#1A48B0\", \"#EB5F8E\", \"#73DAE4\", \"#A1BC4B\", \"#2F8CC9\", \"#7B052D\", \"#BBB7B4\")\n#&gt; Nomes dos países\ncountry_labels &lt;- c(\n  \"Canada\", \"UK\", \"France\", \"US\", \"Spain\", \"Germany\", \"Italy\"\n  )\n\nAs quebras no eixo-x são um pouco difíceis de emular, pois elas fogem do comportamento padrão do ggplot2. Assim, eu preciso definir ela manualmente e ainda fazer um pequeno “hack”: essencialmente, eu crio um vetor que destaca os anos “cheios” (2000, 2005, … 2020) e coloca valores vazios nos anos intermediários.\n\ndate_breaks &lt;- seq(as.Date(\"2000-01-01\"), as.Date(\"2022-01-01\"), by = \"year\")\ndate_labels &lt;- c(date_breaks[c(1, 6, 11, 16, 21)])\nlabels_year &lt;- format(date_labels, \"%Y\")\nlabs &lt;- c(sapply(labels_year, function(x) {c(x, rep(\"\", 4))}))\nlabs &lt;- labs[1:length(date_breaks)]\n\nlabs\n\n [1] \"2000\" \"\"     \"\"     \"\"     \"\"     \"2005\" \"\"     \"\"     \"\"     \"\"    \n[11] \"2010\" \"\"     \"\"     \"\"     \"\"     \"2015\" \"\"     \"\"     \"\"     \"\"    \n[21] \"2020\" \"\"     \"\"    \n\n\nEu mantive a ordem das cores, mas como a ordem dos países mudou com a atualização dos dados, elas não batem com as dos países no gráfico original.\n\n\nCode\nggplot(dat, aes(x = date, y = reindex, color = country)) +\n  geom_line(linewidth = 1) + \n  scale_color_manual(name = \"\", values = cores, labels = country_labels) +\n  scale_y_continuous(position = \"right\") +\n  scale_x_date(breaks = date_breaks, labels = labs) +\n  labs(\n    title = \"The divergent paths of house prices across countries\",\n    subtitle = \"Real house prices (Q1 2000 = 100)\",\n    caption = \"Source: OECD (replica FT)\",\n    x = NULL,\n    y = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCompleto\nPara chegar num resultado mais próximo do original é necessário mexer em vários elementos temáticos. Como fonte, usei a Gill Sans.\n\n\nCode\nfont_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext_auto()\n\nggplot(dat, aes(x = date, y = reindex, color = country)) +\n  geom_line(linewidth = 1.1) + \n  scale_color_manual(name = \"\", values = cores, labels = country_labels) +\n  scale_y_continuous(position = \"right\") +\n  scale_x_date(breaks = date_breaks, labels = labs) +\n  labs(\n    title = \"The divergent paths of house prices across countries\",\n    subtitle = \"Real house prices (Q1 2000 = 100)\",\n    caption = \"Source: OECD (replica)\",\n    x = NULL,\n    y = NULL) +\n  guides(color = guide_legend(nrow = 1)) +\n  theme_minimal() +\n  theme(\n    #&gt; Muda a cor do fundo do gráfico\n    plot.background = element_rect(fill = \"#FEF1E4\", color = NA),\n    #&gt; Remove todos as linhas de grade intermediárias\n    panel.grid.minor = element_blank(),\n    #&gt; Remove as linhas de grade \"verticais\" que partem do eixo-x\n    panel.grid.major.x = element_blank(),\n    #&gt; Altera a cor das linhas de grade \"horizontais\" que partem do eixo-y\n    panel.grid.major.y = element_line(color = \"#EAE3DF\"),\n    \n    #&gt; Altera a fonte e a cor de todos os elementos textuais\n    text = element_text(family = \"Gill Sans\", color = \"#686261\"),\n    #&gt; Ajusta o título do gráfico para ser maior e em preto\n    plot.title = element_text(size = 20, color = \"#000000\"),\n    #&gt; Ajusta o tamanho da legenda\n    plot.subtitle = element_text(size = 12),\n    #&gt; Ajusta a posição da \"Fonte\"\n    plot.caption = element_text(hjust = 0),\n    #&gt; Altera o tamanho e a cor do texto nos eixos\n    axis.text = element_text(size = 11, color = \"#6B6865\"),\n    \n    #&gt; Aumenta as margens do gráfico\n    plot.margin = margin(rep(10, 4)),\n    \n    #&gt; Muda a cor do \"tiquezinho\" no eixo-x e deixa ele mais comprido\n    axis.ticks.x = element_line(color = \"#EADFD8\"),\n    axis.ticks.length = unit(7, \"pt\"),\n    \n    #&gt; Altera a posição da legenda\n    legend.position = c(0.4, 1)\n    \n    )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-original",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-original",
    "title": "Replicando gráficos",
    "section": "Gráfico Original",
    "text": "Gráfico Original\nA imagem é um gráfico de colunas que mostra a taxa de desemprego média mensal no Brasil. A rigor, cada mês é um trimestre móvel. Os dados são da PNAD/C do IBGE. Há dois desafios nesta visualização: primeiro, o eixo-x, que apresenta o trimestre móvel junto com o ano (abreviado) e está virado em 90 graus; segundo, o valor em percentual que aparece no topo da coluna."
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#dados-2",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#dados-2",
    "title": "Replicando gráficos",
    "section": "Dados",
    "text": "Dados\nImporto os dados diretamente da API do SIDRA, o sistema de consultas do IBGE, via o pacote sidrar. Para mais informações sobre este pacote consulte o meu post sobre.\n\n\nCode\nlibrary(sidrar)\n\nunemp &lt;- sidrar::get_sidra(\n  6381,\n  variable = 4099,\n  period = \"202206-202306\"\n  )\n\n#&gt; Limpeza de dados\ntbl_unemp &lt;- unemp |&gt; \n  # simplifica nome das colunas\n  janitor::clean_names() |&gt; \n  # seleciona e renomeia colunas\n  select(\n    qtr = trimestre_movel_codigo,\n    date_label = trimestre_movel,\n    unemp = valor\n  ) |&gt; \n  # faz alguns ajustes estéticos nos dados\n  mutate(\n    date_label = str_remove(date_label, \" 202[0-9]\"),\n    date_label = str_trim(date_label),\n    date = as.Date(paste0(qtr, \"01\"), format = \"%Y%m%d\"),\n    year = lubridate::year(date),\n    date_label = str_glue(\"{date_label}/{substr(year, 3, 4)}\"),\n    pct_label = paste(format(round(unemp, 1), decimal.mark = \",\"), \"%\")\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-2",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-2",
    "title": "Replicando gráficos",
    "section": "Replicando o gráfico",
    "text": "Replicando o gráfico\n\nBásico\nO código abaixo reproduz os elementos mais marcantes do gráfico acima. Note que eu crio um grupo artificial (fill = a) para forçar o gráfico a ter uma legenda de cores.\n\nggplot(tbl_unemp, aes(x = date_label, y = unemp, fill = \"a\")) +\n  geom_col(alpha = 0.9, width = 0.8) +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    aes(label = pct_label),\n    size = 3,\n    position = position_stack(0.95),\n    hjust = 0.5,\n    color = \"white\"\n  ) +\n  scale_y_continuous(breaks = 0:9, labels = \\(x) paste0(x, \"%\")) +\n  scale_fill_manual(\n    name = \"\",\n    values = \"#c6160d\",\n    labels = \"Índice no trimestre\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    legend.key = element_rect(color = NA),\n    axis.text.x = element_text(\n      angle = 90,\n      color = \"gray50\",\n      size = 10,\n      hjust = 1,\n      vjust = 1.5\n      )\n  )\n\n\n\n\n\n\n\n\n\n\nCompleto\nPara melhor reproduzir o gráfico, uso o pacote ggtext que me permite maior flexibilidade no título. Além disso utilizo um pequeno hack (key_glyph='point') para fazer com que a legenda de cores seja no formato de um círculo ao invés de um quadrado.\n\n\nCode\nlibrary(ggtext)\n\nggplot(tbl_unemp, aes(x = date_label, y = unemp, fill = \"a\", color = \"a\")) +\n  geom_col(alpha = 0.9, width = 0.8, key_glyph='point') +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    aes(label = pct_label),\n    size = 3,\n    position = position_stack(0.95),\n    hjust = 0.5,\n    color = \"white\"\n  ) +\n  scale_y_continuous(breaks = 0:9, labels = \\(x) paste0(x, \"%\")) +\n  scale_fill_manual(\n    name = \"\",\n    values = \"#c6160d\",\n    labels = \"Índice no trimestre\"\n  ) +\n  scale_color_manual(\n    name = \"\",\n    values = \"#c6160d\",\n    labels = \"Índice no trimestre\"\n  ) +\n  guides(color = guide_legend(override.aes = list(size = 5))) +\n  labs(\n    title = \"&lt;strong&gt;&lt;span style='font-size:18px'&gt;Evolu&ccedil;&atilde;o da taxa de desemprego no Brasil&lt;/span&gt;&lt;/strong&gt;&lt;br&gt;\n&lt;span style='font-size:16px'&gt;&Iacute;ndice no trimestre&lt;/span&gt;\",\n    caption = \"Fonte: IBGE\",\n    x = NULL,\n    y = NULL) +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.05, 1.1),\n    legend.key = element_rect(color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.x = element_text(angle = 90, color = \"gray50\", size = 10, hjust = 1.85, vjust = 1.5),\n    axis.ticks = element_blank(),\n    plot.title = element_textbox(margin = margin(t = 0, r = 0, b = 30, l = 0), hjust = 0)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico",
    "title": "Replicando gráficos",
    "section": "Gráfico",
    "text": "Gráfico\n\nDados\nOs dados podem ser baixados diretamente do site da OWID ou do meu GitHub.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nÉ preciso fazer apenas alguns ajustes nos dados. Em particular precisamos filtrar o ano e juntar a tabela de países com a de continentes.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")\n\n\n\n\nReplicando o gráfico\n\n\nBásico\nA visualização, fundamentalmente, é um gráfico de dispersão em que as cores representam o continente de cada país e em que o tamanho do círculo é proporcional à população de cada país. É tipo de gráfico também é conhecido como “bubble plot”.\n\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    )\n\n\n\n\n\n\n\n\n\n\nCompleto\nA versão finalizada do gráfico exige bastante trabalho manual, já que é necessário destacar o nome dos países. Fora isso, é preciso ajustar o tamanho dos círculos, trocar a paleta de cores, inserir as informações textuais e ajustar a legenda de cores. Para este gráfico uso as fontes Lato e Playfair Display. Assim como no gráfico do G1, é preciso usar override.aes para modificar a legenda de cores.\n\n\nCode\nfont_add_google(\"Lato\", \"Lato\")\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\n# Tema do gráfico\ntheme_owid &lt;- theme_minimal() +\n  theme(\n    # Linhas de grade\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(linetype = 3, color = \"#DDDDDD\"),\n    \n    # Título, subtítulo e nota de rodapé\n    text = element_text(family = \"Lato\"),\n    title = element_text(family = \"Lato\"),\n    \n    plot.caption = element_text(color = \"#777777\", hjust = 0, size = 8),\n    plot.title = element_text(\n      color = \"#444444\",\n      family = \"Playfair Display\",\n      size = 18),\n    plot.subtitle = element_text(color = \"#666666\", size = 11),\n    # Textos nos eixos\n    axis.title = element_text(color = \"#000000\", size = 9),\n    axis.text = element_text(color = \"#666666\", size = 12),\n    # Legenda de cores\n    legend.key.size = unit(5, \"pt\"),\n    legend.position = \"right\",\n    legend.text = element_text(size = 10),\n    # Margens do gráfico\n    plot.margin = margin(rep(10, 4))\n  )\n\n#&gt; Countries to highlight\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\n#&gt; Caption\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n#&gt; Subtitle\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\n#&gt; x-axis labels\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\n#&gt; Colors\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\n# Gráfico final\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_label_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3,\n    force = 5,\n    family = \"Lato\",\n    label.padding = unit(0.05, \"lines\"),\n    label.size = NA\n    ) +\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  scale_y_continuous(breaks = 3:7) +\n  scale_size_continuous(range = c(1, 15)) +\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  guides(\n    color = \"none\",\n    size = \"none\",\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    ) +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    ) +\n  theme_owid"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#dados-3",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#dados-3",
    "title": "Replicando gráficos",
    "section": "Dados",
    "text": "Dados\nExiste um pacote com os dados do projeto Maddison, chamado maddisson, mas os dados estão um pouco defasados. Para o propósito de replicar o gráfico acima isto não é um problema, já que a série se encerra em 2016. Para atualizar o gráfico, contudo, é preciso baixar os dados manualmente do site. Felizmente, isto é bastante simples no R. O código abaixo mostra as duas opções.\n\n# Usando o pacote\nlibrary(maddison)\n\nmad &lt;- maddison::maddison\n\n# Para trabalhar com dados mais atualizados\n\nlibrary(readxl)\n# url com a tabela em Excel\nurl &lt;- \"https://www.rug.nl/ggdc/historicaldevelopment/maddison/data/mpd2020.xlsx\"\n#&gt; Cria um path temporario e tenta baixar os dados\ntemp_path &lt;- tempfile(\"maddison.xlsx\")\n\ntry(download.file(url = url, destfile = temp_path, mode = \"wb\", quiet = TRUE))\n\nif (file.exists(temp_path)) {\n  mad &lt;- readxl::read_excel(temp_path, sheet = 3)\n}\n\nO código abaixo faz a manipulação de dados necessária. Seleciona-se os países manualmente e cria-se uma variável binária que indica se o país encontra-se em expansão ou recessão em cada ano.\n\nlatam_countries &lt;- c(\n  \"ARG\", \"BOL\", \"BRA\", \"CHL\", \"COL\", \"CUB\", \"DOM\", \"ECU\", \"GTM\", \"HND\", \"HTI\",\n  \"MEX\", \"PER\", \"VEN\")\n\nlatam &lt;- mad |&gt; \n  # Filtra apenas as linhas dos países selecionados\n  filter(\n    countrycode %in% latam_countries,\n    year &gt;= 1900\n    ) |&gt; \n  # Agrupa por país\n  group_by(country) |&gt; \n  mutate(\n    # Calcula a variação do PIB per capita de cada país\n    d_gdppc = gdppc / lag(gdppc) - 1,\n    # Cria uma variável binária para indicar se houve crescimento \n    growth = factor(if_else(d_gdppc &gt; 0, 1L, 0L)),\n    # Remove parêntesis e o texto dentro dele\n    country = str_remove(country, \" \\\\(.+\\\\)\"),\n    # Abrevia o nome da República Dominicana\n    country = str_replace(country, \"Dominican Republic\", \"Dominican Rep.\"),\n    country = factor(country),\n    country = fct_rev(country)\n  ) |&gt; \n  ungroup()"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-3",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-3",
    "title": "Replicando gráficos",
    "section": "Replicando o gráfico",
    "text": "Replicando o gráfico\n\nBásico\nA visualização é, essencialmente, um gráfico de calor de clusters, isto é, são vários quadrados coloridos adjacentes. Para desenhar este gráfico utilizo o geom_tile. Note que também duplico o eixo-x e coloco linhas verticais tracejadas no gráfico. Para conhecer mais sobre este tipo de gráfico veja meu tutorial sobre mapas de calor.\n\nggplot(latam, aes(year, country)) +\n    geom_tile(aes(fill = growth), height = .85, width = 1) +\n    geom_vline(\n      xintercept = c(1900, 1925, 1950, 1975, 2000, 2016),      \n      linetype = 2,                                           \n      colour = \"gray65\"\n      ) +                                      \n    scale_x_continuous(\n      breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n      expand = c(0,0),\n      sec.axis = dup_axis()\n      )\n\n\n\n\n\n\nCompleto\nO gráfico completo faz algumas modificações na legenda de cores e diversas alterações temáticas no gráfico. O portal Nexo utiliza a fonte Gotham Rounded em algumas variantes. Para usar esta fonte é preciso tê-la instalada. Para evitar problemas, o código abaixo verifica se as fontes necessárias estão instaladas; caso contrário, usa-se a fonte Montserrat, do Google Fonts.\n\n# Verifica a fonte do texto\n\ndbfonts &lt;- font_files()\nnexo_fonts &lt;- c(\"Gotham Rounded Bold\", \"Gotham Rounded Light\")\ncond &lt;- str_glue(\"({nexo_fonts[1]})|({nexo_fonts[2]})\")\ncheck_fonts &lt;- all(str_detect(dbfonts$family, cond))\n\nfont &lt;- ifelse(check_fonts, \"Gotham Rounded Bold\", \"Montserrat\")\nfont_axis &lt;- ifelse(check_fonts, \"Gotham Rounded Light\", \"Montserrat\")\nfont_title &lt;- \"Crimson Text\"\n\nif (check_fonts) {\n  # Adiciona as fonts Gotham Rounded Bold e Light\n  font_add(\"Gotham Rounded Bold\", \"Gotham Rounded Bold.otf\")\n  font_add(\"Gotham Rounded Light\", \"Gotham Rounded Light.otf\")\n  } else {\n  # Adiciona Montserrat caso as fontes Gotham nao estejam disponiveis\n  font_add_google(\"Montserrat\", \"Montserrat\")\n  }\n\nfont_add_google(\"Crimson Pro\", \"Crimson Text\")\n\nshowtext_auto()\n\n\n\nCode\nggplot(latam, aes(year, country)) +\n  geom_tile(aes(fill = growth), height = .85, width = 1) +\n  geom_vline(\n    xintercept = c(1900, 1925, 1950, 1975, 2000, 2016),      \n    linetype = 2,                                           \n    colour = \"gray65\"\n    ) +                                      \n  scale_x_continuous(\n    breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n    expand = c(0,0),\n    sec.axis = dup_axis()\n    ) +                         \n  scale_fill_discrete(\n    breaks = c(0,1),\n    name = \"VARIAÇÃO DO PIB NO MUNDO\\nPor ano\",\n    labels = c(\"PIB EM QUEDA\", \"PIB EM CRESCIMENTO\"), \n    na.value = \"gray90\"\n    ) +\n  labs(title = \"América Latina\", x = NULL, y = NULL) +\n  theme(\n    # Fundo do gráfico\n    panel.grid = element_blank(),\n    # Define as margens do gráfico\n    plot.margin = unit(c(1, 1, .5, 1), \"cm\"),\n    # Eixos\n    axis.text.y = element_text(vjust = .4),\n    axis.ticks = element_line(size = .4),\n    axis.text = element_text(family  = font_axis, size = 8),\n    # Legenda\n    legend.position = \"top\",\n    legend.text = element_text(\n      size = 8,\n      colour = \"gray20\",\n      family = font\n      ),\n    legend.title = element_text(\n      size = 12,\n      colour = \"gray20\",\n      family = font\n      ),\n    # Título\n    plot.title = element_text(family = font_title, size = 18)\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-cars/index.html",
    "href": "posts/general-posts/2023-11-wz-cars/index.html",
    "title": "Weekly Viz: Car Dependency in São Paulo",
    "section": "",
    "text": "Automobiles pose a significant challenge in the global effort to combat climate change. Transportation contributes to roughly a fifth of total carbon emissions worldwide, and among contemporary transportation options, petrol-burning cars are notably inefficient. Beyond air pollution, cars require substantial space for operation and inflict various negative externalities on cities.\nIn this post, I revisit São Paulo to assess the extent of the city’s dependence on cars. The data used is available via my R package tidypod.\n\n\nSão Paulo is a megacity with over 12 million inhabitants. The city recently declared its commitment to a significant reduction in greenhouse gas emissions, aiming for net-zero emissions by 2050. Achieving this goal requires a strategic focus on improving public transit options and diminishing reliance on cars. In the Greater São Paulo area, cars currently represent nearly 27% of all trips.\nSimilar to much of the southeastern region of Brazil, São Paulo boasts a predominantly clean energy matrix. The city’s primary challenge lies in the realm of transportation. Specifically, there is a need to reduce the size of the car fleet and enhance public transportation alternatives.\nOne effective measure of car-dependency is assessing the prevalence of car-free households. Using the most recent Origin Destination Survey data I estimate the share of households without cars. The data is aggregated by OD zone: after cleaning, there are 329 valid OD zones in São Paulo1. To quickly get this data I use the tidypod package.\nThe data suggest that constraints related to income, rather than preferences, predominantly keep car usage low. Interestingly, some of the OD zones with the lowest share of car-free households are affluent neighborhoods spatially close to key business districts in the city.\nIt’s important to note that reducing car-dependency doesn’t necessarily mean removing cars from the streets. As professor Dorel Soares Ramos, from the University of São Paulo, explains “renewable gaseous fuels” such as “biomethane” are considered “green for fleet adaptation”2. Similarly, electric automobiles will contribute to lowering carbon emissions. However, irrespective of the energy source for cars, we must still address the numerous negative externalities associated with their use.\n\n\nCar-free households seem to be spatially concentrated at peripheral regions of the city, very far away from any of the cities CBDs. The notable exception is the green mass right at the “old” CBD (Centro Histórico). While this region has very high share of car-free households, it should be noted that is also is not a very populated region. The “newer” CBDs of Paulista and Itaim have much lower shares of car-free households.\n\n\n\n\n\n\n\n\n\nClose to half of São Paulo’s households are entirely car-free, and in certain regions, virtually every household opts to go without any cars.\n\n\n\n\n\n\n\n\n\nA distinct negative correlation exists between income and car-dependency in São Paulo. As income rises, households tend to demand more cars and the likelihood of finding car-free households decreases. The plot below shows average household income (in log-scale) against the percentage share of car-free households across all 329 zones. The size of each circle is scaled to represent the total population living in each zone. The richer, less populated zones, have the lowest shares of car-free households. Meanwhile, the more populated and poor households have some of the highest shares of car-free households3."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-cars/index.html#são-paulo",
    "href": "posts/general-posts/2023-11-wz-cars/index.html#são-paulo",
    "title": "Weekly Viz: Car Dependency in São Paulo",
    "section": "",
    "text": "São Paulo is a megacity with over 12 million inhabitants. The city recently declared its commitment to a significant reduction in greenhouse gas emissions, aiming for net-zero emissions by 2050. Achieving this goal requires a strategic focus on improving public transit options and diminishing reliance on cars. In the Greater São Paulo area, cars currently represent nearly 27% of all trips.\nSimilar to much of the southeastern region of Brazil, São Paulo boasts a predominantly clean energy matrix. The city’s primary challenge lies in the realm of transportation. Specifically, there is a need to reduce the size of the car fleet and enhance public transportation alternatives.\nOne effective measure of car-dependency is assessing the prevalence of car-free households. Using the most recent Origin Destination Survey data I estimate the share of households without cars. The data is aggregated by OD zone: after cleaning, there are 329 valid OD zones in São Paulo1. To quickly get this data I use the tidypod package.\nThe data suggest that constraints related to income, rather than preferences, predominantly keep car usage low. Interestingly, some of the OD zones with the lowest share of car-free households are affluent neighborhoods spatially close to key business districts in the city.\nIt’s important to note that reducing car-dependency doesn’t necessarily mean removing cars from the streets. As professor Dorel Soares Ramos, from the University of São Paulo, explains “renewable gaseous fuels” such as “biomethane” are considered “green for fleet adaptation”2. Similarly, electric automobiles will contribute to lowering carbon emissions. However, irrespective of the energy source for cars, we must still address the numerous negative externalities associated with their use.\n\n\nCar-free households seem to be spatially concentrated at peripheral regions of the city, very far away from any of the cities CBDs. The notable exception is the green mass right at the “old” CBD (Centro Histórico). While this region has very high share of car-free households, it should be noted that is also is not a very populated region. The “newer” CBDs of Paulista and Itaim have much lower shares of car-free households.\n\n\n\n\n\n\n\n\n\nClose to half of São Paulo’s households are entirely car-free, and in certain regions, virtually every household opts to go without any cars.\n\n\n\n\n\n\n\n\n\nA distinct negative correlation exists between income and car-dependency in São Paulo. As income rises, households tend to demand more cars and the likelihood of finding car-free households decreases. The plot below shows average household income (in log-scale) against the percentage share of car-free households across all 329 zones. The size of each circle is scaled to represent the total population living in each zone. The richer, less populated zones, have the lowest shares of car-free households. Meanwhile, the more populated and poor households have some of the highest shares of car-free households3."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-cars/index.html#footnotes",
    "href": "posts/general-posts/2023-11-wz-cars/index.html#footnotes",
    "title": "Weekly Viz: Car Dependency in São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcluding zero-income and zero-population OD zones.↩︎\nOriginal quote: “Um ponto muito importante também é utilização, por exemplo, de combustíveis como o biometano e os combustíveis gasosos renováveis que são considerados verdes para a adaptação da frota”.↩︎\nGiven the considerable variation in OD zone sizes, one could argue that a more appropriate measure would be to utilize population density instead of total population. However, showcasing total population figures makes it easier to understand the overall impact on emissions.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/index.html",
    "href": "posts/general-posts/2023-12-wz-inflation/index.html",
    "title": "Weekly Viz: Brazilian Inflation",
    "section": "",
    "text": "Brazil has a long and painful story with inflation. Though technically Brazilian inflation only classified as a hyperinflation during a brief period in the early 1990s1, few countries had to endure such a long-standing period of prolonged and relentless inflation. The chart below shows the monthly percentage change in Brazil’s official consumer price index IPCA. I highlight some important economic and institutional events. Chiefs of the executive are also shaded in through different colors. It’s important to note that Figueiredo was not a constitutionally elected president, but rather the last leader of the Military Regime.\nThis visualization is heavily inspired by the graphic from the book Saga Brasileira: a longa luta de um povo por sua moeda2, by Miriam Leitão. Total inflation during the “hyperinflation” period surpassed 13 trillion percent! In July of 1994 the ingeniously designed Plano Real stabilizes Brazil’s economy and ends a long period of persistently high inflation. The Brazil post-Plano Real represents a paradigm shift, yet it is not entirely immune to inflation. In the nearly 30 years since the implementation of Plano Real, Brazil has witnessed a cumulative inflation of 560%.\nA nuance that is not captured by the graphic is the frequency of currency changes during this era3. Nearly every new economic plan brought about either a change in currency or a price/wage freeze. The dips in the plot coincide with theses moments, marking brief lapses of time when inflation seemed to magically subside, only to return with renewed vigor.\nOBS: to better see the plot: right-click &gt; “Open image in new tab”.\n\n\n\n\n\n\n\nEven after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Two significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the subsequent decade, the looming threat of inflation resurfaced. Facing a slowing economy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices were increased drastically causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer assumed office.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence came in early 2021. The Covid-19 inflation surge was tackled with inordinary high interest rates: in fact, for many months Brazil had the highest real interest rate in the world! After nearly a year of tight monetary policy, the Brazilian Central Bank started to cut interest rates as inflation converged to its long-run target."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/index.html#footnotes",
    "href": "posts/general-posts/2023-12-wz-inflation/index.html#footnotes",
    "title": "Weekly Viz: Brazilian Inflation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Hyperinflation_in_Brazil↩︎\nLoosely translated: “The Brazilian Saga: the long struggle of a people for its currency”.↩︎\nBefore 1986, Brazil’s currency was the Cruzeiro, though it had been “updated” several times to accommodate inflation. After the Cruzado Plan, the official currency became the “Cruzado”. In 1989, it transitioned to the “Cruzado Novo” (New Cruzado), following the Verão Plan. With the Collor Plan, in 1990, came a new currency that borrowed the old “Cruzeiro” name. Ultimately, in the lead-up to the Real, Brazil introduced a “virtual currency” named URV and the Cruzeiro was renamed as “Cruzeiro Real”.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-census-ages/index.html",
    "href": "posts/general-posts/2023-11-wz-census-ages/index.html",
    "title": "Envelhecimento no Brasil",
    "section": "",
    "text": "O futuro demográfico do Brasil, em grande parte, já é conhecido. Assim como no resto do mundo, a combinação de queda de taxa de fecundidade e aumento de expectativa de vida implica no envelhecimento da população. Pelo índice de envelhecimento1, a razão entre idosos e jovens, vê-se como esta tendência já é realidade em boa parte do território brasileiro.\n\n\n\n\n\nNo começo dos anos 2000 apenas o Rio Grande do Sul e uma parte do Sudeste apresentavam os sinais do envelhecimento. Já no Censo seguinte,\nO mapa abaixo mostra em maiores detalhes o índice de envelhecimento nas cidades brasileiras. Os dados são do mais recente Censo do IBGE. Vale lembrar que valores maiores do que 100 indicam que há uma proporção maior de idosos do que jovens.\n\n\n\n\n\nNo Rio Grande do Sul, quase metade dos municípios tem mais idosos do que jovens. O gráfico abaixo mostra a proporção de cidades onde há mais idosos que jovens, isto é, onde o índice de envelhecimento é maior do que 100. Isto acontece, em parte, pois há muitos municípios pequenos no interior do estado que sofrem com a emigração de jovens. Vale notar que todos os estados do Sul e Sudeste aparecem no ranking.\n\n\n\n\n\n\n\n\n\nConsiderando a população total de cada estado, ao invés da proporção de municípios, o Rio Grande do Sul continua no topo da lista. O ranking continua dominado por estados do Sul e Sudeste, mas vê-se como também o nordeste está envelhecendo."
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-census-ages/index.html#footnotes",
    "href": "posts/general-posts/2023-11-wz-census-ages/index.html#footnotes",
    "title": "Envelhecimento no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFormalmente, o índice de envelhecimento, \\(IE\\), é definido como a razão entre a população idosa (\\(p_{idosa}\\)), com 65 anos ou mais e a população jovem (\\(p_{jovem}\\)), com 14 anos ou menos. \\(IE = 100\\frac{p_{idosa}}{p_{jovem}}\\)↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "Tende-se a pensar que a data de nascimento de um indivíduo é algo completamente aleatório. Afinal, ninguém escolhe precisamente quando vai nascer. Alguns atribuem significado profundo à data de nascimento: a depender do horário, dia e mês a pessoa terá tendências a ser mais de uma forma do que outra. Nascer no mês impróprio pode ser um mal negócio para a vida toda.\nJá na cultura popular é comum especular que os nascimentos seguem alguns ciclos da vida. As estações do ano regulam as safras de comida, a temperatura, a disposição para sair de casa e, muito acreditam, o desejo sexual. Os feriados, as festividades, o carnaval, as vitórias no campeonato de futebol, tudo isso - imagina-se - tem algum efeito sobre a natalidade no país, nove meses no futuro.\nNo campo da economia, pode-se especular que os ciclos de crescimento econômico e, sobretudo, os ciclos de desemprego devem ter algum efeito sobre os nascimentos.\n\n\nUsando dados do IBGE, mais especificamente das Estatísticas do Registro Civil, pode-se calcular o número total de nascimentos em cada mês desde 2003. Grosso modo, nos últimos vinte anos, março, abril e maio foram os três meses com maior número de nascimentos (27,23%). Já os meses do final do ano, outubro, novembro e dezembro foram os meses com menor número de nascimentos (22,95%).\n\n\n\n\n\n\n\n\n\nIsto significa que o maior número de concepções ocorreu no inverno, nos meses de julho a agosto. Já os meses quentes de janeiro a março tiveram os menores números de concepções. Este fato vai diretamente contra a popular percepção de que épocas quentes favorecem o número de nascimentos.\nOlhando para as tendências de médio e longo prazo, vê-se que o número de nascimentos no Brasil permaneceu relativamente estável entre 2000 e 2015, variando de 230 mil a 242 mil nascimentos por mês. A quebra na série coincide com a recessão de 2015-17, período de alta inflação e desemprego recorde: de fato, a série sai do seu pico, acima de 265 mil nascimentos para o seu ponto mais baixo, abaixo de 200 mil nascimentos, neste período.\nO fim da recessão não significou, contudo, um retorno à antiga tendência. Desde 2018-19, o número de nascidos vivos no Brasil segue em tendência de queda.\n\n\n\n\n\n\n\n\n\n\n\n\nA tendência geral, observada no Brasil, repete-se em quase todas as grandes regiões. Apenas a região Norte parece escapar da tendência de queda, ainda que os valores correntes estejam levemente abaixo dos valores observados em 2013-14.\n\n\n\n\n\n\n\n\n\nVisualmente, o padrão sazonal dos nascimentos nas regiões é muito similar ao padrão geral brasileiro. Novamente, a região norte é uma exceção, já que a proporção de nascimentos em cada mês é muito homogênea.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE, afinal, há sazonalidade nos nascimentos no Brasil? Visualmente, parece haver fortes indícios de sazonalidade. Uma maneira simples de simultaneamente testar e mensurar o efeito sazonal é via uma regressão linear. Supondo um modelo simples da forma1:\n\\[\nB_{t} = T_{t}S_{t}E_{t}\n\\]\nonde \\(B_{t}\\) é o número ajustado de nascimentos mensais2, \\(T_{t}\\) é o termo de tendência, \\(S_{t}\\) é o termo sazonal e \\(E_{t}\\) é o termo de resíduo. Aplicando logaritmo natural, temos um modelo aditivo. Para modelar a tendência vamos utilizar o mesmo filtro linear utilizado nos gráficos acima, isto é, uma média móvel 2x12 centrada3. Esta estimativa é descontada da série original para chegar num valor sem tendência. Por fim, vamos supor dummies sazonais da forma:\n\\[\nb_{t} = \\alpha_{0} + \\sum_{i = 2}^{12}\\beta_{i}\\delta_{i} + u_{t}\n\\]\nonde \\(b_{t}\\) agora é o logartimo natural da série de nascimentos livre de tendência. O parâmetro \\(\\alpha_{0}\\) é uma constante e \\(\\delta_{i}\\) é uma variável binária que indica com valor unitário se a observação pertence ao mês \\(i\\).\nA tabela abaixo mostra o resultado da regressão. Assim, como se viu nos gráficos, os três meses do final do ano tem um efeito negativo enquanto os meses de março a maio têm um efeito positivo sobre o número de nascimentos. Além destes, fevereiro e junho aparecem com sinal positivo, mas tem um efeito menor. Agosto também tem um efeito negativo, porém com magnitude inferior aos meses do final do ano.\n\n\nCode\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(gtExtras)\n\ntbl_brazil &lt;- tbl_brazil |&gt; \n  mutate(\n    days = lubridate::days_in_month(date),\n    births_adjusted = total * (365/12) / days\n    )\n\n\nnasc &lt;- ts(log(tbl_brazil$births_adjusted), start = c(2003, 1), frequency = 12)\n\nmm &lt;- stats::filter(nasc, filter = rep(1/12, 12), method = \"convolution\")\nmm &lt;- stats::filter(mm, filter = c(1/2, 1/2), method = \"convolution\")\n\nnasc_mm &lt;- nasc - mm\n\nfit_lm &lt;- tslm(nasc_mm ~ season)\n\ntab_reg &lt;- \n  tbl_regression(\n    fit_lm,\n    estimate_fun = ~style_sigfig(.x, digits = 3)\n    ) %&gt;%\n  bold_labels() %&gt;%\n  bold_p() %&gt;%\n  as_gt() %&gt;%\n  gt_theme_538()\n\ntab_reg\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    season\n\n\n\n        1\n—\n—\n\n        2\n0.048\n\n&lt;0.001\n        3\n0.089\n\n&lt;0.001\n        4\n0.082\n\n&lt;0.001\n        5\n0.073\n\n&lt;0.001\n        6\n0.037\n\n&lt;0.001\n        7\n0.002\n\n0.7\n        8\n-0.031\n\n&lt;0.001\n        9\n-0.002\n\n0.7\n        10\n-0.069\n\n&lt;0.001\n        11\n-0.095\n\n&lt;0.001\n        12\n-0.101\n\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nVisualmente, o resultado da regressão acima pode ser visto no painel abaixo. Note que o padrão sazonal é constante: começa o ano subindo, atinge um pico e aí começa a cair.\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra a decomposição completa da série de nascimentos.\n\n\n\n\n\n\n\n\n\nUm exercício semelhante poderia ser feito analisando cada estado individualmente. Os dados de nascimentos por UF funcionam como um painel longitudinal e pode-se fazer um regressão com efeitos fixos. Por completude, faço este exercício, mas como se vê no gráfico final, o padrão sazonal é muito similar. Em verdade, os efeitos de UF parecem ter ligeiramente intensificado o efeito sazonal."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#o-mês-de-nascimento",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#o-mês-de-nascimento",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "Usando dados do IBGE, mais especificamente das Estatísticas do Registro Civil, pode-se calcular o número total de nascimentos em cada mês desde 2003. Grosso modo, nos últimos vinte anos, março, abril e maio foram os três meses com maior número de nascimentos (27,23%). Já os meses do final do ano, outubro, novembro e dezembro foram os meses com menor número de nascimentos (22,95%).\n\n\n\n\n\n\n\n\n\nIsto significa que o maior número de concepções ocorreu no inverno, nos meses de julho a agosto. Já os meses quentes de janeiro a março tiveram os menores números de concepções. Este fato vai diretamente contra a popular percepção de que épocas quentes favorecem o número de nascimentos.\nOlhando para as tendências de médio e longo prazo, vê-se que o número de nascimentos no Brasil permaneceu relativamente estável entre 2000 e 2015, variando de 230 mil a 242 mil nascimentos por mês. A quebra na série coincide com a recessão de 2015-17, período de alta inflação e desemprego recorde: de fato, a série sai do seu pico, acima de 265 mil nascimentos para o seu ponto mais baixo, abaixo de 200 mil nascimentos, neste período.\nO fim da recessão não significou, contudo, um retorno à antiga tendência. Desde 2018-19, o número de nascidos vivos no Brasil segue em tendência de queda."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#regiões",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#regiões",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "A tendência geral, observada no Brasil, repete-se em quase todas as grandes regiões. Apenas a região Norte parece escapar da tendência de queda, ainda que os valores correntes estejam levemente abaixo dos valores observados em 2013-14.\n\n\n\n\n\n\n\n\n\nVisualmente, o padrão sazonal dos nascimentos nas regiões é muito similar ao padrão geral brasileiro. Novamente, a região norte é uma exceção, já que a proporção de nascimentos em cada mês é muito homogênea."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#sazonalidade",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#sazonalidade",
    "title": "Nascimentos no Brasil",
    "section": "",
    "text": "E, afinal, há sazonalidade nos nascimentos no Brasil? Visualmente, parece haver fortes indícios de sazonalidade. Uma maneira simples de simultaneamente testar e mensurar o efeito sazonal é via uma regressão linear. Supondo um modelo simples da forma1:\n\\[\nB_{t} = T_{t}S_{t}E_{t}\n\\]\nonde \\(B_{t}\\) é o número ajustado de nascimentos mensais2, \\(T_{t}\\) é o termo de tendência, \\(S_{t}\\) é o termo sazonal e \\(E_{t}\\) é o termo de resíduo. Aplicando logaritmo natural, temos um modelo aditivo. Para modelar a tendência vamos utilizar o mesmo filtro linear utilizado nos gráficos acima, isto é, uma média móvel 2x12 centrada3. Esta estimativa é descontada da série original para chegar num valor sem tendência. Por fim, vamos supor dummies sazonais da forma:\n\\[\nb_{t} = \\alpha_{0} + \\sum_{i = 2}^{12}\\beta_{i}\\delta_{i} + u_{t}\n\\]\nonde \\(b_{t}\\) agora é o logartimo natural da série de nascimentos livre de tendência. O parâmetro \\(\\alpha_{0}\\) é uma constante e \\(\\delta_{i}\\) é uma variável binária que indica com valor unitário se a observação pertence ao mês \\(i\\).\nA tabela abaixo mostra o resultado da regressão. Assim, como se viu nos gráficos, os três meses do final do ano tem um efeito negativo enquanto os meses de março a maio têm um efeito positivo sobre o número de nascimentos. Além destes, fevereiro e junho aparecem com sinal positivo, mas tem um efeito menor. Agosto também tem um efeito negativo, porém com magnitude inferior aos meses do final do ano.\n\n\nCode\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(gtExtras)\n\ntbl_brazil &lt;- tbl_brazil |&gt; \n  mutate(\n    days = lubridate::days_in_month(date),\n    births_adjusted = total * (365/12) / days\n    )\n\n\nnasc &lt;- ts(log(tbl_brazil$births_adjusted), start = c(2003, 1), frequency = 12)\n\nmm &lt;- stats::filter(nasc, filter = rep(1/12, 12), method = \"convolution\")\nmm &lt;- stats::filter(mm, filter = c(1/2, 1/2), method = \"convolution\")\n\nnasc_mm &lt;- nasc - mm\n\nfit_lm &lt;- tslm(nasc_mm ~ season)\n\ntab_reg &lt;- \n  tbl_regression(\n    fit_lm,\n    estimate_fun = ~style_sigfig(.x, digits = 3)\n    ) %&gt;%\n  bold_labels() %&gt;%\n  bold_p() %&gt;%\n  as_gt() %&gt;%\n  gt_theme_538()\n\ntab_reg\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    season\n\n\n\n        1\n—\n—\n\n        2\n0.048\n\n&lt;0.001\n        3\n0.089\n\n&lt;0.001\n        4\n0.082\n\n&lt;0.001\n        5\n0.073\n\n&lt;0.001\n        6\n0.037\n\n&lt;0.001\n        7\n0.002\n\n0.7\n        8\n-0.031\n\n&lt;0.001\n        9\n-0.002\n\n0.7\n        10\n-0.069\n\n&lt;0.001\n        11\n-0.095\n\n&lt;0.001\n        12\n-0.101\n\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nVisualmente, o resultado da regressão acima pode ser visto no painel abaixo. Note que o padrão sazonal é constante: começa o ano subindo, atinge um pico e aí começa a cair.\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra a decomposição completa da série de nascimentos.\n\n\n\n\n\n\n\n\n\nUm exercício semelhante poderia ser feito analisando cada estado individualmente. Os dados de nascimentos por UF funcionam como um painel longitudinal e pode-se fazer um regressão com efeitos fixos. Por completude, faço este exercício, mas como se vê no gráfico final, o padrão sazonal é muito similar. Em verdade, os efeitos de UF parecem ter ligeiramente intensificado o efeito sazonal."
  },
  {
    "objectID": "posts/general-posts/2023-10-nascimentos-brasil/index.html#footnotes",
    "href": "posts/general-posts/2023-10-nascimentos-brasil/index.html#footnotes",
    "title": "Nascimentos no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEste é um modelo bastente convencional em séries de tempo, também conhecido, modelo “clássico” ou “decomposição clássica”. Veja, por exemplo Morettin, P & Toloi, C. Análise de Séries Temporais (2006).↩︎\nBecker (1989) sugere corrigir o número de nascimentos pelo número de dias no mês da seguinte maneira: \\(\\tilde{x}_{t} = x_{t}\\frac{365}{12z}\\) onde \\(z\\) é o número de dias do mês.↩︎\nIsto é equivalente a fazer primeiro uma média móvel de dozes meses e depois uma média móvel de dois meses. Na prática, todos os termos têm peso 1/12, exceto pelo primeiro e último que têm peso 1/24.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/index.html#post-plano-real",
    "href": "posts/general-posts/2023-12-wz-inflation/index.html#post-plano-real",
    "title": "Weekly Viz: Brazilian Inflation",
    "section": "",
    "text": "Even after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Two significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the subsequent decade, the looming threat of inflation resurfaced. Facing a slowing economy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices were increased drastically causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer assumed office.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence came in early 2021. The Covid-19 inflation surge was tackled with inordinary high interest rates: in fact, for many months Brazil had the highest real interest rate in the world! After nearly a year of tight monetary policy, the Brazilian Central Bank started to cut interest rates as inflation converged to its long-run target."
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-1-mapas.html",
    "href": "posts/ggplot2-tutorial/12-1-mapas.html",
    "title": "You need a map - Parte 1",
    "section": "",
    "text": "Eu começo este post com uma confissão: eu sofri muito até conseguir fazer mapas minimamente apresentáveis com o R. Há duas dificuldades bem sérias: primeiro, objetos geométricos são uma classe de objetos bastante complexa; segundo, há inúmeras convenções na elaboração de mapas que escapam completamente aos objetivos do economista.\nEste post vai tentar ensinar a fazer mapas, apresentando absolutamente o mínimo necessário sobre convenções cartográficas, tipos de projeção, e geografia de maneira geral. Por questão de completude ele é dividido em três partes: na primeira parte mostro como fazer mapas de pontos de maneira simples sem se preocupar com geometria; na segunda parte mostro como fazer mapas coropléticos, mapas onde as cores usualmente representam o valor das variáveis; por fim, na terceira parte, mostro como fazer mapas estáticos com “elementos de mapa” como ruas, avenidas, rios, etc."
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-1-mapas.html#o-básico",
    "href": "posts/ggplot2-tutorial/12-1-mapas.html#o-básico",
    "title": "You need a map - Parte 1",
    "section": "O básico",
    "text": "O básico\nA abordagem mais simples para tratar dados espaciais é simplesmente tratá-los como dados tabulados quaisquer. Isto funciona perfeitamente bem para objetos geométricos na forma de pontos e linhas, mas torna-se impraticável no caso de polígonos.\nNo plano cartesiano, longitude e latitude são literalmente coordenadas; pode-se mapear estas coordenadas como pontos num gráfico de dispersão. Para tornar os exemplos mais aplicados vamos usar a base de Empreendimentos Residenciais Verticais da Embraesp, disponibilizado pelo Centro de Estudos da Metrópole da FAU/USP. Os dados compreendem todos os lançamentos residenciais na Região Metropolitana de São Paulo entre 1985 e 2013. Para simplificar nossa análise vamos nos restringir somente aos empreendimentos lançados na capital.\n\n#&gt; Os pacotes necessários\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\n\n# Para ler os dados dentro do R a partir do Github\ncem &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/cem_imoveis.zip\"\n  )\n#| Seleciona apenas os empreendimentos lançados em São Paulo (capital)\ncem &lt;- cem |&gt; \n  mutate(code_muni = as.numeric(substr(ap2010, 1, 7))) |&gt; \n  filter(code_muni == 3550308)\n\nNo gráfico abaixo cada ponto é um empreendimento residencial lançado em São Paulo entre 1985 e 2013. Note que os valores de longitude e latitude são fornecidos diretamente aos argumentos x e y.\nA função coord_sf fornece mais estrutura ao sistema de coordenadas. Especificamente, esta função garante que os dados plotados compartilhem o mesmo sistema de coordenadas e permite explicitamente definir qual sistema utilizar. No exemplo abaixo uso crs = sf::st_crs(4326) que é código para a projeção do World Geodetic System.\n\nggplot(cem, aes(x = lon, y = lat)) +\n  geom_point(size = 0.1) +\n  coord_sf(crs = sf::st_crs(4326))\n\n\n\n\n\n\n\n\nA principal vantagem de tratar os pontos, simplesmente como pontos, é que podemos aplicar todo o nosso conhecimento acumulado até agora. Os dados no gráfico seguem a mesma lógica dos gráficos de dispersão, visto anteriormente e a sua manipulação segue as mesmas convenções de dados tabulares convencionais.\nNo exemplo abaixo, o tamanho de cada círculo é proporcional ao número de unidades do empreendimento e a cor sinaliza se o empreendimento é veritical ou horizontal.\n\n#&gt; Seleciona somente empreendimentos lançados em 2008\ncem08 &lt;- cem |&gt; \n  filter(ano_lanc == 2008)\n\nggplot(cem08, aes(x = lon, y = lat)) +\n  geom_point(aes(size = emp_unid, color = as.factor(emp_tipo)), alpha = 0.5) +\n  coord_sf(crs = st_crs(4326))\n\n\n\n\n\n\n\n\nInfelizmente, é necessário repetir o CRS da projeção dos dados em todos os gráficos. No post seguinte, vamos ver como construir objetos “geométricos” com o pacote sf, que funcionam como um data.frame embutido com informação espacial; neste caso, o ggplot2 sabe qual a projeção a ser utilizada e não é necessário declará-la.\nPode-se também fazer gráficos em facets usando os dados de longitude e latitude. No gráfico abaixo, mostro os lançamentos ano a ano no período 2008-2011.\n\n#&gt; Seleciona empreendimentos lançados entre 2008 e 2011\nsub &lt;- cem |&gt; \n  filter(ano_lanc %in% 2008:2011)\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_point(size = 0.4) +\n  coord_sf(crs = st_crs(4326)) +\n  facet_wrap(vars(ano_lanc))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-1-mapas.html#agregando-pontos",
    "href": "posts/ggplot2-tutorial/12-1-mapas.html#agregando-pontos",
    "title": "You need a map - Parte 1",
    "section": "Agregando pontos",
    "text": "Agregando pontos\n\nMapas de contorno\nPode-se agregar os pontos em mapas de calor (contorno). Há duas funções principais:\n\ngeom_density_2d - que desenha linhas de contorno\ngeom_density_2d_filled - que desenha uma mapa de contorno\n\nNos gráficos abaixo uso apenas a subamostra de lançamentos no período 2008-2011.\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d() +\n  coord_sf(crs = st_crs(4326)) +\n  theme(legend.position = \"none\")\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d_filled() +\n  coord_sf(crs = st_crs(4326)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTalvez o argumento mais útil destas funções seja o bins ou binwidth, similar ao argumento de geom_histogram. Os mapas abaixo mostram como este argumento controla o número de quebras nas linhas de contorno. A função que calcula os contornos é a MASS::kde2d() e, em geral, ela escolhe bons valores para o número de bins.\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d_filled(bins = 5) +\n  coord_sf(crs = st_crs(4326))\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d_filled(bins = 8) +\n  coord_sf(crs = st_crs(4326))\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d_filled(bins = 10) +\n  coord_sf(crs = st_crs(4326))\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d_filled(bins = 15) +\n  coord_sf(crs = st_crs(4326))\n\n\n\n\n\n\n\n\n\n\nNa base de dados do CEM cada linha é um tipo de unidade lançada de um empreendimento (uma tipologia). Assim, é possível (e recorrente) que um mesmo empreendimento apareça em várias linhas. A tabela abaixo mostra os empreendimentos lançados pela Cyrella em 2009. Note como o mesmo empreendimento aparece mais do que uma vez.\n\nsub |&gt; \n  filter(construt_a == \"Cyrela\", ano_lanc == 2009) |&gt; \n  select(data_lanc, name_district, cep, emp_andares, emp_unid, emp_dorm, emp_elev, dorm, banh) |&gt; \n  arrange(data_lanc)\n\n# A tibble: 15 × 9\n   data_lanc  name_district cep     emp_andares emp_unid emp_dorm emp_elev  dorm\n   &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 2009-03-15 SANTO AMARO   04754-…          27      108      324        3     3\n 2 2009-03-15 SANTO AMARO   04754-…          27      108      432        3     4\n 3 2009-05-15 SANTO AMARO   04754-…          27      108      324        3     3\n 4 2009-05-15 SANTO AMARO   04754-…          27      108      432        3     4\n 5 2009-08-15 SANTO AMARO   04757-…          26      208      416        5     2\n 6 2009-08-15 SANTO AMARO   04757-…          26      104      312        3     3\n 7 2009-08-15 MORUMBI       05653-…           4       42       84        1     2\n 8 2009-08-15 MORUMBI       05653-…           4      159      318        3     2\n 9 2009-09-15 SANTO AMARO   04754-…          27      216      864        6     4\n10 2009-10-15 SANTO AMARO   04757-…          27      108      324        3     3\n11 2009-10-15 SANTO AMARO   04757-…          27      108      432        3     4\n12 2009-10-15 CASA VERDE    02452-…          26      104      312        3     3\n13 2009-11-15 SANTANA       02460-…          20       80      320        3     4\n14 2009-12-15 MORUMBI       05653-…           4       90       90        5     1\n15 2009-12-15 MORUMBI       05653-…           4      136      272        7     2\n# ℹ 1 more variable: banh &lt;dbl&gt;\n\n\nQuando os dados estão na forma de uma “tabela de frequências” usualmente usa-se o argumento weight. O exemplo abaixo mostra um gráfico de dispersão entre a área útil de cada tipologia e o seu preço final (atualizado pelo IGP-DI). No primeiro caso, cada tipologia tem peso idêntico, ou seja, uma tipologia que foi lançada 200 vezes tem o mesmo peso que uma outra tipologia que foi lançada apenas 20 vezes. No gráfico da direita, o argumento weight = emp_unid cria uma peso para cada observação, de tal maneira que as tipologias mais representativas têm peso maior no ajuste.\n\nggplot(sub, aes(x = ar_ut_unid, y = log(pc_tt_atu))) +\n  geom_point() +\n  geom_smooth(method = \"gam\") +\n  scale_y_continuous(limits = c(NA, 16))\n\nggplot(sub, aes(x = ar_ut_unid, y = log(pc_tt_atu))) +\n  geom_point() +\n  geom_smooth(aes(weight = emp_unid), method = \"gam\") +\n  scale_y_continuous(limits = c(NA, 16))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfelizmente, a função subjacente que a geom_density_2d utiliza não funciona diretamente com o argumento weight. A solução para contornar isto é criar uma tabela de dados onde as observações são repetidas usando a função uncount. Esta função repete as linhas de uma tabela segundo alguma variável: no caso abaixo, cada linha é repetida de acordo com o número de unidades que foram lançadas.\n\ncount_unidades &lt;- sub |&gt; \n  select(emp_code, emp_unid, lat, lon) |&gt; \n  uncount(emp_unid)\n\nggplot(count_unidades, aes(x = lon, y = lat)) +\n  geom_density_2d() +\n  coord_sf(crs = st_crs(4326)) +\n  ggtitle(\"Unidades lançadas\") +\n  labs(x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d() +\n  coord_sf(crs = st_crs(4326)) +\n  ggtitle(\"Empreendimentos lançados\") +\n  labs(x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMais a título de curiosidade, também é possível usar adaptar a função stat_density_2d() para gerar um mapa similar ao do geom_count.\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_count() +\n  coord_sf(crs = st_crs(4326))\n\nggplot(sub, aes(x = lon, y = lat)) +\n  stat_density_2d(\n    geom = \"point\",\n    aes(size = after_stat(density)),\n    n = 20,\n    contour = FALSE\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPode-se separar os contornos em grupos com cores distintas. No gráfico abaixo eu separo os empreendimentos pelo número de andares. Vê-se que os empreendimentos horizontais simples, de apenas um andar, estão concentrados no lado nordeste do gráfico. Já os empreendimentos verticais mais elevados (20 a 35 andares) estão concentrados no sudoeste do gráfico (grosso modo, região do Itaim Bibi) e na região oeste (grosso modo, região da Vila Formosa).\n\nsub &lt;- sub |&gt; \n  mutate(\n    group = findInterval(emp_andares, c(1, 5, 10, 20, 35)),\n    group = factor(\n      group,\n      labels = c(\"1\", \"1 a 5\", \"5 a 10\", \"10 a 20\", \"20 a 35\")\n      )\n  )\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_density_2d(aes(color = group)) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_color_brewer(name = \"Número de andares\", palette = 6, type = \"qual\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nComo de costume, é possível sobrepor vários tipos de geoms num mesmo gráfico. O gráfico abaixo mostra a distribuição espacial dos empreendimentos de 10 a 20 andares.\n\nsub &lt;- sub |&gt; \n  mutate(\n    group = findInterval(emp_andares, c(1, 5, 10, 20, 35)),\n    group = factor(\n      group,\n      labels = c(\"1\", \"1 a 5\", \"5 a 10\", \"10 a 20\", \"20 a 35\")\n      )\n  )\n\nggplot(filter(sub, group == \"10 a 20\"), aes(x = lon, y = lat)) +\n  geom_point(aes(size = emp_unid), alpha = 0.1) +\n  geom_density_2d_filled(alpha = 0.5) +\n  coord_sf(crs = st_crs(4326))\n\n\n\n\n\n\n\n\n\n\nMapas com grids\nPor fim, vale mencionar duas funções simples que permitem agregar os pontos em um grid regular. Há duas opções de grids:\n\nQuadrados com a função geom_bin2d() ou geom_bin_2d()\nHexágonos com a função geom_hex()\n\nTanto quadrados como hexágonos tem boas propriedades geométricas que os tornam aptos a agregar dados espaciais.\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d() +\n  scale_fill_viridis_c(option = \"inferno\")\n\n\n\n\n\n\n\n\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d(bins = 10) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_viridis_c(option = \"inferno\")\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d(bins = 15) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_viridis_c(option = \"inferno\")\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d(bins = 25) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_viridis_c(option = \"inferno\")\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d(bins = 50) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_viridis_c(option = \"inferno\")\n\nExiste algum controle sobre a quantidade de quadrados via o argumento bins ou binwidth (similar ao argumento da função geom_histogram).\n\n\n\n\n\n\n\n\n\nO padrão da função geom_bin2d() é de contar o número de pontos dentro de cada quadrado. Pode-se mudar este valor via o argumento weight dentro de aes. No gráfico da esquerda, os dados representam o número total de unidades lançadas nos empreendimentos. Já no gráfico da direita, os valores representam o valor total (em R$ atualizados pelo IGP-DI) dos empreendimentos lançados\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d(aes(weight = emp_unid), bins = 25) +\n  coord_sf(crs = st_crs(4326)) +\n  ggtitle(\"Número de unidades lançadas\") +\n  scale_fill_viridis_c(option = \"inferno\")\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_bin2d(aes(weight = pc_emp_atu / 1e9), bins = 25) +\n  coord_sf(crs = st_crs(4326)) +\n  ggtitle(\"VGV dos empreendimentos\") +\n  scale_fill_viridis_c(option = \"inferno\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOs mapas com hexágonos funcionam da mesma maneira que os mapas com grids em quadrado. Os mapas com hexágonos se tornaram bastante populares pois emulam o sistema de grid utilizado pela Uber (H3). Os códigos abaixo mostram um exemplo simples de como montar gráficos em hexágonos. Para maior controle sobre os hexágonos vale explorar os pacotes h3 e h3r.\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_hex(bins = 15) +\n  scale_fill_viridis_c()\n\nggplot(sub, aes(x = lon, y = lat)) +\n  geom_hex(bins = 15, aes(weight = pc_emp_atu / 1e9)) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPor fim, vale notar que as técnicas de agregação de pontos que vimos aqui também funcionam com gráficos de dispersão convencionais. De fato, elas podem ser utilizadas para amenizar problemas de overplotting.\n\nggplot(sub, aes(x = ar_ut_unid, y = log(pc_tt_atu))) +\n  geom_point() +\n  geom_smooth(aes(weight = emp_unid), method = \"gam\") +\n  scale_fill_viridis_c()\n\nggplot(sub, aes(x = ar_ut_unid, y = log(pc_tt_atu))) +\n  geom_bin2d(bins = 50) +\n  geom_smooth(aes(weight = emp_unid), method = \"gam\") +\n  scale_fill_viridis_c()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-1-mapas.html#posts-relacionados",
    "href": "posts/ggplot2-tutorial/12-1-mapas.html#posts-relacionados",
    "title": "You need a map - Parte 1",
    "section": "Posts Relacionados",
    "text": "Posts Relacionados\n\nFundamentos: Gráfico de dispersão\nFundamentos: histograma"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html",
    "href": "posts/general-posts/2023-12-bump-plots/index.html",
    "title": "Bump Plots",
    "section": "",
    "text": "Existe um pacote auxiliar específico para criar “bump charts” chamado {ggbump}. O pacote está disponível tanto no CRAN como no Github do autor.\nNeste tipo de gráfico, quer-se comparar o valor de uma variável em diferentes contextos. Podemos ter uma comparação entre os mesmos grupos ao longo do tempo ou os mesmos grupos ao longo de variáveis distintas. Em geral, estes gráficos são organizados em forma de rankings e têm como objetivo facilitar a comparação entre grupos.\nAlguns exemplos de aplicação incluem:\n\nNúmero de medalhas de ouro nas olimpíadas de uma subamostra de países.\nGênero mais escutado de música de um usuário ao longo dos anos.\nPaíses mais populosos do mundo ao longo de décadas.\nRanking de imóveis em vários critérios.\nRanking de times num campeonato a cada semana.\nRanking de países mais ricos segundo diferentes critérios de riqueza\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggbump)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nO primeiro exemplo é tomado emprestado da página de apresentação do pacote e ilustra o básico da função geom_bump. Os dados tem de estar em formato ‘tidy’ (longitudinal) onde a posição dos “pontos” é informada pelos argumentos x e y e os grupos identificados via group. Isto é, essencialmente, temos um grid de pontos, que ocupa 100% do espaço do gráfico, conectados via uma coluna “group”.\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nplayer &lt;- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf &lt;- data.frame(x = year,\n                 y = position,\n                 group = player)\n\n\n\n\n\n\nx\ny\ngroup\n\n\n\n\n2019\n4\nA\n\n\n2020\n2\nA\n\n\n2021\n2\nA\n\n\n2019\n3\nB\n\n\n2020\n1\nB\n\n\n2021\n4\nB\n\n\n2019\n2\nC\n\n\n2020\n3\nC\n\n\n2021\n1\nC\n\n\n2019\n1\nD\n\n\n2020\n4\nD\n\n\n2021\n3\nD\n\n\n\n\n\n\n\nVale tirar um tempo para comparar, com calma, as entradas na tabela acima e o resultado no gráfico abaixo.\n\nggplot(df, aes(year, position, color = player)) +\n  geom_bump()\n\n\n\n\n\n\n\n\n\n\n\nPodemos analisar as cidades com maior número de vendas ao longo dos anos usando a já conhecida txhousing. Quem acompanha meus tutoriais de ggplot2 já deve estar cansado de ver esta base sendo utilizada.\nQueremos montar um gráfico que mostra o ranking de vendas de imóveis ao longo dos anos. Removo o ano de 2015, pois este ano não está completo na amostra. Como há mais de quarenta cidades na amostra eu crio uma subamostra que contém apenas as cidades com maior número de vendas em 2014. O código abaixo mostra o passo-a-passo da manipulação de dados.\n\n#&gt; Encontra as top-15 cidades com maior número de vendas em 2014\ntop_cities &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas o ano de 2014\n  filter(year == 2014) |&gt; \n  #&gt; Calcula o total de vendas em cada cidade\n  summarise(total = sum(listings, na.rm = TRUE), .by = \"city\") |&gt; \n  #&gt; Seleciona o top-15\n  slice_max(total, n = 15) |&gt; \n  pull(city)\n\n#&gt; Calcula o total de vendas anuais na subamostra de cidades e faz o ranking anual\n\nrank_housing &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas cidades dentro da subamostra\n  filter(city %in% top_cities, year &gt; 2005, year &lt; 2015) |&gt; \n  #&gt; Calcula o total de vendas a cada ano\n  summarise(\n    listing_year = sum(listings, na.rm = TRUE),\n    .by = c(\"city\", \"year\")\n    ) |&gt; \n  #&gt; Faz o ranking das cidades dentro de cada ano\n  mutate(rank = rank(-listing_year, \"first\"), .by = \"year\")\n\nNo gráfico abaixo, cada cidade tem uma cor diferente, mas omito a legenda de cores. Note o uso de scale_y_reverse já que, tipicamente, queremos mostrar os menores valores na parte superior do gráfico.\n\nggplot(rank_housing, aes(year, rank, group = city, color = city)) +\n  geom_bump() +\n  scale_y_reverse() +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nPara melhorar a legibilidade do gráfico podemos colocar o nome das cidades ao lado das linhas usando geom_text.\n\nggplot() +\n  geom_bump(\n    data = rank_housing,\n    aes(year, rank, group = city, color = city)\n    ) +\n  geom_text(\n    data = filter(rank_housing, year == max(year)),\n    aes(year, rank, label = city),\n    nudge_x = 0.1,\n    hjust = 0\n  ) +\n  scale_y_reverse() +\n  scale_x_continuous(limits = c(NA, 2017)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nMesmo com o nome das cidades, há muitas linhas para acompanhar no gráfico. Imagine, por exemplo, que queremos destacar apenas algumas das cidades selecionadas. Não parece fazer muito sentido destacar as cidades do top 4 (Houston, Dallas, San Antonio e Austin), já que elas praticamente não se alternam no ranking durante todo o período. Vamos, então, destacar as cidades de Bay Area, El Paso, Corpus Christi e Tyler.\n\nsel_cities &lt;- c(\"Bay Area\", \"El Paso\", \"Corpus Christi\", \"Tyler\")\n\nrank_housing &lt;- rank_housing |&gt; \n  mutate(\n    highlight = if_else(city %in% sel_cities, city, \"\"),\n    is_highlight = factor(if_else(city %in% sel_cities, 1L, 0L))\n  )\n\nO gráfico abaixo exige um código considervalmente mais longo, mas melhora o gráfico original em vários aspectos. Agora temos um maior destaque para as cidades de interesse. Os eixos estão melhor delimitados e as linhas de fundo redundantes foram removidas.\n\n\nCode\nggplot() +\n  #&gt; Linhas em cinza (sem destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 0),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 0.8,\n    smooth = 8\n    ) +\n  #&gt; Linhas coloridas (com destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 1),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 2,\n    smooth = 8\n  ) +\n  #&gt; Pontos \n  geom_point(\n    data = rank_housing,\n    aes(year, rank, color = highlight),\n    size = 4\n  ) +\n  #&gt; Nomes sem destaque\n  geom_text(\n    data = filter(rank_housing, year == max(year), !(city %in% sel_cities)),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    color = \"gray20\"\n  ) +\n  #&gt; Nome com destaque (em negrito)\n  geom_text(\n    data = filter(rank_housing, year == max(year), city %in% sel_cities),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    fontface = \"bold\"\n  ) +\n  #&gt; Adiciona os eixos para melhorar leitura do gráfico\n  scale_y_reverse(breaks = 1:15) +\n  scale_x_continuous(limits = c(NA, 2017), breaks = 2006:2014) +\n  #&gt; Cores\n  scale_color_manual(\n    values = c(\"gray70\", \"#2f4858\", \"#86bbd8\", \"#f6ae2d\", \"#f26419\")\n  ) +\n  #&gt; Elementos temáticos\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#exemplo-simples",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#exemplo-simples",
    "title": "Bump Plots",
    "section": "",
    "text": "O primeiro exemplo é tomado emprestado da página de apresentação do pacote e ilustra o básico da função geom_bump. Os dados tem de estar em formato ‘tidy’ (longitudinal) onde a posição dos “pontos” é informada pelos argumentos x e y e os grupos identificados via group. Isto é, essencialmente, temos um grid de pontos, que ocupa 100% do espaço do gráfico, conectados via uma coluna “group”.\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nplayer &lt;- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf &lt;- data.frame(x = year,\n                 y = position,\n                 group = player)\n\n\n\n\n\n\nx\ny\ngroup\n\n\n\n\n2019\n4\nA\n\n\n2020\n2\nA\n\n\n2021\n2\nA\n\n\n2019\n3\nB\n\n\n2020\n1\nB\n\n\n2021\n4\nB\n\n\n2019\n2\nC\n\n\n2020\n3\nC\n\n\n2021\n1\nC\n\n\n2019\n1\nD\n\n\n2020\n4\nD\n\n\n2021\n3\nD\n\n\n\n\n\n\n\nVale tirar um tempo para comparar, com calma, as entradas na tabela acima e o resultado no gráfico abaixo.\n\nggplot(df, aes(year, position, color = player)) +\n  geom_bump()"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#venda-de-imóveis",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#venda-de-imóveis",
    "title": "Bump Plots",
    "section": "",
    "text": "Podemos analisar as cidades com maior número de vendas ao longo dos anos usando a já conhecida txhousing. Quem acompanha meus tutoriais de ggplot2 já deve estar cansado de ver esta base sendo utilizada.\nQueremos montar um gráfico que mostra o ranking de vendas de imóveis ao longo dos anos. Removo o ano de 2015, pois este ano não está completo na amostra. Como há mais de quarenta cidades na amostra eu crio uma subamostra que contém apenas as cidades com maior número de vendas em 2014. O código abaixo mostra o passo-a-passo da manipulação de dados.\n\n#&gt; Encontra as top-15 cidades com maior número de vendas em 2014\ntop_cities &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas o ano de 2014\n  filter(year == 2014) |&gt; \n  #&gt; Calcula o total de vendas em cada cidade\n  summarise(total = sum(listings, na.rm = TRUE), .by = \"city\") |&gt; \n  #&gt; Seleciona o top-15\n  slice_max(total, n = 15) |&gt; \n  pull(city)\n\n#&gt; Calcula o total de vendas anuais na subamostra de cidades e faz o ranking anual\n\nrank_housing &lt;- txhousing |&gt; \n  #&gt; Seleciona apenas cidades dentro da subamostra\n  filter(city %in% top_cities, year &gt; 2005, year &lt; 2015) |&gt; \n  #&gt; Calcula o total de vendas a cada ano\n  summarise(\n    listing_year = sum(listings, na.rm = TRUE),\n    .by = c(\"city\", \"year\")\n    ) |&gt; \n  #&gt; Faz o ranking das cidades dentro de cada ano\n  mutate(rank = rank(-listing_year, \"first\"), .by = \"year\")\n\nNo gráfico abaixo, cada cidade tem uma cor diferente, mas omito a legenda de cores. Note o uso de scale_y_reverse já que, tipicamente, queremos mostrar os menores valores na parte superior do gráfico.\n\nggplot(rank_housing, aes(year, rank, group = city, color = city)) +\n  geom_bump() +\n  scale_y_reverse() +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nPara melhorar a legibilidade do gráfico podemos colocar o nome das cidades ao lado das linhas usando geom_text.\n\nggplot() +\n  geom_bump(\n    data = rank_housing,\n    aes(year, rank, group = city, color = city)\n    ) +\n  geom_text(\n    data = filter(rank_housing, year == max(year)),\n    aes(year, rank, label = city),\n    nudge_x = 0.1,\n    hjust = 0\n  ) +\n  scale_y_reverse() +\n  scale_x_continuous(limits = c(NA, 2017)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nMesmo com o nome das cidades, há muitas linhas para acompanhar no gráfico. Imagine, por exemplo, que queremos destacar apenas algumas das cidades selecionadas. Não parece fazer muito sentido destacar as cidades do top 4 (Houston, Dallas, San Antonio e Austin), já que elas praticamente não se alternam no ranking durante todo o período. Vamos, então, destacar as cidades de Bay Area, El Paso, Corpus Christi e Tyler.\n\nsel_cities &lt;- c(\"Bay Area\", \"El Paso\", \"Corpus Christi\", \"Tyler\")\n\nrank_housing &lt;- rank_housing |&gt; \n  mutate(\n    highlight = if_else(city %in% sel_cities, city, \"\"),\n    is_highlight = factor(if_else(city %in% sel_cities, 1L, 0L))\n  )\n\nO gráfico abaixo exige um código considervalmente mais longo, mas melhora o gráfico original em vários aspectos. Agora temos um maior destaque para as cidades de interesse. Os eixos estão melhor delimitados e as linhas de fundo redundantes foram removidas.\n\n\nCode\nggplot() +\n  #&gt; Linhas em cinza (sem destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 0),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 0.8,\n    smooth = 8\n    ) +\n  #&gt; Linhas coloridas (com destaque)\n  geom_bump(\n    data = filter(rank_housing, is_highlight == 1),\n    aes(year, rank, group = city, color = highlight),\n    linewidth = 2,\n    smooth = 8\n  ) +\n  #&gt; Pontos \n  geom_point(\n    data = rank_housing,\n    aes(year, rank, color = highlight),\n    size = 4\n  ) +\n  #&gt; Nomes sem destaque\n  geom_text(\n    data = filter(rank_housing, year == max(year), !(city %in% sel_cities)),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    color = \"gray20\"\n  ) +\n  #&gt; Nome com destaque (em negrito)\n  geom_text(\n    data = filter(rank_housing, year == max(year), city %in% sel_cities),\n    aes(year, rank, label = city),\n    nudge_x = 0.2,\n    hjust = 0,\n    fontface = \"bold\"\n  ) +\n  #&gt; Adiciona os eixos para melhorar leitura do gráfico\n  scale_y_reverse(breaks = 1:15) +\n  scale_x_continuous(limits = c(NA, 2017), breaks = 2006:2014) +\n  #&gt; Cores\n  scale_color_manual(\n    values = c(\"gray70\", \"#2f4858\", \"#86bbd8\", \"#f6ae2d\", \"#f26419\")\n  ) +\n  #&gt; Elementos temáticos\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#pacotes",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#pacotes",
    "title": "Bump Plots",
    "section": "",
    "text": "library(ggplot2)\nlibrary(ggbump)\nlibrary(dplyr)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/general-posts/2023-12-bump-plots/index.html#dados",
    "href": "posts/general-posts/2023-12-bump-plots/index.html#dados",
    "title": "Bump Plots",
    "section": "Dados",
    "text": "Dados\nPrimeiro defino alguns objetos úteis como o nome dos países que serão destacados e o nome das colunas que contém as variáveis de PIB. Além disso, crio uma tabela menor que contém apenas as colunas necessárias para a visualização.\n\ncountries_sel &lt;- c(\"Norway\", \"Belgium\", \"Austria\", \"United States\", \"Germany\")\n\nmeasures &lt;- c(\"gdp_over_pop\", \"gdp_ppp_over_pop\", \"gdp_ppp_over_k_hours_worked\")\n\nsub &lt;- dat |&gt; \n  select(country, year, all_of(measures)) |&gt; \n  na.omit()\n\nA transformação essencial é converter os dados em formato tidy e ranquear as observações dentro de cada métrica de PIB.\n\nranking &lt;- sub |&gt; \n  filter(year == max(year)) |&gt; \n  pivot_longer(cols = -c(country, year), names_to = \"measure\") |&gt; \n  mutate(rank = rank(-value), .by = \"measure\")\n\nAgora, mais por conveniência, eu crio algumas variáveis auxiliares que serão úteis para mapear os diferentes elementos estéticos.\n\nranking &lt;- ranking |&gt; \n  mutate(\n    highlight = if_else(country %in% countries_sel, country, \"\"),\n    highlight = factor(highlight, levels = c(countries_sel, \"\")),\n    is_highlight = factor(if_else(country %in% countries_sel, 1L, 0L)),\n    rank_labels = if_else(rank %in% c(1, 5, 10, 15, 20), rank, NA),\n    rank_labels = stringr::str_replace(rank_labels, \"^1$\", \"1st\"),\n    measure = factor(measure, levels = measures)\n    )\n\nPor fim, eu defino as cores das linhas e crio uma tabela auxiliar que contém apenas o texto que vai em cima do gráfico.\n\ncores &lt;- c(\"#101010\", \"#f7443e\", \"#8db0cc\", \"#fa9494\", \"#225d9f\", \"#c7c7c7\")\n\ndf_gdp &lt;- tibble(\n  measure = measures,\n  measure_label = c(\n    \"GDP per person at market rates\",\n    \"Adjusted for cost differences*\",\n    \"Adjusted for costs and hours worked\"\n  ),\n  position = -1.2\n)\n\ndf_gdp &lt;- df_gdp |&gt; \n  mutate(\n    measure = factor(measure, levels = measures),\n    measure_label = stringr::str_wrap(measure_label, width = 12),\n    measure_label = paste0(\"  \", measure_label)\n    )\n\nA versão simplificada do gráfico está resumida no código abaixo. Vale notar o uso da coord_cartesian para “cortar o gráfico” sem perder informação. Não é muito usual utilizar linewidth como um elemento estético dentro de aes mas pode-se ver como isto é bastante simples e como isto economiza algumas linhas de código, quando comparado com o gráfico anterior.\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]]),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2))\n\n\n\n\n\n\n\n\nNote que o gráfico acima inclui alguns países que não aparecem na visualização original como Irlanda e Luxemburgo. Existe um certo debate sobre a inflação do PIB per capita da Irlanda (devido ao grande número de empresas estrangeiras que mantêm suas sedes no país, em função dos baixos impostos da Irlanda). Um argumento similar pode ser feito sobre Luxemburgo. Ainda assim, decidi manter os dois países no gráfico final.\nO código final, como de costume, é bastante extenso. De maneira geral, o resultado é bastante satisfatório.\n\n\nCode\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  #&gt; Nome dos páises sem destaque\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight != 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  #&gt; Nome dos páises com destaque (em negrito)\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight == 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\"\n  ) +\n  #&gt; \"Eixo\" na esquerda (1st, 5, 10, 15, 20)\n  geom_text(\n    data = filter(ranking, measure == measures[[1]]),\n    aes(x = measure, y = rank, label = rank_labels),\n    nudge_x = -0.15,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  #&gt; Texto descritivo acima do gráfico\n  geom_text(\n    data = df_gdp,\n    aes(x = measure, y = position, label = measure_label),\n    inherit.aes = FALSE,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\"\n  ) +\n  #&gt; Posiciona as flechas apontando para baixo\n  annotate(\"text\", x = 1, y = -2.2, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 2, y = -2.2, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 3, y = -2.2, label = expression(\"\\u2193\")) +\n  #&gt; Corta o gráfico\n  coord_cartesian(ylim = c(21, -2)) +\n  #&gt; Cores\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  #&gt; Espessura das linhas\n  scale_linewidth_manual(values = c(0.5, 1.2)) +\n  #&gt; Elementos temáticos\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nO objetivo destes posts é de sempre fazer o máximo possível usando ggplot2 mas, na prática, as caixas de texto acima do gráfico podem ser feitas num software externo. Não é muito fácil usar caracteres especiais (neste caso, flechas) e a própria fonte (Lato) não inclui flechas em unicode. Pode-se melhorar a ordem da sobreposição das linhas usando geom_bump duas vezes como fizemos no gráfico dos imóveis, mas isto exigiria várias linhas adicionais de código."
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-1",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-1",
    "title": "Replicando gráficos",
    "section": "Gráfico 1",
    "text": "Gráfico 1\nO gráfico mostra o ciclo de recessões e expansões nos países da América Latina, desde 1900. Os dados são do projeto Maddison e a fonte para a matéria está aqui. Num post anterior mostrei como reproduzir este gráfico e os outros da matéria.\n\n\n\n\n\n\nDados\nExiste um pacote com os dados do projeto Maddison, chamado maddisson, mas os dados estão um pouco defasados. Para o propósito de replicar o gráfico acima isto não é um problema, já que a série se encerra em 2016. Para atualizar o gráfico, contudo, é preciso baixar os dados manualmente do site. Felizmente, isto é bastante simples no R. O código abaixo mostra as duas opções.\n\n# Usando o pacote\nlibrary(maddison)\n\nmad &lt;- maddison::maddison\n\n# Para trabalhar com dados mais atualizados\n\nlibrary(readxl)\n# url com a tabela em Excel\nurl &lt;- \"https://www.rug.nl/ggdc/historicaldevelopment/maddison/data/mpd2020.xlsx\"\n#&gt; Cria um path temporario e tenta baixar os dados\ntemp_path &lt;- tempfile(\"maddison.xlsx\")\n\ntry(download.file(url = url, destfile = temp_path, mode = \"wb\", quiet = TRUE))\n\nif (file.exists(temp_path)) {\n  mad &lt;- readxl::read_excel(temp_path, sheet = 3)\n}\n\nO código abaixo faz a manipulação de dados necessária. Seleciona-se os países manualmente e cria-se uma variável binária que indica se o país encontra-se em expansão ou recessão em cada ano.\n\nlatam_countries &lt;- c(\n  \"ARG\", \"BOL\", \"BRA\", \"CHL\", \"COL\", \"CUB\", \"DOM\", \"ECU\", \"GTM\", \"HND\", \"HTI\",\n  \"MEX\", \"PER\", \"VEN\")\n\nlatam &lt;- mad |&gt; \n  # Filtra apenas as linhas dos países selecionados\n  filter(\n    countrycode %in% latam_countries,\n    year &gt;= 1900\n    ) |&gt; \n  # Agrupa por país\n  group_by(country) |&gt; \n  mutate(\n    # Calcula a variação do PIB per capita de cada país\n    d_gdppc = gdppc / lag(gdppc) - 1,\n    # Cria uma variável binária para indicar se houve crescimento \n    growth = factor(if_else(d_gdppc &gt; 0, 1L, 0L)),\n    # Remove parêntesis e o texto dentro dele\n    country = str_remove(country, \" \\\\(.+\\\\)\"),\n    # Abrevia o nome da República Dominicana\n    country = str_replace(country, \"Dominican Republic\", \"Dominican Rep.\"),\n    country = factor(country),\n    country = fct_rev(country)\n  ) |&gt; \n  ungroup()\n\n\n\nReplicando o gráfico\n\n\nBásico\nA visualização é, essencialmente, um gráfico de calor de clusters, isto é, são vários quadrados coloridos adjacentes. Para desenhar este gráfico utilizo o geom_tile. Note que também duplico o eixo-x e coloco linhas verticais tracejadas no gráfico. Para conhecer mais sobre este tipo de gráfico veja meu tutorial sobre mapas de calor.\n\nggplot(latam, aes(year, country)) +\n    geom_tile(aes(fill = growth), height = .85, width = 1) +\n    geom_vline(\n      xintercept = c(1900, 1925, 1950, 1975, 2000, 2016),      \n      linetype = 2,                                           \n      colour = \"gray65\"\n      ) +                                      \n    scale_x_continuous(\n      breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n      expand = c(0,0),\n      sec.axis = dup_axis()\n      )\n\n\n\n\n\n\n\n\n\n\nCompleto\nO gráfico completo faz algumas modificações na legenda de cores e diversas alterações temáticas no gráfico. O portal Nexo utiliza a fonte Gotham Rounded em algumas variantes. Para usar esta fonte é preciso tê-la instalada. Para evitar problemas, o código abaixo verifica se as fontes necessárias estão instaladas; caso contrário, usa-se a fonte Montserrat, do Google Fonts.\n\n# Verifica a fonte do texto\n\ndbfonts &lt;- font_files()\nnexo_fonts &lt;- c(\"Gotham Rounded Bold\", \"Gotham Rounded Light\")\ncond &lt;- str_glue(\"({nexo_fonts[1]})|({nexo_fonts[2]})\")\ncheck_fonts &lt;- all(str_detect(dbfonts$family, cond))\n\nfont &lt;- ifelse(check_fonts, \"Gotham Rounded Bold\", \"Montserrat\")\nfont_axis &lt;- ifelse(check_fonts, \"Gotham Rounded Light\", \"Montserrat\")\nfont_title &lt;- \"Crimson Text\"\n\nif (check_fonts) {\n  # Adiciona as fonts Gotham Rounded Bold e Light\n  font_add(\"Gotham Rounded Bold\", \"Gotham Rounded Bold.otf\")\n  font_add(\"Gotham Rounded Light\", \"Gotham Rounded Light.otf\")\n  } else {\n  # Adiciona Montserrat caso as fontes Gotham nao estejam disponiveis\n  font_add_google(\"Montserrat\", \"Montserrat\")\n  }\n\nfont_add_google(\"Crimson Pro\", \"Crimson Text\")\n\nshowtext_auto()\n\n\n\nCode\nggplot(latam, aes(year, country)) +\n  geom_tile(aes(fill = growth), height = .85, width = 1) +\n  geom_vline(\n    xintercept = c(1900, 1925, 1950, 1975, 2000, 2016),      \n    linetype = 2,                                           \n    colour = \"gray65\"\n    ) +                                      \n  scale_x_continuous(\n    breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n    expand = c(0,0),\n    sec.axis = dup_axis()\n    ) +                         \n  scale_fill_discrete(\n    breaks = c(0,1),\n    name = \"VARIAÇÃO DO PIB NO MUNDO\\nPor ano\",\n    labels = c(\"PIB EM QUEDA\", \"PIB EM CRESCIMENTO\"), \n    na.value = \"gray90\"\n    ) +\n  labs(title = \"América Latina\", x = NULL, y = NULL) +\n  theme(\n    # Fundo do gráfico\n    panel.grid = element_blank(),\n    # Define as margens do gráfico\n    plot.margin = unit(c(1, 1, .5, 1), \"cm\"),\n    # Eixos\n    axis.text.y = element_text(vjust = .4),\n    axis.ticks = element_line(size = .4),\n    axis.text = element_text(family  = font_axis, size = 8),\n    # Legenda\n    legend.position = \"top\",\n    legend.text = element_text(\n      size = 8,\n      colour = \"gray20\",\n      family = font\n      ),\n    legend.title = element_text(\n      size = 12,\n      colour = \"gray20\",\n      family = font\n      ),\n    # Título\n    plot.title = element_text(family = font_title, size = 18)\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#dados-4",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#dados-4",
    "title": "Replicando gráficos",
    "section": "Dados",
    "text": "Dados\nOs dados originais estão disponíveis on GitHub da The Economist mas eu não consegui encontrar o código que gera o gráfico acima. Como resultado, vou tentar adivinhar quais colunas, de fato, são utilizadas no gráfico. Além disso, como a fonte da The Economist é proprietária vou utilizar a Lato, da Google Fonts.\n\n\nCode\ndat &lt;- readr::read_csv(here::here(\"static/data/gdp_over_hours_worked_with_estimated_hours_worked.csv\"))\n\ncountries_sel &lt;- c(\"Norway\", \"Belgium\", \"Austria\", \"United States\", \"Germany\")\n\nmeasures &lt;- c(\"gdp_over_pop\", \"gdp_ppp_over_pop\", \"gdp_ppp_over_k_hours_worked\")\n\nsub &lt;- dat |&gt; \n  select(country, year, all_of(measures)) |&gt; \n  na.omit()\n\nranking &lt;- sub |&gt; \n  filter(year == max(year)) |&gt; \n  pivot_longer(cols = -c(country, year), names_to = \"measure\") |&gt; \n  mutate(rank = rank(-value), .by = \"measure\")\n\nranking &lt;- ranking |&gt; \n  mutate(\n    highlight = if_else(country %in% countries_sel, country, \"\"),\n    highlight = factor(highlight, levels = c(countries_sel, \"\")),\n    is_highlight = factor(if_else(country %in% countries_sel, 1L, 0L)),\n    rank_labels = if_else(rank %in% c(1, 5, 10, 15, 20), rank, NA),\n    rank_labels = stringr::str_replace(rank_labels, \"^1$\", \"1st\"),\n    measure = factor(measure, levels = measures)\n    )\n\ncores &lt;- c(\"#101010\", \"#f7443e\", \"#8db0cc\", \"#fa9494\", \"#225d9f\", \"#c7c7c7\")\n\ndf_gdp &lt;- tibble(\n  measure = measures,\n  measure_label = c(\n    \"GDP per person at market rates\",\n    \"Adjusted for cost differences*\",\n    \"Adjusted for costs and hours worked\"\n  ),\n  position = -1.5\n)\n\ndf_gdp &lt;- df_gdp |&gt; \n  mutate(\n    measure = factor(measure, levels = measures),\n    measure_label = stringr::str_wrap(measure_label, width = 12),\n    measure_label = paste0(\"  \", measure_label)\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-4",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-4",
    "title": "Replicando gráficos",
    "section": "Replicando o gráfico",
    "text": "Replicando o gráfico"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#básico-4",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#básico-4",
    "title": "Replicando gráficos",
    "section": "Básico",
    "text": "Básico\nA versão simplificada do gráfico está resumida no código abaixo. Vale notar o uso da coord_cartesian para “cortar o gráfico” sem perder informação. Não é muito usual utilizar linewidth como um elemento estético dentro de aes mas pode-se ver como isto é bastante simples e como isto economiza algumas linhas de código, quando comparado com o gráfico anterior.\n\nlibrary(ggbump)\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]]),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2))"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#completo-4",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#completo-4",
    "title": "Replicando gráficos",
    "section": "Completo",
    "text": "Completo\nO objetivo deste post é de sempre fazer o máximo possível usando ggplot2 mas, na prática, as caixas de texto acima do gráfico podem ser feitas num software externo. Não é muito fácil usar caracteres especiais (neste caso, flechas) e a própria fonte (Lato) não inclui flechas em unicode. Pode-se melhorar a ordem da sobreposição das linhas usando geom_bump duas vezes como fizemos no gráfico dos imóveis, mas isto exigiria várias linhas adicionais de código.\n\n\nCode\nfont_add_google(\"Lato\", \"Lato\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight != 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight == 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\",\n    size = 3\n  ) +\n  geom_text(\n    data = filter(ranking, measure == measures[[1]]),\n    aes(x = measure, y = rank, label = rank_labels),\n    nudge_x = -0.15,\n    hjust = 0,\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = df_gdp,\n    aes(x = measure, y = position, label = measure_label),\n    inherit.aes = FALSE,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\",\n    size = 3\n  ) +\n  annotate(\"text\", x = 1, y = -2.5, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 2, y = -2.5, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 3, y = -2.5, label = expression(\"\\u2193\")) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2)) +\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/test/walkthrough.html",
    "href": "posts/general-posts/test/walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "posts/general-posts/test/walkthrough.html#markdown",
    "href": "posts/general-posts/test/walkthrough.html#markdown",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "posts/general-posts/test/walkthrough.html#code-cell",
    "href": "posts/general-posts/test/walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n8"
  },
  {
    "objectID": "posts/general-posts/test/walkthrough.html#equation",
    "href": "posts/general-posts/test/walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-rename/index.html",
    "href": "posts/general-posts/2024-01-tidyverse-rename/index.html",
    "title": "O novo tidyverse: rename",
    "section": "",
    "text": "O tidyverse é uma coleção poderosa de pacotes, voltados para a manipulação e limpeza de dados. Num outro post, discuti alguns aspectos gerais da filosofia destes pacotes que incluem a sua consistência sintática e o uso de pipes. O tidyverse está em constante expansão, novas funcionalidades são criadas para melhorar a performance e capabilidade de suas funções. Assim, é importante atualizar nosso conhecimento destes pacotes periodicamente. Nesta série de posts vou focar nas funções principais dos pacotes dplyr e tidyr, voltados para a limpeza de dados."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-rename/index.html#o-básico",
    "href": "posts/general-posts/2024-01-tidyverse-rename/index.html#o-básico",
    "title": "O novo tidyverse: rename",
    "section": "O básico",
    "text": "O básico\nOs pacotes utilizados neste tutorial são listados abaixo.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(janitor)\nlibrary(sidrar)\n\nAntes de mais nada, vamos importar uma base de dados qualquer do IBGE usando o pacote sidrar. A base importada mostra o custo médio m² de construção no Brasil da pesquisa SINAPI.\n\ndat &lt;- sidrar::get_sidra(2296, period = \"202201-202301\")\n\n\n\n\n\n\n\n  \n    \n      Nível Territorial (Código)\n      Nível Territorial\n      Unidade de Medida (Código)\n      Unidade de Medida\n      Valor\n    \n  \n  \n    1\nBrasil\n38\nReais\n1525.48\n    1\nBrasil\n38\nReais\n915.79\n    1\nBrasil\n38\nReais\n609.69\n    1\nBrasil\n30\nNúmero-índice\n763.46\n    1\nBrasil\n30\nNúmero-índice\n693.71\n    1\nBrasil\n30\nNúmero-índice\n962.09\n  \n  \n  \n\n\n\n\nA função rename serve para trocar o nome de uma coluna seguindo a sintaxe:\n\nrename(nome_novo = nome_velho)\n\nA forma mais imediata de utilizar a função é simplesmente:\n\ndat_renamed &lt;- rename(dat, valor = Valor)\n\n\n\n\n\n\n\n  \n    \n      Nível Territorial (Código)\n      Nível Territorial\n      Unidade de Medida (Código)\n      Unidade de Medida\n      valor\n    \n  \n  \n    1\nBrasil\n38\nReais\n1525.48\n    1\nBrasil\n38\nReais\n915.79\n    1\nBrasil\n38\nReais\n609.69\n    1\nBrasil\n30\nNúmero-índice\n763.46\n    1\nBrasil\n30\nNúmero-índice\n693.71\n    1\nBrasil\n30\nNúmero-índice\n962.09\n  \n  \n  \n\n\n\n\nPara renomear múliplas colunas, segue-se uma lógica similar, separando os argumentos adicionais por vírgulas. Note que somos forçados a utilizar o sinal `` pois o nome de algumas colunas contêm acentos.\n\ndat_renamed &lt;- rename(\n  dat,\n  valor = Valor,\n  nivel_territorial = `Nível Territorial`,\n  code_unit = `Unidade de Medida (Código)`\n  )\n\n\n\n\n\n\n\n  \n    \n      Nível Territorial (Código)\n      nivel_territorial\n      code_unit\n      Unidade de Medida\n      valor\n    \n  \n  \n    1\nBrasil\n38\nReais\n1525.48\n    1\nBrasil\n38\nReais\n915.79\n    1\nBrasil\n38\nReais\n609.69\n    1\nBrasil\n30\nNúmero-índice\n763.46\n    1\nBrasil\n30\nNúmero-índice\n693.71\n    1\nBrasil\n30\nNúmero-índice\n962.09\n  \n  \n  \n\n\n\n\nAlternativamente, pode-se escrever os nomes usando aspas como no código abaixo.\n\ndat_renamed &lt;- rename(\n  dat,\n  valor = \"Valor\",\n  nivel_territorial = \"Nível Territorial\",\n  code_unit = \"Unidade de Medida (Código)\"\n  )\n\nLiteralmente reescrever o nome das colunas pode ser bastante enfadonho. Há duas maneiras mais interessantes de renomear colunas: (1) utilizando uma função; e (2) utilizando um vetor de “swap”. A primeira abordagem, em geral, é mais simples e pode-se aproveitar funções úteis pré-existentes para manipulação de strings como as do pacote stringr. A segunda abordagem exige mais esforço manual e é recomendada quando não existem regras simples para renomear as colunas.\n\nBoas práticas\nOs nomes das colunas de um data.frame devem:\n\nSer únicos (não-duplicados) e evitar caracteres maiúsculos.\nNão devem incluir caracteres especiais (e.g. !*&@%), nem começar com um número ou caractere especial. Também é recomendável evitar o uso de acentos (é, ç, à, etc.).\nEvitar espaços em branco, que devem ser substituídos por _ ou omitidos (e.g. PIB Agro deve ser reescrito como pibAgro ou pib_agro.\nEvitar nomes “reservados” como for, in, TRUE, if, etc. Estas palavras tem tratamento especial dentro do R e não se deve utilizá-las como nome de coluna1.\n\nExiste uma lógica bastante simples para seguir estas convenções: nomes sintaticamente válidos facilitam (e, em alguns casos, possibilitam) a seleção de colunas. É muito mais simples escrever dat$pib_agro do que dat$`PIB Agrícola (R$)`. Abaixo listo alguns exemplos de nomes e sugestões de como melhorá-los.\n# Ruim\nnomes &lt;- c(\"PIB Agrícola\", \"INFLAÇÃO %\", \"Dia do mês\", \"!Nome D@ Coluna#\", \"If Buyer\")\n\n# Correto\nn1 &lt;- c(\"pib_agricola\", \"pib_agro\")\nn2 &lt;- c(\"inflacao_percent\", \"inflacao_pct\", \"inflacaoPct\")\nn3 &lt;- c(\"dia_do_mes\", \"DiaDoMes\")\nn4 &lt;- c(\"nome_da_coluna\", \"NomeDaColuna\")\nn5 &lt;- c(\"is_buyer\", \"IsBuyer\")"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-rename/index.html#usando-funções",
    "href": "posts/general-posts/2024-01-tidyverse-rename/index.html#usando-funções",
    "title": "O novo tidyverse: rename",
    "section": "Usando funções",
    "text": "Usando funções\nA maneira mais simples de entender o uso de funções para renomear colunas é através de exemplos. Para tornar os exemplos mais sucintos vamos focar apenas no nome das colunas através da função names. Esta abordagem torna mais claro o fato de que os nomes das colunas de um data.frame são, essencialmente, um vetor de texto.\n\nnames(dat)\n\n [1] \"Nível Territorial (Código)\" \"Nível Territorial\"         \n [3] \"Unidade de Medida (Código)\" \"Unidade de Medida\"         \n [5] \"Valor\"                      \"Brasil (Código)\"           \n [7] \"Brasil\"                     \"Mês (Código)\"              \n [9] \"Mês\"                        \"Variável (Código)\"         \n[11] \"Variável\"                  \n\n\nPara aplicar uma função sobre o nome de todas as colunas da base de dados dat usa-se rename_with, uma variação da função rename. A sintaxe da função é simples: rename_with(dados, fn). No caso abaixo converte-se o nome de todas as colunas para maiúculo.\n\ndat_renamed &lt;- rename_with(dat, toupper)\nnames(dat_renamed)\n\n [1] \"NÍVEL TERRITORIAL (CÓDIGO)\" \"NÍVEL TERRITORIAL\"         \n [3] \"UNIDADE DE MEDIDA (CÓDIGO)\" \"UNIDADE DE MEDIDA\"         \n [5] \"VALOR\"                      \"BRASIL (CÓDIGO)\"           \n [7] \"BRASIL\"                     \"MÊS (CÓDIGO)\"              \n [9] \"MÊS\"                        \"VARIÁVEL (CÓDIGO)\"         \n[11] \"VARIÁVEL\"                  \n\n\nAbaixo mostro algumas funções úteis para renomear colunas.\n\n#&gt; Converte para minúsculo\nrename_with(dat, tolower) |&gt; names()\n\n [1] \"nível territorial (código)\" \"nível territorial\"         \n [3] \"unidade de medida (código)\" \"unidade de medida\"         \n [5] \"valor\"                      \"brasil (código)\"           \n [7] \"brasil\"                     \"mês (código)\"              \n [9] \"mês\"                        \"variável (código)\"         \n[11] \"variável\"                  \n\n#&gt; Converte texto para um formato de 'título'\nrename_with(dat, stringr::str_to_title) |&gt; names()\n\n [1] \"Nível Territorial (Código)\" \"Nível Territorial\"         \n [3] \"Unidade De Medida (Código)\" \"Unidade De Medida\"         \n [5] \"Valor\"                      \"Brasil (Código)\"           \n [7] \"Brasil\"                     \"Mês (Código)\"              \n [9] \"Mês\"                        \"Variável (Código)\"         \n[11] \"Variável\"                  \n\n#&gt; Remove acentos\nrename_with(dat, ~stringi::stri_trans_general(.x, id =\"latin-ascii\")) |&gt; names()\n\n [1] \"Nivel Territorial (Codigo)\" \"Nivel Territorial\"         \n [3] \"Unidade de Medida (Codigo)\" \"Unidade de Medida\"         \n [5] \"Valor\"                      \"Brasil (Codigo)\"           \n [7] \"Brasil\"                     \"Mes (Codigo)\"              \n [9] \"Mes\"                        \"Variavel (Codigo)\"         \n[11] \"Variavel\"                  \n\n\nPode-se criar uma função customizada mais completa para renomear colunas. A função abaixo costuma funcionar bem para colunas problemáticas escritas em português.\n\nnice_col_names &lt;- function(x) {\n  \n  #&gt; Remove acentos em geral\n  x &lt;- stringi::stri_trans_general(x, id = \"latin-ascii\")\n  #&gt; Remove pontuação e caracteres especiais (%$#& etc.)\n  x &lt;- stringr::str_replace_all(x, \"[[:punct:]]\", \"\")\n  #&gt; Remove espaços em branco antes e/ou depois do texto\n  x &lt;- stringr::str_trim(x, side = \"both\")\n  #&gt; Substitui espaços em branco por _\n  x &lt;- stringr::str_replace_all(x, \" \", \"_\")\n  #&gt; Converte tudo para minúsculo\n  x &lt;- stringr::str_to_lower(x)\n  \n  #&gt; Avisa se houver nomes duplicados\n  check_dups &lt;- x[duplicated(x)]\n  \n  if (length(check_dups) &gt; 0) {\n    warning(\"Duplicated names!\")\n  }\n  \n  return(x)\n  \n}\n\n\ndat |&gt; \n  rename_with(nice_col_names) |&gt; \n  names()\n\n [1] \"nivel_territorial_codigo\" \"nivel_territorial\"       \n [3] \"unidade_de_medida_codigo\" \"unidade_de_medida\"       \n [5] \"valor\"                    \"brasil_codigo\"           \n [7] \"brasil\"                   \"mes_codigo\"              \n [9] \"mes\"                      \"variavel_codigo\"         \n[11] \"variavel\"                \n\n\nAs funções base make.names e make.unique também podem ser úteis para ajudar a criar nomes de colunas válidos sem causar grandes alterações.\n\ndat |&gt; \n  rename_with(make.names) |&gt; \n  names()\n\n [1] \"Nível.Territorial..Código.\" \"Nível.Territorial\"         \n [3] \"Unidade.de.Medida..Código.\" \"Unidade.de.Medida\"         \n [5] \"Valor\"                      \"Brasil..Código.\"           \n [7] \"Brasil\"                     \"Mês..Código.\"              \n [9] \"Mês\"                        \"Variável..Código.\"         \n[11] \"Variável\"                  \n\n\nPor fim, vale notar que a função jantior::make_clean_names() é bastante eficiente em “limpar” o nome de colunas\n\ndat |&gt; \n  rename_with(janitor::make_clean_names) |&gt; \n  names()\n\n [1] \"nivel_territorial_codigo\" \"nivel_territorial\"       \n [3] \"unidade_de_medida_codigo\" \"unidade_de_medida\"       \n [5] \"valor\"                    \"brasil_codigo\"           \n [7] \"brasil\"                   \"mes_codigo\"              \n [9] \"mes\"                      \"variavel_codigo\"         \n[11] \"variavel\"                \n\n\n\nRenomeando apenas algumas colunas\nEventualmente, pode ser interessante renomear apenas algumas colunas. A função rename_with tem um terceiro argumento opcional que perimite selecionar um subconjunto de colunas que deve ser renomeada utilizando a função definida. A maneira mais simples de selecionar estas colunas é pela sua posição.\n\ndat_renamed &lt;- rename_with(dat, janitor::make_clean_names)\n\ndat_prefix &lt;- rename_with(\n  dat_renamed,\n  ~paste0(\"abs_\", .x),\n  c(1, 2, 3)\n)\n\nnames(dat_prefix)\n\n [1] \"abs_nivel_territorial_codigo\" \"abs_nivel_territorial\"       \n [3] \"abs_unidade_de_medida_codigo\" \"unidade_de_medida\"           \n [5] \"valor\"                        \"brasil_codigo\"               \n [7] \"brasil\"                       \"mes_codigo\"                  \n [9] \"mes\"                          \"variavel_codigo\"             \n[11] \"variavel\"                    \n\n\nComo de praxe, pode-se usar as funções tidyselect para facilitar a vida:\n\n#&gt; Adiciona um prefixo nas colunas numéricas\nrename_with(\n  dat_renamed,\n  ~paste0(\"num_\", .x),\n  where(is.numeric)\n)\n\n#&gt; Adiciona um sufixo nas colunas numéricas\nrename_with(\n  dat_renamed,\n  ~paste0(.x, \"_num\"),\n  where(is.numeric)\n)\n\n#&gt; Converte para maiúsculo as colunas que contem 'codigo' no nome\nrename_with(\n  dat_renamed,\n  stringr::str_to_upper,\n  contains(\"codigo\")\n)"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-rename/index.html#usando-vetores",
    "href": "posts/general-posts/2024-01-tidyverse-rename/index.html#usando-vetores",
    "title": "O novo tidyverse: rename",
    "section": "Usando vetores",
    "text": "Usando vetores\nUm dos elementos básicos do R são os vetores. Um vetor, em linhas gerais, é uma coleção de elementos básicos do mesmo tipo. Uma sequência de números é um exemplo de vetor numérico, como c(2, 4, 6, 8). Um vetor de texto, similarmente, tem a forma c(\"abacaxi\", \"abacate\", \"tomate\").\nUm tipo especial de vetor é um named vector, um vetor com nomes. Um named vector é um vetor onde cada elemento tem um nome, como c(\"a\" = 2, \"b\" = 4, \"c\" = 6, \"d\" = 8). Tipicamente, seleciona-se um elemento específico de um vetor através da sua posição, como em x[2] ou y[5:10]. No caso de um named vector, é possível especificar um elemento através do seu nome.\nNote que a sintaxe c(\"A\" = \"abacaxi\", \"B = \"banana\") é muito similar à sintaxe da função rename. De fato, pode-se usar um vetor deste tipo para renomear as colunas usando um “swap” vector. Este tipo de vetor funciona como c(\"nome_novo\" = \"Nome Velho\"). Antigamente, podia-se simplesmente inserir o vetor dentro da função rename.\n\nswap &lt;- c(\"valor\" = \"Valor\", \"geo_level\" = \"Nível Territorial\")\ndat_renamed &lt;- rename(dat, swap)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(swap)\n\n  # Now:\n  data %&gt;% select(all_of(swap))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nComo deixa claro o aviso, desde a versão 1.1.0 do tidyselect este tipo de sintaxe foi descontinuada e agora é preciso usar mais uma função.\n\nall_of e any_of\nAtualmente, é preciso colocar o vetor swap em uma de duas funções auxiliares: any_of e all_of. A primeira função é a mais branda e substitui o nome de todas as colunas que for possível. A segunda função é mais exigente: se qualquer umas das variáveis estiver ausente retorna-se um erro.\nPara tornar a distinção mais clara, vale criar um novo vetor com o nome de coluna que não consta na nossa base. Note como a função any_of retorna o mesmo output, ignorando a coluna inexistente Ano.\n\nswap &lt;- c(\"valor\" = \"Valor\", \"geo_level\" = \"Nível Territorial\", \"ano\" = \"Ano\")\n\ndat |&gt; \n  rename(any_of(swap)) |&gt; \n  names()\n\n [1] \"Nível Territorial (Código)\" \"geo_level\"                 \n [3] \"Unidade de Medida (Código)\" \"Unidade de Medida\"         \n [5] \"valor\"                      \"Brasil (Código)\"           \n [7] \"Brasil\"                     \"Mês (Código)\"              \n [9] \"Mês\"                        \"Variável (Código)\"         \n[11] \"Variável\"                  \n\n\n\ndat |&gt; \n  rename(all_of(swap)) |&gt; \n  names()\n\nError in `all_of()`:\n! Can't rename columns that don't exist.\n✖ Column `Ano` doesn't exist.\n\n\nNo caso em que se quer renomear exatamente todas as colunas, usa-se a função auxiliar all_of da seguinte maneira.\n\nnew_names &lt;- c(\n  \"geo_code\" = \"Nível Territorial (Código)\",\n  \"geo_level\" = \"Nível Territorial\",\n  \"unit_code\" = \"Unidade de Medida (Código)\",\n  \"unit\" = \"Unidade de Medida\",\n  \"value\" = \"Valor\",\n  \"geo_name_code\" = \"Brasil (Código)\",\n  \"geo_name\" = \"Brasil\",\n  \"month_code\" = \"Mês (Código)\",\n  \"month\" = \"Mês\",\n  \"variable_code\" = \"Variável (Código)\",\n  \"variable\" = \"Variável\"\n)\n\ndat |&gt; \n  rename(all_of(new_names)) |&gt; \n  names()\n\n [1] \"geo_code\"      \"geo_level\"     \"unit_code\"     \"unit\"         \n [5] \"value\"         \"geo_name_code\" \"geo_name\"      \"month_code\"   \n [9] \"month\"         \"variable_code\" \"variable\"     \n\n\nUma maneira mais sucinta de escrever o código acima é aproveitar os nomes pré-existentes da base.\n\nnew_names &lt;- c(\n  \"geo_code\", \"geo_level\", \"unit_code\", \"unit\", \"value\", \"geo_name_code\", \n  \"geo_name\", \"month_code\", \"month\", \"variable_code\", \"variable\"\n)\n\nswap_names &lt;- names(dat)\nnames(swap_names) &lt;- new_names\n\ndat |&gt; \n  rename(all_of(swap_names)) |&gt; \n  names()\n\n [1] \"geo_code\"      \"geo_level\"     \"unit_code\"     \"unit\"         \n [5] \"value\"         \"geo_name_code\" \"geo_name\"      \"month_code\"   \n [9] \"month\"         \"variable_code\" \"variable\"     \n\n\nNa maior parte das aplicações, pode-se usar a função any_of sem grandes preocupações. Apenas em casos quando maior controle sobre o output for necessário deve-se considerar usar a função all_of.\nPor fim, vale notar que um swap vector pode ter algumas “redundâncias”. Imagine, por exemplo, que temos várias bases de dados com pequenas inconsistências de ortografia. Em alguns casos temos “Nível Territorial (Código)”, mas em outras temos “nível territorial (Código)” e em outras “Nível Territorial (código)”, etc. etc. Pode-se construir um vetor que corrige isto sem grandes dificuldades, pois o nome do named vector não precisa ser único. Isto é muito conveniente quando se cria uma função genérica que limpa uma base de dados.\n\nnew_names &lt;- c(\n  \"geo_code\" = \"Nível Territorial (Código)\",\n  \"geo_code\" = \"Nível Territorial (código)\",\n  \"geo_code\" = \"nível territorial (Código)\"\n)\n\ndat |&gt; \n  rename(any_of(new_names)) |&gt; \n  names()\n\n [1] \"geo_code\"                   \"Nível Territorial\"         \n [3] \"Unidade de Medida (Código)\" \"Unidade de Medida\"         \n [5] \"Valor\"                      \"Brasil (Código)\"           \n [7] \"Brasil\"                     \"Mês (Código)\"              \n [9] \"Mês\"                        \"Variável (Código)\"         \n[11] \"Variável\""
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-rename/index.html#usando-funções-e-vetores",
    "href": "posts/general-posts/2024-01-tidyverse-rename/index.html#usando-funções-e-vetores",
    "title": "O novo tidyverse: rename",
    "section": "Usando funções e vetores",
    "text": "Usando funções e vetores\nTambém é possível fazer um mix de vetores e funções. Aqui, infelizmente, a lógica da ordem do vetor se inverte, o que pode causar grande confusão. No exemplo abaixo, monto uma função que “traduz” alguns termos do português para o inglês. A mágica é feita usando a função stringr::str_replace_all que troca um termo por outro seguindo a ordem nome_velho = nome_novo.\n\ntranslate_pt &lt;- function(x) {\n  \n  trans &lt;- c(\n    \"Código\" = \"code\",\n    \"Valor\" = \"value\",\n    \"Nível\" = \"level\",\n    \"Mês\" = \"month\"\n  )\n  \n  stringr::str_replace_all(x, trans)\n  \n}\n\ndat |&gt; \n  rename_with(translate_pt) |&gt; \n  names()\n\n [1] \"level Territorial (code)\" \"level Territorial\"       \n [3] \"Unidade de Medida (code)\" \"Unidade de Medida\"       \n [5] \"value\"                    \"Brasil (code)\"           \n [7] \"Brasil\"                   \"month (code)\"            \n [9] \"month\"                    \"Variável (code)\"         \n[11] \"Variável\""
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html",
    "title": "Uruguay in numbers",
    "section": "",
    "text": "Uruguay has been making the headlines in the past decade in a good way: many consider the country to be an model democracy: topping the Economist’s Democracy Index and being the least corrupt country in Latin America are some factors that bring attention to this small country. It also appears to be one the least unequal countries in the region with a GINI index close to 40 (Brazil’s is somewhere around 52-53 for comparison).\nAs a native Brazilian, who lived most of his life in Porto Alegre - a mere 800km travel away from Uruguay’s capital Montevideo -, and a relatively fluent reader of the Spanish language, I must confess that I know very little about Uruguay. In fact, the little I know about Uruguay’s history is the brief time period when Uruguay was a contested territory between the Kingdoms of Spain and Portugal1.\nSo this saturday afternoon I decided to learn all that I could about Uruguay and (sort of) document my steps."
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#where-to-find-the-data",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#where-to-find-the-data",
    "title": "Uruguay in numbers",
    "section": "Where to find the data?",
    "text": "Where to find the data?\nWhen looking for socioeconomic and demographic data on countries I usually start at usual suspects:\n\nWorld Bank\nIMF\nOur World in Data\nUN\n\nUnfortunately, Uruguay is not a member of the OECD so we can’t use that. For more precise/specific comparison, specially on GDP I also recommend both the Maddison Project and the Penn World Table. All of the aforementioned sources also have convenience packages built for R!\n\n\n\nName\nSite\nR-package\n\n\n\n\nIMF\nlink\nIMFData, imfr\n\n\nWorld Bank\nlink\nWDI\n\n\nUN\nlink\nwpp2022\n\n\nMaddison Project\nlink\nmaddison\n\n\nPenn World Table\nlink\npwt10\n\n\nOECD\nlink\noecd\n\n\nOur World in Data\nlink\nowidR\n\n\n\nThe list above is not intended to be comprehensive but it’s a good starting point (plus, they’re all free). For Latin American countries it might be useful to include Global Data Lab and CEPAL. Also, I always like to check in for the National Statistical Bureau and the Central Bank of the country. In Uruguay’s case that would be the Instituto Nacional de Estadística (INE) and the Banco Central de Uruguay. By the way, some useful glossay/translations for foreigners include:\n\n\n\nSpanish\nEnglish\n\n\n\n\nencuesta\nsurvey/interview\n\n\ningreso/salario\nincome/wage\n\n\ndesarollo\ndevelopment\n\n\ninversión\ninvestment\n\n\nempleo\nemployment\n\n\ndesempleo\nunemployment\n\n\npromedio\naverage\n\n\nalquiler/renta\nrent\n\n\nventa\nsale\n\n\n\nFinally, if you don’t mind working with slightly outdated data (not my case) you can use packages such as gapminder to get a broad view on the country."
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#economy-and-population",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#economy-and-population",
    "title": "Uruguay in numbers",
    "section": "Economy and population",
    "text": "Economy and population\nA good starting point is the Penn World Table included in the {pwt10} package. We can work this table to find some basic facts about Uruguay such as:\n\nReal GDP (rgdpe)\nTotal population (pop)\nAvg. hours worked per year (avh)\n\n\nlibrary(pwt10)\n\npwt = pwt10.0\n\ntab_decade &lt;- pwt %&gt;%\n  # Get only the last year of each decade\n  filter(isocode == \"URY\", year %in% seq(1959, 2019, 10)) %&gt;%\n  mutate(\n    # Real GDP per capita\n    rgdppc = rgdpe / pop,\n    # Convert population\n    pop = pop * 1e6,\n    # Convert year to character for easier formatting of the table\n    year = as.character(year)) %&gt;%\n  select(year, rgdpe, rgdppc, pop, avh, hc, csh_x)\n\n# Swap names for table\nold_names &lt;- names(tab_decade)\nnew_names &lt;- c(\n  \"Year\", \"Real GDP\", \"Real GDP per capita\", \"Population\", \"Avg. Hours Worked\",\n  \"Human Capital Index\", \"Share of Exports (%GDP)\")\nnames(new_names) &lt;- old_names\n\n# The table\ngt(tab_decade) %&gt;%\n  cols_label(.list = new_names) %&gt;%\n  fmt_number(columns = c(rgdpe:pop, avh), decimals = 0) %&gt;%\n  fmt_number(columns = hc, decimals = 2) %&gt;%\n  fmt_percent(columns = csh_x) %&gt;%\n  sub_missing(missing_text = \"-\") %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Year\n      Real GDP\n      Real GDP per capita\n      Population\n      Avg. Hours Worked\n      Human Capital Index\n      Share of Exports (%GDP)\n    \n  \n  \n    1959\n18,377\n7,402\n2,482,770\n-\n1.80\n4.69%\n    1969\n22,091\n7,923\n2,788,100\n-\n1.92\n5.06%\n    1979\n27,998\n9,669\n2,895,688\n-\n2.12\n7.82%\n    1989\n30,551\n9,892\n3,088,595\n-\n2.33\n10.97%\n    1999\n42,008\n12,699\n3,308,012\n1,722\n2.51\n11.26%\n    2009\n53,079\n15,846\n3,349,676\n1,604\n2.57\n16.44%\n    2019\n73,411\n21,206\n3,461,734\n1,533\n2.78\n17.47%\n  \n  \n  \n\n\n\n\nNow, the table above could be improved since variables such as the unemployment rate or average hours worked could be averaged by decade for a more fair presentation. But the goal here is to keep things simple.\nThe first striking feature of the table for me was Uruguay’s population. In the 60 year period from 1959 to 2019, total population increased around 39% reaching 3,46 million. For comparison, the similar sized Brazilian state of Rio Grande do Sul has a population close to 10 million, despite suffering population losses due to migration in recent years."
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#latam",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#latam",
    "title": "Uruguay in numbers",
    "section": "LATAM",
    "text": "LATAM\nLets compare how Uruguay fared against other LATAM countries. For this comparison I will take: Argentina, Brazil, Paraguay and Chile. I also add the most recent HDI estimate from the UN. Here I simply scrape the wikipedia page for “List of countries by Human Development Index”.\n\ncountry_selection &lt;- c(\"Argentina\", \"Brazil\", \"Chile\", \"Paraguay\", \"Uruguay\")\n\ntab_latam &lt;- pwt %&gt;%\n  # Get only the last year of each decade\n  filter(country %in% country_selection, year == 2019) %&gt;%\n  mutate(\n    # Real GDP per capita\n    rgdppc = rgdpe / pop,\n    # Convert population\n    pop = pop * 1e6) %&gt;%\n  select(country, rgdpe, rgdppc, pop, avh, hc, csh_x)\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index\"\n\nhdi &lt;- url |&gt; \n  xml2::read_html() |&gt; \n  html_table() |&gt; \n  pluck(2)\n\nnm &lt;- paste(names(hdi), unlist(hdi[1, ]), sep = \"_\")\ndat &lt;- data.frame(as.matrix(hdi[-1, ]))\nnames(dat) &lt;- janitor::make_clean_names(nm)\ndat &lt;- as_tibble(dat)\n\nhdi_clean &lt;- dat |&gt; \n  select(\n    country = nation,\n    hdi = hdi_na,\n    hdi_growth = percent_growth\n  ) |&gt; \n  mutate(\n    hdi = as.numeric(hdi),\n    hdi_growth = as.numeric(str_remove(hdi_growth, \"%\")) / 100\n  )\n\ntab_latam &lt;- left_join(tab_latam, hdi_clean, by = \"country\")\n\n# Swap names for table\nold_names &lt;- names(tab_latam)\nnew_names &lt;- c(\n  \"Country\", \"Real GDP\", \"Real GDP per capita\", \"Population\", \"Avg. Hours Worked\",\n  \"Human Capital Index\", \"Share of Exports (%GDP)\", \"HDI\", \"HDI Growth\")\nnames(new_names) &lt;- old_names\n\n# The table\ngt(tab_latam) %&gt;%\n  cols_label(.list = new_names) %&gt;%\n  fmt_number(columns = c(rgdpe:pop, avh), decimals = 0) %&gt;%\n  fmt_number(columns = hc, decimals = 2) %&gt;%\n  fmt_number(columns = hdi, decimals = 3) %&gt;%\n  fmt_percent(columns = c(csh_x, hdi_growth)) %&gt;%\n  sub_missing(missing_text = \"-\") %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Country\n      Real GDP\n      Real GDP per capita\n      Population\n      Avg. Hours Worked\n      Human Capital Index\n      Share of Exports (%GDP)\n      HDI\n      HDI Growth\n    \n  \n  \n    Argentina\n990,312\n22,115\n44,780,677\n1,609\n3.10\n10.43%\n0.842\n0.09%\n    Brazil\n3,087,570\n14,630\n211,049,527\n1,708\n3.09\n13.47%\n0.754\n0.38%\n    Chile\n446,942\n23,583\n18,952,038\n1,914\n3.15\n24.32%\n0.855\n0.46%\n    Paraguay\n85,462\n12,132\n7,044,636\n-\n2.66\n15.28%\n0.717\n0.42%\n    Uruguay\n73,411\n21,206\n3,461,734\n1,533\n2.78\n17.47%\n0.809\n0.25%\n  \n  \n  \n\n\n\n\nUruguay’s HDI is barely in the “very high” zone (&gt;=0.8). It’s above both Brazil and Paraguay, but lags behind Argentina and Chile. In terms of real GDP per capita, Uruguay is very close to Argentina and Chile.\n\nPopulation\nIn the next chart, I explore the population dynamics of these countries.\n\ncountry_selection &lt;- c(\"Argentina\", \"Brazil\", \"Chile\", \"Paraguay\", \"Uruguay\")\n\npopindex &lt;- pwt %&gt;%\n  filter(country %in% country_selection, !is.na(pop)) %&gt;%\n  group_by(country) %&gt;%\n  mutate(index_pop = pop / first(pop) * 100)\n\nggplot(popindex, aes(x = year, y = index_pop, color = country)) +\n  geom_hline(yintercept = 100) +\n  geom_line() +\n  scale_color_manual(name = \"\", values = colors_report) +\n  labs(\n    title = \"Falling behind\",\n    subtitle = \"Population growth in selected LATAM countries.\\nUruguay exhibits much lower populational growth than nearby neighbors.\",\n    caption = \"Source: PWT 10.0\",\n    x = NULL,\n    y = \"Index (100 = 1951)\") +\n  theme_report\n\n\n\n\n\n\n\n\nAs suspected, Uruguay has much lower population growth than its neighbors in its recent history. In the same time frame that Uruguay’s population grew around 58,5%, Brazil’s population doubled twice.\nFor some of these comparisons it can be useful to look at regional averages. The issue is grouping countries appropriately. The best way is to rely on standard conventions on country groups and regions. An easy way to get a table with such a grouping is the WDI::WDI_data$country. This is a data.frame with country names (and iso3c codes) with regions defined by the World Bank plus some useful information.\nIn the case of Uruguay, the WB classifies as part of the Latin America & Caribbean reigon and as a high-income country.\n\ncountry_groups &lt;- WDI::WDI_data$country\n\ncountry_groups %&gt;%\n  filter(iso3c == \"URY\")\n\n  iso3c iso2c country                    region    capital longitude latitude\n1   URY    UY Uruguay Latin America & Caribbean Montevideo  -56.0675 -34.8941\n       income lending\n1 High income    IBRD\n\n\nWe can join this table with the pwt table and get averages across LATAM. The code below is probably overkill, since it sums, averages, and indexes all variables across the LATAM countries.\n\n# Drop country name from table to avoid duplicate column name after join\ncountry_groups &lt;- country_groups %&gt;%\n  select(-country)\n\nlatam &lt;- pwt %&gt;%\n  mutate(rgdppc = rgdpe / pop) %&gt;%\n  left_join(country_groups, by = c(\"isocode\" = \"iso3c\")) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    across(\n      where(is.numeric),\n      list(\"total\" = sum, \"avg\" = mean),\n      na.rm = TRUE,\n      .names = \"latam_{.fn}_{.col}\")\n    )\n\nlatam_index &lt;- latam %&gt;%\n  pivot_longer(-year) %&gt;%\n  filter(!is.na(value)) %&gt;%\n  group_by(name) %&gt;%\n  mutate(index = value / first(value) * 100) %&gt;%\n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"name\",\n    values_from = \"index\"\n    )\n\ntab_comparison &lt;- pwt %&gt;%\n  mutate(rgdppc = rgdpe / pop) %&gt;%\n  filter(country == \"Uruguay\") %&gt;%\n  select(year, pop, rgdppc) %&gt;%\n  mutate(\n    uruguay_pop = pop / first(pop) * 100,\n    uruguay_rgdppc = rgdppc / first(rgdppc) * 100) %&gt;%\n  left_join(select(latam_index, year, latam_total_pop, latam_avg_rgdppc))\n\ntab_comparison &lt;- tab_comparison %&gt;%\n  select(-pop, -rgdppc) %&gt;%\n  pivot_longer(-year, names_to = \"series\", values_to = \"index\") %&gt;%\n  separate(series, into = c(\"region\", \"variable\"), extra = \"merge\") %&gt;%\n  mutate(\n    variable = str_remove(variable, \"(total_)|(avg_)\"),\n    variable = factor(variable, labels = c(\"Real GDP per capita\", \"Population\"))\n    )\n\nggplot(tab_comparison, aes(x = year, y = index, color = region)) +\n  geom_hline(yintercept = 100) +\n  geom_line() +\n  scale_color_manual(\n    name = \"\",\n    values = colors_report,\n    labels = c(\"Latin America & Carribean Avg.\", \"Uruguay\")) +\n  facet_wrap(vars(variable)) +\n  labs(\n    caption = \"Source: PWT\",  \n    x = NULL,\n    y = \"Index (100 = 1951)\"\n  ) +\n  theme_report\n\n\n\n\n\n\n\n\nSo, Uruguay is way behind LATAM in growth. Note that in both plots the LATAM series has spikes which most likely indicate that new countries have been introduced in the series. This may not be ideal and in a more thorough analysis we should account for that in some manner. A simple solution would be to simply work with more recent data, since it is less likely to contain missing observations.\n\n\nEconomy\nLets go back to our basic fact table. To better visualize how these variables change we can repeat the analysis above. To avoid copy-pasting and to keep things simple we can build a simple function that compares variables from the pwt table. The code below defines the plot_comparison function that compares the historic values of a variable across the five countries we selected. I also defined two additional arguments that can prove to be helpful. First a start_year to filter the time horizon of the anaylsis. Second a logical index indicating if the variable should be indexed to the first available value (TRUE).\n\npwt &lt;- pwt %&gt;%\n  mutate(\n    # Real GDP per capita\n    rgdppc = rgdpe / pop\n  )\n\nplot_comparison &lt;- function(variable, start_year = 1950, index = TRUE, ...) {\n  \n  dat &lt;- pwt %&gt;%\n    filter(\n      country %in% country_selection,\n      year &gt;= start_year,\n      !is.na({{variable}})\n      )\n  \n  if (isTRUE(index)) {\n    dat &lt;- dat %&gt;%\n       group_by(country) %&gt;%\n      mutate(index_var = {{variable}} / first({{variable}}) * 100)\n    \n    p &lt;- ggplot(dat, aes(x = year, y = index_var, color = country)) +\n      geom_hline(yintercept = 100)\n  } else {\n    \n    p &lt;- ggplot(dat, aes(x = year, y = {{variable}}, color = country))\n    \n    }\n  \n  p &lt;- p +\n    geom_line(lwd = 0.8) +\n    scale_color_manual(name = \"\", values = colors_report) +\n    labs(x = NULL, ...) +\n    theme_report\n  \n  return(p)\n  \n}\n\nTo exemplify how this function works lets see how was the growth in Real GDP per capita across these countries.\n\nplot_comparison(rgdppc, title = \"Real GDP per capita\", y = \"Index (1950 = 100)\")\n\n\n\n\n\n\n\n\nBy default, the values are indexed to first available (i.e. non-NA) value. During this time window the best performing country was Brazil, followed by Argentina. Note that the plot above shows the growth of Real GDP per capita. To find the actual values of the variable we set index = FALSE.\n\nplot_comparison(rgdppc, index = FALSE) +\n  ggtitle(\"Real GDP per capita\") +\n  ylab(\"US$ constant\")\n\n\n\n\n\n\n\n\nWhile Uruguay didn’t fare as well in the full time-period, we could analyze its growth pattern following the most recent Commodity Boom. For those unfamiliar with LATAM economies it is important to note that commodity prices have a strong correlation with boom and bust cycles. The most recent and relevant commodity boom was the 2002-2012 period, fueled by Chinese exceptional growth.\nAs the plot below reveals all countries perform well during this time window. Uruguay has the strongest growth among them but note that all of them slowdown past 2015. Brazil, in fact, faces a strong internal recession and, a few years later, Argentina also goes to bust.\nFinally, since the data is available only until 2019 we don’t see how these economies fared during the COVID years.\n\nplot_comparison(rgdppc, 2002) +\n  ggtitle(\"Real GDP per capita\") +\n  ylab(\"Index (2002 = 100)\")\n\n\n\n\n\n\n\n\nWhen looking at average hours worked we see a similar pattern in all countries. Uruguay has the second highest fall (unfortunately there is no information on Paraguay) meaning the average worker is working ~15% less hours in 2019 than in 1990. This is common trend among most economies.\n\nplot_comparison(avh, 1990) +\n  ggtitle(\"Average hours of work\") +\n  ylab(\"Index (1990 = 100)\")\n\n\n\n\n\n\n\n\nAs mentioned above, most LATAM countries are commodity-exporters and commodity prices have strong repercussions on the domestic economy. This is true, despite most LATAM countries having fairly low exports/imports shares. Exports account for less than 20% of GDP even during the 2002-2012 commodity boom.\nGiven that smaller countries tend to have higher international trade volumes I was surprised by how low both Paraguay and Uruguay appear on this plot. Chile is a LATAM outlier in this sense\n\nplot_comparison(csh_x, index = FALSE) +\n  ggtitle(\"Exports as share of GDP\") +\n  ylab(\"% GDP\")\n\n\n\n\n\n\n\n\nLong-term growth is only possible with sustained productivity growth. A decent proxy for this is the “total factor productivity” of the economy. Historically, LATAM countries have struggled to sustain growth for long periods of time: business cycles tend to be extremely volatile in the region.\nAs seen in the plot, total productivity is growing since 2002 but at very modest rates.\n\nplot_comparison(ctfp, 2002) +\n  ggtitle(\"Productivity\") +\n  ylab(\"TFP, Index (2002 = 100)\")\n\n\n\n\n\n\n\n\nCompare Uruguay to another emerging country such as Poland and we see a stark contrast.\n\npwt |&gt; \n  filter(country %in% c(\"Poland\", \"Uruguay\"), year &gt;= 1980) %&gt;%\n  ggplot(aes(x = year, y = ctfp, color = country)) +\n  geom_line() +\n  scale_color_manual(name = \"\", values = colors_report) +\n  theme_report\n\n\n\n\n\n\n\n\nGrowth in Latin American countries is volatile and Uruguay should be no exception. The plot below shows the year on year growth of real GDP per capita. The smooth line is the 6-year moving average.\n\npwt &lt;- pwt %&gt;%\n  group_by(country) %&gt;%\n  mutate(drgdppc = rgdppc / lag(rgdppc) - 1)\n\npwt %&gt;%\n  filter(country == \"Uruguay\") %&gt;%\n  mutate(trend = RcppRoll::roll_mean(drgdppc, n = 6, fill = NA)) %&gt;%\n  ggplot(aes(x = year)) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(y = drgdppc), color = colors_report[1], linewidth = 1) +\n  geom_line(aes(y = trend), color = colors_report[5]) +\n  theme_report"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#demographics",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#demographics",
    "title": "Uruguay in numbers",
    "section": "Demographics",
    "text": "Demographics\nAs we’ve seen above, Uruguay’s population is not growing much. I suspect this likely due to falling total fertility rates. Also, I’m curious to find out the age structure of the population as I suspect that Uruguay had a relatively fast demographic transition.\nI find that the World Population Projections by the UN is a good fit to answer these sort of questions.\nFirst lets look at the population projections for this century. This data, split by age, is in the popprojAge1dt table. To make for a cleaner visualization I group the ages into \"Less than 14\", \"15-24\", \"25-64\", \"65-84\", \"85+\".\nUruguay’s population is expected to grow only until 2027! From that point onwards it will start shrinking. By 2100 the country’s population is expected to be at around 2,4 million (smaller than what it was in 1959). Interestingly, the actual “structure” of the population seems to stay somewhat constant.\n\nlibrary(wpp2022)\n\n# Projections by age 2022-2100\ndata(\"popprojAge1dt\")\n# Estimates by age 1950-2022\ndata(\"popAge1dt\")\n\nproj_age &lt;- popprojAge1dt %&gt;%\n  filter(name == \"Uruguay\") %&gt;%\n  mutate(\n    year = as.numeric(year),\n    age_group = factor(\n      case_when(\n        age &lt;= 14 ~ \"Less than 14\",\n        age &gt; 14 & age &lt;= 24 ~ \"15-24\",\n        age &gt; 24 & age &lt;= 64 ~ \"25-64\",\n        age &gt; 64 & age &lt;= 84 ~ \"65-84\",\n        age &gt; 84 ~ \"85 or over\"),\n      levels = c(\"Less than 14\", \"15-24\", \"25-64\", \"65-84\", \"85 or over\")\n      )\n  ) %&gt;%\n  group_by(year, age_group) %&gt;%\n  summarise(total_pop = sum(pop)) %&gt;%\n  ungroup()\n\nggplot(proj_age, aes(x = year, y = total_pop, fill = age_group)) +\n  geom_area() +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = seq(2020, 2100, 10)) +\n  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n  scale_fill_manual(name = \"\", values = colors_report) +\n  labs(\n    title = \"Uruguay's population is expected to shrink\",\n    subtitle = \"UN population projections by age group from 2022 to 2100.\",\n    x = NULL,\n    y = \"Population (thousands)\",\n    caption = \"Source: WPP (2022)\"\n  ) +\n  theme_report +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\nThe share of the elder population (above 65 years) is expected to triple in the years to come.\n\nproj_age &lt;- popprojAge1dt %&gt;%\n  filter(name == \"Uruguay\") %&gt;%\n  mutate(\n    year = as.numeric(year),\n    age_group = case_when(\n        age &lt;= 14 ~ \"Young\",\n        age &gt; 14 & age &lt;= 64 ~ \"Adult\",\n        age &gt; 64 ~ \"Elder\")\n  ) %&gt;%\n  group_by(year, age_group) %&gt;%\n  summarise(total_pop = sum(pop)) %&gt;%\n  group_by(year) %&gt;%\n  mutate(share = total_pop / sum(total_pop)) %&gt;%\n  ungroup()\n\nproj_text &lt;- proj_age %&gt;%\n  filter(year == max(year)) %&gt;%\n  mutate(x = 2095, label = paste0(round(share * 100, 1), \"%\"))\n\nggplot(proj_age, aes(x = year, y = share, fill = age_group)) +\n  geom_area() +\n  geom_text(\n    data = proj_text,\n    aes(x = x, y = share, label = label),\n    position = position_stack(vjust = 0.5)) +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = seq(2020, 2100, 10)) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_fill_manual(name = \"\", values = colors_report) +\n  labs(\n    title = \"Uruguay's elder population expected to triple\",\n    subtitle = \"Projected population share of each group age 2022-2100\",\n    caption = \"Source: WPP (2022)\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_report +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n    )\n\n\n\n\n\n\n\n\nThere currently is a 1:1 ratio between the\n\nage_index &lt;- proj_age %&gt;%\n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"age_group\",\n    values_from = \"total_pop\"\n    ) %&gt;%\n  mutate(index = Elder / Young * 100)\n\nggplot(age_index, aes(x = year, y = index)) +\n  geom_hline(yintercept = 100) +\n  geom_line(color = colors_report[1], linewidth = 1) +\n  labs(\n    title = \"Aging Index\",\n    subtitle = \"Proportion of elder population in relation to young population\",\n    caption = \"Source: WPP (2022).\",\n    x = NULL,\n    y = \"Index (elder/young)\"\n  ) +\n  theme_report\n\n\n\n\n\n\n\n\nComparison of the demographic pyramid of Uruguay and Brazil reveals two very different patterns. Notice that Brazil’s pyramid initially takes on a triangular shape, with a broad base indicating a large share of young individuals. As the decades pass, there is a gradual reduction in the base of the pyramid, accompanied by an expansion in both the middle and upper segments, reflecting shifts in the country’s age structure. In contrast, Uruguay’s demographic shifts are more nuanced. While there is a noticeable rise in the elderly population and a decline in the younger demographic, these changes are comparatively less pronounced and lack the dramatic shifts observed in Brazil.\n\nx1 &lt;- seq(0, 80, 5)\nx2 &lt;- seq(4, 84, 5)\n\nxlabels &lt;- c(paste(x1, x2, sep = \"-\"), \"85+\")\n\npop_pyramid &lt;- popAge1dt %&gt;%\n  rename(country = name) %&gt;%\n  filter(\n    country %in% c(\"Brazil\", \"Uruguay\"),\n    year %in% c(1950, 1970, 1990, 2010, 2020)) %&gt;%\n  mutate(\n    age_group = findInterval(age, x2, left.open = TRUE),\n    age_group = factor(age_group, labels = xlabels)) %&gt;%\n  group_by(year, country, age_group) %&gt;%\n  summarise(female = sum(popF), male = sum(popM)) %&gt;%\n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"pop\") %&gt;%\n  group_by(year, country, sex) %&gt;%\n  mutate(share = pop / sum(pop) * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(share = if_else(sex == \"male\", -share, share))\n\nggplot(pop_pyramid, aes(x = age_group, y = share)) +\n  geom_col(aes(fill = sex)) +\n  coord_flip() +\n  scale_y_continuous(\n    breaks = seq(-15, 15, 5),\n    labels = c(15, 10, 5, 0, 5, 10, 15)) +\n  scale_fill_manual(values = colors_report[c(1, 3)]) +\n  guides(fill = \"none\") +\n  facet_grid(rows = vars(country), cols = vars(year)) +\n  labs(\n    title = \"Uruguay had a smaller demographic bonus\",\n    subtitle = \"Age pyramids for Brazil and Uruguay\",\n    caption = \"Source: WPP (2022)\",\n    y = \"%\",\n    x = NULL) +\n  theme_report +\n  theme(\n    text = element_text(size = 6),\n    panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#montevideo",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#montevideo",
    "title": "Uruguay in numbers",
    "section": "Montevideo",
    "text": "Montevideo\nObtaining reliable information about cities tends to be more challenging compared to data available for countries. In this analysis, I use the latest Mercer Quality of Living City Ranking to assess and rank the major cities in Latin America. The findings highlight Montevideo, the capital of Uruguay, as the highest-ranking city among its Latin American counterparts. However, it’s noteworthy that Montevideo only secures a place just within the top 100 on the overall list.\n\nlibrary(rvest)\nlibrary(emoji)\n\nmercer_url &lt;- \"https://mobilityexchange.mercer.com/Insights/quality-of-living-rankings\"\n\ncountry_latam &lt;- c(\n  \"Argentina\", \"Bolivia\", \"Brazil\", \"Chile\", \"Colombia\", \"Ecuador\", \n  \"Guyana\", \"Paraguay\", \"Peru\", \"Suriname\", \"Uruguay\", \"Venezuela\",\n  \"Mexico\", \"Panama\", \"Cuba\", \"Costa Rica\"\n  )\n\ntables &lt;- mercer_url %&gt;%\n  xml2::read_html() %&gt;%\n  html_table()\n\n# mercer &lt;- bind_rows(tables[[1]], tables[[2]])\nmercer &lt;- tables[[1]]\n\nmercer &lt;- mercer %&gt;%\n  janitor::clean_names() %&gt;%\n  rename(country = location, rank = ranking_2023) %&gt;%\n  mutate(\n    city = str_to_title(city),\n    country = str_to_title(country)\n    )\n  \n\n# mercer &lt;- mercer %&gt;%\n#   janitor::clean_names() %&gt;%\n#   unite(\"country\", c(country_region, country_region_2), na.rm = TRUE)\n\ntab_rank &lt;- mercer %&gt;%\n  filter(country %in% country_latam) %&gt;%\n  mutate(flag = map_chr(country, emoji::flag)) %&gt;%\n  select(rank, city, flag)\n\ntab_rank %&gt;%\n  gt() %&gt;%\n  cols_label(rank = \"Rank\", city = \"City\", flag = \"\") %&gt;%\n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      Rank\n      City\n      \n    \n  \n  \n    89\nMontevideo\n🇺🇾\n    100\nBuenos Aires\n🇦🇷\n    103\nPanama City\n🇵🇦\n    104\nSantiago\n🇨🇱\n    108\nSao Paulo\n🇧🇷\n    115*\nSan Jose\n🇨🇷\n    115*\nRio De Janeiro\n🇧🇷\n    119\nBrasilia\n🇧🇷\n    120\nBelo Horizonte\n🇧🇷\n    121\nQuito\n🇪🇨\n    122\nMonterrey\n🇲🇽\n    123\nAsuncion\n🇵🇾\n    129\nBogota\n🇨🇴\n    130\nMexico City\n🇲🇽\n    133\nLima\n🇵🇪\n    150\nManaus\n🇧🇷\n    167*\nLa Paz\n🇧🇴\n    200\nHavana\n🇨🇺\n    208*\nCaracas\n🇻🇪\n  \n  \n  \n\n\n\n\n\nwiki_url &lt;- \"https://en.wikipedia.org/wiki/List_of_cities_in_Uruguay\"\n\ntables &lt;- html_table(xml2::read_html(wiki_url))\n\npop_muni &lt;- tables[[1]]\n\nas_numeric_string &lt;- Vectorize(function(x) {\n  \n  digits &lt;- str_extract_all(x, \"[[:digit:]]\")\n  digits &lt;- paste(unlist(digits), collapse = \"\")\n  as.numeric(digits)\n  \n  })\n\npop_muni &lt;- pop_muni %&gt;%\n  janitor::clean_names() %&gt;%\n  select(city, department, starts_with(\"pop\")) %&gt;%\n  mutate(across(starts_with(\"pop\"), as_numeric_string))\n\npop_depto &lt;- pop_muni %&gt;%\n  group_by(department) %&gt;%\n  summarise(total_pop = sum(population2011_census, na.rm = TRUE))\n\nFinally, I map the population of each district (departamento) of Uruguay.\n\nlibrary(geouy)\n\ndeptos &lt;- load_geouy(\"Departamentos\")\n\nReading layer `departamentos_v2' from data source \n  `https://mapas.mides.gub.uy/geoserver/IDE/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=IDE:departamentos_v2' \n  using driver `GML'\nSimple feature collection with 19 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 366582.2 ymin: 6127919 xmax: 858252.1 ymax: 6671738\nProjected CRS: WGS 84 / UTM zone 21S\n\ndeptos &lt;- st_make_valid(deptos)\n\npop_depto &lt;- pop_depto %&gt;%\n  mutate(\n    depto_name = str_remove(department, \"Department\"),\n    depto_name = str_to_upper(depto_name),\n    depto_name = stringi::stri_trans_general(depto_name, \"latin-ascii\"),\n    depto_name = str_trim(depto_name)\n  )\n\ndeptos_pop &lt;- left_join(deptos, pop_depto, by = c(\"nombre\" = \"depto_name\"))\n\n\nlibrary(tmap)\nlibrary(tmaptools)\ntmap_mode(mode = \"view\")\n\nm &lt;- tm_shape(deptos_pop) +\n  tm_fill(\n    col = \"total_pop\",\n    style = \"jenks\",\n    n = 5,\n    title = \"Population\",\n    id = \"department\") +\n  tm_borders() +\n  tm_basemap(server = \"CartoDB.Positron\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-uruguay-numbers/index.html#footnotes",
    "href": "posts/general-posts/2023-09-uruguay-numbers/index.html#footnotes",
    "title": "Uruguay in numbers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a brief period, Uruguay was a part of Brazil (technically, the United Kingdom of Portugal, Brazil, and the Algarves). To this day, both Uruguay and Rio Grande do Sul, the southernmost Brazilian state, that borders Uruguay, have some degree of shared culture, mostly amongst the gaucho figure.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-rename/index.html#footnotes",
    "href": "posts/general-posts/2024-01-tidyverse-rename/index.html#footnotes",
    "title": "O novo tidyverse: rename",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara verificar a lista completa de palavras reservadas consulte ?Reserved. Vale notar que não se deve criar objetos com estes nomes também, i.e., não se deve fazer TRUE &lt;- c(1, 2, 3).↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html",
    "href": "posts/general-posts/2023-12-carros-renda/index.html",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Este post investiga a relação entre a “demanda por automóveis”, entendida simplesmente como a posse (ou não) de um automóvel. Usando dados da Pesquisa Origem e Destino do Metrô de 2017, estima-se um modelo de escolha discreta para investigar o efeito da renda domiciliar sobre a escolha de ter um carro. Encontrou-se um efeito forte e significante da renda sobre a “demanda” por carros. Idade, educação e filhos também são fatores importantes e que aumentam a probabilidade do domicílio ter um automóvel. O único fator encontrado que reduz a probabilidade do domicílio ter um automóvel é ele ser chefiado por uma mulher.\nVale notar que não se pode falar propriamente em “demanda” por automóveis já que temos somente dados de um único momento no tempo. Eventualmente, quando os dados da POD 2022 forem liberados, será possível construir uma curva de demanda por automóveis.\n\n\nA Região Metropolitana de São Paulo abriga aproximadamente 20 milhões de pessoas e gera mais de 40 milhões de viagens todos os dias. Segundo Relatório do Metrô as viagens na Região Metropolitana de São Paulo dividem-se da seguinte maneira: um terço das viagens é feita por modos não-motorizados (a pé ou de bicicleta), enquanto dois terços das viagens é feito por modos motorizados. Neste último grupo, 55% das viagens são feitas com modais coletivos (ônibus, metrô, trem) e 45% das viagens são feitas com modais individuais (carro, táxi, moto). Ao todo, as viagens em automóveis particulares (excluindo táxis) representam cerca de 27% de todas as viagens diárias; ou seja, cerca de 1 em cada 4 viagens na RMSP é feita com carro particular.\nO diagrama abaixo esquematiza todas as viagens diárias realizadas na RMSP. Vale notar que os dados são de 2017, então muitas das estações das linhas 4-amarela e 5-coral ainda estavam sob construção. Similarmente, as linhas 13-Jade, 15-Prata (monotrilho) ainda não estavam operando. Por simplicidade considerei o universo de todas as viagens: ainda que os deslocamentos casa-trabalho componham a maior parte deste conjunto, inclui-se todo tipo de deslocamento.\n\n\n\n\n\n\n\n\n\nPouco mais da metade dos domicílios na RMSP possui algum automóvel: 53% dos domicílios possui ao menos um carro, enquanto 47% dos domicílios não possui carro particular. Mais especificamente, 43,8% possui somente um carro. Domicílios com 2 automóveis são apenas 8% do total e domicílios com 3 automóveis ou mais são pouco mais de 1% do total.\n\n\n\n\n\n\n\n\n\nNa média da RMSP, há 0,635 carros por domicílios. Na capital, o número é similar, 0,6. Olhando para os distritos de São Paulo, vê-se que os menos “carro-dependentes” são distritos centrais como República e Sé. Distritos de baixa renda média como Parelheiros e Cidade Tiradentes também tem uma baixa razão de carros por domicílio.\nOs distritos mais carro-dependentes têm rendas elevadas e estão dentro ou próximos do Centro Expandido da cidade. É curioso notar que bairros verticalizados como Moema e Morumbi aparecem junto com o distrito de Alto de Pinheiros que é majoritariamente composto por residências horizontais. Isto sugere que a verticalização não acompanha menor dependência do automóvel particular.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a taxa de motorização, a razão de carros por domicílios, nos distritos da RMSP. Vê-se um padrão espacial onde as regiões com maiores taxas de motorização concentram-se no quadrante sudoeste do Centro Expandido da capital. No começo da zona oeste, na região do Tatuapé, também vê-se taxas mais elevadas. Fora da capital, vale notar que a cidade de Santana de Parnaíba tem elevada taxa de motorização, comparável aos distritos mais carro-dependentes de São Paulo.\n\n\n\n\n\n\n\nO share de domicílios sem carro segue um padrão espacial similar. O mapa detalha as zonas OD da cidade de São Paulo. Em outro post, discuti a distribuição desta métrica na cidade.\n\n\n\n\n\n\n\n\n\n\n\n\nA demanda por automóveis tende a aumentar junto com a renda das famílias. Mesmo famílias que moram em regiões centrais, próximas de polos de empregos e com boa oferta de infraestrutura urbana, preferem ter mais automóveis. O gráfico abaixo mostra a relação entre a renda domiciliar média e a razão de carros por domicílio; os dados são agregados por zona OD. A linha de tendência mostra uma relação quadrática ponderada (usando a população total de cada zona como peso).\nÉ interessante notar as zonas que “fogem” da tendência esperada. A região do Morumbi, por exemplo, tem renda similar a do Paraíso, mas tem uma taxa de motorização consideravelmente melhor. Possivelmente, isto reflete a infraestrutura de cada bairro: o Morumbi não tem acesso fácil nem a trem e nem a metrô; já o bairro do Paraíso tem acesso fácil a duas linhas de metrô, ciclovias e corredores de ônibus.\n\n\n\n\n\n\n\n\n\n\n\nA demanda por automóveis também parece variar por região. O gráfico abaixo agrupa as zonas por regiões: Centro Expandido, São Paulo (capital) e RMSP (resto da região metropolitana). Nota-se que as zonas dentro do CE da capital tem uma tendência a ter menos carros do que regiões de renda equivalente na RMSP.\nNote que agora os gráficos agora mostram a renda domiciliar per capita em escala logarítmica e usa-se uma tendência linear. A regressão continua sendo ponderada pela população total de cada zona, mas todos os círculos no painel da direita tem igual tamanho para facilitar a visualização dos dados.\n\n\n\n\n\n\n\n\n\n\n\n\nIndo um pouco mais a fundo, pode-se fazer uma regressão linear para melhor avaliar a diferença entre os grupos. Visualmente, as zonas dentro do CE parecem ter uma proporção menor de carros por habitante do que as zonas na RMSP, relativamente à renda familiar. A tabela abaixo mostra o resultado da regressão, onde adiciona-se também algumas variáveis auxiliares como a proporção de adultos (18-64 anos) que habita na zona e a densidade populacional da zona.\nA relação de interesse aparece nas duas últimas linhas que indica a relação entre a renda domiciliar per capita e a localização (estrato) para a proporção de carros por domicílio. Tanto regiões dentro da capital como dentro do CE apresentam uma menor proporção de carros por domicílios, relativamente às zonas da RMSP.\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    Prop. Adultos (%)\n-0.00298\n-0.00762, 0.00166\n0.2\n    Densidade Populacional\n-0.00058\n-0.00076, -0.00040\n&lt;0.001\n    Prop Ensino Superior (%)\n0.00026\n-0.00229, 0.00280\n0.8\n    Renda Domiciliar per capita (log)\n0.54064\n0.45712, 0.62415\n&lt;0.001\n    Estrato\n\n\n\n        RMSP\n—\n—\n\n        São Paulo\n0.49744\n-0.07001, 1.06489\n0.086\n        Centro Expandido\n0.55672\n-0.34798, 1.46142\n0.2\n    Renda Domiciliar per capita (log) * Estrato\n\n\n\n        Renda Domiciliar per capita (log) * São Paulo\n-0.07532\n-0.15368, 0.00303\n0.060\n        Renda Domiciliar per capita (log) * Centro Expandido\n-0.10456\n-0.22138, 0.01227\n0.079\n    R²\n0.664\n\n\n    Adjusted R²\n0.658\n\n\n    AIC\n-464\n\n\n    Sigma\n25.1\n\n\n    Log-likelihood\n242\n\n\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nVisualmente, pode-se verificar isto no gráfico abaixo que mostra a relação entre a renda domiciliar per capita e a “demanda por automóveis” em cada um dos estratos geográficos.\n\n\n\n\n\n\n\n\n\n\n\n\nO mapa abaixo apresenta visualmente a relação entre renda e a taxa de automóveis. A classificação das zonas é simples e segue o algoritmo de Jenks sem considerar explicitamente a dependência espacial entre as zonas. Ainda assim, é possível distinguir entre as regiões. A região da Paraíso, por exemplo, é de renda alta e taxa de motorização média; Pinheiros é de renda média e taxa de motorização média; e Jardim Europa é de renda alta e taxa alta. Algumas regiões do Centro-Sul como Brooklin e Campo Belo são médio-alto (renda média e taxa alta). O centro antigo da cidade é baixo-baixo, assim como boa parte da periferia da cidade.\n\n\n\n\n\n\n\nA maior parte das zonas classificadas é do tipo “low-low”, isto é, de renda baixa e de baixa razão carros/domicílios. De fato, quase metada das zonas entra no grupo de “renda baixa” (R$2000 a R$4500). Dentre as zonas de renda alta (&gt; R$ 8000), a maior parte tem uma razão alta de carros/domicílio; ainda assim, uma proporção relativamente expressiva têm razão média de carros/domicílios (36%). Apenas duas zonas (MASP e Rodrigues Alves) são classificados como renda alta e baixa razão carros/domicílios.\n\n\n\n\n\n\n  \n    \n    \n      Renda - Carros\n      Número de Zonas\n      Proporção de zonas\n    \n  \n  \n    Low-Low\n175\n35.57%\n    Medium-Medium\n114\n23.17%\n    Low-Medium\n63\n12.80%\n    High-High\n52\n10.57%\n    Medium-Low\n42\n8.54%\n    High-Medium\n31\n6.30%\n    Medium-High\n13\n2.64%\n    High-Low\n2\n0.41%\n  \n  \n  \n\n\n\n\nA tabela abaixo ilustra a classificação acima trazendo alguns exemplos de cada grupo.\n\n\n\n\n\n\n  \n    \n    \n      Renda - Carros\n      Exemplos de Região\n    \n  \n  \n    Low-Low\nCascatas, Morro do Índio, Sacadura Cabral, Itapevi, Jardim Presidente Dutra\n    Low-Medium\nEstrada do Carneiro, Riacho Grande, Taboão, Melhoramentos, Cotia\n    Medium-Low\nPenha, Vila São Rafael, Quarta Parada, Jardim Primavera, Belenzinho\n    Medium-Medium\nJardim Piratininga, Vila Miranda, Itaquera, Vila Esperança, Vila Zelina\n    Medium-High\nVila Bertioga, Rudge Ramos, Parque da Mooca, Granja Viana, Bosque da Saúde\n    High-Low\nMasp, Rodrigues Alves\n    High-Medium\nBela Vista, Vila Sônia, Chácara Itaim, Alfredo Pujol, Vila Clementino\n    High-High\nSanta Cruz, Parque Ibirapuera, Gavião Peixoto, Santo Amaro, Vila Nova Conceição\n  \n  \n  \n\n\n\n\n\n\n\n\nAté agora este post assumiu que o deslocamento total de carro de uma família é, de alguma forma, proporcional ao número de automóveis que ela possui. Isto é, o número total de quilômetros rodados em automóvel particular é uma função da posse do automóvel.\nEsta é uma hipótese razoável, mas como viu-se acima, a maioria dos domicílios ou não têm um carro ou tem somente um. Assim, é possível fazer ainda mais uma simplificação: considerar a posse ou não de um automóvel (ou mais).\n\n\nHá dois fatores bastante intuitivos, indicados pela literatura, que estão relacionados com a posse de automóvel: renda e idade. O gráfico abaixo mostra a relação entre a posse de automóveis e a idade do chefe da família. Como se vê, há uma relação não-linear entre as variáveis: a proporção de famílias com automóvel cresce até certo ponto e depois diminui.\nOs pontos agrupam as idades em grupos de cinco anos (18-22, 23-27, etc.) e representam a média de cada grupo. A linha de ajuste é de uma regressão polinomial de segundo grau, ajustada pelo peso proporcional da cada grupo na população total.\n\n\n\n\n\n\n\n\n\nO gráfico de colunas abaixo mostra a proporção de domicílios que possui ao menos um carro por decil de renda. Abaixo dos eixos, apresenta-se o intervalo de renda considerado. As barrinhas pequenas são o intervalo de confiança de cada estimativa.\n\n\n\n\n\n\n\n\n\n\n\n\nComo agora temos uma variável binária (posse ou não-posse de automóvel) pode-se usar um modelo de escolha discreta para estudar a relação entre renda e automóveis. Na regressão abaixo, mostro os resultados de uma regressão Logística e de uma regressão Probit. A regressão é feita sobre os microdados da Pesquisa Origem Destino (identificados por domicílio) usando os fatores de expansão como pesos.\nPara simplificar a análise, restringiu-se a amostra às zonas dentro da cidade de São Paulo (capital). Além disso, removeu-se domicílios com renda muita baixa (menos de 1/4 do salário mínimo da época) e domicílios chefiados por um responsável com menos de 18 anos. Esta regressão inclui algumas variáveis adicionais relevantes:\n\nCriança: variável binária que indica se há pelo menos uma criança no domicílio (menor de 18 anos com vínculo familiar)\nEnsino superior: variável binária que indica se o responsável pelo domicílio possui ensino superior.\nEmpregado: variável binária que indica se o responsável pelo domicílio estava empregado.\nFeminino: variável binária que indica se a responsável pelo domicílio se identifica como do sexo feminino.\nCentro Exp: variável binária que indica se a zona do domicílio se encontra dentro do Centro Expandido da cidade.\nA variável de idade foi “discretizada” em grupos para melhor capturar o seu efeito não-linear.\n\nA tabela abaixo ilustra o efeito das variáveis binárias acima sobre a frequência de domicílios em relação à posse de automóveis. A tabela soma o número de domicílios com ou sem automóvel e compara as famílias com e sem crianças. Proporcionalmente, as famílias com ao menos um filho tem uma chance cerca de 1,5 maior de ter um carro (em contraste com uma família sem filhos).\n\n\n\n\n\nPara levar em consideração o efeito espacial das variáveis incluiu-se efeitos fixos de cada zona OD. Existem métodos mais sofisticados para modelar a dependências espacial entre as regiões, mas para os propósitos deste post esta abordagem é suficiente. Os resultados das duas regressões é apresentado abaixo.\nVale notar que apresento os odd-ratios ao invés dos coeficientes na regressão logística1. Assim, pode-se ver mais imediatamente o efeito das variáveis. De maneira geral, todos os fatores tem efeito de aumentar a chance que um domicílio tenha um automóvel; apenas “Feminino” e “Centro Exp.” têm valores menores do que um, indicando que domicílios chefiados por mulheres e/ou dentro do CE têm uma chance menor de ter automóvel. Um domicílio que recebe incremento de 1% na sua renda tem um aumento de 6,74% na chance de ter um carro. Similarmente, um domicílio com criança tem um aumento de 1,33 na chance de ter um carro em relação a um domicílio sem criança.\n\n\n\n\n\nA interpretação dos coeficientes de uma regressão Probit não é tão imediato. Vale notar que o sinal de todos os coeficientes corrobora o modelo Logit: com exceção de “feminino”, todas as variáveis têm efeito positivo, isto é, aumentam a probabilidade do domicílio possuir um automóvel.\nO gráfico abaixo simula a probabilidade esperada de um domicílio de Pinheiros chefiado por um indivíduo de 35 a 44 anos possuir um automóvel, condicional a alguns fatores:\n\n\n\nCriança. Se a família possui pelo menos um filho no domicílio.\nEnsino Universitário. Se a pessoa responsável pelo domicílio possui ensino supeior.\nRenda. O nível de renda domiciliar (ajustado pelo IPC-Fipe).\n\n\n\nEm todos os casos analisados, ter um filho aumenta a probabilidade de ter um automóvel, mas este efeito varia de acordo com a renda.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCornut, B., & Madre, J. L. (2017). A longitudinal perspective on car ownership and use in relation with income inequalities in the Paris metropolitan area. Transport Reviews, 37(2), 227-244.\nDargay, J. M. (2001). The effect of income on car ownership: evidence of asymmetry. Transportation Research Part A: Policy and Practice, 35(9), 807-821.\nDargay, J., & Gately, D. (1999). Income’s effect on car and vehicle ownership, worldwide: 1960–2015. Transportation Research Part A: Policy and Practice, 33(2), 101-138.\nNolan, A. (2010). A dynamic analysis of household car ownership. Transportation research part A: policy and practice, 44(6), 446-455."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#são-paulo-fatos-gerais",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#são-paulo-fatos-gerais",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "A Região Metropolitana de São Paulo abriga aproximadamente 20 milhões de pessoas e gera mais de 40 milhões de viagens todos os dias. Segundo Relatório do Metrô as viagens na Região Metropolitana de São Paulo dividem-se da seguinte maneira: um terço das viagens é feita por modos não-motorizados (a pé ou de bicicleta), enquanto dois terços das viagens é feito por modos motorizados. Neste último grupo, 55% das viagens são feitas com modais coletivos (ônibus, metrô, trem) e 45% das viagens são feitas com modais individuais (carro, táxi, moto). Ao todo, as viagens em automóveis particulares (excluindo táxis) representam cerca de 27% de todas as viagens diárias; ou seja, cerca de 1 em cada 4 viagens na RMSP é feita com carro particular.\nO diagrama abaixo esquematiza todas as viagens diárias realizadas na RMSP. Vale notar que os dados são de 2017, então muitas das estações das linhas 4-amarela e 5-coral ainda estavam sob construção. Similarmente, as linhas 13-Jade, 15-Prata (monotrilho) ainda não estavam operando. Por simplicidade considerei o universo de todas as viagens: ainda que os deslocamentos casa-trabalho componham a maior parte deste conjunto, inclui-se todo tipo de deslocamento."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Pouco mais da metade dos domicílios na RMSP possui algum automóvel: 53% dos domicílios possui ao menos um carro, enquanto 47% dos domicílios não possui carro particular. Mais especificamente, 43,8% possui somente um carro. Domicílios com 2 automóveis são apenas 8% do total e domicílios com 3 automóveis ou mais são pouco mais de 1% do total.\n\n\n\n\n\n\n\n\n\nNa média da RMSP, há 0,635 carros por domicílios. Na capital, o número é similar, 0,6. Olhando para os distritos de São Paulo, vê-se que os menos “carro-dependentes” são distritos centrais como República e Sé. Distritos de baixa renda média como Parelheiros e Cidade Tiradentes também tem uma baixa razão de carros por domicílio.\nOs distritos mais carro-dependentes têm rendas elevadas e estão dentro ou próximos do Centro Expandido da cidade. É curioso notar que bairros verticalizados como Moema e Morumbi aparecem junto com o distrito de Alto de Pinheiros que é majoritariamente composto por residências horizontais. Isto sugere que a verticalização não acompanha menor dependência do automóvel particular.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a taxa de motorização, a razão de carros por domicílios, nos distritos da RMSP. Vê-se um padrão espacial onde as regiões com maiores taxas de motorização concentram-se no quadrante sudoeste do Centro Expandido da capital. No começo da zona oeste, na região do Tatuapé, também vê-se taxas mais elevadas. Fora da capital, vale notar que a cidade de Santana de Parnaíba tem elevada taxa de motorização, comparável aos distritos mais carro-dependentes de São Paulo.\n\n\n\n\n\n\n\nO share de domicílios sem carro segue um padrão espacial similar. O mapa detalha as zonas OD da cidade de São Paulo. Em outro post, discuti a distribuição desta métrica na cidade."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio-e-renda",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#carros-por-domicílio-e-renda",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "A demanda por automóveis tende a aumentar junto com a renda das famílias. Mesmo famílias que moram em regiões centrais, próximas de polos de empregos e com boa oferta de infraestrutura urbana, preferem ter mais automóveis. O gráfico abaixo mostra a relação entre a renda domiciliar média e a razão de carros por domicílio; os dados são agregados por zona OD. A linha de tendência mostra uma relação quadrática ponderada (usando a população total de cada zona como peso).\nÉ interessante notar as zonas que “fogem” da tendência esperada. A região do Morumbi, por exemplo, tem renda similar a do Paraíso, mas tem uma taxa de motorização consideravelmente melhor. Possivelmente, isto reflete a infraestrutura de cada bairro: o Morumbi não tem acesso fácil nem a trem e nem a metrô; já o bairro do Paraíso tem acesso fácil a duas linhas de metrô, ciclovias e corredores de ônibus.\n\n\n\n\n\n\n\n\n\n\n\nA demanda por automóveis também parece variar por região. O gráfico abaixo agrupa as zonas por regiões: Centro Expandido, São Paulo (capital) e RMSP (resto da região metropolitana). Nota-se que as zonas dentro do CE da capital tem uma tendência a ter menos carros do que regiões de renda equivalente na RMSP.\nNote que agora os gráficos agora mostram a renda domiciliar per capita em escala logarítmica e usa-se uma tendência linear. A regressão continua sendo ponderada pela população total de cada zona, mas todos os círculos no painel da direita tem igual tamanho para facilitar a visualização dos dados.\n\n\n\n\n\n\n\n\n\n\n\n\nIndo um pouco mais a fundo, pode-se fazer uma regressão linear para melhor avaliar a diferença entre os grupos. Visualmente, as zonas dentro do CE parecem ter uma proporção menor de carros por habitante do que as zonas na RMSP, relativamente à renda familiar. A tabela abaixo mostra o resultado da regressão, onde adiciona-se também algumas variáveis auxiliares como a proporção de adultos (18-64 anos) que habita na zona e a densidade populacional da zona.\nA relação de interesse aparece nas duas últimas linhas que indica a relação entre a renda domiciliar per capita e a localização (estrato) para a proporção de carros por domicílio. Tanto regiões dentro da capital como dentro do CE apresentam uma menor proporção de carros por domicílios, relativamente às zonas da RMSP.\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    Prop. Adultos (%)\n-0.00298\n-0.00762, 0.00166\n0.2\n    Densidade Populacional\n-0.00058\n-0.00076, -0.00040\n&lt;0.001\n    Prop Ensino Superior (%)\n0.00026\n-0.00229, 0.00280\n0.8\n    Renda Domiciliar per capita (log)\n0.54064\n0.45712, 0.62415\n&lt;0.001\n    Estrato\n\n\n\n        RMSP\n—\n—\n\n        São Paulo\n0.49744\n-0.07001, 1.06489\n0.086\n        Centro Expandido\n0.55672\n-0.34798, 1.46142\n0.2\n    Renda Domiciliar per capita (log) * Estrato\n\n\n\n        Renda Domiciliar per capita (log) * São Paulo\n-0.07532\n-0.15368, 0.00303\n0.060\n        Renda Domiciliar per capita (log) * Centro Expandido\n-0.10456\n-0.22138, 0.01227\n0.079\n    R²\n0.664\n\n\n    Adjusted R²\n0.658\n\n\n    AIC\n-464\n\n\n    Sigma\n25.1\n\n\n    Log-likelihood\n242\n\n\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nVisualmente, pode-se verificar isto no gráfico abaixo que mostra a relação entre a renda domiciliar per capita e a “demanda por automóveis” em cada um dos estratos geográficos.\n\n\n\n\n\n\n\n\n\n\n\n\nO mapa abaixo apresenta visualmente a relação entre renda e a taxa de automóveis. A classificação das zonas é simples e segue o algoritmo de Jenks sem considerar explicitamente a dependência espacial entre as zonas. Ainda assim, é possível distinguir entre as regiões. A região da Paraíso, por exemplo, é de renda alta e taxa de motorização média; Pinheiros é de renda média e taxa de motorização média; e Jardim Europa é de renda alta e taxa alta. Algumas regiões do Centro-Sul como Brooklin e Campo Belo são médio-alto (renda média e taxa alta). O centro antigo da cidade é baixo-baixo, assim como boa parte da periferia da cidade.\n\n\n\n\n\n\n\nA maior parte das zonas classificadas é do tipo “low-low”, isto é, de renda baixa e de baixa razão carros/domicílios. De fato, quase metada das zonas entra no grupo de “renda baixa” (R$2000 a R$4500). Dentre as zonas de renda alta (&gt; R$ 8000), a maior parte tem uma razão alta de carros/domicílio; ainda assim, uma proporção relativamente expressiva têm razão média de carros/domicílios (36%). Apenas duas zonas (MASP e Rodrigues Alves) são classificados como renda alta e baixa razão carros/domicílios.\n\n\n\n\n\n\n  \n    \n    \n      Renda - Carros\n      Número de Zonas\n      Proporção de zonas\n    \n  \n  \n    Low-Low\n175\n35.57%\n    Medium-Medium\n114\n23.17%\n    Low-Medium\n63\n12.80%\n    High-High\n52\n10.57%\n    Medium-Low\n42\n8.54%\n    High-Medium\n31\n6.30%\n    Medium-High\n13\n2.64%\n    High-Low\n2\n0.41%\n  \n  \n  \n\n\n\n\nA tabela abaixo ilustra a classificação acima trazendo alguns exemplos de cada grupo.\n\n\n\n\n\n\n  \n    \n    \n      Renda - Carros\n      Exemplos de Região\n    \n  \n  \n    Low-Low\nCascatas, Morro do Índio, Sacadura Cabral, Itapevi, Jardim Presidente Dutra\n    Low-Medium\nEstrada do Carneiro, Riacho Grande, Taboão, Melhoramentos, Cotia\n    Medium-Low\nPenha, Vila São Rafael, Quarta Parada, Jardim Primavera, Belenzinho\n    Medium-Medium\nJardim Piratininga, Vila Miranda, Itaquera, Vila Esperança, Vila Zelina\n    Medium-High\nVila Bertioga, Rudge Ramos, Parque da Mooca, Granja Viana, Bosque da Saúde\n    High-Low\nMasp, Rodrigues Alves\n    High-Medium\nBela Vista, Vila Sônia, Chácara Itaim, Alfredo Pujol, Vila Clementino\n    High-High\nSanta Cruz, Parque Ibirapuera, Gavião Peixoto, Santo Amaro, Vila Nova Conceição"
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#carros-e-renda-analisando-a-relação",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#carros-e-renda-analisando-a-relação",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Até agora este post assumiu que o deslocamento total de carro de uma família é, de alguma forma, proporcional ao número de automóveis que ela possui. Isto é, o número total de quilômetros rodados em automóvel particular é uma função da posse do automóvel.\nEsta é uma hipótese razoável, mas como viu-se acima, a maioria dos domicílios ou não têm um carro ou tem somente um. Assim, é possível fazer ainda mais uma simplificação: considerar a posse ou não de um automóvel (ou mais).\n\n\nHá dois fatores bastante intuitivos, indicados pela literatura, que estão relacionados com a posse de automóvel: renda e idade. O gráfico abaixo mostra a relação entre a posse de automóveis e a idade do chefe da família. Como se vê, há uma relação não-linear entre as variáveis: a proporção de famílias com automóvel cresce até certo ponto e depois diminui.\nOs pontos agrupam as idades em grupos de cinco anos (18-22, 23-27, etc.) e representam a média de cada grupo. A linha de ajuste é de uma regressão polinomial de segundo grau, ajustada pelo peso proporcional da cada grupo na população total.\n\n\n\n\n\n\n\n\n\nO gráfico de colunas abaixo mostra a proporção de domicílios que possui ao menos um carro por decil de renda. Abaixo dos eixos, apresenta-se o intervalo de renda considerado. As barrinhas pequenas são o intervalo de confiança de cada estimativa.\n\n\n\n\n\n\n\n\n\n\n\n\nComo agora temos uma variável binária (posse ou não-posse de automóvel) pode-se usar um modelo de escolha discreta para estudar a relação entre renda e automóveis. Na regressão abaixo, mostro os resultados de uma regressão Logística e de uma regressão Probit. A regressão é feita sobre os microdados da Pesquisa Origem Destino (identificados por domicílio) usando os fatores de expansão como pesos.\nPara simplificar a análise, restringiu-se a amostra às zonas dentro da cidade de São Paulo (capital). Além disso, removeu-se domicílios com renda muita baixa (menos de 1/4 do salário mínimo da época) e domicílios chefiados por um responsável com menos de 18 anos. Esta regressão inclui algumas variáveis adicionais relevantes:\n\nCriança: variável binária que indica se há pelo menos uma criança no domicílio (menor de 18 anos com vínculo familiar)\nEnsino superior: variável binária que indica se o responsável pelo domicílio possui ensino superior.\nEmpregado: variável binária que indica se o responsável pelo domicílio estava empregado.\nFeminino: variável binária que indica se a responsável pelo domicílio se identifica como do sexo feminino.\nCentro Exp: variável binária que indica se a zona do domicílio se encontra dentro do Centro Expandido da cidade.\nA variável de idade foi “discretizada” em grupos para melhor capturar o seu efeito não-linear.\n\nA tabela abaixo ilustra o efeito das variáveis binárias acima sobre a frequência de domicílios em relação à posse de automóveis. A tabela soma o número de domicílios com ou sem automóvel e compara as famílias com e sem crianças. Proporcionalmente, as famílias com ao menos um filho tem uma chance cerca de 1,5 maior de ter um carro (em contraste com uma família sem filhos).\n\n\n\n\n\nPara levar em consideração o efeito espacial das variáveis incluiu-se efeitos fixos de cada zona OD. Existem métodos mais sofisticados para modelar a dependências espacial entre as regiões, mas para os propósitos deste post esta abordagem é suficiente. Os resultados das duas regressões é apresentado abaixo.\nVale notar que apresento os odd-ratios ao invés dos coeficientes na regressão logística1. Assim, pode-se ver mais imediatamente o efeito das variáveis. De maneira geral, todos os fatores tem efeito de aumentar a chance que um domicílio tenha um automóvel; apenas “Feminino” e “Centro Exp.” têm valores menores do que um, indicando que domicílios chefiados por mulheres e/ou dentro do CE têm uma chance menor de ter automóvel. Um domicílio que recebe incremento de 1% na sua renda tem um aumento de 6,74% na chance de ter um carro. Similarmente, um domicílio com criança tem um aumento de 1,33 na chance de ter um carro em relação a um domicílio sem criança.\n\n\n\n\n\nA interpretação dos coeficientes de uma regressão Probit não é tão imediato. Vale notar que o sinal de todos os coeficientes corrobora o modelo Logit: com exceção de “feminino”, todas as variáveis têm efeito positivo, isto é, aumentam a probabilidade do domicílio possuir um automóvel.\nO gráfico abaixo simula a probabilidade esperada de um domicílio de Pinheiros chefiado por um indivíduo de 35 a 44 anos possuir um automóvel, condicional a alguns fatores:\n\n\n\nCriança. Se a família possui pelo menos um filho no domicílio.\nEnsino Universitário. Se a pessoa responsável pelo domicílio possui ensino supeior.\nRenda. O nível de renda domiciliar (ajustado pelo IPC-Fipe).\n\n\n\nEm todos os casos analisados, ter um filho aumenta a probabilidade de ter um automóvel, mas este efeito varia de acordo com a renda."
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#footnotes",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#footnotes",
    "title": "Carros e Renda em Sao Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIsto é equivalente a tomar a exponencial do coeficiente. Isto é, ao invés de mostrar \\(\\beta\\) mostra-se \\(exp(\\beta)\\).↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-carros-renda/index.html#referências",
    "href": "posts/general-posts/2023-12-carros-renda/index.html#referências",
    "title": "Carros e Renda em Sao Paulo",
    "section": "",
    "text": "Cornut, B., & Madre, J. L. (2017). A longitudinal perspective on car ownership and use in relation with income inequalities in the Paris metropolitan area. Transport Reviews, 37(2), 227-244.\nDargay, J. M. (2001). The effect of income on car ownership: evidence of asymmetry. Transportation Research Part A: Policy and Practice, 35(9), 807-821.\nDargay, J., & Gately, D. (1999). Income’s effect on car and vehicle ownership, worldwide: 1960–2015. Transportation Research Part A: Policy and Practice, 33(2), 101-138.\nNolan, A. (2010). A dynamic analysis of household car ownership. Transportation research part A: policy and practice, 44(6), 446-455."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html",
    "title": "O novo tidyverse: mutate",
    "section": "",
    "text": "O tidyverse é uma coleção poderosa de pacotes, voltados para a manipulação e limpeza de dados. Num outro post, discuti alguns aspectos gerais da filosofia destes pacotes que incluem a sua consistência sintática e o uso de pipes. A filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes” que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático.\nO tidyverse está em constante expansão, novas funcionalidades são criadas para melhorar a performance e capabilidade de suas funções. Assim, é importante atualizar nosso conhecimento destes pacotes periodicamente. Nesta série de posts vou focar nas funções principais dos pacotes dplyr e tidyr, voltados para a limpeza de dados."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#limpeza-de-dados",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#limpeza-de-dados",
    "title": "O novo tidyverse: mutate",
    "section": "",
    "text": "Dados tabulares são armazenados dentro de objetos chamados data.frame. Todo gráfico de ggplot começa com um (ou mais) data.frame. Apesar de ser possível montar gráficos a partir de vetores de dados dispersos, recomendo fortemente que sempre se utilize dados dentro de data.frame. Isto garante um código mais organizado e menos propenso a erros.\nApesar de funcionar como um repositório de pacotes, o R já vem com diversas funções “de fábrica” que permitem a importação e manipulação de dados. Estas funções que já vem carregadas no R são chamadas de funções “base” ou “base-R”. Alguns pacotes foram criados para melhorar estas funções “base”."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#o-básico",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#o-básico",
    "title": "O novo tidyverse: mutate",
    "section": "O básico",
    "text": "O básico\nOs pacotes utilizados neste tutorial são listados abaixo.\n\nlibrary(dplyr)\nlibrary(readr)\n\nPara praticar as funções vamos utilizar uma tabela que traz informações sobre as cidades do Brasil.\n\ntbl &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/cities_brazil.csv\"\n  )\n\nA função mutate serve para criar novas colunas a partir de colunas pré-existentes. Ela segue a seguinte sintaxe:\nmutate(coluna_nova = f(coluna_velha))\nonde f() designa algum transformação que é feita sobre os dados antigos. Em geral, esta transformação é alguma operação matemática (+, -, log, etc.), transformação de classe (e.g. as.numeric), ou função em geral. Abaixo mostra-se alguns exemplos de transformações simples. Como as saídas ocupam muito espaço, vou omiti-las.\nA primeira linha cria uma coluna onde todas as entradas são iguais a 1. O segundo exemplo aplica a função log sobre a variável pib. O terceiro exemplo divide a coluna household por um milhão. Por fim, o quarto exemplo mostra como dropar uma coluna usando a função mutate.\n\n#&gt; Cria uma coluna onde todas as entradas são iguais a 1\nmutate(tbl, id = 1)\n#&gt; Cria a coluna 'lpib' igual ao logaritmo natural do 'pib'\nmutate(tbl, lpib = log(pib))\n#&gt; Cria a coluna hh igual a 'household' dividido por 1 milhão\nmutate(tbl, hh = household / 1e6)\n#&gt; Dropa a coluna pib\nmutate(tbl, pib = NULL)\n\nVale notar que, assim como a função filter é mais eficiente juntar todas as operações dentro de um único mutate:\n\ntbl |&gt; \n  mutate(\n    id = 1,\n    lpib = log(pib),\n    hh = household / 1e6,\n    pib = NULL\n  )\n\nUm fato conveniente da função mutate é que ela vai criando as colunas sequencialmente, assim é possível fazer diversas transformações numa mesma chamada à função. No caso abaixo, pode-se criar a variável lpibpc a partir das colunas lpib e lpop; similarmente, pode-se criar a coluna lpibs a partir de pibserv, que é a soma de pib_services e pib_govmt_services\n\ntbl |&gt; \n  mutate(\n    lpib = log(pib),\n    lpop = log(population),\n    #&gt; Criando uma variável a partir de duas colunas criadas anteriormente\n    lpibpc = lpib - lpop,\n    pibserv = pib_services + pib_govmt_services,\n    #&gt; Criando uma variável a partir de duas colunas criadas anteriormente\n    lpibs = log(pibserv)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#grupos",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#grupos",
    "title": "O novo tidyverse: mutate",
    "section": "Grupos",
    "text": "Grupos\nA expressão mutate sempre é aplicada dentro de grupos. No caso em que não existe um grupo, a expressão é aplicada para todos os dados disponíveis. O código abaixo, por exemplo, encontra a participação percentual do PIB de cada município no PIB brasileiro.\n\ntbl |&gt; \n  select(name_muni, abbrev_state, pib) |&gt; \n  mutate(pib_share = pib / sum(pib) * 100) |&gt; \n  arrange(desc(pib_share))\n\n# A tibble: 5,570 × 4\n   name_muni      abbrev_state       pib pib_share\n   &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 São Paulo      SP           748759007     9.84 \n 2 Rio de Janeiro RJ           331279902     4.35 \n 3 Brasília       DF           265847334     3.49 \n 4 Belo Horizonte MG            97509893     1.28 \n 5 Manaus         AM            91768773     1.21 \n 6 Curitiba       PR            88308728     1.16 \n 7 Osasco         SP            76311814     1.00 \n 8 Porto Alegre   RS            76074563     1.00 \n 9 Guarulhos      SP            65849311     0.865\n10 Campinas       SP            65419717     0.860\n# ℹ 5,560 more rows\n\n\nJá este segundo código encontra a participação percentual do PIB de cada município dentro do seu respectivo estado.\n\ntbl |&gt; \n  select(name_muni, abbrev_state, pib) |&gt; \n  mutate(pib_share = pib / sum(pib) * 100, .by = \"abbrev_state\") |&gt; \n  arrange(desc(pib_share))\n\n# A tibble: 5,570 × 4\n   name_muni      abbrev_state       pib pib_share\n   &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 Brasília       DF           265847334     100  \n 2 Manaus         AM            91768773      79.1\n 3 Boa Vista      RR            11826207      73.8\n 4 Macapá         AP            11735557      63.5\n 5 Rio Branco     AC             9579592      58.1\n 6 Rio de Janeiro RJ           331279902      43.9\n 7 Fortaleza      CE            65160893      39.0\n 8 Teresina       PI            21578875      38.3\n 9 Porto Velho    RO            19448762      37.7\n10 Aracaju        SE            16447105      36.2\n# ℹ 5,560 more rows\n\n\nVale notar que a sintaxe .by = \"coluna\" é nova e ainda está em fase experimental. Ela substitui a sintaxe mais antiga do group_by. O código acima é equivalente ao código abaixo.\n\ntbl |&gt; \n  select(name_muni, abbrev_state, pib) |&gt; \n  group_by(abbre_state) |&gt; \n  mutate(pib_share = pib / sum(pib) * 100) |&gt; \n  ungroup()\n\nUma das vantagens de usar .by é que não é necessário usar ungroup já que os dados são desagrupados automaticamente."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#transformando-múltiplas-colunas",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#transformando-múltiplas-colunas",
    "title": "O novo tidyverse: mutate",
    "section": "Transformando múltiplas colunas",
    "text": "Transformando múltiplas colunas\nA função mutate tem um par importante na função across, que permite aplicar uma mesma função a múltiplas colunas com facilidade. Imagine o seguinte caso, onde quer-se aplicar a função scale, que serve para “normalizar” vetores numéricos, em todas as colunas de uma base. Tipicamente, seria necessário escrever e nomear cada coluna\n\ntbl |&gt; \n  mutate(\n    scaled_pib = scale(pib),\n    scaled_pop = scale(population),\n    scaled_agriculture = scale(pib_agriculture),\n    scaled_industrial = scale(pib_industrial),\n    ...\n  )\n\nEm linhas gerais, o resultado do código acima pode ser replicado simplesmente com:\n\ntbl |&gt; \n  mutate(\n    across(where(is.numeric), scale)\n  )\n\nA função across serve para aplicar uma função sobre um subconjunto de colunas seguindo: across(colunas, funcao). Ela funciona com os tidyselectors1, facilitando a seleção de colunas a ser transformadas. Funções mais complexas podem ser utilizadas via função anônima usando o operador ~2.\nO primeiro exemplo abaixo mostra como aplicar a função log em todas as colunas cujo nome começa com pib. Já o segundo exemplo mostra como converter todas as colunas do tipo character para factor. O terceiro exemplo mostra como converter as colunas de factor para numeric utilizando o operador ~. Os últimos dois exemplos mostram outras aplicações do mesmo operador.\n\n#&gt; Aplica uma transformação log em todas as colunas que começam com pib\nmutate(tbl, across(starts_with(\"pib\"), log))\n#&gt; Converte todas as colunas de strings para factors\nmutate(tbl, across(where(is.character), as.factor))\n#&gt; Converte as colunas de factors para numeric\nmutate(tbl, across(where(is.factor), ~ as.numeric(as.character(.x))))\n#&gt; Divide por pib e multiplica por 100 todas as colunas entre pib_taxes e\n#&gt; pib_govmt_services\nmutate(tbl, across(pib_taxes:pib_govmt_services, ~.x / pib * 100))\n#&gt; Normaliza todas as colunas numéricas\nmutate(tbl, across(where(is.numeric), ~ as.numeric(scale(.x))))\n\nPor fim, existe um argumento opcional .names que permite renomear as novas colunas usando uma sintaxe estilo glue3. Esta sintaxe tem dois tipos especiais importantes: {.col}, que faz referência ao nome original da coluna, e {.fn}, que faz referência ao nome da função utilizada. O exemplo abaixo refina o primeiro caso que vimos acima. Agora aplica-se a função as.numeric(scale(x)) sobre cada uma das colunas numéricas. As novas colunas têm o nome \"scaled_NomeOriginalDaColuna\".\n\ntbl |&gt; \n  mutate(\n    across(\n      where(is.numeric),\n      ~ as.numeric(scale(.x)),\n      .names = \"scaled_{.col}\"\n      )\n  )\n\nO tipo especial {.fn} é bastante útil com a função summarise, que permite aplicar uma lista de múltiplas funções simultaneamente. Ainda assim, é possível utilizá-lo com a função mutate. A sintaxe tem de ser adaptada, pois {.fn} espera que a função tenha sido passada como uma lista com nomes. No exemplo abaixo, aplica-se a função log sobre todas as colunas númericas e as colunas resultantes são renomeadas. Vale notar que, na maioria dos casos, não vale a pena utilizar {.fn} no contexto do mutate.\n\ntbl |&gt; \n  mutate(\n    across(\n      where(is.numeric),\n      list(\"ln\" = log),\n      .names = \"{.fn}_{.col}\"\n      )\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#outros-argumentos",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#outros-argumentos",
    "title": "O novo tidyverse: mutate",
    "section": "Outros argumentos",
    "text": "Outros argumentos\nA função mutate tem alguns outros argumentos, de uso diverso. Os argumentos .before e .after permitem selecionar a posição das novas colunas. O padrão da função é de sempre adicionar as novas colunas ao final do data.frame. Estes argumentos aceitam o nome de alguma das colunas ou mesmo funções tidyselect. No caso abaixo, cria-se a coluna lpib que é posta no início do data.frame.\n\ntbl |&gt; \n  mutate(\n    lpib = log(pib),\n    .before = everything()\n  ) |&gt; \n  select(1:5)\n\n# A tibble: 5,570 × 5\n    lpib code_muni name_muni             code_state name_state\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;     \n 1  13.3   1100015 Alta Floresta D'Oeste         11 Rondônia  \n 2  14.9   1100023 Ariquemes                     11 Rondônia  \n 3  12.0   1100031 Cabixi                        11 Rondônia  \n 4  14.7   1100049 Cacoal                        11 Rondônia  \n 5  13.3   1100056 Cerejeiras                    11 Rondônia  \n 6  12.8   1100064 Colorado do Oeste             11 Rondônia  \n 7  12.5   1100072 Corumbiara                    11 Rondônia  \n 8  12.5   1100080 Costa Marques                 11 Rondônia  \n 9  13.4   1100098 Espigão D'Oeste               11 Rondônia  \n10  13.8   1100106 Guajará-Mirim                 11 Rondônia  \n# ℹ 5,560 more rows\n\n\nO outro argumento opcional é o .keep que permite controlar quais colunas devem ser preservadas após a aplicação da função mutate. O padrão da função, naturalmente, é de preservar todas as colunas, isto é, .keep = \"all\". Contudo, pode-se usar .keep = \"used\" para manter somente as colunas que foram utilizadas.\n\ntbl |&gt; \n  mutate(\n    code_muni = as.character(code_muni),\n    lpib = log(pib),\n    .keep = \"used\"\n    )\n\n# A tibble: 5,570 × 3\n   code_muni     pib  lpib\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 1100015    570272  13.3\n 2 1100023   2818049  14.9\n 3 1100031    167190  12.0\n 4 1100049   2519353  14.7\n 5 1100056    600670  13.3\n 6 1100064    366931  12.8\n 7 1100072    268381  12.5\n 8 1100080    261978  12.5\n 9 1100098    666331  13.4\n10 1100106    984586  13.8\n# ℹ 5,560 more rows\n\n\nVale notar que .keep = \"used\" sempre preserva as colunas “agrupadoras”.\n\ntbl |&gt; \n  mutate(\n    code_muni = as.character(code_muni),\n    lpib = log(pib),\n    .by = \"code_state\",\n    .keep = \"used\"\n    )\n\n# A tibble: 5,570 × 4\n   code_muni code_state     pib  lpib\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 1100015           11  570272  13.3\n 2 1100023           11 2818049  14.9\n 3 1100031           11  167190  12.0\n 4 1100049           11 2519353  14.7\n 5 1100056           11  600670  13.3\n 6 1100064           11  366931  12.8\n 7 1100072           11  268381  12.5\n 8 1100080           11  261978  12.5\n 9 1100098           11  666331  13.4\n10 1100106           11  984586  13.8\n# ℹ 5,560 more rows"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#funções-úteis",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#funções-úteis",
    "title": "O novo tidyverse: mutate",
    "section": "Funções úteis",
    "text": "Funções úteis\nAbaixo segue uma lista de funções úteis.\n\ntbl |&gt; \n  mutate(\n    #&gt; Cria um ranking da variável\n    rank_pib = rank(pib),\n    #&gt; Cria um rakning (em percentil) da variável\n    rank_perc_pib = percent_rank(pib),\n    #&gt; Agrupa em decis \n    group_decile_pib = ntile(pib, 10),\n    #&gt; Cria um id\n    id = row_number(),\n    #&gt; Aplica uma transformação condicional a uma condição lógica\n    lpib = ifelse(pib &gt; 0, log(pib), 1),\n    #&gt; Aplica uma transformação condicional a múltiplas condições lógicas\n    type = case_when(\n      code_state %in% c(11, 12, 13) ~ \"grupo_1\",\n      code_state %in% c(14, 15, 16) ~ \"grupo_2\",\n      TRUE ~ \"outros\"\n    ),\n    #&gt; Soma cumulativa\n    spib = cumsum(pib),\n    #&gt; Diferença percentual usando o valor imediatamente anterior\n    diff_pib = pib / lag(pib) - 1,\n    #&gt; Participação relativa da variável\n    share_pib = pib / sum(pib, na.rm = TRUE) * 100,\n    #&gt; Normalizar variável\n    scaled_pib = as.numeric(scale(pib))\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-mutate/index.html#footnotes",
    "href": "posts/general-posts/2024-01-tidyverse-mutate/index.html#footnotes",
    "title": "O novo tidyverse: mutate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara mais detalhes sobre os tidyselectors veja o post sobre a função select.↩︎\nA função across foi uma mudança significativa de paradigma na evolução do dplyr. Esta função tornou obsoletas diversas funções que eram distinguidas pelos sufixos (_at, _all, _if). Para mais detalhes veja o post do blog do dplyr.↩︎\nDo pacote glue. Veja mais aqui.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-10-sazonalidade/index.html",
    "href": "posts/general-posts/2023-10-sazonalidade/index.html",
    "title": "Sazonalidade",
    "section": "",
    "text": "Séries de tempo costumam exibir alguns padrões similares. Uma abordagem particularmente útil é de decompor uma série de tempo em componentes, que representam características específicas. Pode-se pensar numa série de tempo como uma conjunção de três componentes:\n\nTendência\nSazonalidade\nResíduo (resto, ruído, etc.)\n\nEm geral, a tendência varia pouco no tempo, segue algum ciclo longo, como a tendência de crescimento do PIB de um país. O movimento sazonal de uma série reflete algum tipo de variação períodica. Muitas séries possuem sazonalidade, como na demanda de sorvetes ao longo do ano, o número diário de acidentes de trânsito a cada semana, a temperatura média ao longo do dia, etc.\nEntender a sazonalidade de um fenômeno é essencial para a sua modelagem estatística. Vale notar que, usualmente, o interesse de economistas e econometristas não está na sazonalidade em si, mas sim na série dessazonalizada, isto é, livre de qualquer sazonalidade. Esta abordagem enfatiza mais a busca pela tendência “limpa” da série do que pelo efeito sazonal.\nA sazonalidade se expressa em várias frequências, mas a ênfase deste post será em sazonalidades mensais e trimestrais. Sazonalidades complexas (mistas, por exemplo) ou de alta frequência (intradiária, diária, semanal), em geral, exigem um pouco mais de esforço no setup e pacotes específicos1."
  },
  {
    "objectID": "posts/general-posts/2023-10-sazonalidade/index.html#testando-sazonalidade",
    "href": "posts/general-posts/2023-10-sazonalidade/index.html#testando-sazonalidade",
    "title": "Sazonalidade",
    "section": "Testando sazonalidade",
    "text": "Testando sazonalidade\nExistem testes estatísticos formais para verificar a presença de sazonalidade em uma série. Pessoalmente, foram raras as ocasiões em que vi alguém testando a presença de sazonalidade; em geral, a inspeção visual é suficiente. Um teste bastante simples é verificar se coeficientes das dummies sazonais, dos modelos de regressão acima, são conjuntamente iguais a zero, isto é:\n\\[\nH_{o} = \\alpha_{1} = \\alpha_{2} = \\dots = \\alpha_{11} = 0\n\\]\nNo caso em que a hipótese nula é rejeitada, tem-se evidência de sazonalidade na série.\nHá também dois possíveis testes não-paramétricos: (1) Teste de Kruskal-Wallis; e (2) Teste de Friedman. O teste de Kruskal-Wallis verifica se amostras distintas foram geradas pela mesma distribuição. No caso de séries de tempo, podemos adaptar este teste. No caso de uma série mensal, cada ano fornece 12 observações e queremos verificar se existe uma diferença entre os anos. Assim temos k amostras (k anos) de 12 observações.\n\ndf = data.frame(\n  date = as.Date.ts(ipi),\n  value = as.numeric(ipi)\n)\n\ntab = df |&gt; \n  dplyr::filter(!is.na(value)) |&gt; \n  dplyr::mutate(\n    rank = rank(value, ties.method = \"average\"),\n    mes = lubridate::month(date),\n    ano = lubridate::year(date)\n    )\n\ntab |&gt; \n  tidyr::pivot_wider(\n    id_cols = \"ano\",\n    names_from = \"mes\",\n    values_from = \"rank\",\n    names_sort = TRUE\n  )\n\n# A tibble: 12 × 13\n     ano   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2012 113     115 131     117 132   121   129   139   128.  140.  130   110  \n 2  2013 118     109 125     135 133   124   138   140.  137   142   134   103  \n 3  2014 111     112 120     116 122   108   123   128.  126   136   119    90  \n 4  2015 100      92 114     102 101    91   104.  104.   95   106    78     7  \n 5  2016  19      32  74      51  66.5  57    72.5  86    45    46    34     3  \n 6  2017  11.5     8  54       9  36.5  28.5  39    75    43.5  56    47    11.5\n 7  2018  20      14  40.5    31  15    40.5  52    82.5  48    80.5  42     6  \n 8  2019  25      27  28.5    33  60    30    71    80.5  60    82.5  49     5  \n 9  2020  23      24  17.5     1   2    22    89    93    99   107    97    64  \n10  2021  69      65  96      76  88    84    94    98    85    79    66.5  16  \n11  2022  17.5    26  62.5    35  72.5  50    68    87    54    70    36.5   4  \n12  2023  10      13  58      21  60    43.5  62.5  77    38    54    NA    NA  \n\n\nA hipótese nula do teste é de que as diferentes amostras foram geradas pela mesma distribuição. Como se vê abaixo, o teste é rejeitado, implicando que há presença de sazonalidade.\nVale notar que o teste tem duas hipóteses bastante frágeis no contexto de séries de tempo. Primeiro, o teste supõe que as observações dentro de cada grupo são independentes, isto é, de que as observações de cada mês (janeiro, fevereiro, etc.) são indepedentes. Segundo, o teste supõe que as amostras (anos) são independentes entre si.\n\nkruskal.test(tab$rank, tab$ano)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tab$rank and tab$ano\nKruskal-Wallis chi-squared = 97.133, df = 11, p-value = 6.589e-16\n\n\nO teste de Friedman é muito similar, mas não exige independência das observações dentro de cada grupo (mês). O código abaixo executa o teste. Note que é preciso remover os anos incompletos, pois o teste somente funciona quando as amostras tem tamanho igual. Novamente, rejeita-se a hipótese nula, sugerindo que existe um padrão sazonal na série.\n\ntab = df |&gt; \n  dplyr::mutate(\n    mes = lubridate::month(date),\n    ano = lubridate::year(date)\n    ) |&gt; \n  dplyr::filter(!is.na(value), ano &gt; 2012, ano &lt; 2023) |&gt; \n  dplyr::mutate(rank = rank(value), .by = \"ano\")\n\nfriedman.test(tab$rank, tab$mes, tab$ano)\n\n\n    Friedman rank sum test\n\ndata:  tab$rank, tab$mes and tab$ano\nFriedman chi-squared = 73.326, df = 11, p-value = 2.837e-11"
  },
  {
    "objectID": "posts/general-posts/2023-10-sazonalidade/index.html#stl",
    "href": "posts/general-posts/2023-10-sazonalidade/index.html#stl",
    "title": "Sazonalidade",
    "section": "STL",
    "text": "STL\nA decomposição STL é mais sofisticada do que a decomposição clássica. A metodologia STL foi apresentada no influente artigo STL: A Seasonal-Trend Decomposition Procedure Based on Loess dos autores Robert Cleveland, William Cleveland, Jean McRae e Irma Terpenning. Assim como a decomposição clássica, a decomposição STL divide uma série em três componentes: um componente de tendência (trend), uma componente sazonal (seasonal) e um componente aleatório (remainder).\nO STL foi feito para ser um método versátil, resistente a outliers e eficiente (do ponto de vista computacional). Tipicamente, a decomposição STL funciona com qualquer série (mesmo quando há observações ausentes) independentemente da sua frequência. Para modelar a tendência e sazonalidade da série, o STL usa uma regressão LOESS. Além de ser mais flexível do que a média móvel, que vimos acima, a regressão LOESS não perde observações da série.\nO código abaixo mostra como calcular a decomposição STL e apresenta os resultados visualmente usando a série co2 que vem pré-carregada no R. Esta série é similar a uma das utilizadas no artigo original.\n\nstl_13 &lt;- stl(co2, s.window = 13)\n\nautoplot(stl_13) + theme_series\n\n\n\n\n\n\n\n\nA função stl tem diversos parâmetros de “suaviazação”, que servem para escolher o tamanho e intensidade dos ciclos de tendência e sazonalidade. Via de regra quanto maiores os valores dos argumentos x.window mais suave será o ajuste final. Outro argumento potencialmente útil é definir robust = TRUE quando a série possui outliers.\nNo presente contexto, o argumento mais relevante do comando stl é o s.window ou \\(n_{(s)}\\) na nomenclatura do artigo original. Este parâmetro, em linhas gerais, define o grau de suaviazação da tendência sazonal. Ele deve ser um número ímpar e pelo menos igual a 7. O artigo original sugere uma ferramenta visual para escolher o parâmetro, mas concede que a escolha final é arbitrária e depende da sensibilidade do usuário.\nO gráfico apresentado é o seasonal-diagnostic plot. As linhas mostram a “intensidade” do efeito sazonal e os pontos indicam como este efeito sazonal varia com o ruído aleatório. Note como as variações da curva coincidem com as variações dos pontos, sugerindo que as oscilações sazonais estão sendo afetadas pelo ruído da série.\n\n\nCode\ncomponents &lt;- stl_13$time.series\n\ndat = data.frame(\n  date = as.Date.ts(components),\n  coredata(components)\n)\n\ndat$month = lubridate::month(dat$date, label = TRUE, locale = \"pt_BR\")\ndat$year = lubridate::year(dat$date)\n\ndat &lt;- dat |&gt; \n  mutate(s_avg = mean(seasonal), .by = \"month\") |&gt; \n  mutate(s1 = seasonal - s_avg, s2 = seasonal - s_avg + remainder)\n\nggplot(dat) +\n  geom_line(aes(x = year, y = s1), lwd = 0.8) +\n  geom_point(aes(x = year, y = s2), size = 0.8, shape = 21) +\n  facet_wrap(vars(month)) +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico abaixo repete o mesmo exercício mas utilizando s.window = 35 para suavizar a série original. Note como as curvas estão mais suaves.\n\n\nCode\nstl_35 &lt;- stl(co2, s.window = 35)\ncomponents &lt;- stl_35$time.series\n\ndat = data.frame(\n  date = as.Date.ts(components),\n  coredata(components)\n)\n\ndat$month = lubridate::month(dat$date, label = TRUE, locale = \"pt_BR\")\ndat$year = lubridate::year(dat$date)\n\ndat &lt;- dat |&gt; \n  mutate(s_avg = mean(seasonal), .by = \"month\") |&gt; \n  mutate(s1 = seasonal - s_avg, s2 = seasonal - s_avg + remainder)\n\nggplot(dat) +\n  geom_line(aes(x = year, y = s1), lwd = 0.8) +\n  geom_point(aes(x = year, y = s2), size = 0.8, shape = 21) +\n  facet_wrap(vars(month)) +\n  theme_series\n\n\n\n\n\n\n\n\n\nA função forecast::mstl oferece uma opção menos manual do ajuste STL. Esta função executa seis janelas distintas para s.window iterativamente. Os resultados costumam ser satisfatórios, mas como de costume, é necessário revisar o ajuste final.\n\nstl_auto = mstl(co2, lambda = \"auto\")\nautoplot(stl_auto)"
  },
  {
    "objectID": "posts/general-posts/2023-10-sazonalidade/index.html#x13-arima",
    "href": "posts/general-posts/2023-10-sazonalidade/index.html#x13-arima",
    "title": "Sazonalidade",
    "section": "X13-ARIMA",
    "text": "X13-ARIMA\nO X13-ARIMA ou X13-ARIMA-SEATS é a versão mais recente do X11-ARIMA, uma metodologia desenvolvida pelo US Census Bureau para lidar com a sazonalidade de séries de tempo. Este método foi pensado para lidar com o tipo de sazonalidade comumemente encontrada em séries econômicas. Os métodos de ajuste são implementados no R via o pacote seasonal.\nA função principal do pacote seas realiza um ajuste sazonal automático. Para verificar os principais resultados do ajuste usa-se a função summary. O ajuste automático costuma ser bom, mas é sempre necessário revisar o ajuste.\n\n#&gt; Consumo mensal de energia elétrica - Residencial\nenerg = rbcb::get_series(1403, as = \"ts\", start_date = as.Date(\"2002-01-01\"))\n#&gt; Executa a rotina do X13-ARIMA\nsenerg = seas(energ)\n#&gt; Resumo dos resultados\nsummary(senerg)\n\n\nCall:\nseas(x = energ)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \nMon               -0.003563   0.002277  -1.565 0.117624    \nTue               -0.005363   0.002274  -2.358 0.018350 *  \nWed               -0.001028   0.002238  -0.459 0.646059    \nThu                0.007674   0.002255   3.404 0.000665 ***\nFri                0.004144   0.002226   1.862 0.062633 .  \nSat               -0.000279   0.002237  -0.125 0.900713    \nEaster[15]        -0.028472   0.004792  -5.942 2.82e-09 ***\nLS2002.Apr         0.075729   0.016719   4.529 5.91e-06 ***\nAO2014.Feb         0.078577   0.015565   5.048 4.46e-07 ***\nLS2015.Mar        -0.071656   0.014564  -4.920 8.65e-07 ***\nMA-Nonseasonal-01  0.451032   0.060950   7.400 1.36e-13 ***\nMA-Nonseasonal-02  0.196966   0.062239   3.165 0.001553 ** \nMA-Seasonal-12     0.740178   0.049735  14.882  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 2)(0 1 1)  Obs.: 263  Transform: log\nAICc:  3358, BIC:  3405  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 12.63   Shapiro (normality): 0.9962  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nO gráfico abaixo mostra a série original e a série sazonalmente ajustada em vermelho. Para extrair a série ajustada usa-se a função final().\n\nautoplot(energ) +\n  autolayer(final(senerg)) +\n  guides(color = \"none\") +\n  theme_series\n\n\n\n\n\n\n\n\nO pacote seasonal é muito bem documentado e inclui um artigo de apresentação com exemplos, um texto mostrando como rodar os exemplos oficiais do X13 dentro do R e até uma ferramenta interativa. Neste post não vou explorar todas as nuances do pacote.\nPara conseguir um ajuste mais adequado é preciso explorar os argumentos adicionais da função seas além de adaptar as variáveis de calendário. Na sua configuração padrão, a função seas não considera o efeito do carnaval, por exemplo. É possível criar eventos de calendário usando a função genhol (de “generate holiday”).\nUm recurso bastante útil para conseguir as datas das festividades brasileiras é o site do prof. Roberto Cabral de Mello Borges. O código abaixo extrai a tabela com as datas da Páscoa, Carnaval e Corpus Christi de 1951-2078.\n\n\nCode\nlibrary(rvest)\nurl = \"https://www.inf.ufrgs.br/~cabral/tabela_pascoa.html\"\ntabela = url |&gt; \n  read_html() |&gt; \n  html_table()\ntab = tabela[[1]]\n\nferiados_bra = data.frame(tab[-1, ])\nnames(feriados_bra) = as.character(tab[1, ])\n\nhead(feriados_bra)\n\n\n   Ano  a b c  d e d+e Dia   Mês      Páscoa    Carnaval Corpus Christi\n1 1951 13 3 5  1 2   3  25 Março 25/mar/1951 06/fev/1951    24/mai/1951\n2 1952 14 0 6 20 2  22  13 Abril 13/abr/1952 26/fev/1952    12/jun/1952\n3 1953 15 1 0  9 5  14   5 Abril 05/abr/1953 17/fev/1953    04/jun/1953\n4 1954 16 2 1 28 6  34  18 Abril 18/abr/1954 02/mar/1954    17/jun/1954\n5 1955 17 3 2 17 2  19  10 Abril 10/abr/1955 22/fev/1955    09/jun/1955\n6 1956 18 0 3  6 4  10   1 Abril 01/abr/1956 14/fev/1956    31/mai/1956\n\n\nPara utilizar este dado no X13-ARIMA é necessário converter a coluna Carnaval num tipo Date e então utilizar a função genhol. Para facilitar a leitura das datas uso o pacote readr.\n\nlibrary(readr)\n\nferiados_bra$date_carnaval = parse_date(\n  feriados_bra$Carnaval,\n  format = \"%d/%b/%Y\",\n  locale = locale(\"pt\")\n  )\n\ncarnaval = genhol(feriados_bra$date_carnaval, start = -3, end = 1, frequency = 12)\n\nAs datas de carnaval são inseridas dentro do X13-ARIMA via o argumento xreg.\n\nsenerg = seas(\n  energ,\n  xreg = carnaval,\n  regression.variables = \"td1coef\",\n  arima.model = c(0, 1, 1, 0, 1, 1)\n)\n\nsummary(senerg)\n\n\nCall:\nseas(x = energ, xreg = carnaval, regression.variables = \"td1coef\", \n    arima.model = c(0, 1, 1, 0, 1, 1))\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \nxreg              -0.027410   0.006868  -3.991 6.58e-05 ***\nEaster[15]        -0.043053   0.006399  -6.728 1.71e-11 ***\nLS2015.Mar        -0.079565   0.016364  -4.862 1.16e-06 ***\nMA-Nonseasonal-01  0.613688   0.048339  12.696  &lt; 2e-16 ***\nMA-Seasonal-12     0.720574   0.048169  14.959  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 263  Transform: log\nAICc:  3415, BIC:  3436  QS (no seasonality in final):5.775 .\nBox-Ljung (no autocorr.): 108.7 *** Shapiro (normality): 0.9934  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nO gráfico abaixo mostra o resulado do ajuste com as datas de feriado modificadas.\n\nautoplot(energ) +\n  autolayer(final(senerg)) +\n  guides(color = \"none\") +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2023-10-sazonalidade/index.html#sarima",
    "href": "posts/general-posts/2023-10-sazonalidade/index.html#sarima",
    "title": "Sazonalidade",
    "section": "SARIMA",
    "text": "SARIMA\nOs modelos SARIMA incluem um componente de “sazonalidade estocástica” nos modelos ARIMA. Já escrevi um post onde detalho melhor alguns aspectos teóricos deste tipo de modelo. Vale notar o modelo SARIMA não realiza uma decomposição de tendência e sazonalidade com visto acima. Quando se aplica um SARIMA, implicitamente, supõe-se que a série possui uma tendência sazonal estocástica (i.e. raiz unitária sazonal) e não uma tendência sazonal determinística5.\nComo último exemplo vamos analisar a demanda mensal por gasolina. A série é da ANP e registra o total de vendas de gasolina C no Brasil em metros cúbicos.\n\ngasolina = read_csv(\"...\")\n\n\ngas = ts(na.omit(gasolina$demand), start = c(2012, 1), frequency = 12)\nautoplot(gas) + theme_series\n\n\n\n\n\n\n\n\nA série parece exibir algum componente de sazonalidade. Parece haver um pico de consumo todo mês de dezembro e uma queda todo mês de fevereiro.\n\nggseasonplot(gas) +\n  scale_color_viridis_d() +\n  theme_series\n\n\n\n\n\n\n\n\nA escolha da ordem do modelo SARIMA exige a inspeção visual do correlograma da série. Para mais detalhes consulte meu post mais detalhado sobre o assunto. Aqui, por simplicidade, uso um modelo simples, que costuma funcionar bem para série em geral.\n\nsarima_model = Arima(log(gas), order = c(0, 1, 1), seasonal = c(0, 1, 1))\n\nsarima_model\n\nSeries: log(gas) \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.2719  -0.7227\ns.e.   0.0973   0.0972\n\nsigma^2 = 0.003158:  log likelihood = 179.5\nAIC=-353.01   AICc=-352.81   BIC=-344.52\n\n\nUm dos pontos fortes dos modelos SARIMA é de gerar boas previsões de curto prazo com grande facilidade. Como comentei anteriormente, não há uma interpretação simples para a sazonalidade neste tipo de abordagem.\n\nautoplot(forecast(sarima_model), include = 48) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2023-10-sazonalidade/index.html#footnotes",
    "href": "posts/general-posts/2023-10-sazonalidade/index.html#footnotes",
    "title": "Sazonalidade",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara uma referência para séries com sazonalidade complexa no R veja Hyndman e Athansopoulos (2021) Forecasting: Principles and Practice.↩︎\nPode-se usar um polinômio de qualquer grau, mas polinômios de ordens muito elevadas costumam se ajustar “perfeitamente” aos dados e vão absorver toda a sazonalidade da série.↩︎\nSempre coloca-se uma variável binária a menos do que períodos sazonais pela questão do posto da matriz de regressores. Na prática, se houvesse uma dummy para cada período sazonal a matriz de regressão seria uma matriz identidade.↩︎\nÉ comum ver esta expressão nos textos de séries de tempo; em geral o termo é utilizado em contraste com modelos SARIMA onde a sazonalidade é estocástica, mas o termo “determinístico” não tem implicação causal. Na prática, quer dizer que a sazonalidade não varia no tempo e é sempre a mesma o que pode gerar previsões ruins a depender do caso.↩︎\nPara uma boa apresentação sobre raiz unitária e sobre tendências determinísticas veja Enders (2009) Applied Econometric Time Series.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "O Gradient Descent (GD), também conhecido como steepest descent ou até como Stochastic Gradient Descent (SGD)1, é um algoritmo fundamental em otimização que desempenha um papel crucial no ajuste de parâmetros de modelos de machine learning. Basicamente, o GD utiliza a informação do gradiente de uma função, que se quer otimizar, para encontrar o “caminho” em que ela decaí mais rapidamente.\nNo caso mais típico, da minimização de uma função de perda, o GD ou SGD usa o gradiente da função para encontrar os valores dos parâmetros que minimizam esta função. O algoritmo opera iterativamente e, idealmente, aproxima-se gradualmente da solução correta.\nAssim, a utilidade geral do GD reside na sua capacidade de iterativamente encontrar o mínimo de funções usando relativamente pouca informação.\n\n\n\n\nPara tornar as coisas mais concretas vamos considerar o caso de uma regressão linear simples. Temos uma variável de resposta \\(y\\) que será “explicada” por uma variável independente (feature) \\(x\\) . A relação é modelada de maneira linear como:\n\\[\ny = \\beta_{0} + \\beta_{1}x + \\varepsilon\n\\]\nA equação acima descreve o nosso modelo, que será estimado posteriormente. Com este modelo podemos calcular os valores preditos \\(\\hat{y_{i}}\\) que serão comparados com os valores observados (reais) \\(y_{i}\\). Nosso objetivo é ajustar os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\) para minimizar a função de custo, que pode ser representada como o Erro Quadrático Médio (MSE, do inglês Mean Squared Error):\n\\[\n\\text{MSE} =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_{i}})^2= \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 X_i))^2\n\\]\nPara atualizar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) usando GD, precisamos calcular o gradiente da função de custo em relação a esses parâmetros. A primeira derivada da função de custo em relação a \\(\\beta_0\\) e \\(\\beta_1\\) nos dá as direções em que devemos ajustar esses parâmetros para minimizar a função de custo.\nA primeira derivada da função de custo em relação a \\(\\beta_0\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nE a primeira derivada da função de custo em relação a \\(\\beta_1\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_1} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nEssas derivadas nos fornecem a direção em que devemos ajustar os parâmetros para minimizar a função de custo.\n\n\n\nO algortimo de GD funciona iterativamente O algoritmo de SGD atualiza o parâmetro \\(\\beta^{t}\\) a cada iteração t, onde \\(\\beta^{0}\\) é dado, usando o gradiente \\(\\nabla_{\\beta}^t\\) da seguinte maneira:\n\\[\n\\beta^{t+1} = \\beta^{t} - \\gamma\\nabla_{\\beta}^{t}\n\\]\nonde \\(\\gamma\\) é um número real não-negativo, tipicamente próximo de 0.001, chamado “learning rate”. Quanto maior for o valor de \\(\\gamma\\) maiores serão os “passos” no processo de atualização; inversamente, quanto menor for o valor de \\(\\gamma\\) menores serão os “passos”no processo iterativo.\nPara implementar o passo-a-passo do algoritmo vamos usar a base mtcars. O primeiro passo é ajustar os dados usando a função scale. Em seguida, separamos alguns objetos úteis para facilitar a exposição.\n\n#&gt; Regularizar os dados\nmtcars_scaled &lt;- as.data.frame(scale(mtcars))\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\nVamos fazer a regressão de mpg (milhas por galão), uma medida da eficiência do veículo, contra wt (peso), o peso do veículo. Visualmente, parece haver uma relação linear decrescente entre as variáveis.\n\nplot(y ~ x)\n\n\n\n\nAntes de fazer o loop, vamos decompor o algoritmo em etapas. Primeiro, precisa-se de valores iniciais para os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\). Por simplicidade, vamos sortear números aleatórios entre 0 e 1 a partir de uma distribuição uniforme. Com estes valores será possível calcular o valor de \\(\\hat{y}_{i}^0\\), onde 0 indica que estamos na iteração de valor 0.\n\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\n(yhat &lt;- b0 + b1 * x)\n\n [1]  0.33357324  0.43639854  0.21260228  0.57349896  0.66422717  0.67229190\n [7]  0.71664792  0.56341804  0.54728858  0.66422717  0.66422717  0.91826617\n[13]  0.78116576  0.80132758  1.39408525  1.46424840  1.43239272  0.16421390\n[19] -0.07167945  0.01703258  0.27107158  0.69648609  0.66221099  0.82552177\n[25]  0.82753796  0.05735623  0.14001971 -0.11280958  0.55535331  0.39405871\n[31]  0.71664792  0.39809108\n\n\nNão é necessário, mas é instrutivo calcular a função de perda.\n\n(mse &lt;- mean((y - yhat)^2))\n\n[1] 2.112771\n\n\nAgora, calculamos o valor do gradiente neste ponto.\n\n(gb0 &lt;- sum(y - yhat) * (-2/N))\n\n[1] 1.148812\n\n(gb1 &lt;- sum((y - yhat) * x) * (-2/N))\n\n[1] 2.44553\n\n\nPor fim, o valor dos parâmetros é atualizado segundo a fórmula matemática do algoritmo. Utiliza-se \\(g = 0.01\\) como valor para a learning-rate.\n\ng = 0.01\n\n(b0_new &lt;- b0 - g * gb0)\n\n[1] 0.5629181\n\n(b1_new &lt;- b1 - g * gb1)\n\n[1] 0.3700945\n\n\nEste processo será repetido \\(T\\) vezes até que se atinja algum critério de convergência. Em geral, estabelece-se\n\nUm número máximo de iterações. (10.000 iterações, por exemplo).\nUm valor mínimo de mudança na estimativa dos parâmetros. Isto é, quando o valor das estimativas para de mudar significativamente, entende-se que ele convergiu para um valor satisfatório.\n\nPara deixar o loop abaixo mais simples, vou simplesmente estabelecer um número máximo de 5000 iterações. O código segue abaixo. Note que algumas partes do código acima foram repetidas por conveniência da leitura.\n\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\ng &lt;- 0.01\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nfor (i in seq_len(num_iterations)) {\n\n  if (i %% 100 == 0) cat(\"Iteração: \", i, \"\\n\")\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  if (i %% 100 == 0) {\n    cat(\"Valor da perda: \", as.numeric(mse), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2/N)\n  gb1 &lt;- sum((y - yhat) * x) * (-2/N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - g * gb0\n  b1_new &lt;- b1 - g * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n\n  if (i %% 100 == 0) {\n    cat(\"Betas: \", c(b0, b1), \"\\n\\n\")\n  }\n\n}\n\nIteração:  100 \nValor da perda:  0.2860836 \nBetas:  0.112221 -0.6852286 \n\nIteração:  200 \nValor da perda:  0.2403437 \nBetas:  0.0148827 -0.8418728 \n\nIteração:  300 \nValor da perda:  0.2394607 \nBetas:  0.001973738 -0.8640144 \n\nIteração:  400 \nValor da perda:  0.2394436 \nBetas:  0.0002617562 -0.8671442 \n\nIteração:  500 \nValor da perda:  0.2394432 \nBetas:  3.471399e-05 -0.8675866 \n\nIteração:  600 \nValor da perda:  0.2394432 \nBetas:  4.603754e-06 -0.8676491 \n\nIteração:  700 \nValor da perda:  0.2394432 \nBetas:  6.105479e-07 -0.8676579 \n\nIteração:  800 \nValor da perda:  0.2394432 \nBetas:  8.097059e-08 -0.8676592 \n\nIteração:  900 \nValor da perda:  0.2394432 \nBetas:  1.073828e-08 -0.8676593 \n\nIteração:  1000 \nValor da perda:  0.2394432 \nBetas:  1.424107e-09 -0.8676594 \n\n\nPara recuperar o valor final dos betas.\n\nc(b0, b1)\n\n[1]  1.424107e-09 -8.676594e-01\n\n\nPara efeito didático, vamos comparar estas estimativas finais contra os valores estimados pela função lm. Note que os valores estão muito similares\n\nsummary(model_lm &lt;- lm(mpg ~ wt, data = mtcars_scaled))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars_scaled)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75381 -0.39236 -0.02077  0.23388  1.14033 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.040e-15  8.934e-02   0.000        1    \nwt          -8.677e-01  9.077e-02  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5054 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\n\n\nSempre que possível, é útil visualizar o funcionamento do algoritmo em gráficos. Como estamos trabalhando com um exemplo simples, pode-se plotar os resultados gradativamente num gráfico de dispersão. O código abaixo mostra como a linha de ajuste (linha de regressão) vai se alterando à medida que se aumenta o número de amostras.\n\n\nCode\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\nalpha &lt;- 0.01\n\ny &lt;- dat$mpg\nx &lt;- dat$wt\nN &lt;- nrow(dat)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nbetas &lt;- matrix(ncol = 2, nrow = num_iterations)\nfor (i in seq_len(num_iterations)) {\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2/N)\n  gb1 &lt;- sum((y - yhat) * dat$wt) * (-2/N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - alpha * gb0\n  b1_new &lt;- b1 - alpha * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n  \n  betas[i, 1] &lt;- b0_new\n  betas[i, 2] &lt;- b1_new\n\n\n}\n\nsel &lt;- c(1:10, seq(20, 100, 10), seq(100, 1000, 50))\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tibble)\n\ntbl_betas &lt;- tibble(\n  iter = sel,\n  beta0 = betas[sel, 1],\n  beta1 = betas[sel, 2]\n)\n\nggplot() +\n  geom_point(\n    data = dat,\n    aes(x = wt, y = mpg),\n    shape = 21\n  ) +\n  geom_abline(\n    data = tbl_betas,\n    aes(intercept = beta0, slope = beta1),\n    color = \"#CB181D\",\n    lwd = 0.8\n  ) +\n  geom_text(\n    data = tbl_betas,\n    aes(x = 1.8, y = 1.8, label = paste(\"Iteration:\", iter)),\n    size = 5\n  ) +\n  transition_states(iter) +\n  enter_fade() + \n  exit_shrink() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAgora vamos considerar o caso de regressão múltipla, onde temos várias variáveis independentes. A equação do modelo é generalizada para:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\n\\]\nIsto é, agora temos \\(k\\) variáveis independentes, \\(x_1, x_2, ..., x_k\\), e temos \\(k\\) coeficientes, \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k\\), a ser estimados. A função de custo torna-se:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_{1} x_{i1} + \\beta_2 x_{i2} + ... + \\beta_k X_{ik}))^2\n\\]\nAgora, as derivadas parciais da função de custo em relação a cada parâmetro \\(\\beta\\) são calculadas e utilizadas para atualizar os coeficientes durante o GD.\n\\[\n\\nabla_{\\beta} = (\\frac{\\partial\\text{MSE}}{\\partial\\beta_{0}}, \\frac{\\partial\\text{MSE}}{\\partial\\beta_{1}}, ..., \\frac{\\partial\\text{MSE}}{\\partial\\beta_{k}})\n\\]\n\n\nNa regressão múltipla, podemos representar os dados de entrada \\(X\\) e os parâmetros do modelo \\(\\beta\\) como matrizes. Esta forma de representação é mais prática quando temos muitas variáveis e permite dispensar o uso de somatórios.\nSuponha que tenhamos \\(n\\) observações e \\(k\\) variáveis independentes.\nAs observações de entrada podem ser organizadas em uma matriz \\(X\\) de dimensão \\(n \\times (k+1)\\), onde a primeira coluna é composta por \\(1\\)s para representar o intercepto do modelo. Assim, a matriz \\(X\\) é dada por:\n\\[\nX = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\]\nOs parâmetros do modelo \\(\\beta\\) podem ser representados como um vetor de coeficientes de dimensão \\((k+1) \\times 1\\). Assim, o vetor \\(\\beta\\) é dado por:\n\\[\n\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix}\n\\]\nA resposta \\(y\\) pode ser representada como um vetor de dimensão \\(n \\times 1\\).\n\\[\ny = \\begin{bmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\]\nNote que o problema de minimização acima é equivalente a minimizar a soma do erro ao quadrado do modelo. Isto acontece pois o erro, no caso mais simples, é simplesmente\n\\[\n\\varepsilon = y_i - \\hat{y_i} = y_i - \\beta_{0} - \\beta_{1}x_i\n\\]\ne o problema de minimizar:\n\\[\n\\text{min } \\varepsilon^2 = (y_i - \\hat{y_i})^2\n\\]\nEm termos matriciais temos:\n\\[\ne = y - \\hat{y} = y - X\\beta\n\\] e agora o problema de minimizar\n\\[\n\\text{min } e^te = (y - X\\beta)^t(y-XB)\n\\]\nPara encontrar o gradiente da função de custo \\(e^te\\), em relação aos parâmetros \\(\\beta\\), podemos usar cálculo matricial.\nO gradiente \\(\\nabla_{\\beta} (e'e)\\) é dado por:\n\\[\n\\nabla_{\\beta} (e'e) = -2X^T(y - X\\beta)\n\\]\nOnde \\(X^T\\) representa a transposta da matriz X. Este gradiente nos fornece a direção em que devemos ajustar os (múltiplos) parâmetros \\(\\beta\\) para minimizar a função de custo \\(e'e\\).\n\n\n\n\nDesta vez, vamos implementar tanto o gradiente como a função de perda como functions no R.\n\ngrad &lt;- function(beta) {\n\n  (2/N) * t(X) %*% (X %*% beta - y)\n\n}\n\nloss &lt;- function(beta) {\n\n  e = y - X %*% beta\n\n  t(e) %*% e\n\n}\n\n\n\nNosso modelo de regressão agora terá a forma:\n\\[\n\\text{mpg} = \\beta_{0} + \\beta_{1}\\text{wt} + \\beta_{2}\\text{qsec}+ \\beta_{3}\\text{am}\n\\]\nonde mpg e wt tem as mesmas definições dadas acima; já qsec é uma medida de velocidade do veículo e am é uma variável binária que indica se o câmbio do veículo é manual ou automático.\n\n\n\nDesta vez, o preparo dos dados será feito usando o pacote dplyr.\n\nlibrary(dplyr)\n\ndat &lt;- mtcars |&gt;\n  select(c(\"mpg\", \"wt\", \"qsec\", \"am\")) |&gt;\n  mutate(across(everything(), ~as.numeric(scale(.x))))\n\ny &lt;- dat$mpg\nX &lt;- as.matrix(dat[, c(\"wt\", \"qsec\", \"am\")])\nX &lt;- cbind(1, X)\ncolnames(X)[1] &lt;- c(\"coef\")\nN &lt;- nrow(X)\n\n\n\n\nNovamente, para ganhar um pouco de intuição vamos rodar o\n\n#&gt; Valor inicial para os betas\nbeta &lt;- runif(ncol(X))\n\n# Opcional\n#&gt; Computa a \"predição\" do modelo\nyhat &lt;- X %*% beta\n#&gt; Calcula o valor da função de perda\nl &lt;- loss(beta)\n\n#&gt; Atualiza o valor dos beta\nbeta_new &lt;- beta - alpha * grad(beta)\n\n\n\n\nO código abaixo mostra o loop completo. Fora algumas pequenas modificações, ele é exatamente igual ao loop anterior. Neste segundo exemplo, eu reduzo o valor da learning-rate e aumento o número de iterações.\n\n\nCode\nbeta &lt;- runif(ncol(X))\nnum_iterations &lt;- 10000\nalpha &lt;- 0.001\n\nfor (i in seq_len(num_iterations)) {\n\n  if (i %% 1000 == 0) cat(\"Iteração: \", i, \"\\n\")\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- X %*% beta\n\n  #&gt; Calcula a \"função de perda\"\n  vl_loss &lt;- loss(beta)\n\n  if (i %% 1000 == 0) {\n    cat(\"Valor da perda: \", as.numeric(vl_loss), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  grad_current &lt;- grad(beta)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  beta_current &lt;- beta - alpha * grad_current\n\n  beta &lt;- beta_current\n\n  if (i %% 1000 == 0) {\n    cat(\"Betas: \", beta, \"\\n\\n\")\n  }\n\n}\n\n\nIteração:  1000 \nValor da perda:  9.455926 \nBetas:  0.1309512 -0.08362166 0.5981053 0.7911146 \n\nIteração:  2000 \nValor da perda:  6.562313 \nBetas:  0.01768686 -0.2744187 0.5436181 0.6104938 \n\nIteração:  3000 \nValor da perda:  5.516612 \nBetas:  0.002388867 -0.3935967 0.4873342 0.4898478 \n\nIteração:  4000 \nValor da perda:  5.047922 \nBetas:  0.0003226512 -0.4729525 0.4471217 0.4090441 \n\nIteração:  5000 \nValor da perda:  4.835839 \nBetas:  4.357873e-05 -0.5262559 0.4197881 0.3547316 \n\nIteração:  6000 \nValor da perda:  4.739834 \nBetas:  5.88594e-06 -0.5621091 0.4013647 0.318196 \n\nIteração:  7000 \nValor da perda:  4.696374 \nBetas:  7.949817e-07 -0.5862306 0.3889652 0.293615 \n\nIteração:  8000 \nValor da perda:  4.676701 \nBetas:  1.073738e-07 -0.6024597 0.3806222 0.2770767 \n\nIteração:  9000 \nValor da perda:  4.667796 \nBetas:  1.45024e-08 -0.6133789 0.3750089 0.2659495 \n\nIteração:  10000 \nValor da perda:  4.663764 \nBetas:  1.95876e-09 -0.6207255 0.3712321 0.2584629 \n\n\nPor fim, temos o valor final dos betas estimados.\n\nbeta_current\n\n              [,1]\ncoef  1.958760e-09\nwt   -6.207255e-01\nqsec  3.712321e-01\nam    2.584629e-01\n\n\nNovamente, podemos comparar estas estimativas com aquelas calculadas pela função lm. Note que, neste caso, mesmo após 10.000 iterações ainda há algumas pequenas divergências entre os valores.\n\nsummary(model_lm &lt;- lm(mpg ~ wt + qsec + am, data = dat))\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5776 -0.2581 -0.1204  0.2341  0.7734 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.485e-15  7.212e-02   0.000 1.000000    \nwt          -6.358e-01  1.155e-01  -5.507 6.95e-06 ***\nqsec         3.635e-01  8.559e-02   4.247 0.000216 ***\nam           2.431e-01  1.168e-01   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.408 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11\n\n\n\n\n\n\nO Gradiente Descendente é uma ferramenta poderosa para otimização em aprendizado de máquina e outros campos. Neste post, explicamos a matemática por trás do GD, mostramos como derivar o gradiente tanto para regressão linear simples quanto múltipla, e como entender o funcionamento deste algoritmo fundamental."
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-simples",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-simples",
    "title": "Gradient Descent",
    "section": "",
    "text": "Para tornar as coisas mais concretas vamos considerar o caso de uma regressão linear simples. Temos uma variável de resposta \\(y\\) que será “explicada” por uma variável independente (feature) \\(x\\) . A relação é modelada de maneira linear como:\n\\[\ny = \\beta_{0} + \\beta_{1}x + \\varepsilon\n\\]\nA equação acima descreve o nosso modelo, que será estimado posteriormente. Com este modelo podemos calcular os valores preditos \\(\\hat{y_{i}}\\) que serão comparados com os valores observados (reais) \\(y_{i}\\). Nosso objetivo é ajustar os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\) para minimizar a função de custo, que pode ser representada como o Erro Quadrático Médio (MSE, do inglês Mean Squared Error):\n\\[\n\\text{MSE} =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_{i}})^2= \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 X_i))^2\n\\]\nPara atualizar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) usando GD, precisamos calcular o gradiente da função de custo em relação a esses parâmetros. A primeira derivada da função de custo em relação a \\(\\beta_0\\) e \\(\\beta_1\\) nos dá as direções em que devemos ajustar esses parâmetros para minimizar a função de custo.\nA primeira derivada da função de custo em relação a \\(\\beta_0\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_0} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nE a primeira derivada da função de custo em relação a \\(\\beta_1\\) é dada por:\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\beta_1} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i (y_i - (\\beta_0 + \\beta_1 x_i))\n\\]\nEssas derivadas nos fornecem a direção em que devemos ajustar os parâmetros para minimizar a função de custo.\n\n\n\nO algortimo de GD funciona iterativamente O algoritmo de SGD atualiza o parâmetro \\(\\beta^{t}\\) a cada iteração t, onde \\(\\beta^{0}\\) é dado, usando o gradiente \\(\\nabla_{\\beta}^t\\) da seguinte maneira:\n\\[\n\\beta^{t+1} = \\beta^{t} - \\gamma\\nabla_{\\beta}^{t}\n\\]\nonde \\(\\gamma\\) é um número real não-negativo, tipicamente próximo de 0.001, chamado “learning rate”. Quanto maior for o valor de \\(\\gamma\\) maiores serão os “passos” no processo de atualização; inversamente, quanto menor for o valor de \\(\\gamma\\) menores serão os “passos”no processo iterativo.\nPara implementar o passo-a-passo do algoritmo vamos usar a base mtcars. O primeiro passo é ajustar os dados usando a função scale. Em seguida, separamos alguns objetos úteis para facilitar a exposição.\n\n#&gt; Regularizar os dados\nmtcars_scaled &lt;- as.data.frame(scale(mtcars))\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\nVamos fazer a regressão de mpg (milhas por galão), uma medida da eficiência do veículo, contra wt (peso), o peso do veículo. Visualmente, parece haver uma relação linear decrescente entre as variáveis.\n\nplot(y ~ x)\n\n\n\n\nAntes de fazer o loop, vamos decompor o algoritmo em etapas. Primeiro, precisa-se de valores iniciais para os parâmetros \\(\\beta_{0}\\) e \\(\\beta_{1}\\). Por simplicidade, vamos sortear números aleatórios entre 0 e 1 a partir de uma distribuição uniforme. Com estes valores será possível calcular o valor de \\(\\hat{y}_{i}^0\\), onde 0 indica que estamos na iteração de valor 0.\n\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\n(yhat &lt;- b0 + b1 * x)\n\n [1]  0.33357324  0.43639854  0.21260228  0.57349896  0.66422717  0.67229190\n [7]  0.71664792  0.56341804  0.54728858  0.66422717  0.66422717  0.91826617\n[13]  0.78116576  0.80132758  1.39408525  1.46424840  1.43239272  0.16421390\n[19] -0.07167945  0.01703258  0.27107158  0.69648609  0.66221099  0.82552177\n[25]  0.82753796  0.05735623  0.14001971 -0.11280958  0.55535331  0.39405871\n[31]  0.71664792  0.39809108\n\n\nNão é necessário, mas é instrutivo calcular a função de perda.\n\n(mse &lt;- mean((y - yhat)^2))\n\n[1] 2.112771\n\n\nAgora, calculamos o valor do gradiente neste ponto.\n\n(gb0 &lt;- sum(y - yhat) * (-2/N))\n\n[1] 1.148812\n\n(gb1 &lt;- sum((y - yhat) * x) * (-2/N))\n\n[1] 2.44553\n\n\nPor fim, o valor dos parâmetros é atualizado segundo a fórmula matemática do algoritmo. Utiliza-se \\(g = 0.01\\) como valor para a learning-rate.\n\ng = 0.01\n\n(b0_new &lt;- b0 - g * gb0)\n\n[1] 0.5629181\n\n(b1_new &lt;- b1 - g * gb1)\n\n[1] 0.3700945\n\n\nEste processo será repetido \\(T\\) vezes até que se atinja algum critério de convergência. Em geral, estabelece-se\n\nUm número máximo de iterações. (10.000 iterações, por exemplo).\nUm valor mínimo de mudança na estimativa dos parâmetros. Isto é, quando o valor das estimativas para de mudar significativamente, entende-se que ele convergiu para um valor satisfatório.\n\nPara deixar o loop abaixo mais simples, vou simplesmente estabelecer um número máximo de 5000 iterações. O código segue abaixo. Note que algumas partes do código acima foram repetidas por conveniência da leitura.\n\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\ng &lt;- 0.01\n\ny &lt;- mtcars_scaled$mpg\nx &lt;- mtcars_scaled$wt\nN &lt;- nrow(mtcars_scaled)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nfor (i in seq_len(num_iterations)) {\n\n  if (i %% 100 == 0) cat(\"Iteração: \", i, \"\\n\")\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  if (i %% 100 == 0) {\n    cat(\"Valor da perda: \", as.numeric(mse), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2/N)\n  gb1 &lt;- sum((y - yhat) * x) * (-2/N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - g * gb0\n  b1_new &lt;- b1 - g * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n\n  if (i %% 100 == 0) {\n    cat(\"Betas: \", c(b0, b1), \"\\n\\n\")\n  }\n\n}\n\nIteração:  100 \nValor da perda:  0.2860836 \nBetas:  0.112221 -0.6852286 \n\nIteração:  200 \nValor da perda:  0.2403437 \nBetas:  0.0148827 -0.8418728 \n\nIteração:  300 \nValor da perda:  0.2394607 \nBetas:  0.001973738 -0.8640144 \n\nIteração:  400 \nValor da perda:  0.2394436 \nBetas:  0.0002617562 -0.8671442 \n\nIteração:  500 \nValor da perda:  0.2394432 \nBetas:  3.471399e-05 -0.8675866 \n\nIteração:  600 \nValor da perda:  0.2394432 \nBetas:  4.603754e-06 -0.8676491 \n\nIteração:  700 \nValor da perda:  0.2394432 \nBetas:  6.105479e-07 -0.8676579 \n\nIteração:  800 \nValor da perda:  0.2394432 \nBetas:  8.097059e-08 -0.8676592 \n\nIteração:  900 \nValor da perda:  0.2394432 \nBetas:  1.073828e-08 -0.8676593 \n\nIteração:  1000 \nValor da perda:  0.2394432 \nBetas:  1.424107e-09 -0.8676594 \n\n\nPara recuperar o valor final dos betas.\n\nc(b0, b1)\n\n[1]  1.424107e-09 -8.676594e-01\n\n\nPara efeito didático, vamos comparar estas estimativas finais contra os valores estimados pela função lm. Note que os valores estão muito similares\n\nsummary(model_lm &lt;- lm(mpg ~ wt, data = mtcars_scaled))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars_scaled)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75381 -0.39236 -0.02077  0.23388  1.14033 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.040e-15  8.934e-02   0.000        1    \nwt          -8.677e-01  9.077e-02  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5054 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\n\n\nSempre que possível, é útil visualizar o funcionamento do algoritmo em gráficos. Como estamos trabalhando com um exemplo simples, pode-se plotar os resultados gradativamente num gráfico de dispersão. O código abaixo mostra como a linha de ajuste (linha de regressão) vai se alterando à medida que se aumenta o número de amostras.\n\n\nCode\nnum_iterations &lt;- 1000\n#&gt; Learning-rate\nalpha &lt;- 0.01\n\ny &lt;- dat$mpg\nx &lt;- dat$wt\nN &lt;- nrow(dat)\n\n#&gt; Valores iniciais para estimativas dos parâmetros\nb0 &lt;- runif(1)\nb1 &lt;- runif(1)\n\nbetas &lt;- matrix(ncol = 2, nrow = num_iterations)\nfor (i in seq_len(num_iterations)) {\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- b0 + b1 * x\n\n  #&gt; Calcula a \"função de perda\"\n  error &lt;- y - yhat\n  mse &lt;- mean(error^2)\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  gb0 &lt;- sum(y - yhat) * (-2/N)\n  gb1 &lt;- sum((y - yhat) * dat$wt) * (-2/N)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  b0_new &lt;- b0 - alpha * gb0\n  b1_new &lt;- b1 - alpha * gb1\n\n  b0 &lt;- b0_new\n  b1 &lt;- b1_new\n  \n  betas[i, 1] &lt;- b0_new\n  betas[i, 2] &lt;- b1_new\n\n\n}\n\nsel &lt;- c(1:10, seq(20, 100, 10), seq(100, 1000, 50))\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tibble)\n\ntbl_betas &lt;- tibble(\n  iter = sel,\n  beta0 = betas[sel, 1],\n  beta1 = betas[sel, 2]\n)\n\nggplot() +\n  geom_point(\n    data = dat,\n    aes(x = wt, y = mpg),\n    shape = 21\n  ) +\n  geom_abline(\n    data = tbl_betas,\n    aes(intercept = beta0, slope = beta1),\n    color = \"#CB181D\",\n    lwd = 0.8\n  ) +\n  geom_text(\n    data = tbl_betas,\n    aes(x = 1.8, y = 1.8, label = paste(\"Iteration:\", iter)),\n    size = 5\n  ) +\n  transition_states(iter) +\n  enter_fade() + \n  exit_shrink() +\n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-múltipla",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#regressão-múltipla",
    "title": "Gradient Descent",
    "section": "",
    "text": "Agora vamos considerar o caso de regressão múltipla, onde temos várias variáveis independentes. A equação do modelo é generalizada para:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\n\\]\nIsto é, agora temos \\(k\\) variáveis independentes, \\(x_1, x_2, ..., x_k\\), e temos \\(k\\) coeficientes, \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k\\), a ser estimados. A função de custo torna-se:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_{1} x_{i1} + \\beta_2 x_{i2} + ... + \\beta_k X_{ik}))^2\n\\]\nAgora, as derivadas parciais da função de custo em relação a cada parâmetro \\(\\beta\\) são calculadas e utilizadas para atualizar os coeficientes durante o GD.\n\\[\n\\nabla_{\\beta} = (\\frac{\\partial\\text{MSE}}{\\partial\\beta_{0}}, \\frac{\\partial\\text{MSE}}{\\partial\\beta_{1}}, ..., \\frac{\\partial\\text{MSE}}{\\partial\\beta_{k}})\n\\]\n\n\nNa regressão múltipla, podemos representar os dados de entrada \\(X\\) e os parâmetros do modelo \\(\\beta\\) como matrizes. Esta forma de representação é mais prática quando temos muitas variáveis e permite dispensar o uso de somatórios.\nSuponha que tenhamos \\(n\\) observações e \\(k\\) variáveis independentes.\nAs observações de entrada podem ser organizadas em uma matriz \\(X\\) de dimensão \\(n \\times (k+1)\\), onde a primeira coluna é composta por \\(1\\)s para representar o intercepto do modelo. Assim, a matriz \\(X\\) é dada por:\n\\[\nX = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\]\nOs parâmetros do modelo \\(\\beta\\) podem ser representados como um vetor de coeficientes de dimensão \\((k+1) \\times 1\\). Assim, o vetor \\(\\beta\\) é dado por:\n\\[\n\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix}\n\\]\nA resposta \\(y\\) pode ser representada como um vetor de dimensão \\(n \\times 1\\).\n\\[\ny = \\begin{bmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\]\nNote que o problema de minimização acima é equivalente a minimizar a soma do erro ao quadrado do modelo. Isto acontece pois o erro, no caso mais simples, é simplesmente\n\\[\n\\varepsilon = y_i - \\hat{y_i} = y_i - \\beta_{0} - \\beta_{1}x_i\n\\]\ne o problema de minimizar:\n\\[\n\\text{min } \\varepsilon^2 = (y_i - \\hat{y_i})^2\n\\]\nEm termos matriciais temos:\n\\[\ne = y - \\hat{y} = y - X\\beta\n\\] e agora o problema de minimizar\n\\[\n\\text{min } e^te = (y - X\\beta)^t(y-XB)\n\\]\nPara encontrar o gradiente da função de custo \\(e^te\\), em relação aos parâmetros \\(\\beta\\), podemos usar cálculo matricial.\nO gradiente \\(\\nabla_{\\beta} (e'e)\\) é dado por:\n\\[\n\\nabla_{\\beta} (e'e) = -2X^T(y - X\\beta)\n\\]\nOnde \\(X^T\\) representa a transposta da matriz X. Este gradiente nos fornece a direção em que devemos ajustar os (múltiplos) parâmetros \\(\\beta\\) para minimizar a função de custo \\(e'e\\)."
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#implementando-o-algoritmo-1",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#implementando-o-algoritmo-1",
    "title": "Gradient Descent",
    "section": "",
    "text": "Desta vez, vamos implementar tanto o gradiente como a função de perda como functions no R.\n\ngrad &lt;- function(beta) {\n\n  (2/N) * t(X) %*% (X %*% beta - y)\n\n}\n\nloss &lt;- function(beta) {\n\n  e = y - X %*% beta\n\n  t(e) %*% e\n\n}\n\n\n\nNosso modelo de regressão agora terá a forma:\n\\[\n\\text{mpg} = \\beta_{0} + \\beta_{1}\\text{wt} + \\beta_{2}\\text{qsec}+ \\beta_{3}\\text{am}\n\\]\nonde mpg e wt tem as mesmas definições dadas acima; já qsec é uma medida de velocidade do veículo e am é uma variável binária que indica se o câmbio do veículo é manual ou automático.\n\n\n\nDesta vez, o preparo dos dados será feito usando o pacote dplyr.\n\nlibrary(dplyr)\n\ndat &lt;- mtcars |&gt;\n  select(c(\"mpg\", \"wt\", \"qsec\", \"am\")) |&gt;\n  mutate(across(everything(), ~as.numeric(scale(.x))))\n\ny &lt;- dat$mpg\nX &lt;- as.matrix(dat[, c(\"wt\", \"qsec\", \"am\")])\nX &lt;- cbind(1, X)\ncolnames(X)[1] &lt;- c(\"coef\")\nN &lt;- nrow(X)\n\n\n\n\nNovamente, para ganhar um pouco de intuição vamos rodar o\n\n#&gt; Valor inicial para os betas\nbeta &lt;- runif(ncol(X))\n\n# Opcional\n#&gt; Computa a \"predição\" do modelo\nyhat &lt;- X %*% beta\n#&gt; Calcula o valor da função de perda\nl &lt;- loss(beta)\n\n#&gt; Atualiza o valor dos beta\nbeta_new &lt;- beta - alpha * grad(beta)\n\n\n\n\nO código abaixo mostra o loop completo. Fora algumas pequenas modificações, ele é exatamente igual ao loop anterior. Neste segundo exemplo, eu reduzo o valor da learning-rate e aumento o número de iterações.\n\n\nCode\nbeta &lt;- runif(ncol(X))\nnum_iterations &lt;- 10000\nalpha &lt;- 0.001\n\nfor (i in seq_len(num_iterations)) {\n\n  if (i %% 1000 == 0) cat(\"Iteração: \", i, \"\\n\")\n\n  #&gt; Calcula o valor previsto\n  yhat &lt;- X %*% beta\n\n  #&gt; Calcula a \"função de perda\"\n  vl_loss &lt;- loss(beta)\n\n  if (i %% 1000 == 0) {\n    cat(\"Valor da perda: \", as.numeric(vl_loss), \"\\n\")\n  }\n\n  #&gt; Calcula o gradiente nos pontos atuais\n  grad_current &lt;- grad(beta)\n  #&gt; Atualiza o valor dos parâmetros usando o gradiente\n  beta_current &lt;- beta - alpha * grad_current\n\n  beta &lt;- beta_current\n\n  if (i %% 1000 == 0) {\n    cat(\"Betas: \", beta, \"\\n\\n\")\n  }\n\n}\n\n\nIteração:  1000 \nValor da perda:  9.455926 \nBetas:  0.1309512 -0.08362166 0.5981053 0.7911146 \n\nIteração:  2000 \nValor da perda:  6.562313 \nBetas:  0.01768686 -0.2744187 0.5436181 0.6104938 \n\nIteração:  3000 \nValor da perda:  5.516612 \nBetas:  0.002388867 -0.3935967 0.4873342 0.4898478 \n\nIteração:  4000 \nValor da perda:  5.047922 \nBetas:  0.0003226512 -0.4729525 0.4471217 0.4090441 \n\nIteração:  5000 \nValor da perda:  4.835839 \nBetas:  4.357873e-05 -0.5262559 0.4197881 0.3547316 \n\nIteração:  6000 \nValor da perda:  4.739834 \nBetas:  5.88594e-06 -0.5621091 0.4013647 0.318196 \n\nIteração:  7000 \nValor da perda:  4.696374 \nBetas:  7.949817e-07 -0.5862306 0.3889652 0.293615 \n\nIteração:  8000 \nValor da perda:  4.676701 \nBetas:  1.073738e-07 -0.6024597 0.3806222 0.2770767 \n\nIteração:  9000 \nValor da perda:  4.667796 \nBetas:  1.45024e-08 -0.6133789 0.3750089 0.2659495 \n\nIteração:  10000 \nValor da perda:  4.663764 \nBetas:  1.95876e-09 -0.6207255 0.3712321 0.2584629 \n\n\nPor fim, temos o valor final dos betas estimados.\n\nbeta_current\n\n              [,1]\ncoef  1.958760e-09\nwt   -6.207255e-01\nqsec  3.712321e-01\nam    2.584629e-01\n\n\nNovamente, podemos comparar estas estimativas com aquelas calculadas pela função lm. Note que, neste caso, mesmo após 10.000 iterações ainda há algumas pequenas divergências entre os valores.\n\nsummary(model_lm &lt;- lm(mpg ~ wt + qsec + am, data = dat))\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5776 -0.2581 -0.1204  0.2341  0.7734 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.485e-15  7.212e-02   0.000 1.000000    \nwt          -6.358e-01  1.155e-01  -5.507 6.95e-06 ***\nqsec         3.635e-01  8.559e-02   4.247 0.000216 ***\nam           2.431e-01  1.168e-01   2.081 0.046716 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.408 on 28 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.8336 \nF-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11"
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#conclusão",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#conclusão",
    "title": "Gradient Descent",
    "section": "",
    "text": "O Gradiente Descendente é uma ferramenta poderosa para otimização em aprendizado de máquina e outros campos. Neste post, explicamos a matemática por trás do GD, mostramos como derivar o gradiente tanto para regressão linear simples quanto múltipla, e como entender o funcionamento deste algoritmo fundamental."
  },
  {
    "objectID": "posts/general-posts/2024-02-gradient-descent/index.html#footnotes",
    "href": "posts/general-posts/2024-02-gradient-descent/index.html#footnotes",
    "title": "Gradient Descent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApesar dos métodos serem formalmente diferentes, a sua essência é idêntica a ponto de ser comum confundi-los. O SGD é uma “aproximação” do GD, onde apenas uma parte (amostra) dos dados é utilizada para calcular o gradiente. Neste sentido, o SGD é muito mais eficiente do ponto de vista computacional.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html",
    "href": "posts/general-posts/2023-08-firjan-app/index.html",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório\n\n\n\n\nO app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16.\n\n\n\n\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html#introdução",
    "href": "posts/general-posts/2023-08-firjan-app/index.html#introdução",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório"
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html#sobre-o-app",
    "href": "posts/general-posts/2023-08-firjan-app/index.html#sobre-o-app",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "O app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16."
  },
  {
    "objectID": "posts/general-posts/2023-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "href": "posts/general-posts/2023-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Um caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/repost-pacotes-essenciais-r/index.html",
    "href": "posts/general-posts/repost-pacotes-essenciais-r/index.html",
    "title": "Pacotes Essenciais R",
    "section": "",
    "text": "R é uma das linguagens de programação mais populares para ciência de dados, estatística, economia e várias outras áreas quantitativas. O R já vem com alguns pacotes “imbutidos” que, em geral, são referidos como base R, são os pacotes que são instalados automaticamente junto com o R como o stats, utils, graphics, datasets entre outros.\nOu seja, já é possível importar dados e limpá-los, fazer análises estatísticas, gráficos e tabelas sem carregar nenhum pacote adicional.\nMas para o usufruir de todo o potencial que o R pode oferecer é essencial conhecer os melhores pacotes e as suas funções."
  },
  {
    "objectID": "posts/general-posts/repost-pacotes-essenciais-r/index.html#dplyr-x-data.table",
    "href": "posts/general-posts/repost-pacotes-essenciais-r/index.html#dplyr-x-data.table",
    "title": "Pacotes Essenciais R",
    "section": "dplyr x data.table",
    "text": "dplyr x data.table\nO dplyr é bastante intuitivo, poderoso e há centenas de tutoriais e livros sobre ele. Ele serve para transformar os dados: criar colunas, filtrar linhas, reordernar dados, etc. Ele integra um “ecossistema” de pacotes chamado tidyverse, uma coleção de pacotes para processamento e visualização de dados, construídos em torno de uma filosofia comum.\nO data.table surgiu com o intuito de fazer o R funcionar melhor num mundo de big data. Seu desenvolvimento foi focado em ser o mais rápido, eficiente e sucinto possível, permitindo trabalhar com bases de dados muito grandes dentro do R (~100GB). Em termos de velocidade, o data.table ganha facilmente de praticamente todas as outras linguagens populares em data science. Ele é centena de vezes mais rápido que base R e também ganha com folga do seu competidor dplyr.\nSe o data.table é tão mais eficiente então como o dplyr se tornou mais popular do que ele?\nAcontece que o dplyr é melhor integrado com vários outros pacotes podersos do tidyverse. Também há muito material de apoio ao dplyr na forma de tutoriais, vídeos, livros, etc. disponíveis na internet. Quase todo curso de R ensina a usar o dplyr. Assim, a curva de aprendizado fica mais fácil.\nO data.table funciona muito bem com funções do base R como lapply, colMeans(), etc. e tem uma sintaxe concisa e flexível, onde o usuário acaba tendo bastante liberdade para criar as suas soluções. Já o dplyr vai pelo caminho de tentar facilitar ao máximo a vida do usuário criando várias funções com usos bastante específicos para otimizar pequenas tarefas do dia-a-dia do processamento de dados.\nOutra pequena desvantagem do data.table é que ele não funciona de primeira no Mac. Como ele usa OpenMP é preciso baixar outros programas e modificar algumas configurações no R o que pode ser bem trabalhoso e chato.\nNa comparação fica o seguinte:\n\n\n\n\n\n\n\ndplyr\ndata.table\n\n\n\n\nMais funções para aprender. Em geral, o código fica mais comprido\nSintaxe sucinta\n\n\nFunções melhor integradas com o tidyverse\nFunções melhor integradas com base R\n\n\nMuito mais veloz que o base R\nOpção mais veloz possível\n\n\nSintaxe mais intuitiva, mais material de apoio na internet, mais popular.\nMenos material de apoio disponível"
  },
  {
    "objectID": "posts/general-posts/repost-pacotes-essenciais-r/index.html#tidyverse",
    "href": "posts/general-posts/repost-pacotes-essenciais-r/index.html#tidyverse",
    "title": "Pacotes Essenciais R",
    "section": "tidyverse",
    "text": "tidyverse\nIndepedentemente da sua escolha entre dplyr e/ou data.table (recomendo usar os dois!) vale a pena explorar os demais pacotes do tidyverse.\n\ntidyr\nO tidyr acrescenta algumas importantes funcionalidades que faltam no dplyr. Talvez as duas funções mais importantes do pacote sejam pivot_longer e pivot_wider que servem para converter seus dados de transversais (wide) para longitudinais (long) e vice-versa. O tidyr também traz um novo tipo de objeto o tibble, uma versão moderna do data.frame.\nVale também explorar algumas funções muito úteis como separate, fill, complete, replace_na entre outras.\n\nlibrary(tidyr)\nlibrary(dplyr)\n# Exemplo de tibble\ndata &lt;- as_tibble(USPersonalExpenditure)\ndata &lt;- mutate(data, variable = rownames(USPersonalExpenditure))\ndata\n\n# A tibble: 5 × 6\n  `1940` `1945` `1950` `1955` `1960` variable           \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              \n1 22.2   44.5    59.6    73.2  86.8  Food and Tobacco   \n2 10.5   15.5    29      36.5  46.2  Household Operation\n3  3.53   5.76    9.71   14    21.1  Medical and Health \n4  1.04   1.98    2.45    3.4   5.4  Personal Care      \n5  0.341  0.974   1.8     2.6   3.64 Private Education  \n\n\n\n# Exemplo de dado longitudinal\nlong &lt;- pivot_longer(data, cols = !variable, names_to = \"year\")\nlong\n\n# A tibble: 25 × 3\n   variable            year  value\n   &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt;\n 1 Food and Tobacco    1940   22.2\n 2 Food and Tobacco    1945   44.5\n 3 Food and Tobacco    1950   59.6\n 4 Food and Tobacco    1955   73.2\n 5 Food and Tobacco    1960   86.8\n 6 Household Operation 1940   10.5\n 7 Household Operation 1945   15.5\n 8 Household Operation 1950   29  \n 9 Household Operation 1955   36.5\n10 Household Operation 1960   46.2\n# ℹ 15 more rows\n\n\n\n# Calcula a variação e apresenta de maneira transversal\nlong %&gt;%\n  group_by(variable) %&gt;%\n  mutate(variacao = (value / lag(value) - 1) * 100) %&gt;%\n  na.omit() %&gt;%\n  pivot_wider(\n    id_cols = \"variable\",\n    names_from = \"year\",\n    values_from = \"variacao\")\n\n# A tibble: 5 × 5\n# Groups:   variable [5]\n  variable            `1945` `1950` `1955` `1960`\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Food and Tobacco     100.    33.9   22.8   18.6\n2 Household Operation   47.6   87.1   25.9   26.6\n3 Medical and Health    63.2   68.6   44.2   50.7\n4 Personal Care         90.4   23.7   38.8   58.8\n5 Private Education    186.    84.8   44.4   40  \n\n\n\n\nstringr\nO stringr é um pacote voltado para facilitar a manipulação de vetores de texto (character). Todas as funções do pacote são convenientemente precedidas pelo prefixo str_ e a lista dos argumentos segue um padrão uniforme. O nome das funções também é bastante intuitivo.\nNovamente, há funções base como gsub, grep, strsplit, sub, entre outras, que servem para manipular vetores de string, mas essas funções carecem de “coesão interna” e em alguns casos elas podem retornar valores inesperados ou “erros silenciosos”.\nUma função muito divertida do pacote é a str_glue que facilita na hora de concatenar strings. Note como é simples incluir variáveis como parte do texto e inclusive fazer transformações nas variáveis.\n\nlibrary(stringr)\n\nnome &lt;- \"Vinicius Oike\"\nidade &lt;- 29\nnfav &lt;- runif(1, min = 0, max = 10)\n\nstr_glue(\"Olá, meu nome é {nome}, tenho {idade} anos. Ano que vem, terei {idade + 1} anos. Meu número favorito é {round(nfav)}.\")\n\nOlá, meu nome é Vinicius Oike, tenho 29 anos. Ano que vem, terei 30 anos. Meu número favorito é 8.\n\n\n\n\nlubridate\nO lubridate é um pacote exclusivamente focado em manipulação de vetores de datas e tudo relacionado ao tempo como variável. Variáveis de datas podem ser uma dor de cabeça tremenda no processo de limpeza de dados, pois há inúmeros formatos diferentes, que variam para cada país, fora os problemas potenciais de ano bissextos, fusos-horários diferentes, etc.\nEspecialmente para quem precisa trabalhar com séries de tempo ou dados em painel, o lubridate é um pacote é essencial.\nO pacote também facilita “operações aritméticas” com datas como no exemplo abaixo:\n\nlibrary(lubridate)\n\nstart &lt;- ymd(\"2020-01-15\")\nstart + years(1) + months(3)\n\n[1] \"2021-04-15\"\n\n\nEle também permite a extração de qualquer informação específica de uma data\n\nagora &lt;- ymd_hms(\"2022-06-10 20:36:15\")\n\nComo o dia da semana:\n\nwday(agora)\n\n[1] 6\n\n\nO trimestre:\n\nquarter(agora)\n\n[1] 2\n\n\nOu o mês (com a opção de ter a abreviação do mês já no padrão desejado!):\n\nmonth(agora, label = TRUE, locale = \"pt_BR\")\n\n[1] Jun\n12 Levels: Jan &lt; Fev &lt; Mar &lt; Abr &lt; Mai &lt; Jun &lt; Jul &lt; Ago &lt; Set &lt; ... &lt; Dez\n\n\n\n\ndbplyr\nO dbplyr é um pacote muito interessante que transforma o R numa “interface” para o SQL. O pacote traduz código escrito em dplyr para código em SQL. Assim, é possível acessar, transformar e baixar dados de uma Base de Dados em SQL, PostGreSQL, etc. diretamente no R.\nO código abaixo é um exemplo (bem artficial) que demonstra o funcionamento da pacote. A função show_query() não costuma ser usada na prática, mas ela serve para mostrar o que o pacote está fazendo.\n\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Abre um servidor SQL para servir de exemplo\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\n# Faz o update da tabela mtcars para esse servidor\ncopy_to(con, mtcars)\n# Acessa a tabela do servidor usando tbl\ndb &lt;- tbl(con, \"mtcars\")\n\n# Trabalha normalmente usando comandos do dplyr\ndb %&gt;%\n  filter(cyl %in% c(4, 6)) %&gt;%\n  mutate(disp = log(round(disp))) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(a1 = mean(disp, na.rm = TRUE), a2 = median(wt, na.rm = TRUE)) %&gt;%\n  arrange(desc(a1)) %&gt;%\n  # No lugar de show_query() use collect() para baixar os dados\n  show_query()\n\n&lt;SQL&gt;\nSELECT `cyl`, AVG(`disp`) AS `a1`, MEDIAN(`wt`) AS `a2`\nFROM (\n  SELECT\n    `mpg`,\n    `cyl`,\n    LOG(ROUND(`disp`, 0)) AS `disp`,\n    `hp`,\n    `drat`,\n    `wt`,\n    `qsec`,\n    `vs`,\n    `am`,\n    `gear`,\n    `carb`\n  FROM `mtcars`\n  WHERE (`cyl` IN (4.0, 6.0))\n)\nGROUP BY `cyl`\nORDER BY `a1` DESC"
  },
  {
    "objectID": "posts/general-posts/repost-pacotes-essenciais-r/index.html#rmarkdown",
    "href": "posts/general-posts/repost-pacotes-essenciais-r/index.html#rmarkdown",
    "title": "Pacotes Essenciais R",
    "section": "RMarkdown",
    "text": "RMarkdown\nUma análise de dados, em geral, vira um de dois produtos: um relatório/artigo ou um aplicativo interativo. Novamente o R tem funcionalidades incríveis para ambos os objetivos. Existe um tipo de arquivo chamado RMarkdown que junto com o pacote knitr permite compilar um arquivo misto de Markdown e R em formato pdf, html, doc ou ppt.\nEssa extensão é tão poderosa que não só é possível escrever um artigo científico com ela, mas também, um dashboard interativo, um livro inteiro, ou até um blog/site (este blog é escrito em RMarkdown).\nAtualmente o formato do código é ainda mais flexível, permitindo misturar linguagens como R, Python, SQL e outras num mesmo arquivo. Um resumo das funcionalidades pode ser visto aqui."
  },
  {
    "objectID": "posts/general-posts/repost-pacotes-essenciais-r/index.html#shiny",
    "href": "posts/general-posts/repost-pacotes-essenciais-r/index.html#shiny",
    "title": "Pacotes Essenciais R",
    "section": "Shiny",
    "text": "Shiny\nO pacote shiny tem se tornado cada vez mais potente nos últimos anos. Inicialmente, ele servia para montar dashboards relativamente simples, que permitiam ao usuário mais liberdade para explorar e montar suas próprias análises. Atualmente, tanto o pacote como as suas extensões melhoraram muito o seu potencial.\nA galeria de apps do RStudio já está um pouco desatualizada, mas dá um sabor do que é possível fazer com shiny.\nAlguns exemplos:\n\nRadiant - Esta é, sem dúvida, uma das aplicações mais completas e impressionantes de Shiny, vale um post inteiro por si só.\nMonitor de Covid\nAnaálise do Perfil de Eleitor no Brasil"
  },
  {
    "objectID": "posts/general-posts/repost-pacotes-essenciais-r/index.html#quarto",
    "href": "posts/general-posts/repost-pacotes-essenciais-r/index.html#quarto",
    "title": "Pacotes Essenciais R",
    "section": "Quarto",
    "text": "Quarto\nRecentemente lançado, o quarto não é um pacote de R propriamente dito. O Quarto é como um RMarkdown turbinado, é um meio de publicar análises de dados com textos. De maneira geral o Quarto te permite:\n\nEscrever e rodar análises de dados em R, python e julia\nPublicar relatórios, dashboards, livros, sites, etc.\nPublicar artigos científicos utilizando equações, citações, etc.\n\nPara conhecer mais sobre o Quarto vale a pena checar o site. Como o Quarto foi desenvolvido pela Posit, que desenvolvou o RStudio a IDE mais popular de R, o Quarto e o R funcionam muito bem no RStudio."
  },
  {
    "objectID": "posts/general-posts/2023-08-recife/index.html",
    "href": "posts/general-posts/2023-08-recife/index.html",
    "title": "Weekly Viz: Recife em mapas",
    "section": "",
    "text": "Semana que vem vou estar no Recife a trabalho e para conhecer um pouco da cidade resolvi tirar um tempo para fazer alguns mapas da cidade.\n\n\n\n\n\n\n\nOs dados dos dois primeiros mapas, das principais vias e dos corpos d’água, são do OpenStreetMap. Pode-se ver, por exemplo, a Rodovia Mario Covas, que corta a cidade de norte a sul e o famoso Rio Capibaribe que atravessa a cidade.\nPara o terceiro mapa puxei os dados do Ciclomapa. Apesar do título, eu incluí tanto ciclovias, ciclofaixas e ciclorrotas.\nOs mapas da segunda linha foram todos feitos com dados do projeto de Acesso a Oportunidades do IPEA. Os dados de população e renda são do Censo de 2010 e espacialmente interpolados com os hexágonos H3, da Uber. Para facilitar a visualização usei o algoritmo de Jenks para agrupar os dados em 7 grupos. Além disso, usei uma transformação log tanto na renda como na população para reduzir um pouco da variância nos dados.\nA renda da cidade é visivelmente concentrada na região sul-sudeste, próxima do litoral e na região centro-leste, no que me parece ser a região dos bairros Madalena e Boa Vista. Já a densidade populacional não segue um padrão simples; a população está espalhada por toda a cidade.\nComo medida de acesso a empregos usei o percentual de oportunidades de emprego acessíveis a 15 minutos de carro em horário de pico. Segundo os dados do Censo, a maior parte dos deslocamentos casa-trabalho na RM de Recife são de menos de uma hora (83,4%). Assim, 15 minutos de carro (em horário de pico) me parece ser um “luxo” que as pessoas estão dispostas a pagar e que deve se refletir, em algum nível, no preço dos imóveis.\nO mapa sugere que Recife é uma cidade monocêntrica, com a maior parte dos empregos concentrada na região central. Interessante notar que, apesar da distância geográfica, a região sul, de Boa Viagem, Pina, etc. continua com indicadores de acessibilidade relativamente altos.\nPara tentar mensurar a verticalização calculei a proporção de apartamentos em relação ao total de domicílios em cada região. Novamente os dados vem do Censo e são agrupados a nível de setor censitário. Eu faço uma interpolação espacial simples com os hexágonos H3, na mesma resolução 9 do projeto do IPEA, para manter o padrão. Grosso modo, parece que as regiões de rendas mais altas coincidem com as regiões mais verticalizadas.\nPor fim, os dados de anúncios e preço provém de anúncios online de venda de imóveis ativos entre janeiro e junho de 2023. Por simplicidade, eu escolhi trabalhar apenas com anúncios de apartamentos e removi algumas observações discrepantes para conseguir um valor médio mais razoável. O preço médio observado na cidade neste período foi de R$7.900, com a maior parte das observações caindo dentro do intervalo R$3.800-R$12.700."
  },
  {
    "objectID": "posts/general-posts/2023-08-recife/index.html#detalhes-do-mapa",
    "href": "posts/general-posts/2023-08-recife/index.html#detalhes-do-mapa",
    "title": "Weekly Viz: Recife em mapas",
    "section": "",
    "text": "Os dados dos dois primeiros mapas, das principais vias e dos corpos d’água, são do OpenStreetMap. Pode-se ver, por exemplo, a Rodovia Mario Covas, que corta a cidade de norte a sul e o famoso Rio Capibaribe que atravessa a cidade.\nPara o terceiro mapa puxei os dados do Ciclomapa. Apesar do título, eu incluí tanto ciclovias, ciclofaixas e ciclorrotas.\nOs mapas da segunda linha foram todos feitos com dados do projeto de Acesso a Oportunidades do IPEA. Os dados de população e renda são do Censo de 2010 e espacialmente interpolados com os hexágonos H3, da Uber. Para facilitar a visualização usei o algoritmo de Jenks para agrupar os dados em 7 grupos. Além disso, usei uma transformação log tanto na renda como na população para reduzir um pouco da variância nos dados.\nA renda da cidade é visivelmente concentrada na região sul-sudeste, próxima do litoral e na região centro-leste, no que me parece ser a região dos bairros Madalena e Boa Vista. Já a densidade populacional não segue um padrão simples; a população está espalhada por toda a cidade.\nComo medida de acesso a empregos usei o percentual de oportunidades de emprego acessíveis a 15 minutos de carro em horário de pico. Segundo os dados do Censo, a maior parte dos deslocamentos casa-trabalho na RM de Recife são de menos de uma hora (83,4%). Assim, 15 minutos de carro (em horário de pico) me parece ser um “luxo” que as pessoas estão dispostas a pagar e que deve se refletir, em algum nível, no preço dos imóveis.\nO mapa sugere que Recife é uma cidade monocêntrica, com a maior parte dos empregos concentrada na região central. Interessante notar que, apesar da distância geográfica, a região sul, de Boa Viagem, Pina, etc. continua com indicadores de acessibilidade relativamente altos.\nPara tentar mensurar a verticalização calculei a proporção de apartamentos em relação ao total de domicílios em cada região. Novamente os dados vem do Censo e são agrupados a nível de setor censitário. Eu faço uma interpolação espacial simples com os hexágonos H3, na mesma resolução 9 do projeto do IPEA, para manter o padrão. Grosso modo, parece que as regiões de rendas mais altas coincidem com as regiões mais verticalizadas.\nPor fim, os dados de anúncios e preço provém de anúncios online de venda de imóveis ativos entre janeiro e junho de 2023. Por simplicidade, eu escolhi trabalhar apenas com anúncios de apartamentos e removi algumas observações discrepantes para conseguir um valor médio mais razoável. O preço médio observado na cidade neste período foi de R$7.900, com a maior parte das observações caindo dentro do intervalo R$3.800-R$12.700."
  },
  {
    "objectID": "posts/general-posts/2023-09-inflation/index.html",
    "href": "posts/general-posts/2023-09-inflation/index.html",
    "title": "Hyperinflation: a Brazilian journey",
    "section": "",
    "text": "For most of its modern history, Brazil has lived high inflation. Brazil’s official consumer price index was created only in 1980 so for a broader historical perspective we must look at other indices. The IGP-DI, devised by the Getulio Vargas Foundation, is a broad price index\n\n\nFrom January 1980 to July 1994, Brazil accumulated over a 13 trillion percentage inflation. In the following 14 years, total inflation would be much lower, close to 320%, and ever since Plano Real, inflation has totaled around 560%\n\n\n\n\n\n\n\n\nEven after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Wwo significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the following decade, the specter of inflation appeared once again. Facing a slowing econonmy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices are increased causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer takes stance.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence"
  },
  {
    "objectID": "posts/general-posts/2023-09-inflation/index.html#hyperinflation-under-the-scope",
    "href": "posts/general-posts/2023-09-inflation/index.html#hyperinflation-under-the-scope",
    "title": "Hyperinflation: a Brazilian journey",
    "section": "",
    "text": "From January 1980 to July 1994, Brazil accumulated over a 13 trillion percentage inflation. In the following 14 years, total inflation would be much lower, close to 320%, and ever since Plano Real, inflation has totaled around 560%"
  },
  {
    "objectID": "posts/general-posts/2023-09-inflation/index.html#post-real-brazil",
    "href": "posts/general-posts/2023-09-inflation/index.html#post-real-brazil",
    "title": "Hyperinflation: a Brazilian journey",
    "section": "",
    "text": "Even after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Wwo significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the following decade, the specter of inflation appeared once again. Facing a slowing econonmy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices are increased causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer takes stance.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-2-mapas.html",
    "href": "posts/ggplot2-tutorial/12-2-mapas.html",
    "title": "You need a map - Parte 2",
    "section": "",
    "text": "Dados espaciais são um tipo especial de dado geralmente representados por uma de três formas geométricas: pontos, linhas ou polígonos. Em alguns casos, como visto no post anterior, é possível tratar estes dados como dados tabulares quaisquer. Na maioria dos casos, contudo, será necessário maior cuidado. Isto acontece não somente por causa da complexidade das figuras geométricas, mas também porque dados espaciais carregam uma quantidade relativamente expressiva de metadados. Estes metadados caracterizam diversos aspectos sobre estes dados. Uma das funções destes metadados é de explicar ao R como aqueles dados estão projetados no espaço.\nEsta é uma discussão bastante técnica e neste post vou apresentar apenas o essencial para a construção de mapas. Isto é, vou ignorar os detalhes da projeção e até de convenções cartográficas e vou me focar em casos aplicados simples. Uma introdução a estes assuntos pode ser vista em Geocomputation with R.\nA lista completa de pacotes necessários para acompanhar este post segue abaixo.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tibble)\nlibrary(sf)\nlibrary(spData)\nlibrary(geobr)\nlibrary(censobr)\nlibrary(arrow)\nlibrary(BAMMtools)\n\n\n\nO pacote {sf} funciona como um data.frame com uma coluna especial chamada geometry. É possível ignorar a existência desta coluna e trabalhar normalmente com seus dados, usando todo o seu conhecimento acumulado previamente com dados tabulares convencionais. Eventualmente, será necessário juntar um shapefile com uma base de dados e até com outro shapefile; o {sf} também facilita isto bastante.\nComo comentado acima, vou ignorar a maior parte das convenções cartográficas (escala, rosa dos ventos, etc.). Vale comentar, contudo, que é possível incluir estes elementos no R sem grandes problemas.\nVou discutir majoritariamente sobre objetos espaciais no formato de vetores (vector shape file) já que eles são mais prevalentes nas ciências socias em geral.\n\n\nHá três peças fundamentais que vamos ver: pontos, linhas e polígonos. Nos três casos, usa-se a mesma função geom_sf() para montar a visualização. Os elementos estéticos seguem a mesma lógica das funções geom_point(), geom_line() e geom_rect(), respectivamente. Assim, para tornar um objeto espacial, em formato de linha, mais grosso, usa-se linewidth = 2, por exemplo.\nVou começar assumindo um conhecimento mínimo de R apesar deste ser um capítulo já avançado na série de tutoriais de ggplot2.\nA estrutura de objetos espaciais é um sf/data.frame ou um “spatial data frame”. Este objeto é essencialmente igual a um data.frame convencional, mas ele possui uma coluna especial chamada geom ou geometry. Para criar um objeto sf a partir de um data.frame ou tibble usa-se a função st_as_sf.\n\n\n\nO tipo mais simples de objeto geométrico é o ponto, definido por uma coordenada horizontal e uma coordenada vertical. O exemplo abaixo mostra como criar um spatial data.frame de pontos a partir dos dados de latitude e longitude da base do CEM. O primeiro código abaixo importa a base de dados tabulares e filtra os dados apenas para o ano de 2008 em São Paulo.\n\ncem &lt;- read_csv(\"...\")\n\ncem &lt;- cem |&gt; \n  mutate(code_muni = as.numeric(substr(ap2010, 1, 7))) |&gt; \n  filter(code_muni == 3550308)\n\ncem08 &lt;- filter(cem, ano_lanc == 2008)\n\nNote que é preciso escolher um sistema de projeção e eu escolho o SIRGAS 2000 EPSG: 4674, um sistema de projeção de referência para a América Latina. Para verificar o tipo de geometria dos objetos uso st_geometry.\n\npoints_cem &lt;- st_as_sf(cem08, coords = c(\"lon\", \"lat\"), crs = 4674)\n\nst_geometry(points_cem)\n\nGeometry set for 394 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.79478 ymin: -23.70684 xmax: -46.43215 ymax: -23.45133\nGeodetic CRS:  SIRGAS 2000\nFirst 5 geometries:\n\n\nPara gerar um gráfico a partir dos nossos dados, usa-se a função geom_sf, sem argumento algum. Note que este gráfico é idêntico ao que geramos antes usando geom_point.\n\nggplot(points_cem) +\n  geom_sf()\n\n\n\n\n\n\n\n\nComo comentei, há três tipos principais de objetos geométricos: pontos, linhas e polígonos. É possível gerar linhas e polígonos arbitrários a partir de coordenadas usando as funções do pacote sf. Na prática, é pouco usual criar objetos geométricos desta maneira e a sintaxe para desenhar mesmo formas geométricas simples como triângulos e quadrados é bastante trabalhosa, como o código abaixo deixa evidente.\n\n#&gt; Define as coordenadas do quadrado\nquadrado = list(\n  rbind(\n    c(0, 0),\n    c(0, 1),\n    c(1, 1),\n    c(1, 0),\n    #&gt; Note que é preciso repetir (0, 0)\n    c(0, 0)\n  )\n)\n#&gt; Converte as coordenads em um POLYGON\nquadrado &lt;- st_polygon(quadrado)\n#&gt; Converte em SFC\nquadrado &lt;- st_sfc(quadrado)\n#&gt; Plota o quadrado\nggplot(quadrado) +\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\nO pacote spData traz várias bases de dados espaciais interessantes. Para exemplificar um caso de linhas, vamos explorar a base seine que mostra os rios Seine, Marne e Yonne, na França.\n\nlibrary(spData)\nspData::seine\n\nSimple feature collection with 3 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 518344.7 ymin: 6660431 xmax: 879955.3 ymax: 6938864\nProjected CRS: RGF93 / Lambert-93\n   name                       geometry\n1 Marne MULTILINESTRING ((879955.3 ...\n2 Seine MULTILINESTRING ((828893.6 ...\n3 Yonne MULTILINESTRING ((773482.1 ...\n\n\nNovamente, podemos verificar o tipo de geometria deste objeto. Note que os dados estão em uma projeção específica (RGF93 / EPSG2154) em que os dados não estão em latitude/longitude, mas sim em metros.\n\nst_geometry(seine)\n\nGeometry set for 3 features \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 518344.7 ymin: 6660431 xmax: 879955.3 ymax: 6938864\nProjected CRS: RGF93 / Lambert-93\n\n\nO gráfico abaixo mostra o gráfico padrão usando geom_sf e mais alguns exemplos de customização. Note que os argumentos são essencialmente os mesmos que se fornece à função geom_line/geom_path.\n\nggplot(seine) + geom_sf()\nggplot(seine) + geom_sf(lwd = 1)\nggplot(seine) + geom_sf(lwd = 0.4, color = \"dodgerblue3\")\nggplot(seine) + geom_sf(linetype = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos começar importando um shapefile dos estados do Brasil, usando o excelente {geobr}. Note que este objeto possui uma coluna geom que guarda um objeto do tipo MULTIPOLYGON. Este é um tipo de polígono.\n\nufs &lt;- geobr::read_state(showProgress = FALSE)\n\nufs\n\nSimple feature collection with 27 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.99045 ymin: -33.75208 xmax: -28.83594 ymax: 5.271841\nGeodetic CRS:  SIRGAS 2000\nFirst 10 features:\n   code_state abbrev_state name_state code_region name_region\n1          11           RO   Rondônia           1       Norte\n2          12           AC       Acre           1       Norte\n3          13           AM   Amazonas           1       Norte\n4          14           RR    Roraima           1       Norte\n5          15           PA       Pará           1       Norte\n6          16           AP      Amapá           1       Norte\n7          17           TO  Tocantins           1       Norte\n8          21           MA   Maranhão           2    Nordeste\n9          22           PI      Piauí           2    Nordeste\n10         23           CE      Ceará           2    Nordeste\n                             geom\n1  MULTIPOLYGON (((-63.32721 -...\n2  MULTIPOLYGON (((-73.18253 -...\n3  MULTIPOLYGON (((-67.32609 2...\n4  MULTIPOLYGON (((-60.20051 5...\n5  MULTIPOLYGON (((-54.95431 2...\n6  MULTIPOLYGON (((-51.1797 4....\n7  MULTIPOLYGON (((-48.35878 -...\n8  MULTIPOLYGON (((-45.84073 -...\n9  MULTIPOLYGON (((-41.74605 -...\n10 MULTIPOLYGON (((-41.16703 -...\n\n\nO código abaixo monta o mapa.\n\nggplot(ufs) +\n  geom_sf()\n\n\n\n\n\n\n\n\nA função geom_sf aceita vários argumentos estéticos já que ela desenha pontos, linhas e polígonos. No caso do mapa abaixo, color define a cor do contorno/fronteira do polígono, enquanto fill define a cor do interior do polígono.\n\nggplot(ufs) +\n  geom_sf(color = \"gray90\", aes(fill = name_region), lwd = 0.05) +\n  scale_fill_brewer(name = \"\", type = \"qual\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nPor fim, vale notar que é possível usar a função coord_sf para fazer um “zoom” no mapa.\n\nggplot(ufs) +\n  geom_sf(aes(fill = name_region), lwd = 0.05) +\n  scale_fill_brewer(name = \"\", type = \"qual\") +\n  theme(legend.position = \"top\") +\n  coord_sf(xlim = c(-60, -48), ylim = c(-35, -20))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-2-mapas.html#introdução-ao-sf",
    "href": "posts/ggplot2-tutorial/12-2-mapas.html#introdução-ao-sf",
    "title": "You need a map - Parte 2",
    "section": "",
    "text": "O pacote {sf} funciona como um data.frame com uma coluna especial chamada geometry. É possível ignorar a existência desta coluna e trabalhar normalmente com seus dados, usando todo o seu conhecimento acumulado previamente com dados tabulares convencionais. Eventualmente, será necessário juntar um shapefile com uma base de dados e até com outro shapefile; o {sf} também facilita isto bastante.\nComo comentado acima, vou ignorar a maior parte das convenções cartográficas (escala, rosa dos ventos, etc.). Vale comentar, contudo, que é possível incluir estes elementos no R sem grandes problemas.\nVou discutir majoritariamente sobre objetos espaciais no formato de vetores (vector shape file) já que eles são mais prevalentes nas ciências socias em geral.\n\n\nHá três peças fundamentais que vamos ver: pontos, linhas e polígonos. Nos três casos, usa-se a mesma função geom_sf() para montar a visualização. Os elementos estéticos seguem a mesma lógica das funções geom_point(), geom_line() e geom_rect(), respectivamente. Assim, para tornar um objeto espacial, em formato de linha, mais grosso, usa-se linewidth = 2, por exemplo.\nVou começar assumindo um conhecimento mínimo de R apesar deste ser um capítulo já avançado na série de tutoriais de ggplot2.\nA estrutura de objetos espaciais é um sf/data.frame ou um “spatial data frame”. Este objeto é essencialmente igual a um data.frame convencional, mas ele possui uma coluna especial chamada geom ou geometry. Para criar um objeto sf a partir de um data.frame ou tibble usa-se a função st_as_sf.\n\n\n\nO tipo mais simples de objeto geométrico é o ponto, definido por uma coordenada horizontal e uma coordenada vertical. O exemplo abaixo mostra como criar um spatial data.frame de pontos a partir dos dados de latitude e longitude da base do CEM. O primeiro código abaixo importa a base de dados tabulares e filtra os dados apenas para o ano de 2008 em São Paulo.\n\ncem &lt;- read_csv(\"...\")\n\ncem &lt;- cem |&gt; \n  mutate(code_muni = as.numeric(substr(ap2010, 1, 7))) |&gt; \n  filter(code_muni == 3550308)\n\ncem08 &lt;- filter(cem, ano_lanc == 2008)\n\nNote que é preciso escolher um sistema de projeção e eu escolho o SIRGAS 2000 EPSG: 4674, um sistema de projeção de referência para a América Latina. Para verificar o tipo de geometria dos objetos uso st_geometry.\n\npoints_cem &lt;- st_as_sf(cem08, coords = c(\"lon\", \"lat\"), crs = 4674)\n\nst_geometry(points_cem)\n\nGeometry set for 394 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.79478 ymin: -23.70684 xmax: -46.43215 ymax: -23.45133\nGeodetic CRS:  SIRGAS 2000\nFirst 5 geometries:\n\n\nPara gerar um gráfico a partir dos nossos dados, usa-se a função geom_sf, sem argumento algum. Note que este gráfico é idêntico ao que geramos antes usando geom_point.\n\nggplot(points_cem) +\n  geom_sf()\n\n\n\n\n\n\n\n\nComo comentei, há três tipos principais de objetos geométricos: pontos, linhas e polígonos. É possível gerar linhas e polígonos arbitrários a partir de coordenadas usando as funções do pacote sf. Na prática, é pouco usual criar objetos geométricos desta maneira e a sintaxe para desenhar mesmo formas geométricas simples como triângulos e quadrados é bastante trabalhosa, como o código abaixo deixa evidente.\n\n#&gt; Define as coordenadas do quadrado\nquadrado = list(\n  rbind(\n    c(0, 0),\n    c(0, 1),\n    c(1, 1),\n    c(1, 0),\n    #&gt; Note que é preciso repetir (0, 0)\n    c(0, 0)\n  )\n)\n#&gt; Converte as coordenads em um POLYGON\nquadrado &lt;- st_polygon(quadrado)\n#&gt; Converte em SFC\nquadrado &lt;- st_sfc(quadrado)\n#&gt; Plota o quadrado\nggplot(quadrado) +\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\nO pacote spData traz várias bases de dados espaciais interessantes. Para exemplificar um caso de linhas, vamos explorar a base seine que mostra os rios Seine, Marne e Yonne, na França.\n\nlibrary(spData)\nspData::seine\n\nSimple feature collection with 3 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 518344.7 ymin: 6660431 xmax: 879955.3 ymax: 6938864\nProjected CRS: RGF93 / Lambert-93\n   name                       geometry\n1 Marne MULTILINESTRING ((879955.3 ...\n2 Seine MULTILINESTRING ((828893.6 ...\n3 Yonne MULTILINESTRING ((773482.1 ...\n\n\nNovamente, podemos verificar o tipo de geometria deste objeto. Note que os dados estão em uma projeção específica (RGF93 / EPSG2154) em que os dados não estão em latitude/longitude, mas sim em metros.\n\nst_geometry(seine)\n\nGeometry set for 3 features \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 518344.7 ymin: 6660431 xmax: 879955.3 ymax: 6938864\nProjected CRS: RGF93 / Lambert-93\n\n\nO gráfico abaixo mostra o gráfico padrão usando geom_sf e mais alguns exemplos de customização. Note que os argumentos são essencialmente os mesmos que se fornece à função geom_line/geom_path.\n\nggplot(seine) + geom_sf()\nggplot(seine) + geom_sf(lwd = 1)\nggplot(seine) + geom_sf(lwd = 0.4, color = \"dodgerblue3\")\nggplot(seine) + geom_sf(linetype = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos começar importando um shapefile dos estados do Brasil, usando o excelente {geobr}. Note que este objeto possui uma coluna geom que guarda um objeto do tipo MULTIPOLYGON. Este é um tipo de polígono.\n\nufs &lt;- geobr::read_state(showProgress = FALSE)\n\nufs\n\nSimple feature collection with 27 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.99045 ymin: -33.75208 xmax: -28.83594 ymax: 5.271841\nGeodetic CRS:  SIRGAS 2000\nFirst 10 features:\n   code_state abbrev_state name_state code_region name_region\n1          11           RO   Rondônia           1       Norte\n2          12           AC       Acre           1       Norte\n3          13           AM   Amazonas           1       Norte\n4          14           RR    Roraima           1       Norte\n5          15           PA       Pará           1       Norte\n6          16           AP      Amapá           1       Norte\n7          17           TO  Tocantins           1       Norte\n8          21           MA   Maranhão           2    Nordeste\n9          22           PI      Piauí           2    Nordeste\n10         23           CE      Ceará           2    Nordeste\n                             geom\n1  MULTIPOLYGON (((-63.32721 -...\n2  MULTIPOLYGON (((-73.18253 -...\n3  MULTIPOLYGON (((-67.32609 2...\n4  MULTIPOLYGON (((-60.20051 5...\n5  MULTIPOLYGON (((-54.95431 2...\n6  MULTIPOLYGON (((-51.1797 4....\n7  MULTIPOLYGON (((-48.35878 -...\n8  MULTIPOLYGON (((-45.84073 -...\n9  MULTIPOLYGON (((-41.74605 -...\n10 MULTIPOLYGON (((-41.16703 -...\n\n\nO código abaixo monta o mapa.\n\nggplot(ufs) +\n  geom_sf()\n\n\n\n\n\n\n\n\nA função geom_sf aceita vários argumentos estéticos já que ela desenha pontos, linhas e polígonos. No caso do mapa abaixo, color define a cor do contorno/fronteira do polígono, enquanto fill define a cor do interior do polígono.\n\nggplot(ufs) +\n  geom_sf(color = \"gray90\", aes(fill = name_region), lwd = 0.05) +\n  scale_fill_brewer(name = \"\", type = \"qual\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nPor fim, vale notar que é possível usar a função coord_sf para fazer um “zoom” no mapa.\n\nggplot(ufs) +\n  geom_sf(aes(fill = name_region), lwd = 0.05) +\n  scale_fill_brewer(name = \"\", type = \"qual\") +\n  theme(legend.position = \"top\") +\n  coord_sf(xlim = c(-60, -48), ylim = c(-35, -20))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-2-mapas.html#juntando-shapes-e-dados",
    "href": "posts/ggplot2-tutorial/12-2-mapas.html#juntando-shapes-e-dados",
    "title": "You need a map - Parte 2",
    "section": "Juntando shapes e dados",
    "text": "Juntando shapes e dados\nPara compor este tipo de mapa é necessário combinar dados tabulares com objetos geométricos. No exemplo abaixo, vamos novamente considerar a tabela de tipo de propriedade do domicílio por UF.\n\n\nCode\nrented &lt;- tibble::tribble(\n  ~abbrev_state, ~share_apto, ~share_rented, ~name_region, \n  #------------#------------#--------------#------------#      \n           \"RO\",           8,          11.1, \"Norte\", \n           \"AC\",        7.47,          6.93, \"Norte\",\n           \"AM\",        15.8,          7.26, \"Norte\",\n           \"RR\",        14.3,          11.7, \"Norte\", \n           \"PA\",        4.53,          6.39, \"Norte\",\n           \"AP\",        10.7,          6.34, \"Norte\",\n           \"TO\",        3.17,          11.7, \"Norte\", \n           \"MA\",        4.06,          5.74, \"Nordeste\",\n           \"PI\",        4.08,          5.15, \"Nordeste\",\n           \"CE\",        9.85,          9.68, \"Nordeste\",\n           \"RN\",        9.27,          10.8, \"Nordeste\", \n           \"PB\",        11.9,          9.18, \"Nordeste\",\n           \"PE\",        10.4,          9.98, \"Nordeste\",\n           \"AL\",        6.32,          10.0, \"Nordeste\", \n           \"SE\",        12.0,            11, \"Nordeste\",   \n           \"BA\",        12.0,          7.61, \"Nordeste\",\n           \"MG\",        14.0,          10.5, \"Sudeste\", \n           \"ES\",        21.7,          10.7, \"Sudeste\", \n           \"RJ\",        26.8,          10.7, \"Sudeste\", \n           \"SP\",        19.4,          12.7, \"Sudeste\", \n           \"PR\",        11.9,          11.7, \"Sul\", \n           \"SC\",          17,          11.8, \"Sul\", \n           \"RS\",        16.5,          8.46, \"Sul\",\n           \"MS\",        3.67,          11.9, \"Centro Oeste\", \n           \"MT\",        3.29,          12.7, \"Centro Oeste\", \n           \"GO\",        9.88,          13.6, \"Centro Oeste\", \n           \"DF\",        35.4,          17.8, \"Centro Oeste\"\n  )\n\n\nPara juntar esta base com o shapefile das UFs, gerado pelo geobr, basta usar a função left_join do pacote dplyr. Esta função tem o cuidado de preservar a geometria do objeto espacial. Note que o objeto espacial (sf) deve sempre vir na esquerda: left_join(objeto_espacial, base_de_dados, by = \"chave\").\n\nufs_rent &lt;- left_join(ufs, rented, by = \"abbrev_state\")\n\nggplot(ufs_rent) + \n  geom_sf(aes(fill = share_rented)) +\n  scale_fill_distiller(direction = 1) +\n  coord_sf(xlim = c(NA, -35))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-2-mapas.html#temas-para-mapas",
    "href": "posts/ggplot2-tutorial/12-2-mapas.html#temas-para-mapas",
    "title": "You need a map - Parte 2",
    "section": "Temas para mapas",
    "text": "Temas para mapas\nNão há grande variedade de temas para mapas. Em geral, usa-se ou ggthemes::theme_map() ou theme_void(). Estes temas apagam a maioria dos elementos temáticos do gráfico, como eixos e linhas de grade.\n\nbase_map &lt;- ggplot(ufs_rent) + \n  geom_sf(aes(fill = share_rented)) +\n  scale_fill_distiller(direction = 1, palette = 2) +\n  coord_sf(xlim = c(NA, -35))\n\nbase_map + \n  ggtitle(\"theme_map()\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\nbase_map + \n  ggtitle(\"theme_void()\") +\n  theme_void()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/12-2-mapas.html#discretizando-dados-contínuos",
    "href": "posts/ggplot2-tutorial/12-2-mapas.html#discretizando-dados-contínuos",
    "title": "You need a map - Parte 2",
    "section": "Discretizando dados contínuos",
    "text": "Discretizando dados contínuos\nO olho humano não é particularmente apto a distinguir diferenças sutis entre cores, quando dispotas lado a lado. Neste sentido, pode ser difícil captar certas nuances nos mapas coropléticos acima.\nHá três maneiras simples de agrupar dados contínuos para fazer mapas:\n\nUsar alguma variação de percentil, isto, quintis, decis, etc.\nUsar o algoritmo de Jenks.\nUsar a distribuição dos dados.\nDefinir quebras manualmente.\n\nVamos montar o mapa da renda domiciliar per capita de Porto Alegre a nível de setor censitário. Para importar os dados vamos usar o pacote censobr. O código abaixo é adaptado da páginas de exemplos do pacote.\n\n\nCode\nlibrary(arrow)\nlibrary(censobr)\n\n#&gt; Importa o shapefile dos setores censitários de Porto Alegre\ntracts &lt;- geobr::read_census_tract(\n  code_tract = 4314902,\n  year = 2010,\n  simplified = FALSE,\n  showProgress = FALSE\n  )\n\n#&gt; Importa a tabela Basico do Censo (2010)\ntract_basico &lt;- read_tracts(\n  year = 2010,\n  dataset = \"Basico\", \n  showProgress = FALSE\n  )\n#&gt; Importa a tabela DomicilioRenda do Censo (2010)\ntract_income &lt;- read_tracts(\n  year = 2010,\n  dataset = \"DomicilioRenda\", \n  showProgress = FALSE\n  )\n\n#&gt; Seleciona as colunas relevantes\ntract_basico &lt;- tract_basico |&gt;\n  dplyr::filter(code_muni == \"4314902\") |&gt; \n  dplyr::select('code_tract','V002')\n\ntract_income &lt;- tract_income |&gt; \n  dplyr::filter(code_muni == \"4314902\") |&gt; \n  dplyr::select('code_tract','V003')\n\n#&gt; Juntas as bases e importa os dados\ntracts_df &lt;- dplyr::left_join(tract_basico, tract_income)\ntracts_df &lt;- dplyr::collect(tracts_df)\n#&gt; Calcula a renda per capita\ntracts_df &lt;- dplyr::mutate(tracts_df, income_pc = V003 / V002)\n#&gt; Junta os dados com o shapefile\npoa_tracts &lt;- left_join(tracts, tracts_df, by = \"code_tract\")\n\n\nO mapa abaixo mostra o renda domiciliar per capita na zona central de Porto Alegre em cada setor censitário. Pode-se ver, de maneira geral, uma concentração de alta renda na região central em torno dos bairros Moinhos de Vento e Auxiliadora. Já as regiões periféricas da cidade tem renda mais baixa, com exceção de uma parte da Zona Sul da cidade.\n\nggplot(poa_tracts) +\n  geom_sf(aes(fill = income_pc), color = \"white\", lwd = 0.01) +\n  scale_fill_distiller(name = \"\", palette = \"Greens\") +\n  coord_sf(xlim = c(-51.32, -51.1), ylim = c(-30.15, -30)) +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\nQuintis e decis\nÉ relativamente simples classificar os dados em percentis usando a função dplyr::ntile.\n\n# Quintis e decis\npoa_tracts &lt;- poa_tracts |&gt; \n  mutate(\n    quintil = factor(ntile(income_pc, 5)),\n    decil = factor(ntile(income_pc, 10))\n    )\n\nO mapa abaixo mostra a distribuição espacial do rendimento domiciliar per capita em Porto Alegre por quintis. Nesta visualização, é bastante imediato perceber os grupos de renda da cidade. Note que a legenda do mapa, infelizmente, não mostra o intervalo de renda de cada grupo. Para montar uma legenda mais instrutiva é preciso um pouco de criatividade. A solução mais imediata, que consome menos linhas de código, é usar a função cut em conjunto com a função quantile. Na minha experiência, os resultados desta combinação costumam ser pouco satisfatórios; o trabalho adicional de se construir a legenda costuma compensar.\n\nggplot(poa_tracts) +\n  geom_sf(aes(fill = quintil), color = \"white\", lwd = 0.1) +\n  scale_fill_brewer(name = \"\", palette = \"Greens\") +\n  ggtitle(\"Mapa de quintis\") +\n  coord_sf(xlim = c(-51.32, -51.1), ylim = c(-30.15, -30)) +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\nO código abaixo calcula os quantis e monta uma legenda. O código pode parecer um pouco grande, mas é fácil criar uma função que reproduz este funcionamento.\n\nq &lt;- quantile(poa_tracts$income_pc, seq(0.2, 0.8, 0.2), na.rm = TRUE)\nq &lt;- format(round(q), big.mark = \".\", trim = TRUE)\n\nl &lt;- paste(q, q[-1], sep = \"-\")\nl &lt;- c(paste(\"&lt;\", q[1]), l)\nl[length(l)] &lt;- paste(\"&gt;\", q[length(q)])\n\nl\n\n[1] \"&lt; 531\"       \"531-928\"     \"928-1.526\"   \"1.526-2.372\" \"&gt; 2.372\"    \n\n\n\nget_legend_discrete &lt;- function(x) {\n  \n  y &lt;- suppressWarnings(format(round(x), big.mark = \".\", trim = TRUE))\n  \n  l &lt;- paste(y, y[-1], sep = \"-\")\n  l &lt;- c(paste(\"&lt;\", y[1]), l)\n  l[length(l)] &lt;- paste(\"&gt;\", y[length(y)])\n  \n  return(l)\n  \n}\n\nO mapa abaixo mostra a renda per capita em decis. Agora usamos a escala de cores viridis que costuma funcionar bem com mapas.\n\ndecis &lt;- quantile(poa_tracts$income_pc, seq(0.1, 0.9, 0.1), na.rm = TRUE)\nlabel_decil &lt;- get_legend_discrete(decis)\n\nggplot(poa_tracts) +\n  geom_sf(aes(fill = decil), color = \"white\", lwd = 0.1) +\n  scale_fill_viridis_d(name = \"\", label = label_decil) +\n  ggtitle(\"Mapa de Decis\") +\n  coord_sf(xlim = c(-51.32, -51.1), ylim = c(-30.15, -30)) +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nAlgoritmo de Jenks\nO algoritmo de Jenks, também conhecido como algoritmo de quebras naturais, serve para classificar dados em grupos. Essencialmente, o algoritmo tenta encontrar \\(k\\) grupos que sejam o mais homogêneos possíveis. Este método foi desenvolvido pelo cartógrafo George Jenks e costuma funcionar muito bem para a construção de mapas. A implementação mais eficiente do algoritmo no R é feita via BAMMtools::getJenksBreaks.\nNote que é preciso escolher manualmente o número de grupos \\(k\\). Em geral, vale experimentar com alguns valores entre 3-9. Para melhor entender o funcionamento do algoritmo, o gráfico abaixo mostra o histograma da variável de renda per capita, onde as linhas retas indicam as quebras entre os grupos.\n\njenks &lt;- BAMMtools::getJenksBreaks(poa_tracts$income_pc, k = 7)\n\nggplot(poa_tracts, aes(x = income_pc)) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = jenks)\n\n\n\n\n\n\n\n\nO mapa final é feito abaixo.\n\npoa_tracts &lt;- poa_tracts |&gt; \n  mutate(\n    jenks_groups = factor(findInterval(income_pc, jenks))\n    )\n\n#&gt; Construir a legenda\nlabel_jenks &lt;- get_legend_discrete(jenks)\nlabel_jenks[length(label_jenks)] &lt;- NA\n\nggplot(poa_tracts) +\n  geom_sf(aes(fill = jenks_groups), color = NA) +\n  scale_fill_brewer(\n    name = \"\",\n    palette = \"Greens\",\n    labels = label_jenks,\n    na.value = \"gray70\") +\n  ggtitle(\"Mapa de Quebras Naturais (Jenks)\") +\n  coord_sf(xlim = c(-51.32, -51.1), ylim = c(-30.15, -30)) +\n  ggthemes::theme_map() +\n  theme(plot.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\n\n\nMapa de desvio-padrão\nTambém é possível construir mapas usando medidas de dispersão estatísticas tradicionais, como a distância interquartílica ou desvio-padrão. Neste exemplo, vamos classificar os dados como desvios-padrão em relação à média. Para melhorar a visualização, vamos primeiro usar uma transformação log nos dados de renda para que a distribuição fique mais próxima de uma normal.\nEste tipo de gráfico funciona bem com escalas de cores divergentes. Note como este mapa destaca as regiões de renda muito alta e também as regiões vulneráveis de renda muito baixa.\n\npoa_tracts &lt;- poa_tracts |&gt; \n  mutate(\n    lincome_pc = log(income_pc),\n    scale_income = as.numeric(scale(lincome_pc)),\n    sd_groups = factor(findInterval(scale_income, c(-2, -1, 0, 1, 2)))\n  )\n\nl &lt;- c(\"&lt; -2\", \"-2 a -1\", \"-1 a 0\", \"0 a 1\", \"1 a 2\", \"&gt; 2\")\nl &lt;- paste(l, \"SD\")\n\nggplot(poa_tracts) +\n  geom_sf(aes(fill = sd_groups), color = NA) +\n  scale_fill_brewer(\n    name = \"\",\n    palette = \"RdBu\",\n    labels = l,\n    na.value = \"gray70\") +\n  ggtitle(\"Mapa de Desvio Padrão\") +\n  coord_sf(xlim = c(-51.32, -51.1), ylim = c(-30.15, -30)) +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html",
    "title": "Tendência e Sazonalidade",
    "section": "",
    "text": "Séries de tempo costumam exibir alguns padrões similares. Uma abordagem particularmente útil é de decompor uma série de tempo em componentes, que representam características específicas. Pode-se pensar numa série de tempo como uma conjunção de três componentes:\n\nTendência\nSazonalidade\nResíduo (resto, ruído, etc.)\n\nEm geral, a tendência varia pouco no tempo, segue algum ciclo longo, como a tendência de crescimento do PIB de um país. O movimento sazonal de uma série reflete algum tipo de variação períodica. Muitas séries possuem sazonalidade, como nfa demanda por energia elétrica ao longo dos trimestres, o número de nascimentos a cada mês, o número diário de acidentes de trânsito a cada semana, a temperatura média ao longo do dia, etc.\nEntender a sazonalidade de um fenômeno é essencial para a sua modelagem estatística. Vale notar que, usualmente, o interesse de economistas e econometristas não está na sazonalidade em si, mas sim na série dessazonalizada, isto é, livre de qualquer sazonalidade. Esta abordagem enfatiza mais a busca pela tendência “limpa” da série do que pelo efeito sazonal.\nA sazonalidade se expressa em várias frequências, mas a ênfase deste post será em sazonalidades mensais e trimestrais. Sazonalidades complexas (mistas, por exemplo) ou de alta frequência (intradiária, diária, semanal), em geral, exigem um pouco mais de esforço no setup e pacotes específicos1."
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#stl",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#stl",
    "title": "Tendência e Sazonalidade",
    "section": "STL",
    "text": "STL\nA decomposição STL é mais sofisticada do que a decomposição clássica. A metodologia STL foi apresentada no influente artigo STL: A Seasonal-Trend Decomposition Procedure Based on Loess dos autores Robert Cleveland, William Cleveland, Jean McRae e Irma Terpenning. Assim como a decomposição clássica, a decomposição STL divide uma série em três componentes: um componente de tendência (trend), uma componente sazonal (seasonal) e um componente aleatório (remainder).\nO STL foi feito para ser um método versátil, resistente a outliers e eficiente (do ponto de vista computacional). Tipicamente, a decomposição STL funciona com qualquer série (mesmo quando há observações ausentes) independentemente da sua frequência. Para modelar a tendência e sazonalidade da série, o STL usa uma regressão LOESS. Além de ser mais flexível do que a média móvel, que vimos acima, a regressão LOESS não perde observações da série.\nO código abaixo mostra como calcular a decomposição STL e apresenta os resultados visualmente usando a série co2 que vem pré-carregada no R. Esta série é similar a uma das utilizadas no artigo original.\n\nstl_13 &lt;- stl(co2, s.window = 13)\n\nautoplot(stl_13) + theme_series\n\n\n\n\n\n\n\n\nA função stl tem diversos parâmetros de “suaviazação”, que servem para escolher o tamanho e intensidade dos ciclos de tendência e sazonalidade. Via de regra quanto maiores os valores dos argumentos x.window mais suave será o ajuste final. Outro argumento potencialmente útil é definir robust = TRUE quando a série possui outliers.\nNo presente contexto, o argumento mais relevante do comando stl é o s.window ou \\(n_{(s)}\\) na nomenclatura do artigo original. Este parâmetro, em linhas gerais, define o grau de suaviazação da tendência sazonal. Ele deve ser um número ímpar e pelo menos igual a 7. O artigo original sugere uma ferramenta visual para escolher o parâmetro, mas concede que a escolha final é arbitrária e depende da sensibilidade do usuário.\nO gráfico apresentado é o seasonal-diagnostic plot. As linhas mostram a “intensidade” do efeito sazonal e os pontos indicam como este efeito sazonal varia com o ruído aleatório. Note como as variações da curva coincidem com as variações dos pontos, sugerindo que as oscilações sazonais estão sendo afetadas pelo ruído da série.\n\n\nCode\ncomponents &lt;- stl_13$time.series\n\ndat = data.frame(\n  date = as.Date.ts(components),\n  coredata(components)\n)\n\ndat$month = lubridate::month(dat$date, label = TRUE, locale = \"pt_BR\")\ndat$year = lubridate::year(dat$date)\n\ndat &lt;- dat |&gt; \n  mutate(s_avg = mean(seasonal), .by = \"month\") |&gt; \n  mutate(s1 = seasonal - s_avg, s2 = seasonal - s_avg + remainder)\n\nggplot(dat) +\n  geom_line(aes(x = year, y = s1), lwd = 0.8) +\n  geom_point(aes(x = year, y = s2), size = 0.8, shape = 21) +\n  facet_wrap(vars(month)) +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico abaixo repete o mesmo exercício mas utilizando s.window = 35 para suavizar a série original. Note como as curvas estão mais suaves.\n\n\nCode\nstl_35 &lt;- stl(co2, s.window = 35)\ncomponents &lt;- stl_35$time.series\n\ndat = data.frame(\n  date = as.Date.ts(components),\n  coredata(components)\n)\n\ndat$month = lubridate::month(dat$date, label = TRUE, locale = \"pt_BR\")\ndat$year = lubridate::year(dat$date)\n\ndat &lt;- dat |&gt; \n  mutate(s_avg = mean(seasonal), .by = \"month\") |&gt; \n  mutate(s1 = seasonal - s_avg, s2 = seasonal - s_avg + remainder)\n\nggplot(dat) +\n  geom_line(aes(x = year, y = s1), lwd = 0.8) +\n  geom_point(aes(x = year, y = s2), size = 0.8, shape = 21) +\n  facet_wrap(vars(month)) +\n  theme_series\n\n\n\n\n\n\n\n\n\nA função forecast::mstl oferece uma opção menos manual do ajuste STL. Esta função executa seis janelas distintas para s.window iterativamente. Os resultados costumam ser satisfatórios, mas como de costume, é necessário revisar o ajuste final.\n\nstl_auto = mstl(co2, lambda = \"auto\")\nautoplot(stl_auto)"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#x13-arima",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#x13-arima",
    "title": "Tendência e Sazonalidade",
    "section": "X13-ARIMA",
    "text": "X13-ARIMA\nO X13-ARIMA ou X13-ARIMA-SEATS é a versão mais recente do X11-ARIMA, uma metodologia desenvolvida pelo US Census Bureau para lidar com a sazonalidade de séries de tempo. Este método foi pensado para lidar com o tipo de sazonalidade comumemente encontrada em séries econômicas. Os métodos de ajuste são implementados no R via o pacote seasonal.\nA função principal do pacote seas realiza um ajuste sazonal automático. Para verificar os principais resultados do ajuste usa-se a função summary. O ajuste automático costuma ser bom, mas é sempre necessário revisar o ajuste.\n\n#&gt; Consumo mensal de energia elétrica - Residencial\nenerg = rbcb::get_series(1403, as = \"ts\", start_date = as.Date(\"2002-01-01\"))\n#&gt; Executa a rotina do X13-ARIMA\nsenerg = seas(energ)\n#&gt; Resumo dos resultados\nsummary(senerg)\n\n\nCall:\nseas(x = energ)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \nMon               -0.0042020  0.0023735  -1.770  0.07666 .  \nTue               -0.0059006  0.0023706  -2.489  0.01281 *  \nWed                0.0001782  0.0023459   0.076  0.93944    \nThu                0.0071610  0.0023445   3.054  0.00226 ** \nFri                0.0042435  0.0023327   1.819  0.06889 .  \nSat               -0.0009477  0.0023489  -0.403  0.68662    \nEaster[15]        -0.0285829  0.0051214  -5.581 2.39e-08 ***\nLS2002.Apr         0.0741254  0.0171309   4.327 1.51e-05 ***\nAO2014.Feb         0.0807054  0.0158664   5.087 3.65e-07 ***\nLS2015.Mar        -0.0791423  0.0154974  -5.107 3.28e-07 ***\nMA-Nonseasonal-01  0.5255629  0.0539651   9.739  &lt; 2e-16 ***\nMA-Seasonal-12     0.7236629  0.0507009  14.273  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 264  Transform: log\nAICc:  3379, BIC:  3423  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 16.46   Shapiro (normality): 0.9953  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nO gráfico abaixo mostra a série original e a série sazonalmente ajustada em vermelho. Para extrair a série ajustada usa-se a função final().\n\nautoplot(energ) +\n  autolayer(final(senerg)) +\n  guides(color = \"none\") +\n  theme_series\n\n\n\n\n\n\n\n\nO pacote seasonal é muito bem documentado e inclui um artigo de apresentação com exemplos, um texto mostrando como rodar os exemplos oficiais do X13 dentro do R e até uma ferramenta interativa. Neste post não vou explorar todas as nuances do pacote.\nPara conseguir um ajuste mais adequado é preciso explorar os argumentos adicionais da função seas além de adaptar as variáveis de calendário. Na sua configuração padrão, a função seas não considera o efeito do carnaval, por exemplo. É possível criar eventos de calendário usando a função genhol (de “generate holiday”).\nUm recurso bastante útil para conseguir as datas das festividades brasileiras é o site do prof. Roberto Cabral de Mello Borges. O código abaixo extrai a tabela com as datas da Páscoa, Carnaval e Corpus Christi de 1951-2078.\n\n\nCode\nlibrary(rvest)\nurl = \"https://www.inf.ufrgs.br/~cabral/tabela_pascoa.html\"\ntabela = url |&gt; \n  read_html() |&gt; \n  html_table()\ntab = tabela[[1]]\n\nferiados_bra = data.frame(tab[-1, ])\nnames(feriados_bra) = as.character(tab[1, ])\n\nhead(feriados_bra)\n\n\n   Ano  a b c  d e d+e Dia   Mês      Páscoa    Carnaval Corpus Christi\n1 1951 13 3 5  1 2   3  25 Março 25/mar/1951 06/fev/1951    24/mai/1951\n2 1952 14 0 6 20 2  22  13 Abril 13/abr/1952 26/fev/1952    12/jun/1952\n3 1953 15 1 0  9 5  14   5 Abril 05/abr/1953 17/fev/1953    04/jun/1953\n4 1954 16 2 1 28 6  34  18 Abril 18/abr/1954 02/mar/1954    17/jun/1954\n5 1955 17 3 2 17 2  19  10 Abril 10/abr/1955 22/fev/1955    09/jun/1955\n6 1956 18 0 3  6 4  10   1 Abril 01/abr/1956 14/fev/1956    31/mai/1956\n\n\nPara utilizar este dado no X13-ARIMA é necessário converter a coluna Carnaval num tipo Date e então utilizar a função genhol. Para facilitar a leitura das datas uso o pacote readr.\n\nlibrary(readr)\n\nferiados_bra$date_carnaval = parse_date(\n  feriados_bra$Carnaval,\n  format = \"%d/%b/%Y\",\n  locale = locale(\"pt\")\n  )\n\ncarnaval = genhol(feriados_bra$date_carnaval, start = -3, end = 1, frequency = 12)\n\nAs datas de carnaval são inseridas dentro do X13-ARIMA via o argumento xreg.\n\nsenerg = seas(\n  energ,\n  xreg = carnaval,\n  regression.variables = \"td1coef\",\n  arima.model = c(0, 1, 1, 0, 1, 1)\n)\n\nsummary(senerg)\n\n\nCall:\nseas(x = energ, xreg = carnaval, regression.variables = \"td1coef\", \n    arima.model = c(0, 1, 1, 0, 1, 1))\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \nxreg              -0.027271   0.006800  -4.011 6.05e-05 ***\nEaster[15]        -0.042946   0.006329  -6.786 1.16e-11 ***\nLS2015.Mar        -0.079623   0.016601  -4.796 1.62e-06 ***\nMA-Nonseasonal-01  0.597775   0.049154  12.161  &lt; 2e-16 ***\nMA-Seasonal-12     0.707968   0.049235  14.379  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 264  Transform: log\nAICc:  3431, BIC:  3451  QS (no seasonality in final):4.891 .\nBox-Ljung (no autocorr.):   108 *** Shapiro (normality): 0.9928  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nO gráfico abaixo mostra o resulado do ajuste com as datas de feriado modificadas.\n\nautoplot(energ) +\n  autolayer(final(senerg)) +\n  guides(color = \"none\") +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#sarima",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#sarima",
    "title": "Tendência e Sazonalidade",
    "section": "SARIMA",
    "text": "SARIMA\nOs modelos SARIMA incluem um componente de “sazonalidade estocástica” nos modelos ARIMA. Já escrevi um post onde detalho melhor alguns aspectos teóricos deste tipo de modelo. Vale notar o modelo SARIMA não realiza uma decomposição de tendência e sazonalidade com visto acima. Quando se aplica um SARIMA, implicitamente, supõe-se que a série possui uma tendência sazonal estocástica (i.e. raiz unitária sazonal) e não uma tendência sazonal determinística5.\nComo último exemplo vamos analisar a demanda mensal por gasolina. A série é da ANP e registra o total de vendas de gasolina C no Brasil em metros cúbicos.\n\ngasolina = read_csv(\"...\")\n\n\ngas = ts(na.omit(gasolina$demand), start = c(2012, 1), frequency = 12)\nautoplot(gas) + theme_series\n\n\n\n\n\n\n\n\nA série parece exibir algum componente de sazonalidade. Parece haver um pico de consumo todo mês de dezembro e uma queda todo mês de fevereiro.\n\nggseasonplot(gas) +\n  scale_color_viridis_d() +\n  theme_series\n\n\n\n\n\n\n\n\nA escolha da ordem do modelo SARIMA exige a inspeção visual do correlograma da série. Para mais detalhes consulte meu post mais detalhado sobre o assunto. Aqui, por simplicidade, uso um modelo simples, que costuma funcionar bem para série em geral.\n\nsarima_model = Arima(log(gas), order = c(0, 1, 1), seasonal = c(0, 1, 1))\n\nsarima_model\n\nSeries: log(gas) \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.2719  -0.7227\ns.e.   0.0973   0.0972\n\nsigma^2 = 0.003158:  log likelihood = 179.5\nAIC=-353.01   AICc=-352.81   BIC=-344.52\n\n\nUm dos pontos fortes dos modelos SARIMA é de gerar boas previsões de curto prazo com grande facilidade. Como comentei anteriormente, não há uma interpretação simples para a sazonalidade neste tipo de abordagem.\n\nautoplot(forecast(sarima_model), include = 48) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-01-sazonalidade/index.html#footnotes",
    "href": "posts/general-posts/2024-01-sazonalidade/index.html#footnotes",
    "title": "Tendência e Sazonalidade",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara uma referência para séries com sazonalidade complexa no R veja Hyndman e Athansopoulos (2021) Forecasting: Principles and Practice.↩︎\nPode-se usar um polinômio de qualquer grau, mas polinômios de ordens muito elevadas costumam se ajustar “perfeitamente” aos dados e vão absorver toda a sazonalidade da série.↩︎\nSempre coloca-se uma variável binária a menos do que períodos sazonais pela questão do posto da matriz de regressores. Na prática, se houvesse uma dummy para cada período sazonal a matriz de regressão seria uma matriz identidade.↩︎\nÉ comum ver esta expressão nos textos de séries de tempo; em geral o termo é utilizado em contraste com modelos SARIMA onde a sazonalidade é estocástica, mas o termo “determinístico” não tem implicação causal. Na prática, quer dizer que a sazonalidade não varia no tempo e é sempre a mesma o que pode gerar previsões ruins a depender do caso.↩︎\nPara uma boa apresentação sobre raiz unitária e sobre tendências determinísticas veja Enders (2009) Applied Econometric Time Series.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html",
    "href": "posts/general-posts/2024-02-media-movel/index.html",
    "title": "Médias móveis",
    "section": "",
    "text": "Uma média móvel, como o nome sugere, calcula a média de uma série temporal em janelas móveis. Tipicamente, a média móvel serve como uma estimativa da tendência da série; também é bastante comum usar a média móvel para identificar ciclos ou padrões numa série. Aplicar uma média móvel numa série de tempo \\(y_t\\) produz uma nova série \\(z_t\\):\n\\[\nz_t = \\sum_{j = -k}^{k}a_{j}y_{t+j}\n\\] No caso mais simples, k = 1, temos:\n\\[\nz_{t} = a_1y_{t-1} + a_2 y_{t} + a_3 y_{t+1}\n\\]\nSupondo que os pesos devem ser todos iguais e somar um temos a média móvel simples abaixo:\n\\[\nz_{t} = \\frac{1}{3}y_{t-1} +\\frac{1}{3} y_{t} + \\frac{1}{3} y_{t+1}\n\\]\nO exemplo acima, mostra uma média móvel simétrica de ordem 3, onde todos os pesos são iguais. Cada ponto na nova série \\(z_t\\) é uma média entre os valores vizinhos da série original. Tipicamente, valores próximos uns aos outros no tempo costumam ser similares; na prática, isto torna o filtro de médias móveis bastante suave.\nEvidentemente, não é possível calcular este filtro no início e no final da série \\(y_t\\). Isto implica que a série \\(z_t\\) tem menos observações do que a série original \\(y_t\\). Isto, de fato, é um ponto negativo quando se usa filtros de médias móveis.\nMédias móveis simétricas sempre tem um número ímpar de termos: no exemplo acima, \\(k=1\\) resultou num filtro com três termos; se tivéssemos usado \\(k=2\\) teríamos cinco termos e assim por diante. É possível fazer médias móveis não-simétricas variando a janela temporal. A equação abaixo mostra um filtro que soma as últimas duas observações e tira a média junto com a observação atual e a próxima observação.\n\\[\nz_{t} = \\frac{1}{4} y_{t-2} + \\frac{1}{4} y_{t-1} + \\frac{1}{4} y_{t} + \\frac{1}{4} y_{t+1}\n\\]\nNão existe forte “contraindicação” sobre o uso de médias móveis não-simétricas. Vale notar, contudo, que é relativamente fácil transformar uma média móvel não-simétrica em uma média móvel simétrica. Vamos tirar uma média móvel sobre a média móvel acima:\n\\[\n\\begin{align}\nx_{t} & = \\frac{1}{2}z_{t} + \\frac{1}{2}z_{t+1} \\\\\nx_{t} & = \\frac{1}{2}(\\frac{1}{4}(y_{t-2} + y_{t-1} + y_{t} + y_{t+1}) + \\frac{1}{4}(y_{t-1} + y_{t} + y_{t+1} + y_{t+2})) \\\\\nx_{t} & = \\frac{1}{8} y_{t-2} + \\frac{1}{4} y_{t-1} + \\frac{1}{4} y_{t} + \\frac{1}{4} y_{t+1} + \\frac{1}{8} y_{t+2}\n\\end{align}\n\\]\nA série final é uma média móvel simétrica com menor peso nas pontas. Esta é uma média móvel de ordem 2x4. Este tipo de filtro também é conhecido como média móvel ponderada.\nNa primeira demonstração acima, definimos que todos \\(a_j\\) teriam o mesmo valor. Isto não é necessário. Imagine que queremos um filtro que olha somente para o passado e atribui pesos decrescentes à medida que a observação se afasta no tempo. No caso de uma janela com apenas dois períodos teríamos algo da forma:\n\\[\nz_{t} = \\frac{1}{2}y_{t} + \\frac{1}{3}y_{t-1} + \\frac{1}{6}y_{t-2}\n\\]\nNote que os pesos somam 1 e vão decaindo no tempo. De forma mais geral,\n\\[\n\\text{WMA}_M = \\frac{ny_{t} + (n-1)y_{t-1} + \\dots + y_{M-n+1}}{\\frac{n(n+1)}{2}}\n\\]\nPode-se definir uma média móvel ponderada com pesos que apresentam algum tipo de decaimento. O filtro acima também é conhecido como média móvel linearmente ponderada, pois os pesos decaem linearmente. Outra opção seria usar pesos que apresentam decaimento exponencial como\n\\[\nz_{t} = \\alpha y_{t} + \\alpha(1-\\alpha) y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2}\n\\]\nTomando o valor \\(\\alpha = \\frac{1}{2}\\) temos que:\n\\[\nz_{t} = \\frac{1}{2}y_t + \\frac{1}{4}y_{t-1} + \\frac{1}{8} y_{t-2}\n\\]\nComparando os termos da expressão acima com a média móvel ponderada anterior, vemos que os termos do passado agora tem menor peso. De maneira mais geral, o filtro de médias móveis exponencial é dado por\n\\[\nz_t = \\alpha \\sum_{j = 0}^{\\infty}(1-\\alpha)^j y_{t-j}\n\\]\n\n\nPara montar um exemplo de médias móveis vamos importar uma das rubricas da balança de pagamentos do Brasil. O gráfico abaixo mostra a série anual da rubrica “passe de atletas”1.\n\nlibrary(rbcb)\nlibrary(ggplot2)\nlibrary(forecast)\n\ntheme_series = theme_bw(base_size = 10, base_family = \"sans\") +\n    theme(\n      legend.position = \"top\",\n      panel.grid.minor = element_blank(),\n      axis.text.x = element_text(angle = 90),\n      axis.title.x = element_blank()\n      )\n\natletas = rbcb::get_series(23617, as = \"ts\")\n\nautoplot(atletas) + theme_series\n\n\n\n\n\n\n\n\nA tabela abaixo mostra o cálculo de três filtros de médias móveis aplicados na série.\n\nma3 = stats::filter(atletas, filter = rep(1/3, 3), method = \"convolution\")\nma5 = stats::filter(atletas, filter = rep(1/5, 5), method = \"convolution\")\nma11 = stats::filter(atletas, filter = rep(1/11, 11), method = \"convolution\")\n\nmts = ts.intersect(atletas, ma3, ma5, ma11)\n\ntbl_ma = data.frame(\n  ano = as.numeric(time(mts)),\n  zoo::coredata(mts)\n)\n\n\n\n\n\n\n\n  \n    \n      ano\n      atletas\n      ma3\n      ma5\n      ma11\n    \n  \n  \n    1995\n18.0\n—\n—\n—\n    1996\n46.6\n49.5\n—\n—\n    1997\n84.0\n60.3\n51.9\n—\n    1998\n50.4\n65.0\n73.6\n—\n    1999\n60.7\n79.2\n85.0\n—\n    2000\n126.4\n96.8\n80.2\n80.4\n    2001\n103.4\n96.6\n86.5\n89.2\n    2002\n60.0\n81.8\n97.8\n106.5\n    2003\n82.1\n86.4\n99.6\n113.9\n    2004\n117.0\n111.5\n101.9\n125.8\n    2005\n135.4\n122.4\n137.3\n138.4\n    2006\n114.9\n162.5\n154.0\n143.8\n    2007\n237.1\n172.5\n166.9\n145.1\n    2008\n165.5\n194.8\n179.7\n162.4\n    2009\n181.7\n182.1\n193.7\n170.2\n    2010\n199.1\n188.7\n170.0\n179.2\n    2011\n185.2\n167.5\n186.9\n183.4\n    2012\n118.3\n184.5\n184.1\n193.9\n    2013\n250.0\n178.8\n187.6\n201.5\n    2014\n168.0\n211.4\n186.8\n210.9\n    2015\n216.3\n188.5\n209.3\n214.3\n    2016\n181.3\n209.5\n223.2\n210.2\n    2017\n230.8\n244.0\n243.6\n208.6\n    2018\n319.8\n273.5\n244.1\n218.9\n    2019\n269.8\n269.5\n238.6\n—\n    2020\n218.9\n214.2\n226.0\n—\n    2021\n153.9\n180.1\n208.4\n—\n    2022\n167.6\n184.4\n—\n—\n    2023\n231.8\n—\n—\n—\n  \n  \n  \n\n\n\n\nO gráfico abaixo mostra o ajuste de cada um dos filtros.\n\nautoplot(mts) + \n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\nNum caso aplicado, seria interessante ter uma maneira simples de escalar o processo acima para múltiplas séries de tempo. Para calcular médias móveis com grande velocidade, vamos usar o pacote RcppRoll, onde cpp significa C++, a linguagem subjacente do pacote. A função roll_mean() aplica uma média móvel sobre uma série.\n\nlibrary(RcppRoll)\nroll_mean(atletas, n = 3)\n\n [1]  49.53333  60.33333  65.03333  79.16667  96.83333  96.60000  81.83333\n [8]  86.36667 111.50000 122.43333 162.46667 172.50000 194.76667 182.10000\n[15] 188.66667 167.53333 184.50000 178.76667 211.43333 188.53333 209.46667\n[22] 243.96667 273.46667 269.50000 214.20000 180.13333 184.43333\n\n\nObjetos ts foram criados especificamente para armazenar séries de tempo, têm muitas praticidades, e são pré-carregados no R. A vasta maioria dos dados, e análises de dados, segue o paradigma de bases retangulares, de data.frame s por assim dizer. Neste sentido, o ts ou mts acaba sendo pouco prático2 quando se trabalha com múltiplas séries de tempo ou até mesmo com séries de tempo com altas frequências.\nNo exemplo abaixo vamos importar 9 séries do Índice de Produção Industrial (IPI) e aplicar uma média móvel 2x12 em cada uma delas. O código abaixo mostra como importar as séries e empilhá-las num único tibble. Para manipular os dados vamos utilizar o popular pacote dplyr3.\n\nlibrary(dplyr)\n\ncode = 28503:28511\nseries = get_series(code)\nseries = lapply(series, setNames, c(\"date\", \"value\"))\nnames(series) = code\nseries = bind_rows(series, .id = \"series_id\")\n\nggplot(series, aes(date, value)) + \n  geom_line() + \n  facet_wrap(vars(series_id)) +\n  theme_series\n\n\n\n\n\n\n\n\nÉ relativamente simples aplicar o filtro sobre cada uma das séries. O gráfico mostra o resultado final.\n\nseries = series |&gt; \n  group_by(series_id) |&gt; \n  mutate(\n    trend = roll_mean(value, n = 12, fill = NA),\n    trend = roll_mean(trend, n = 2, fill = NA)\n  )\n\nggplot(series, aes(date)) + \n  geom_line(aes(y = value), alpha = 0.8, color = \"#126782\") + \n  geom_line(aes(y = trend), lwd = 0.8, color = \"#023047\") +\n  facet_wrap(vars(series_id)) +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\nMédias móveis fornecem uma estimativa simples da tendência de uma série. Neste sentido, é comum utilizar o filtro de médias móveis para decompor uma série. O código abaixo usa a base USMacroG para importar algumas séries macroeconômicas trimestrais dos EUA no período 1947-19974. Vamos modelar a tendência das séries usando uma janela de quatro anos (dois anos para trás e dois anos para frente), ou seja, uma média móvel de ordem 25.\n\n\nCode\nlibrary(AER)\nlibrary(tidyr)\ndata(\"USMacroG\")\n\nmacro = tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nsubmacro = macro |&gt; \n  filter(date &gt;= as.Date(\"1947-01-01\"), date &lt;= as.Date(\"1997-01-01\")) |&gt; \n  mutate(across(gdp:m1, log))\n\nmacro_trend = submacro |&gt; \n  select(date, gdp, consumption, invest, m1, inflation, unemp) |&gt; \n  pivot_longer(col = -date, names_to = \"name_series\") |&gt; \n  group_by(name_series) |&gt; \n  mutate(\n    trend = roll_mean(value, n = 25, fill = NA),\n    detrend = value - trend) |&gt; \n  ungroup()\n\nggplot(macro_trend, aes(date)) +\n  geom_line(aes(y = value), alpha = 0.5) +\n  geom_line(aes(y = trend), lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico das séries livres de tendência é apresentado abaixo. Note que as séries não foram dessazonalisadas, portanto, elas ainda apresentam oscilação sazonal.\n\nggplot(macro_trend, aes(date, detrend)) +\n  geom_line(lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\nApesar de simples, o filtro de médias móveis é bastante utilizado e tem boas propriedades estatísticas. O gráfico abaixo compara o ajuste de um filtro de médias móveis com o filtro Baxter-King5, que é bastante mais sofisticado. Como se vê, ambos os filtros chegam em resultados muito similares.\n\n\nCode\nlibrary(mFilter)\n\ntbl_unemp = select(macro, date, unemp)\nbk = mFilter::bkfilter(tbl_unemp$unemp, 6, 32, nfix = 12)\n\ntbl_unemp = tbl_unemp |&gt; \n  mutate(\n    ma = roll_mean(unemp, n = 25, fill = NA, align = \"center\"),\n    bk = mFilter::bkfilter(unemp, 6, 32, nfix = 12)$trend\n  ) |&gt; \n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(tbl_unemp, aes(date, value, color = series)) +\n  geom_line(data = filter(tbl_unemp, series == \"unemp\")) +\n  geom_line(data = filter(tbl_unemp, series != \"unemp\"), lwd = 0.9) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#FFD166\", \"#0CB0A9\", \"#073B4C\"),\n    labels = c(\"Baxter-King\", \"MA25\", \"Original\")\n  ) +\n  theme_series\n\n\n\n\n\n\n\n\n\n\n\n\nO filtro de médias móveis exponenciais ou EWMA (exponentially weighted moving average) é bastante popular em finanças. Na prática, ele é um caso específico de suvização exponencial6. Tecnicamente, seria possível implementá-lo usando stats::filter fornecendo os pesos adequados. O código abaixo replica o primeiro exemplo, da série “passe de atletas”.\n\newma_wgt = function(alpha = 0.5, order = 3) { rev((1 - alpha)^(seq(1, order))) }\n\newma3 = stats::filter(atletas, filter = c(1/8, 1/4, 1/2), sides = 1L)\newma5 = stats::filter(atletas, filter = ewma_wgt(order = 5), sides = 1L)\newma11 = stats::filter(atletas, filter = ewma_wgt(order = 11), sides = 1L)\n\n\nmts = ts.intersect(atletas, ewma3, ewma5, ewma11)\n\nautoplot(mts) + \n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series\n\n\n\n\n\n\n\n\nO exemplo acima é um tanto quanto artificial. Tipicamente, o filtro EWMA é utilizado em séries financeiras de alta frequência. O código abaixo usa o pacote TTR (Technical Trading Rules), especializado em funções para finanças, para computar o EWMA de uma série de preço. Para tornar o exemplo mais simples vamos aproveitar a base ttrc do próprio pacote.\n\n\nCode\nlibrary(TTR)\ndata(ttrc)\n\nttrc = ttrc |&gt; \n  rename_with(tolower) |&gt; \n  dplyr::filter(date &gt;= as.Date(\"2004-01-01\")) |&gt; \n  mutate(\n    trend07 = EMA(close, n = 7),\n    trend30 = EMA(close, n = 30),\n    trend50 = EMA(close, n = 50)\n    ) |&gt; \n  select(date, close, starts_with(\"trend\")) |&gt; \n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(ttrc, aes(x = date, y = value, color = series)) +\n  geom_line(data = dplyr::filter(ttrc, series == \"close\"), alpha = 0.8) +\n  geom_line(data = dplyr::filter(ttrc, series != \"close\"), lwd = 0.5) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"EWMA7\", \"EWMA25\", \"EWMA50\")) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#exemplo-simples",
    "href": "posts/general-posts/2024-02-media-movel/index.html#exemplo-simples",
    "title": "Médias móveis",
    "section": "",
    "text": "Para montar um exemplo de médias móveis vamos importar uma das rubricas da balança de pagamentos do Brasil. O gráfico abaixo mostra a série anual da rubrica “passe de atletas”1.\n\nlibrary(rbcb)\nlibrary(ggplot2)\nlibrary(forecast)\n\ntheme_series = theme_bw(base_size = 10, base_family = \"sans\") +\n    theme(\n      legend.position = \"top\",\n      panel.grid.minor = element_blank(),\n      axis.text.x = element_text(angle = 90),\n      axis.title.x = element_blank()\n      )\n\natletas = rbcb::get_series(23617, as = \"ts\")\n\nautoplot(atletas) + theme_series\n\n\n\n\n\n\n\n\nA tabela abaixo mostra o cálculo de três filtros de médias móveis aplicados na série.\n\nma3 = stats::filter(atletas, filter = rep(1/3, 3), method = \"convolution\")\nma5 = stats::filter(atletas, filter = rep(1/5, 5), method = \"convolution\")\nma11 = stats::filter(atletas, filter = rep(1/11, 11), method = \"convolution\")\n\nmts = ts.intersect(atletas, ma3, ma5, ma11)\n\ntbl_ma = data.frame(\n  ano = as.numeric(time(mts)),\n  zoo::coredata(mts)\n)\n\n\n\n\n\n\n\n  \n    \n      ano\n      atletas\n      ma3\n      ma5\n      ma11\n    \n  \n  \n    1995\n18.0\n—\n—\n—\n    1996\n46.6\n49.5\n—\n—\n    1997\n84.0\n60.3\n51.9\n—\n    1998\n50.4\n65.0\n73.6\n—\n    1999\n60.7\n79.2\n85.0\n—\n    2000\n126.4\n96.8\n80.2\n80.4\n    2001\n103.4\n96.6\n86.5\n89.2\n    2002\n60.0\n81.8\n97.8\n106.5\n    2003\n82.1\n86.4\n99.6\n113.9\n    2004\n117.0\n111.5\n101.9\n125.8\n    2005\n135.4\n122.4\n137.3\n138.4\n    2006\n114.9\n162.5\n154.0\n143.8\n    2007\n237.1\n172.5\n166.9\n145.1\n    2008\n165.5\n194.8\n179.7\n162.4\n    2009\n181.7\n182.1\n193.7\n170.2\n    2010\n199.1\n188.7\n170.0\n179.2\n    2011\n185.2\n167.5\n186.9\n183.4\n    2012\n118.3\n184.5\n184.1\n193.9\n    2013\n250.0\n178.8\n187.6\n201.5\n    2014\n168.0\n211.4\n186.8\n210.9\n    2015\n216.3\n188.5\n209.3\n214.3\n    2016\n181.3\n209.5\n223.2\n210.2\n    2017\n230.8\n244.0\n243.6\n208.6\n    2018\n319.8\n273.5\n244.1\n218.9\n    2019\n269.8\n269.5\n238.6\n—\n    2020\n218.9\n214.2\n226.0\n—\n    2021\n153.9\n180.1\n208.4\n—\n    2022\n167.6\n184.4\n—\n—\n    2023\n231.8\n—\n—\n—\n  \n  \n  \n\n\n\n\nO gráfico abaixo mostra o ajuste de cada um dos filtros.\n\nautoplot(mts) + \n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#maior-eficiência",
    "href": "posts/general-posts/2024-02-media-movel/index.html#maior-eficiência",
    "title": "Médias móveis",
    "section": "",
    "text": "Num caso aplicado, seria interessante ter uma maneira simples de escalar o processo acima para múltiplas séries de tempo. Para calcular médias móveis com grande velocidade, vamos usar o pacote RcppRoll, onde cpp significa C++, a linguagem subjacente do pacote. A função roll_mean() aplica uma média móvel sobre uma série.\n\nlibrary(RcppRoll)\nroll_mean(atletas, n = 3)\n\n [1]  49.53333  60.33333  65.03333  79.16667  96.83333  96.60000  81.83333\n [8]  86.36667 111.50000 122.43333 162.46667 172.50000 194.76667 182.10000\n[15] 188.66667 167.53333 184.50000 178.76667 211.43333 188.53333 209.46667\n[22] 243.96667 273.46667 269.50000 214.20000 180.13333 184.43333\n\n\nObjetos ts foram criados especificamente para armazenar séries de tempo, têm muitas praticidades, e são pré-carregados no R. A vasta maioria dos dados, e análises de dados, segue o paradigma de bases retangulares, de data.frame s por assim dizer. Neste sentido, o ts ou mts acaba sendo pouco prático2 quando se trabalha com múltiplas séries de tempo ou até mesmo com séries de tempo com altas frequências.\nNo exemplo abaixo vamos importar 9 séries do Índice de Produção Industrial (IPI) e aplicar uma média móvel 2x12 em cada uma delas. O código abaixo mostra como importar as séries e empilhá-las num único tibble. Para manipular os dados vamos utilizar o popular pacote dplyr3.\n\nlibrary(dplyr)\n\ncode = 28503:28511\nseries = get_series(code)\nseries = lapply(series, setNames, c(\"date\", \"value\"))\nnames(series) = code\nseries = bind_rows(series, .id = \"series_id\")\n\nggplot(series, aes(date, value)) + \n  geom_line() + \n  facet_wrap(vars(series_id)) +\n  theme_series\n\n\n\n\n\n\n\n\nÉ relativamente simples aplicar o filtro sobre cada uma das séries. O gráfico mostra o resultado final.\n\nseries = series |&gt; \n  group_by(series_id) |&gt; \n  mutate(\n    trend = roll_mean(value, n = 12, fill = NA),\n    trend = roll_mean(trend, n = 2, fill = NA)\n  )\n\nggplot(series, aes(date)) + \n  geom_line(aes(y = value), alpha = 0.8, color = \"#126782\") + \n  geom_line(aes(y = trend), lwd = 0.8, color = \"#023047\") +\n  facet_wrap(vars(series_id)) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#footnotes",
    "href": "posts/general-posts/2024-02-media-movel/index.html#footnotes",
    "title": "Médias móveis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNome completo: “Transfer rights on sporting club players - annual - net”.↩︎\nExistem, na verdade, uma infinitude de classes de objetos para lidar com séries temporais no R. Para uma referência veja o pacote tsbox que funciona como uma pedra de Rosetta entre esses diferentes tipos de objetos.↩︎\nApesar de muito bom, o dplyr tem um problema chato quando se trabalha com séries de tempo. Há um conflito entre as funções dplyr::filter e stats::filter, que se utiliza para calcular uma média móvel. É muito comum carregar o pacote e esquecer deste detalhe. Para resolver os conflitos basta sempre declarar a função usando o operador “quatro pontos”, ::.↩︎\nPara a definição das variáveis, consulte ?USMacroG↩︎\nM. Baxter and R.G. King. Measuring business cycles: Approximate bandpass filters. The Review of Economics and Statistics, 81(4):575-93, 1999.↩︎\nPara uma boa introdução a suvização exponencial usando o pacote forecast consulte Forecasting: Principles and Practice (2ed).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#extraindo-tendência",
    "href": "posts/general-posts/2024-02-media-movel/index.html#extraindo-tendência",
    "title": "Médias móveis",
    "section": "",
    "text": "Médias móveis fornecem uma estimativa simples da tendência de uma série. Neste sentido, é comum utilizar o filtro de médias móveis para decompor uma série. O código abaixo usa a base USMacroG para importar algumas séries macroeconômicas trimestrais dos EUA no período 1947-19974. Vamos modelar a tendência das séries usando uma janela de quatro anos (dois anos para trás e dois anos para frente), ou seja, uma média móvel de ordem 25.\n\n\nCode\nlibrary(AER)\nlibrary(tidyr)\ndata(\"USMacroG\")\n\nmacro = tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nsubmacro = macro |&gt; \n  filter(date &gt;= as.Date(\"1947-01-01\"), date &lt;= as.Date(\"1997-01-01\")) |&gt; \n  mutate(across(gdp:m1, log))\n\nmacro_trend = submacro |&gt; \n  select(date, gdp, consumption, invest, m1, inflation, unemp) |&gt; \n  pivot_longer(col = -date, names_to = \"name_series\") |&gt; \n  group_by(name_series) |&gt; \n  mutate(\n    trend = roll_mean(value, n = 25, fill = NA),\n    detrend = value - trend) |&gt; \n  ungroup()\n\nggplot(macro_trend, aes(date)) +\n  geom_line(aes(y = value), alpha = 0.5) +\n  geom_line(aes(y = trend), lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series\n\n\n\n\n\n\n\n\n\nO gráfico das séries livres de tendência é apresentado abaixo. Note que as séries não foram dessazonalisadas, portanto, elas ainda apresentam oscilação sazonal.\n\nggplot(macro_trend, aes(date, detrend)) +\n  geom_line(lwd = 0.8) +\n  facet_wrap(vars(name_series), scales = \"free_y\") +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#filtros-complexos",
    "href": "posts/general-posts/2024-02-media-movel/index.html#filtros-complexos",
    "title": "Médias móveis",
    "section": "",
    "text": "Apesar de simples, o filtro de médias móveis é bastante utilizado e tem boas propriedades estatísticas. O gráfico abaixo compara o ajuste de um filtro de médias móveis com o filtro Baxter-King5, que é bastante mais sofisticado. Como se vê, ambos os filtros chegam em resultados muito similares.\n\n\nCode\nlibrary(mFilter)\n\ntbl_unemp = select(macro, date, unemp)\nbk = mFilter::bkfilter(tbl_unemp$unemp, 6, 32, nfix = 12)\n\ntbl_unemp = tbl_unemp |&gt; \n  mutate(\n    ma = roll_mean(unemp, n = 25, fill = NA, align = \"center\"),\n    bk = mFilter::bkfilter(unemp, 6, 32, nfix = 12)$trend\n  ) |&gt; \n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(tbl_unemp, aes(date, value, color = series)) +\n  geom_line(data = filter(tbl_unemp, series == \"unemp\")) +\n  geom_line(data = filter(tbl_unemp, series != \"unemp\"), lwd = 0.9) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#FFD166\", \"#0CB0A9\", \"#073B4C\"),\n    labels = c(\"Baxter-King\", \"MA25\", \"Original\")\n  ) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-media-movel/index.html#médias-móveis-exponenciais",
    "href": "posts/general-posts/2024-02-media-movel/index.html#médias-móveis-exponenciais",
    "title": "Médias móveis",
    "section": "",
    "text": "O filtro de médias móveis exponenciais ou EWMA (exponentially weighted moving average) é bastante popular em finanças. Na prática, ele é um caso específico de suvização exponencial6. Tecnicamente, seria possível implementá-lo usando stats::filter fornecendo os pesos adequados. O código abaixo replica o primeiro exemplo, da série “passe de atletas”.\n\newma_wgt = function(alpha = 0.5, order = 3) { rev((1 - alpha)^(seq(1, order))) }\n\newma3 = stats::filter(atletas, filter = c(1/8, 1/4, 1/2), sides = 1L)\newma5 = stats::filter(atletas, filter = ewma_wgt(order = 5), sides = 1L)\newma11 = stats::filter(atletas, filter = ewma_wgt(order = 11), sides = 1L)\n\n\nmts = ts.intersect(atletas, ewma3, ewma5, ewma11)\n\nautoplot(mts) + \n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"MA3\", \"MA5\", \"MA11\")) +\n  theme_series\n\n\n\n\n\n\n\n\nO exemplo acima é um tanto quanto artificial. Tipicamente, o filtro EWMA é utilizado em séries financeiras de alta frequência. O código abaixo usa o pacote TTR (Technical Trading Rules), especializado em funções para finanças, para computar o EWMA de uma série de preço. Para tornar o exemplo mais simples vamos aproveitar a base ttrc do próprio pacote.\n\n\nCode\nlibrary(TTR)\ndata(ttrc)\n\nttrc = ttrc |&gt; \n  rename_with(tolower) |&gt; \n  dplyr::filter(date &gt;= as.Date(\"2004-01-01\")) |&gt; \n  mutate(\n    trend07 = EMA(close, n = 7),\n    trend30 = EMA(close, n = 30),\n    trend50 = EMA(close, n = 50)\n    ) |&gt; \n  select(date, close, starts_with(\"trend\")) |&gt; \n  pivot_longer(cols = -date, names_to = \"series\")\n\nggplot(ttrc, aes(x = date, y = value, color = series)) +\n  geom_line(data = dplyr::filter(ttrc, series == \"close\"), alpha = 0.8) +\n  geom_line(data = dplyr::filter(ttrc, series != \"close\"), lwd = 0.5) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#073B4C\", \"#FFD166\", \"#0CB0A9\", \"#F4592A\"),\n    labels = c(\"Original\", \"EWMA7\", \"EWMA25\", \"EWMA50\")) +\n  theme_series"
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "Uma tarefa rotineira em pesquisa econômica é de encontrar tendências e ciclos em séries de tempo macroeconômicas. Dada uma série de tempo \\(y_t\\), tenta-se encontrar alguma decomposição que resulte em:\n\\[\ny_t = \\text{cycle}_t + \\text{trend}_t + \\text{remainder}_t\n\\]\nDiferentes teorias e abordagens foram levantadas para extair as tendências de curto e longo prazo de séries.\nDe maneira geral, há dois tipos de tendências: (1) tendências determinísticas e (2) tendências estocásticas. A forma mais simples de tendência determinística é de uma tendência temporal linear. Algo na forma\n\\[\ny_t = \\alpha_{0} + \\alpha_{1}t + u_{t}\n\\]\nNaturalmente, também é possível propor polinômios de ordem mais elevada para modelar a tendência acima. Contudo, não é usual usar polinômios maiores do que de grau 3, isto é,\n\\[\ny_t = \\alpha_{0} + \\alpha_{1}t + \\alpha_{2}t^2 + \\alpha_{3}t^3 + u_{t}\n\\]\nJá o exemplo mais simples de série com tendência estocástica é um random-walk\n\\[\ny_t = y_{t-1} + u_{t}\n\\]\nA depender do tipo de série que se considera, pode fazer sentido remover diferenças em ordens específicas. Na modelagem SARIMA, por exemplo, tira-se “diferenças sazonais”. No caso de uma série trimestral, por exemplo, pode fazer sentido tirar uma diferença trimestral\n\\[\nz_{t} = y_{t} - y_{t-4}\n\\]\n\n\nO gráfico abaixo mostra a produção de automóveis no Brasil entre 1993 e 2007. Os dados são da Anfavea e baixados no R via API do Banco Central. A série não foi ajustada sazonalmente.\n\n\nCode\nggplot(subcarros, aes(x = date, y = lcar)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  labs(\n    subtitle = \"Produção de automóveis e comerciais leves\",\n    x = NULL,\n    y = \"Unidades (log)\",\n    caption = \"Fonte: BCB\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra as séries sem tendência e o gráfico de autocorrelação do resíduo do ajuste. Note que é possível perceber visualmente a presença de uma componente sazonal que permanece na série. Os resíduos do ajuste com polinômio de terceiro grau ainda apresentam bastante autocorrelação, sugerindo que a série não é tendência-estacionária. O ajuste na primeira diferença parece ser mais adequado neste caso.\n\n\nCode\np1 &lt;- ggplot(subcarros, aes(x = date, y = resid_tt)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  labs(x = NULL, title = \"Tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np2 &lt;- ggplot(subcarros, aes(x = date, y = resid_sto)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  labs(x = NULL, title = \"Tendência estocástica I(1)\") +\n  theme_bw()\n\np3 &lt;- ggAcf(ts(subcarros$resid_tt, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np4 &lt;- ggAcf(ts(subcarros$resid_sto, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência I(1)\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\n\n\n\nUma abordagem bastante comum para encontrar a tendência/ciclo em séries macroeconômicas é o filtro Hodrick-Prescott (HP). O filtro HP separa uma série em suas componentes de tendência e ciclo. Sua formulação matemática baseia-se na minimização de uma função objetivo que busca encontrar uma estimativa suave da tendência subjacente, ao mesmo tempo em que penaliza variações abruptas.\n\n\nDada uma série temporal \\(y_t\\), o objetivo é encontrar uma tendência \\(g_t\\) que minimize a seguinte função objetivo:\n\\[\n\\min_{g_t} \\left[ \\sum_{t=1}^{T} \\left( y_t - g_t \\right)^2 + \\lambda \\sum_{t=2}^{T-1} \\left( g_{t+1} - 2g_t + g_{t-1} \\right)^2 \\right]\n\\]\nO primeiro termo representa a soma dos quadrados dos resíduos entre a série observada e a estimativa da tendência, enquanto o segundo termo é uma penalidade que desencoraja variações abruptas na tendência. O filtro HP suaviza a série observada, permitindo identificar movimentos de longo prazo enquanto remove flutuações de curto prazo. O parâmetro \\(\\lambda\\) desempenha um papel crucial na determinação do nível de suavização da tendência, com valores maiores resultando em tendências mais suaves.\nNote que se \\(\\lambda = 0\\) o valor ótimo de \\(g_{t}\\) é simplesmente \\(y_{t}\\). Quando \\(\\lambda \\to \\infty\\) o componente \\(g_{t}\\) se aproxima de uma tendência temporal linear. A escolha de \\(\\lambda\\) não é simples e é usual amparar-se em regras de bolso. Para séries trimestrais, costuma-se usar \\(\\lambda = 1600\\); para séries mensais \\(\\lambda = 14400\\)1.\nApesar de ter sido desenvolvida para séries macroeconômicas, não há problema, em princípio, em aplicar o filtro HP a qualquer tipo de série. O gráfico abaixo mostra o resultado da aplicação do filtro HP à série de produção de veículos.\n\n\nCode\ny &lt;- ts(log(subcarros$value), frequency = 12)\nhpy &lt;- hpfilter(y, freq = 14400, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\nggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(subtitle = \"Filtro HP: Série e Tendência\", x = NULL, y = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nO passeio aleatório é um processo estocástico comum em economia. Variáveis importantes como preços futuros, preços de ações, preços de petróleo, conusmo, inflação, e a taxa de crescimento do estoque de moeda podem ser modeladas como passeios aleatórios.\nSeja um passeio aleatório \\((1-L)y_{t} = u_{t}\\) com \\(T\\) observações e frequência trimestral, onde \\(L\\) é o operador defasagem usual: \\(Lx_{t} = x_{t-1}\\). Segundo Hamilton (2017), Quando se aplica o filtro HP sobre \\(y_{t}\\) encontra-se o seguinte termo de ciclo, \\(c_t\\)\n\\[\nc_{t} = \\frac{\\lambda(1-L)^3}{F(L)}u_{t+2}\n\\]\nUsando \\(\\lambda = 1600\\), a sugestão usual para séries trimestrais,\n\\[\nc_t = 89.72 \\{ - q_{0,t+2} + \\sum^{\\infty}_{j=0} (0.8941)^j[\\cos(0.1117j)+8.916\\sin(0.1117j)](q_{1,t+2-j} + q_{2,t+2+j}) \\}\n\\]\nonde\n\n\\(q_{0t} = \\varepsilon_t - 3\\varepsilon_{t-1} + 3\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\\(q_{1t} = \\varepsilon_t - 3.79\\varepsilon_{t-1} + 5.37\\varepsilon_{t-2} - 3.37\\varepsilon_{t-3} + 0.79\\varepsilon_{t-4}\\)\n\\(q_{2t} = -0.79\\varepsilon_{t+1} + 3.37\\varepsilon_t - 5.37\\varepsilon_{t-1} + 3.79\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\nque é uma expressão bastante longa. O importante a se notar é que o termo \\(c_{t}\\) tem uma estrutura recursiva em função das defasagens de \\(\\varepsilon_t\\). Note que a série subjacente é simplesmente uma soma de ruídos aleatórios sem nenhum padrão:\n\\[\n\\begin{align}\n(1-L)y_{t} & = \\varepsilon_{t} \\\\\ny_{t} & = \\frac{\\varepsilon_{t}}{(1-L)} \\\\\ny_{t} & = \\sum_{i = 0}^{\\infty}e_{i}\n\\end{align}\n\\]\nmas o ciclo desta série tem uma estrutura artifical, que foi acrescentada pelo uso do filtro HP. Em linhas gerais, a exposição acima resume o argumento de Hamilton contra o uso indiscriminado do filtro HP em séries macroeconômicas não-estacionárias. O filtro HP acaba “criando” uma estrutura nos dados que não existia previamente.\n\n\nCode\nset.seed(1984)\ny &lt;- ts(cumsum(rnorm(100)), frequency = 4)\n\nhpy &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  cycle = as.numeric(hpy$cycle),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\np1 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(\"Filtro HP aplicado no Random Walk\", subtitle = \"Série e Tendência\", x = NULL, y = NULL) +\n  theme_bw()\n\np2 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = cycle), lwd = 0.8, color = \"#023047\") +\n  labs(x = NULL, y = NULL, subtitle = \"Ciclo\") +\n  theme_bw()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\nPara um exemplo mais aplicado, vamos usar a base USMacroG do pacote {AER}. O código abaixo aplica o filtro HP sobre o logaritmo das séries trimestrais do PIB, consumo, investimento e M1. As séries são todas completas com 204 observações entre 1950 e 2000.\nO gráfico mostra o ajuste do filtro HP em cada uma das quatro séries.\n\n\nCode\nlibrary(AER)\ndata(\"USMacroG\")\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro &lt;- macro |&gt; \n  mutate(across(gdp:m1, log))\n\nnest_macro &lt;- macro |&gt; \n  select(date, gdp, consumption, invest, m1) |&gt; \n  pivot_longer(col = -date, names_to = \"name_series\") |&gt; \n  group_by(name_series) |&gt; \n  nest()\n\nmacro_hp &lt;- nest_macro |&gt; \n  mutate(hpy = map(data, \\(x) {\n    y &lt;- ts(x$value, frequency = 4, start = c(1950, 1))\n    hp &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n    \n    tibble(\n      cycle = as.numeric(hp$cycle),\n      trend = as.numeric(hp$trend)\n    )\n  })) |&gt; \n  unnest(c(data, hpy)) |&gt; \n  ungroup()\n\np1 &lt;- ggplot(macro_hp, aes(x = date, y = value)) +\n  geom_line(aes(y = value), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  facet_wrap(~name_series, scales = \"free_y\", ncol = 1) +\n  labs(x = NULL, y = NULL)+\n  theme_bw()\n\np2 &lt;- ggplot(macro_hp, aes(x = date, y = cycle)) +\n  geom_line(lwd = 0.8, color = \"#023047\") +\n  geom_hline(yintercept = 0) +\n  facet_wrap(~name_series, ncol = 1) +\n  labs(x = NULL, y = NULL) +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nA análise das autocorrelações mostra que há bastante autocorrelação cruzada entre as séries.\n\n\nCode\nmacro_detrend &lt;- macro_hp |&gt; \n  mutate(detrend = value - trend) |&gt; \n  pivot_wider(id_cols = \"date\", names_from = \"name_series\", values_from = \"detrend\") |&gt; \n  select(-date)\n\nggAcf(ts(macro_detrend, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: séries com filtro HP\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSe tivéssemos assumido, contudo, que as séries são I(1), não haveria muita correlação cruzada entre as séries.\n\n\nCode\nmacro_d1 &lt;- macro |&gt; \n  select(gdp, consumption, invest, m1) |&gt; \n  mutate(across(everything(), ~c(NA, diff(.x))))\n\n\nggAcf(ts(macro_d1, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: primeira diferença das séries\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo alternativa ao filtro HP Hamilton (2017) sugere um procedimento bastante simples. Seja \\(y_t\\) uma série não-estacionária. Então fazemos uma regressão linear de \\(y_{t+h}\\) contra os \\(p\\) valores mais recentes.\n\\[\ny_{t+h} = \\beta_0 + \\beta_{1} y_{t} + \\beta_{2} y_{t-1} + \\beta_{3} y_{t-2} + \\beta_{4} y_{t-3} + u_{t+h}\n\\]\nPara o caso específico de dados trimestrais, ele sugere uma formulação do tipo:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nOu, alternativamente, um modelo ainda mais simples, em função apenas de \\(y_{t-8}\\) na forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + u_{t+h}\n\\]\nDe modo geral, Hamilton argumenta que este filtro garante a estacionaridade de séries, sendo elas tendência-estacionárias ou diferença-estacionárias. Mais detalhes sobre o filtro podem ser verificados no Working Paper.\n\n\nO primeiro passo necessário é preparar os dados. Novamente, vamos trabalhar com as séries num padrão retangular, usando um tibble. A base de dados utilizada é a USMacroG, descrita acima; neste caso, vamos adicionar as séries de gasto do governo e inflação. Isto é, temos agora seis séries: PIB, consumo, investimento, gasto do governo, inflação e M1. Todas as variáveis, exceto pela taxa de inflação, serão transformadas usando log, seguindo o paper de Hamilton.\nO código abaixo tem um trecho interessante onde se usa uma combinação de map e partial para criar várias colunas com os valores defasados das variáveis.\n\ndata(USMacroG)\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro_long &lt;- macro |&gt; \n  select(date, gdp, consumption, invest, m1, government, inflation) |&gt; \n  mutate(across(gdp:government, ~log10(.x) * 100)) |&gt; \n  pivot_longer(-date, names_to = \"name_series\")\n\nmacro_lags &lt;- macro_long |&gt; \n  group_by(name_series) |&gt; \n  mutate(\n    across(value,\n           map(1:12, ~partial(dplyr::lag, n = .x)),\n           .names = \"l{.fn}\")\n  )\n\n\n\n\nHamilton sugere duas maneiras de tirar a tendência dos dados: usando uma regressão linear e tirando uma diferença simples. No caso de séries trimestrais, o autor sugere \\(p=4\\) e \\(h=8\\). Assim, temos de estimar um modelo linear da forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nE a série livre de tendência, \\(z_t\\) será dada por:\n\\[\nz_{t} = y_{t} - \\hat{\\beta_0} - \\hat{\\beta_1}y_{t-8} - \\hat{\\beta_2}y_{t-9} - \\hat{\\beta_3}y_{t-10} - \\hat{\\beta_4}y_{t-11}\n\\]\nNo segundo “modelo”, vamos simplesmente tirar uma diferença na forma:\n\\[\nz_{t} = y_{t} - y_{t-8}\n\\]\nO primeiro modelo será chamado de “regression”, enquanto o segundo modelo será chamado de “Random Walk”. O código abaixo faz a regressão em todas as séries e grava os resultados do fit e os resíduos de todos os modelos.\n\nmacro_models &lt;- macro_lags |&gt; \n  mutate(resid_rw = value - l8) |&gt; \n  nest() |&gt; \n  mutate(\n    model_reg = map(data, \\(d) lm(value ~ l8 + l9 + l10 + l11, data = d)),\n    trend = map(model_reg, fitted),\n    resid = map(model_reg, residuals)\n  )\n\n\n\n\nIdealmente, deve-se verificar o comportamento dos resíduos do modelo de regressão. Neste caso, estes resíduos são considerados o componente cíclico da série original. O painel abaixo mostra os resíduos da primeira série, do PIB. O gráfico de cima é simplesmente o gráfico do resíduo. No gráfico inferior-esquerdo temos o gráfico de autocorrelação da série e no gráfico inferior-direito temos o historgrama dos resíduos com a densidade da distribuição normal superimposta.\nVale lembrar que não estamos buscando limpar a série totalmente de autocorrelação, como na modelagem ARIMA.\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 417.09, df = 10, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\nNULL\n\n\nEvidentemente, não é fácil inspecionar os resíduos de todos os modelos, especialmente quando o número de séries é muito grande. O painel abaixo mostra o gráfico de autocorrelação entre as seis séries consideradas. Na diagonal principal temos o gráfico do resíduo da própria série; nos demais gráficos, mostra-se a autocorrelação cruzada entre os resíduos.\n\n\n\n\n\n\n\n\n\n\n\n\nO código abaixo junta todas as séries para facilitar a sua visualização. Como há várias séries e muita repetição, crio uma função plot_series para facilitar a construção dos painéis.\n\n\nCode\nm1 &lt;- macro_models |&gt; \n  ungroup() |&gt; \n  select(name_series, trend, resid) |&gt; \n  unnest(cols = c(\"trend\", \"resid\")) |&gt; \n  mutate(id = as.numeric(names(trend)), .before = everything())\n\nmacro_trend &lt;- macro_models |&gt; \n  select(name_series, data) |&gt; \n  unnest(cols = c(data)) |&gt; \n  mutate(id = vctrs::vec_group_id(date), .before = everything()) |&gt; \n  ungroup()\n\nmacro_trend &lt;- macro_trend |&gt; \n  left_join(m1, by = c(\"id\", \"name_series\"))\n\nseries_trend &lt;- macro_trend |&gt; \n  select(date, name_series, trend, resid, resid_rw) |&gt; \n  pivot_longer(cols = -c(date, name_series), names_to = \"name_decomp\") |&gt;\n  mutate(decomp = if_else(str_detect(name_decomp, \"trend\"), \"trend\", \"resid\")) |&gt; \n  filter(!is.na(value))\n\nplot_series &lt;- function(x, ylim = c(NA, NA)) {\n  \n  p1 &lt;- ggplot() +\n  geom_line(\n    data = dplyr::filter(series_trend, name_series == x, decomp == \"trend\"),\n    aes(x = date, y = value),\n    lwd = 0.8,\n    color = \"steelblue\"\n    ) +\n  labs(x = NULL, y = NULL) +\n  ggtitle(glue::glue(\"Series: {x}\")) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 10))\n\np2 &lt;- ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_line(\n    data = dplyr::filter(series_trend, name_series == x, decomp == \"resid\"),\n    aes(x = date, y = value, color = name_decomp),\n    lwd = 0.8\n    ) +\n  scale_x_date(\n    breaks = seq(as.Date(\"1950-01-01\"), as.Date(\"2000-01-01\"), by = \"10 year\"),\n    date_labels = \"%Y\"\n    ) +\n  scale_y_continuous(limits = ylim) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#003049\", \"#d62828\"),\n    labels = c(\"Regression\", \"Random Walk\")) +\n  labs(x = NULL, y = NULL) +\n  ggtitle(glue::glue(\"Cyclical Component: {x}\")) +\n  theme_bw() +\n  theme(legend.position = \"top\", plot.title = element_text(size = 10))\n\n  return(list(trend = p1, cycle = p2))\n  \n}\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de PIB e de consumo.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de investimento e gasto do governo.\n\n\n\n\n\n\n\n\n\nFinalmente, o gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de M1 e de inflação.\n\n\n\n\n\n\n\n\n\nO filtro proposto por Hamilton tem diversas características interessantes. A simplicidade e intuição do filtro, em especial, são pontos atrativos. Um paper recente, de Viv Hall e Peter Thompson (2021), contudo, não encontrou grandes vantagens deste filtro em relação aos tradicionais filtro HP e filtro Baxter-King. Os autores testam os filtros usando um amplo conjunto de séries macroeconômicas da Nova Zelândia. De maneira geral, o filtro de Hamilton gerou séries com maior volatilidade e pior capacidade preditiva. Considerando a literatura de filtros, parece improvável, de fato, que um filtro tão simples possa ter uma melhor performance do que filtros no domínio da frequência."
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#produção-de-automóveis",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#produção-de-automóveis",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "O gráfico abaixo mostra a produção de automóveis no Brasil entre 1993 e 2007. Os dados são da Anfavea e baixados no R via API do Banco Central. A série não foi ajustada sazonalmente.\n\n\nCode\nggplot(subcarros, aes(x = date, y = lcar)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  labs(\n    subtitle = \"Produção de automóveis e comerciais leves\",\n    x = NULL,\n    y = \"Unidades (log)\",\n    caption = \"Fonte: BCB\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nO painel abaixo mostra as séries sem tendência e o gráfico de autocorrelação do resíduo do ajuste. Note que é possível perceber visualmente a presença de uma componente sazonal que permanece na série. Os resíduos do ajuste com polinômio de terceiro grau ainda apresentam bastante autocorrelação, sugerindo que a série não é tendência-estacionária. O ajuste na primeira diferença parece ser mais adequado neste caso.\n\n\nCode\np1 &lt;- ggplot(subcarros, aes(x = date, y = resid_tt)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  labs(x = NULL, title = \"Tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np2 &lt;- ggplot(subcarros, aes(x = date, y = resid_sto)) +\n  geom_line(lwd = 0.8, color = \"steelblue\") +\n  geom_hline(yintercept = 0) +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  labs(x = NULL, title = \"Tendência estocástica I(1)\") +\n  theme_bw()\n\np3 &lt;- ggAcf(ts(subcarros$resid_tt, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência cúbica\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\np4 &lt;- ggAcf(ts(subcarros$resid_sto, frequency = 12), lag.max = 84) +\n  ggtitle(\"ACF: resíduos tendência I(1)\") +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw()\n\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#filtro-hp",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#filtro-hp",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "Uma abordagem bastante comum para encontrar a tendência/ciclo em séries macroeconômicas é o filtro Hodrick-Prescott (HP). O filtro HP separa uma série em suas componentes de tendência e ciclo. Sua formulação matemática baseia-se na minimização de uma função objetivo que busca encontrar uma estimativa suave da tendência subjacente, ao mesmo tempo em que penaliza variações abruptas.\n\n\nDada uma série temporal \\(y_t\\), o objetivo é encontrar uma tendência \\(g_t\\) que minimize a seguinte função objetivo:\n\\[\n\\min_{g_t} \\left[ \\sum_{t=1}^{T} \\left( y_t - g_t \\right)^2 + \\lambda \\sum_{t=2}^{T-1} \\left( g_{t+1} - 2g_t + g_{t-1} \\right)^2 \\right]\n\\]\nO primeiro termo representa a soma dos quadrados dos resíduos entre a série observada e a estimativa da tendência, enquanto o segundo termo é uma penalidade que desencoraja variações abruptas na tendência. O filtro HP suaviza a série observada, permitindo identificar movimentos de longo prazo enquanto remove flutuações de curto prazo. O parâmetro \\(\\lambda\\) desempenha um papel crucial na determinação do nível de suavização da tendência, com valores maiores resultando em tendências mais suaves.\nNote que se \\(\\lambda = 0\\) o valor ótimo de \\(g_{t}\\) é simplesmente \\(y_{t}\\). Quando \\(\\lambda \\to \\infty\\) o componente \\(g_{t}\\) se aproxima de uma tendência temporal linear. A escolha de \\(\\lambda\\) não é simples e é usual amparar-se em regras de bolso. Para séries trimestrais, costuma-se usar \\(\\lambda = 1600\\); para séries mensais \\(\\lambda = 14400\\)1.\nApesar de ter sido desenvolvida para séries macroeconômicas, não há problema, em princípio, em aplicar o filtro HP a qualquer tipo de série. O gráfico abaixo mostra o resultado da aplicação do filtro HP à série de produção de veículos.\n\n\nCode\ny &lt;- ts(log(subcarros$value), frequency = 12)\nhpy &lt;- hpfilter(y, freq = 14400, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\nggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(subtitle = \"Filtro HP: Série e Tendência\", x = NULL, y = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nO passeio aleatório é um processo estocástico comum em economia. Variáveis importantes como preços futuros, preços de ações, preços de petróleo, conusmo, inflação, e a taxa de crescimento do estoque de moeda podem ser modeladas como passeios aleatórios.\nSeja um passeio aleatório \\((1-L)y_{t} = u_{t}\\) com \\(T\\) observações e frequência trimestral, onde \\(L\\) é o operador defasagem usual: \\(Lx_{t} = x_{t-1}\\). Segundo Hamilton (2017), Quando se aplica o filtro HP sobre \\(y_{t}\\) encontra-se o seguinte termo de ciclo, \\(c_t\\)\n\\[\nc_{t} = \\frac{\\lambda(1-L)^3}{F(L)}u_{t+2}\n\\]\nUsando \\(\\lambda = 1600\\), a sugestão usual para séries trimestrais,\n\\[\nc_t = 89.72 \\{ - q_{0,t+2} + \\sum^{\\infty}_{j=0} (0.8941)^j[\\cos(0.1117j)+8.916\\sin(0.1117j)](q_{1,t+2-j} + q_{2,t+2+j}) \\}\n\\]\nonde\n\n\\(q_{0t} = \\varepsilon_t - 3\\varepsilon_{t-1} + 3\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\\(q_{1t} = \\varepsilon_t - 3.79\\varepsilon_{t-1} + 5.37\\varepsilon_{t-2} - 3.37\\varepsilon_{t-3} + 0.79\\varepsilon_{t-4}\\)\n\\(q_{2t} = -0.79\\varepsilon_{t+1} + 3.37\\varepsilon_t - 5.37\\varepsilon_{t-1} + 3.79\\varepsilon_{t-2} - \\varepsilon_{t-3}\\)\n\nque é uma expressão bastante longa. O importante a se notar é que o termo \\(c_{t}\\) tem uma estrutura recursiva em função das defasagens de \\(\\varepsilon_t\\). Note que a série subjacente é simplesmente uma soma de ruídos aleatórios sem nenhum padrão:\n\\[\n\\begin{align}\n(1-L)y_{t} & = \\varepsilon_{t} \\\\\ny_{t} & = \\frac{\\varepsilon_{t}}{(1-L)} \\\\\ny_{t} & = \\sum_{i = 0}^{\\infty}e_{i}\n\\end{align}\n\\]\nmas o ciclo desta série tem uma estrutura artifical, que foi acrescentada pelo uso do filtro HP. Em linhas gerais, a exposição acima resume o argumento de Hamilton contra o uso indiscriminado do filtro HP em séries macroeconômicas não-estacionárias. O filtro HP acaba “criando” uma estrutura nos dados que não existia previamente.\n\n\nCode\nset.seed(1984)\ny &lt;- ts(cumsum(rnorm(100)), frequency = 4)\n\nhpy &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n\ntbl_hp &lt;- tibble(\n  date = as.numeric(time(y)),\n  cycle = as.numeric(hpy$cycle),\n  trend = as.numeric(hpy$trend),\n  series = as.numeric(y)\n)\n\np1 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = series), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  labs(\"Filtro HP aplicado no Random Walk\", subtitle = \"Série e Tendência\", x = NULL, y = NULL) +\n  theme_bw()\n\np2 &lt;- ggplot(tbl_hp, aes(x = date)) +\n  geom_line(aes(y = cycle), lwd = 0.8, color = \"#023047\") +\n  labs(x = NULL, y = NULL, subtitle = \"Ciclo\") +\n  theme_bw()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\nPara um exemplo mais aplicado, vamos usar a base USMacroG do pacote {AER}. O código abaixo aplica o filtro HP sobre o logaritmo das séries trimestrais do PIB, consumo, investimento e M1. As séries são todas completas com 204 observações entre 1950 e 2000.\nO gráfico mostra o ajuste do filtro HP em cada uma das quatro séries.\n\n\nCode\nlibrary(AER)\ndata(\"USMacroG\")\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro &lt;- macro |&gt; \n  mutate(across(gdp:m1, log))\n\nnest_macro &lt;- macro |&gt; \n  select(date, gdp, consumption, invest, m1) |&gt; \n  pivot_longer(col = -date, names_to = \"name_series\") |&gt; \n  group_by(name_series) |&gt; \n  nest()\n\nmacro_hp &lt;- nest_macro |&gt; \n  mutate(hpy = map(data, \\(x) {\n    y &lt;- ts(x$value, frequency = 4, start = c(1950, 1))\n    hp &lt;- hpfilter(y, freq = 1600, type = \"lambda\")\n    \n    tibble(\n      cycle = as.numeric(hp$cycle),\n      trend = as.numeric(hp$trend)\n    )\n  })) |&gt; \n  unnest(c(data, hpy)) |&gt; \n  ungroup()\n\np1 &lt;- ggplot(macro_hp, aes(x = date, y = value)) +\n  geom_line(aes(y = value), lwd = 0.8, color = \"#023047\") +\n  geom_line(aes(y = trend), lwd = 0.8, color = \"#ffb703\") +\n  facet_wrap(~name_series, scales = \"free_y\", ncol = 1) +\n  labs(x = NULL, y = NULL)+\n  theme_bw()\n\np2 &lt;- ggplot(macro_hp, aes(x = date, y = cycle)) +\n  geom_line(lwd = 0.8, color = \"#023047\") +\n  geom_hline(yintercept = 0) +\n  facet_wrap(~name_series, ncol = 1) +\n  labs(x = NULL, y = NULL) +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nA análise das autocorrelações mostra que há bastante autocorrelação cruzada entre as séries.\n\n\nCode\nmacro_detrend &lt;- macro_hp |&gt; \n  mutate(detrend = value - trend) |&gt; \n  pivot_wider(id_cols = \"date\", names_from = \"name_series\", values_from = \"detrend\") |&gt; \n  select(-date)\n\nggAcf(ts(macro_detrend, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: séries com filtro HP\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSe tivéssemos assumido, contudo, que as séries são I(1), não haveria muita correlação cruzada entre as séries.\n\n\nCode\nmacro_d1 &lt;- macro |&gt; \n  select(gdp, consumption, invest, m1) |&gt; \n  mutate(across(everything(), ~c(NA, diff(.x))))\n\n\nggAcf(ts(macro_d1, frequency = 4), lag.max = 28) +\n  ggtitle(\"ACF: primeira diferença das séries\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#hamilton",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#hamilton",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "",
    "text": "Como alternativa ao filtro HP Hamilton (2017) sugere um procedimento bastante simples. Seja \\(y_t\\) uma série não-estacionária. Então fazemos uma regressão linear de \\(y_{t+h}\\) contra os \\(p\\) valores mais recentes.\n\\[\ny_{t+h} = \\beta_0 + \\beta_{1} y_{t} + \\beta_{2} y_{t-1} + \\beta_{3} y_{t-2} + \\beta_{4} y_{t-3} + u_{t+h}\n\\]\nPara o caso específico de dados trimestrais, ele sugere uma formulação do tipo:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nOu, alternativamente, um modelo ainda mais simples, em função apenas de \\(y_{t-8}\\) na forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + u_{t+h}\n\\]\nDe modo geral, Hamilton argumenta que este filtro garante a estacionaridade de séries, sendo elas tendência-estacionárias ou diferença-estacionárias. Mais detalhes sobre o filtro podem ser verificados no Working Paper.\n\n\nO primeiro passo necessário é preparar os dados. Novamente, vamos trabalhar com as séries num padrão retangular, usando um tibble. A base de dados utilizada é a USMacroG, descrita acima; neste caso, vamos adicionar as séries de gasto do governo e inflação. Isto é, temos agora seis séries: PIB, consumo, investimento, gasto do governo, inflação e M1. Todas as variáveis, exceto pela taxa de inflação, serão transformadas usando log, seguindo o paper de Hamilton.\nO código abaixo tem um trecho interessante onde se usa uma combinação de map e partial para criar várias colunas com os valores defasados das variáveis.\n\ndata(USMacroG)\n\nmacro &lt;- tibble(\n  date = zoo::as.Date.ts(USMacroG),\n  as.data.frame(USMacroG)\n)\n\nmacro_long &lt;- macro |&gt; \n  select(date, gdp, consumption, invest, m1, government, inflation) |&gt; \n  mutate(across(gdp:government, ~log10(.x) * 100)) |&gt; \n  pivot_longer(-date, names_to = \"name_series\")\n\nmacro_lags &lt;- macro_long |&gt; \n  group_by(name_series) |&gt; \n  mutate(\n    across(value,\n           map(1:12, ~partial(dplyr::lag, n = .x)),\n           .names = \"l{.fn}\")\n  )\n\n\n\n\nHamilton sugere duas maneiras de tirar a tendência dos dados: usando uma regressão linear e tirando uma diferença simples. No caso de séries trimestrais, o autor sugere \\(p=4\\) e \\(h=8\\). Assim, temos de estimar um modelo linear da forma:\n\\[\ny_{t} = \\beta_0 + \\beta_{1} y_{t-8} + \\beta_{2} y_{t-9} + \\beta_{3} y_{t-10} + \\beta_{4} y_{t-11} + u_{t+h}\n\\]\nE a série livre de tendência, \\(z_t\\) será dada por:\n\\[\nz_{t} = y_{t} - \\hat{\\beta_0} - \\hat{\\beta_1}y_{t-8} - \\hat{\\beta_2}y_{t-9} - \\hat{\\beta_3}y_{t-10} - \\hat{\\beta_4}y_{t-11}\n\\]\nNo segundo “modelo”, vamos simplesmente tirar uma diferença na forma:\n\\[\nz_{t} = y_{t} - y_{t-8}\n\\]\nO primeiro modelo será chamado de “regression”, enquanto o segundo modelo será chamado de “Random Walk”. O código abaixo faz a regressão em todas as séries e grava os resultados do fit e os resíduos de todos os modelos.\n\nmacro_models &lt;- macro_lags |&gt; \n  mutate(resid_rw = value - l8) |&gt; \n  nest() |&gt; \n  mutate(\n    model_reg = map(data, \\(d) lm(value ~ l8 + l9 + l10 + l11, data = d)),\n    trend = map(model_reg, fitted),\n    resid = map(model_reg, residuals)\n  )\n\n\n\n\nIdealmente, deve-se verificar o comportamento dos resíduos do modelo de regressão. Neste caso, estes resíduos são considerados o componente cíclico da série original. O painel abaixo mostra os resíduos da primeira série, do PIB. O gráfico de cima é simplesmente o gráfico do resíduo. No gráfico inferior-esquerdo temos o gráfico de autocorrelação da série e no gráfico inferior-direito temos o historgrama dos resíduos com a densidade da distribuição normal superimposta.\nVale lembrar que não estamos buscando limpar a série totalmente de autocorrelação, como na modelagem ARIMA.\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals\nQ* = 417.09, df = 10, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 10\n\n\nNULL\n\n\nEvidentemente, não é fácil inspecionar os resíduos de todos os modelos, especialmente quando o número de séries é muito grande. O painel abaixo mostra o gráfico de autocorrelação entre as seis séries consideradas. Na diagonal principal temos o gráfico do resíduo da própria série; nos demais gráficos, mostra-se a autocorrelação cruzada entre os resíduos.\n\n\n\n\n\n\n\n\n\n\n\n\nO código abaixo junta todas as séries para facilitar a sua visualização. Como há várias séries e muita repetição, crio uma função plot_series para facilitar a construção dos painéis.\n\n\nCode\nm1 &lt;- macro_models |&gt; \n  ungroup() |&gt; \n  select(name_series, trend, resid) |&gt; \n  unnest(cols = c(\"trend\", \"resid\")) |&gt; \n  mutate(id = as.numeric(names(trend)), .before = everything())\n\nmacro_trend &lt;- macro_models |&gt; \n  select(name_series, data) |&gt; \n  unnest(cols = c(data)) |&gt; \n  mutate(id = vctrs::vec_group_id(date), .before = everything()) |&gt; \n  ungroup()\n\nmacro_trend &lt;- macro_trend |&gt; \n  left_join(m1, by = c(\"id\", \"name_series\"))\n\nseries_trend &lt;- macro_trend |&gt; \n  select(date, name_series, trend, resid, resid_rw) |&gt; \n  pivot_longer(cols = -c(date, name_series), names_to = \"name_decomp\") |&gt;\n  mutate(decomp = if_else(str_detect(name_decomp, \"trend\"), \"trend\", \"resid\")) |&gt; \n  filter(!is.na(value))\n\nplot_series &lt;- function(x, ylim = c(NA, NA)) {\n  \n  p1 &lt;- ggplot() +\n  geom_line(\n    data = dplyr::filter(series_trend, name_series == x, decomp == \"trend\"),\n    aes(x = date, y = value),\n    lwd = 0.8,\n    color = \"steelblue\"\n    ) +\n  labs(x = NULL, y = NULL) +\n  ggtitle(glue::glue(\"Series: {x}\")) +\n  theme_bw() +\n  theme(plot.title = element_text(size = 10))\n\np2 &lt;- ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_line(\n    data = dplyr::filter(series_trend, name_series == x, decomp == \"resid\"),\n    aes(x = date, y = value, color = name_decomp),\n    lwd = 0.8\n    ) +\n  scale_x_date(\n    breaks = seq(as.Date(\"1950-01-01\"), as.Date(\"2000-01-01\"), by = \"10 year\"),\n    date_labels = \"%Y\"\n    ) +\n  scale_y_continuous(limits = ylim) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"#003049\", \"#d62828\"),\n    labels = c(\"Regression\", \"Random Walk\")) +\n  labs(x = NULL, y = NULL) +\n  ggtitle(glue::glue(\"Cyclical Component: {x}\")) +\n  theme_bw() +\n  theme(legend.position = \"top\", plot.title = element_text(size = 10))\n\n  return(list(trend = p1, cycle = p2))\n  \n}\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de PIB e de consumo.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de investimento e gasto do governo.\n\n\n\n\n\n\n\n\n\nFinalmente, o gráfico abaixo mostra o ajuste do filtro de Hamilton nas séries de M1 e de inflação.\n\n\n\n\n\n\n\n\n\nO filtro proposto por Hamilton tem diversas características interessantes. A simplicidade e intuição do filtro, em especial, são pontos atrativos. Um paper recente, de Viv Hall e Peter Thompson (2021), contudo, não encontrou grandes vantagens deste filtro em relação aos tradicionais filtro HP e filtro Baxter-King. Os autores testam os filtros usando um amplo conjunto de séries macroeconômicas da Nova Zelândia. De maneira geral, o filtro de Hamilton gerou séries com maior volatilidade e pior capacidade preditiva. Considerando a literatura de filtros, parece improvável, de fato, que um filtro tão simples possa ter uma melhor performance do que filtros no domínio da frequência."
  },
  {
    "objectID": "posts/general-posts/2024-02-hamilton-trend/index.html#footnotes",
    "href": "posts/general-posts/2024-02-hamilton-trend/index.html#footnotes",
    "title": "Filtro HP e Filtro de Hamilton",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara uma discussão sobre a escolha de \\(\\lambda\\) em séries macroeconômicas, veja Ravn e Uhlig (2002)↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-energy/index.html",
    "href": "posts/general-posts/2024-02-wz-energy/index.html",
    "title": "Energia Elétrica e Crescimento Econômico no Brasil",
    "section": "",
    "text": "Energia e PIB\nTipicamente, entende-se que a demanda ou produção de energia elétrica é uma variável proxy razoável para o PIB. O aumento da renda tende a ser acompanhado de maior demanda por energia elétrica pelo lado dos consumidores. Por outro lado, a indústria, comércio e serviços em geral utilizam a energia elétrica como importante insumo; assim, um aumento da produção será acompanhado por maior demanda de energia elétrica.\nQuando se olha para o comportamento destas séries de tempo no Brasil surge um curioso gráfico. De 2003 a 2014, o crescimento das séries do PIB, do IBC-Br e de demanda total de energia elétrica no país são muito similares. A partir deste ponto, parece haver uma divergência na taxa de crescimento das séries. O consumo por energia elétrica rapidamente se recupera e volta a exibir uma tendência de crescimento. Já o PIB e especialmente o IBC-Br passam um maior tempo estagnados e demoram a voltar a crescer.\nNo gráfico abaixo, tanto a série do PIB como a do IBC-Br estão dessazonalizadas. Mostra-se a tendência do consumo de energia elétrica, usando uma decomposição STL.\n\n\n\n\n\n\n\n\n\nO segundo gráfico divide o consumo em três grandes grupos: comercial, industrial e residencial. Neste gráfico, vê-se como a demanda por energia elétrica da indústria começa a divergir das demais após a Crise de 2008 e depois cai novamente na Crise de 2014. De maneira geral, o consumo de energia elétrica residencial parece ser o menos afetado por crises econômicas."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-house-prices/index.html",
    "href": "posts/general-posts/2024-02-wz-house-prices/index.html",
    "title": "Preços de Imóveis no Brasil",
    "section": "",
    "text": "# A tibble: 121 × 5\n   date       value cumprod new_index source\n   &lt;date&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n 1 2014-01-01  0.55   0.550      100  IPCA  \n 2 2014-02-01  0.69   1.24       101. IPCA  \n 3 2014-03-01  0.92   2.18       102. IPCA  \n 4 2014-04-01  0.67   2.86       103. IPCA  \n 5 2014-05-01  0.46   3.33       103. IPCA  \n 6 2014-06-01  0.4    3.75       104. IPCA  \n 7 2014-07-01  0.01   3.76       104. IPCA  \n 8 2014-08-01  0.25   4.02       104. IPCA  \n 9 2014-09-01  0.57   4.61       105. IPCA  \n10 2014-10-01  0.42   5.05       105. IPCA  \n# ℹ 111 more rows\n\n\n\nPreços de Imóveis no Brasil\nExiste uma percepção generalizada na população de que os preços dos imóveis no Brasil são muito caros. Isto pode ser resultado tanto de uma ignorância sobre a dinâmica do preço dos imóveis como do excesso de notícias sobre recordes de preços (nominais) que se vê na imprensa.\nEsta narrativa não é totalmente desprovida de embasamento. Houve uma recuperação significativa dos preços dos imóveis no período da pandemia, alimentado pela baixa taxa de juros da época. Contudo, também houve um aumento significativo do preço dos insumos da construção civil assim como dos preços em geral da economia.\nEspeculo que isto se deve a alguns fatores:\n\nEvidência anedótica. Quando se discute preços de imóveis é certo que aparecem casos específicos de regiões, bairros, ou ruas onde os preços dos imóveis aumentaram muito acima da inflação. Estes casos são reais, como foi, por exemplo,\nIgnorância sobre a dinâmica de preços. Dificilmente se observa uma queda nominal do preço dos imóveis; mesmo em períodos de fraca demanda ou de crise, os proprietários de imóveis tendem a\nA narrativa da Housing Crisis. Narrativa parcialmente importada dos Estados Unidos e de boa parte do mundo desenvolvido"
  },
  {
    "objectID": "posts/general-posts/2024-01-wz-house-prices/index.html",
    "href": "posts/general-posts/2024-01-wz-house-prices/index.html",
    "title": "Preços de Imóveis no Brasil",
    "section": "",
    "text": "Preços de Imóveis no Brasil\nExiste uma percepção generalizada na população de que os preços dos imóveis no Brasil são muito caros. Isto pode ser resultado tanto de uma ignorância sobre a dinâmica do mercado como do excesso de notícias sobre recordes de preços que se vê na imprensa. Também não faltam casos de regiões ou mesmo de ruas que, após significativo processo de revitalização, apresentaram aumentos de preços muito acima da inflação. De maneira geral, contudo, os dados apontam que os preços dos imóveis no Brasil andaram de lado: em termos reais, isto é, descontando a inflação, o nível atual do preço dos imóveis está praticamente idêntico ao que se observava em 2010.\n\nPreços do Brasil x Mundo\n\n\n\n\n\n\n\n\n\n\n\nPreços do Brasil x Países Desenvolvidos\n\n\n\n\n\n\n\n\n\n\n\nPreços do Brasil x Países LATAM"
  },
  {
    "objectID": "posts/general-posts/2023-01-tidyverse-filter/index.html",
    "href": "posts/general-posts/2023-01-tidyverse-filter/index.html",
    "title": "O novo tidyverse: filter",
    "section": "",
    "text": "O tidyverse é uma coleção poderosa de pacotes, voltados para a manipulação e limpeza de dados. Num outro post, discuti alguns aspectos gerais da filosofia destes pacotes que incluem a sua consistência sintática e o uso de pipes. A filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes” que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático.\nO tidyverse está em constante expansão, novas funcionalidades são criadas para melhorar a performance e capabilidade de suas funções. Assim, é importante atualizar nosso conhecimento destes pacotes periodicamente. Nesta série de posts vou focar nas funções principais dos pacotes dplyr e tidyr, voltados para a limpeza de dados."
  },
  {
    "objectID": "posts/general-posts/2023-01-tidyverse-filter/index.html#o-básico",
    "href": "posts/general-posts/2023-01-tidyverse-filter/index.html#o-básico",
    "title": "O novo tidyverse: filter",
    "section": "O básico",
    "text": "O básico\nOs pacotes utilizados neste tutorial são listados abaixo.\n\nlibrary(dplyr)\nlibrary(readr)\n\nPara praticar as funções vamos utilizar uma tabela que traz informações sobre as cidades do Brasil.\n\ndat &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/cities_brazil.csv\"\n  )\n\n# dat &lt;- select(dat, 1:7, population, population_growth, pib)\n\nA função filter é talvez uma das que menos mudou ao longo do desenvolvimento do pacote dplyr. A função serve para filtrar as linhas de um data.frame segundo alguma condição lógica.\n\nfiltered_dat &lt;- filter(dat, population_growth &lt; 0)\n\nnrow(filtered_dat)\n\n[1] 2399\n\n\nOs principais operadores lógicos no R:\n\n“Maior que”, “Menor que”: &gt;, &lt;, &gt;=, &lt;=\nE/ou: &, |\n“Negação”: !\n“Igual a”: ==\n“Dentro de”: %in%\n\nAs funções is_* também são bastante importantes; em particular a função is.na() é útil para encontrar ou remover observações ausentes.\nO exemplo abaixo mostra como filtrar linhas baseado num string. Note que quando se usa múltiplos strings é preciso usar o %in%.\n\nfilter(dat, name_muni == \"São Paulo\")\nfilter(dat, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\"))\n\ncities &lt;- c(\"São Paulo\", \"Rio de Janeiro\")\nfilter(dat, name_muni %in% cities)\n\n\nfilter(dat, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\")) |&gt; \n  print_table()\n\nPara negar a igualdade, basta usar o operador !. No caso do operador %in% há duas maneiras válidas de negá-lo: pode-se colocar o ! no começo da expressão ou colocar a expressão inteira dentro de um parêntesis. Eu tendo a preferir a segunda sintaxe.\n\n#&gt; Remove todas as cidades da região Sudeste\nfilter(dat, name_region != \"Sudeste\")\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(dat, !name_region %in% c(\"Sudeste\", \"Norte\"))\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(dat, !(name_region %in% c(\"Sudeste\", \"Norte\")))\n\nEm geral, pode-se omitir o operador E (&), já que se pode concatenar várias condições lógicas dentro uma mesma chamada para a função filter, separando as condições por vírgulas. Esta sintaxe costuma ser preferida pois ela é mais eficiente do que chamar a função a função filter múltiplas vezes. Além disso, a escrita do código fica mais limpa, pois é fácil separar as condições em linhas distintas. As três versões do código abaixo geram o mesmo resultado.\n\n# Mais eficiente e mais fácil de ler\nd1 &lt;- dat |&gt; \n  filter(\n    name_region == \"Nordeste\",\n    !(name_state %in% c(\"Pernambuco\", \"Piauí\")),\n    !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n  )\n# Igualmente eficiente, leitura fica um pouco pior\nd2 &lt;- dat |&gt; \n  filter(\n    name_region == \"Nordeste\" & \n      !(name_state %in% c(\"Pernambuco\", \"Piauí\")) & \n      !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n    )\n\n# Menos eficiente\nd3 &lt;- dat |&gt; \n  filter(name_region == \"Nordeste\") |&gt; \n  filter(!(name_state %in% c(\"Pernambuco\", \"Piauí\"))) |&gt; \n  filter(!(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\")))\n\nall.equal(d1, d2)\nall.equal(d2, d3)\nall.equal(d3, d1)\n\nRelações de grandeza funcionam naturalmente com números. A tabela abaixo mostra todos os municípios com mais do que um milhão de habitantes.\n\nfilter(dat, population &gt; 1e6) \n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npopulation\npopulation_density\n\n\n\n\nSão Paulo\nSP\n11.451.245\n7.528\n\n\nRio de Janeiro\nRJ\n6.211.423\n5.175\n\n\nBrasília\nDF\n2.817.068\n489\n\n\nFortaleza\nCE\n2.428.678\n7.775\n\n\nSalvador\nBA\n2.418.005\n3.487\n\n\nBelo Horizonte\nMG\n2.315.560\n6.988\n\n\nManaus\nAM\n2.063.547\n181\n\n\nCuritiba\nPR\n1.773.733\n4.079\n\n\nRecife\nPE\n1.488.920\n6.804\n\n\nGoiânia\nGO\n1.437.237\n1.971\n\n\nPorto Alegre\nRS\n1.332.570\n2.690\n\n\nBelém\nPA\n1.303.389\n1.230\n\n\nGuarulhos\nSP\n1.291.784\n4.054\n\n\nCampinas\nSP\n1.138.309\n1.433\n\n\nSão Luís\nMA\n1.037.775\n1.780\n\n\n\n\n\n\n\n\nTambém pode-se usar alguma função que retorne um valor numérico. Nos exemplos abaixo filtra-se apenas os municípios com PIB acima da média e os municípios no top 1% da distribuição do PIB.\n\nfilter(dat, pib &gt; mean(pib))\nfilter(dat, pib &gt; quantile(pib, probs = 0.99))\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npib\npib_share_uf\n\n\n\n\nSão Paulo\nSP\n748.759.007\n31\n\n\nRio de Janeiro\nRJ\n331.279.902\n44\n\n\nBrasília\nDF\n265.847.334\n100\n\n\nBelo Horizonte\nMG\n97.509.893\n14\n\n\nManaus\nAM\n91.768.773\n79\n\n\nCuritiba\nPR\n88.308.728\n18\n\n\nOsasco\nSP\n76.311.814\n3\n\n\nPorto Alegre\nRS\n76.074.563\n16\n\n\nGuarulhos\nSP\n65.849.311\n3\n\n\nCampinas\nSP\n65.419.717\n3\n\n\nFortaleza\nCE\n65.160.893\n39\n\n\nSalvador\nBA\n58.938.115\n19\n\n\nGoiânia\nGO\n51.961.311\n23\n\n\nBarueri\nSP\n51.254.572\n2\n\n\nJundiaí\nSP\n51.235.050\n2\n\n\nRecife\nPE\n50.311.002\n26\n\n\nSão Bernardo do Campo\nSP\n48.614.342\n2\n\n\nDuque de Caxias\nRJ\n47.153.673\n6\n\n\nNiterói\nRJ\n40.949.495\n5\n\n\nSão José dos Campos\nSP\n39.148.012\n2\n\n\nPaulínia\nSP\n38.572.766\n2\n\n\nParauapebas\nPA\n38.014.863\n18\n\n\nUberlândia\nMG\n37.631.537\n6\n\n\nSorocaba\nSP\n36.723.769\n2\n\n\nJoinville\nSC\n36.391.912\n10\n\n\nMaricá\nRJ\n35.618.327\n5\n\n\nRibeirão Preto\nSP\n35.218.869\n1\n\n\nItajaí\nSC\n33.084.145\n9\n\n\nSão Luís\nMA\n33.074.010\n31\n\n\nBelém\nPA\n30.835.763\n14\n\n\nCampo Grande\nMS\n30.121.789\n25\n\n\nContagem\nMG\n29.558.094\n4\n\n\nSanto André\nSP\n29.440.477\n1\n\n\nPiracicaba\nSP\n27.172.817\n1\n\n\nCuiabá\nMT\n26.528.839\n15\n\n\nBetim\nMG\n26.185.005\n4\n\n\nCaxias do Sul\nRS\n25.965.161\n6\n\n\nCamaçari\nBA\n25.697.266\n8\n\n\nVitória\nES\n25.473.898\n18\n\n\nSerra\nES\n25.079.657\n18\n\n\nCampos dos Goytacazes\nRJ\n23.841.837\n3\n\n\nMaceió\nAL\n22.872.756\n36\n\n\nNatal\nRN\n22.729.773\n32\n\n\nCanaã dos Carajás\nPA\n22.522.725\n10\n\n\nSantos\nSP\n22.073.535\n1\n\n\nSão José dos Pinhais\nPR\n21.975.612\n4\n\n\nLondrina\nPR\n21.729.852\n4\n\n\nTeresina\nPI\n21.578.875\n38\n\n\nFlorianópolis\nSC\n21.312.447\n6\n\n\nCajamar\nSP\n20.798.646\n1\n\n\nJoão Pessoa\nPB\n20.766.551\n30\n\n\nMaringá\nPR\n20.005.630\n4\n\n\nAraucária\nPR\n19.724.416\n4\n\n\nPorto Velho\nRO\n19.448.762\n38\n\n\nSão Gonçalo\nRJ\n19.002.883\n3\n\n\nSão José do Rio Preto\nSP\n18.694.213\n1"
  },
  {
    "objectID": "posts/general-posts/2023-01-tidyverse-filter/index.html#grupos",
    "href": "posts/general-posts/2023-01-tidyverse-filter/index.html#grupos",
    "title": "O novo tidyverse: filter",
    "section": "Grupos",
    "text": "Grupos\nA função de filtro segue uma regra lógica que é aplicada sobre a tabela como um todo. É possível filtrar dentro de grupos usando o argumento .by = \"nome_do_grupo\".\nNo código abaixo, novamente filtra-se os municípios com PIB acima da média. No segundo exemplo, contudo, este filtro é aplicado dentro de cada região, segundo a coluna/grupo name_region. A regra lógica pib &gt; mean(pib) é aplicada dentro de cada região, isto é, filtra-se todos os municípios que têm PIB superior à média do PIB da sua região.\n\ndat |&gt; filter(pib &gt; mean(pib)) \ndat |&gt; filter(pib &gt; mean(pib), .by = \"name_region\")\n\nVale notar que a a sintaxe .by = \"grupo\" ainda está em fase experimental. Ela oferece um substituto mais sucinto à antiga sintaxe que usava a função group_by() com a vantagem de sempre aplicar a função ungroup() ao final do processo, isto é, o resultado final da função acima será uma tabela sem grupos. O código acima é equivalente ao código abaixo.\n\ndat |&gt; \n  group_by(name_region) |&gt; \n  filter(pib &gt; mean(pib)) |&gt; \n  ungroup()\n\nEste outro exemplo enfatiza como o resultado da função filter muda quando é aplicada em diferentes grupos.\n\ndat |&gt; filter(pib == max(pib))\ndat |&gt; filter(pib == max(pib), .by = \"name_state\")\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npib\npib_share_uf\n\n\n\n\nPorto Velho\nRO\n19.448.762\n38\n\n\nRio Branco\nAC\n9.579.592\n58\n\n\nManaus\nAM\n91.768.773\n79\n\n\nBoa Vista\nRR\n11.826.207\n74\n\n\nParauapebas\nPA\n38.014.863\n18\n\n\nMacapá\nAP\n11.735.557\n64\n\n\nPalmas\nTO\n9.940.091\n23\n\n\nSão Luís\nMA\n33.074.010\n31\n\n\nTeresina\nPI\n21.578.875\n38\n\n\nFortaleza\nCE\n65.160.893\n39\n\n\nNatal\nRN\n22.729.773\n32\n\n\nJoão Pessoa\nPB\n20.766.551\n30\n\n\nRecife\nPE\n50.311.002\n26\n\n\nMaceió\nAL\n22.872.756\n36\n\n\nAracaju\nSE\n16.447.105\n36\n\n\nSalvador\nBA\n58.938.115\n19\n\n\nBelo Horizonte\nMG\n97.509.893\n14\n\n\nVitória\nES\n25.473.898\n18\n\n\nRio de Janeiro\nRJ\n331.279.902\n44\n\n\nSão Paulo\nSP\n748.759.007\n31\n\n\nCuritiba\nPR\n88.308.728\n18\n\n\nJoinville\nSC\n36.391.912\n10\n\n\nPorto Alegre\nRS\n76.074.563\n16\n\n\nCampo Grande\nMS\n30.121.789\n25\n\n\nCuiabá\nMT\n26.528.839\n15\n\n\nGoiânia\nGO\n51.961.311\n23\n\n\nBrasília\nDF\n265.847.334\n100"
  },
  {
    "objectID": "posts/general-posts/2023-01-tidyverse-filter/index.html#if_any-e-if_all",
    "href": "posts/general-posts/2023-01-tidyverse-filter/index.html#if_any-e-if_all",
    "title": "O novo tidyverse: filter",
    "section": "if_any e if_all",
    "text": "if_any e if_all\nA função filter não funciona em conjunção com a função across(). Esta função foi desenvolvida para funcionar apenas com mutate e summarise e aplica uma mesma regra/função sobre múltiplas colunas.\nJá a função filter recebeu duas funções auxiliares: if_any e if_all. Elas seguem o mesmo padrão das funções base any e all. Estas funções servem para agregar condições lógicas. A função any, por exemplo, testa múltiplas condições lógicas e retorna um único TRUE se houver ao menos um TRUE entre as condições lógicas. Já a função all retorna um único TRUE se absolutamente todas as condições lógicas testadas também retornaram TRUE.\nA função if_any aplica uma mesma regra em múltiplas colunas e retorna todas as linhas que atendem esta regra. No exemplo abaixo\n\ndat |&gt; filter(if_any(starts_with(\"pib\"), ~ . &gt; 100000))\n\nO exemplo seguinte é mais interessante. Neste caso, todas as variáveis numéricas da tabela são normalizadas (por região) e retorna-se apenas os municípios onde o valor de cada coluna é superior a 1. Como as variáveis estão normalizadas isto é equivalente a retornar os municípios que estão 1 desvio-padrão acima da média da sua região em todos os atributos numéricos considerados.\n\ndat |&gt; \n  select(-contains(\"code\")) |&gt; \n  select(where(~all(.x &gt; 0))) |&gt; \n  mutate(across(where(is.numeric), ~as.numeric(scale(log(.x)))), .by = \"name_region\") |&gt; \n  filter(if_all(everything(), ~ . &gt; 1))\n\n# A tibble: 2 × 15\n  name_muni            name_state abbrev_state name_region population city_area\n  &lt;chr&gt;                &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 Paranaguá            Paraná     PR           Sul               2.32      1.13\n2 São José dos Pinhais Paraná     PR           Sul               3.00      1.28\n  population_density households dwellers_per_household   pib pib_taxes\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1               1.52       2.24                   2.25  2.80      2.81\n2               2.10       2.97                   1.04  3.27      3.31\n  pib_added_value pib_industrial pib_services pib_govmt_services\n            &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n1            2.76           2.36         2.71               2.57\n2            3.18           2.76         2.92               3.17\n\n\nO último exemplo é similar ao anterior. As variáveis numéricas novamente são normalizadas mas desta vez busca-se somente os municípios que estão 3 desvios-padrão, acima da média do seu estado, ou na população ou no PIB.\n\ndat |&gt; \n  select(-contains(\"code\")) |&gt; \n  select(where(~all(.x &gt; 0))) |&gt; \n  mutate(across(where(is.numeric), ~as.numeric(scale(log(.x)))), .by = \"name_state\") |&gt; \n  filter(if_any(c(population, pib), ~ . &gt; 3))\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npopulation\npib\npopulation_density\npib_services\ncity_area\n\n\n\n\nPorto Velho\nRO\n3,249\n3,508\n1,129\n2,954\n2,622\n\n\nRio Branco\nAC\n3,254\n3,337\n2,525\n3,112\n0,628\n\n\nManaus\nAM\n5,214\n5,439\n3,420\n5,136\n-0,239\n\n\nBoa Vista\nRR\n3,299\n3,310\n3,059\n3,172\n-0,709\n\n\nBelém\nPA\n4,041\n3,410\n3,122\n3,670\n-0,672\n\n\nCanaã dos Carajás\nPA\n0,950\n3,151\n0,511\n2,260\n0,069\n\n\nParauapebas\nPA\n2,305\n3,582\n0,816\n2,795\n0,602\n\n\nAraguaína\nTO\n3,837\n3,459\n2,184\n3,492\n1,201\n\n\nGurupi\nTO\n3,062\n2,857\n2,255\n3,018\n0,381\n\n\nPalmas\nTO\n4,467\n4,166\n3,269\n4,073\n0,580\n\n\nBalsas\nMA\n2,045\n3,190\n-0,898\n3,117\n2,899\n\n\nImperatriz\nMA\n3,236\n3,599\n2,353\n3,654\n0,373\n\n\nSão José de Ribamar\nMA\n3,103\n2,412\n4,270\n2,681\n-1,894\n\n\nSão Luís\nMA\n4,845\n5,096\n4,541\n4,903\n-0,581\n\n\nParnaíba\nPI\n3,676\n3,413\n3,580\n3,577\n-0,564\n\n\nPicos\nPI\n2,881\n3,018\n2,643\n3,340\n-0,266\n\n\nTeresina\nPI\n5,667\n5,526\n4,092\n5,417\n0,676\n\n\nUruçuí\nPI\n1,464\n3,101\n-1,185\n2,756\n2,604\n\n\nCaucaia\nCE\n3,041\n2,955\n1,882\n2,814\n0,878\n\n\nFortaleza\nCE\n5,212\n4,963\n5,137\n4,922\n-0,641\n\n\nMaracanaú\nCE\n2,569\n3,239\n3,900\n3,146\n-1,851\n\n\nParnamirim\nRN\n3,428\n3,265\n4,138\n3,371\n-0,674\n\n\nMossoró\nRN\n3,475\n3,434\n1,213\n3,512\n2,658\n\n\nNatal\nRN\n4,538\n4,421\n4,968\n4,488\n-0,323\n\n\nCabedelo\nPB\n2,206\n3,090\n3,771\n3,125\n-2,247\n\n\nCampina Grande\nPB\n4,163\n4,287\n2,668\n4,093\n1,407\n\n\nJoão Pessoa\nPB\n4,893\n4,951\n4,328\n4,721\n0,137\n\n\nSanta Rita\nPB\n3,070\n3,029\n1,490\n2,768\n1,645\n\n\nIpojuca\nPE\n1,478\n3,168\n0,696\n2,718\n0,530\n\n\nJaboatão dos Guararapes\nPE\n3,469\n3,148\n2,836\n3,085\n-0,142\n\n\nRecife\nPE\n4,360\n4,261\n3,673\n4,202\n-0,303\n\n\nArapiraca\nAL\n3,021\n2,913\n3,084\n3,187\n0,645\n\n\nMaceió\nAL\n4,588\n4,318\n4,478\n4,537\n1,190\n\n\nAracaju\nSE\n3,638\n3,676\n4,258\n3,886\n-0,094\n\n\nCamaçari\nBA\n3,357\n4,191\n2,547\n3,774\n0,001\n\n\nFeira de Santana\nBA\n4,229\n3,722\n2,744\n3,800\n0,506\n\n\nJuazeiro\nBA\n3,068\n2,623\n0,357\n2,778\n2,135\n\n\nLuís Eduardo Magalhães\nBA\n2,123\n3,040\n0,108\n3,046\n1,628\n\n\nSalvador\nBA\n5,882\n4,927\n4,579\n5,007\n-0,122\n\n\nSão Francisco do Conde\nBA\n0,853\n3,510\n1,627\n2,947\n-1,059\n\n\nVitória da Conquista\nBA\n3,615\n3,054\n1,439\n3,230\n1,414\n\n\nBelo Horizonte\nMG\n5,103\n4,613\n5,077\n4,620\n-0,186\n\n\nBetim\nMG\n3,489\n3,638\n3,490\n3,365\n-0,148\n\n\nContagem\nMG\n3,874\n3,728\n4,373\n3,712\n-0,715\n\n\nExtrema\nMG\n1,581\n3,027\n1,961\n3,055\n-0,487\n\n\nGovernador Valadares\nMG\n3,049\n2,630\n1,342\n2,768\n1,768\n\n\nIpatinga\nMG\n2,935\n3,004\n3,619\n2,862\n-0,882\n\n\nJuiz de Fora\nMG\n3,744\n3,312\n2,450\n3,412\n1,280\n\n\nMontes Claros\nMG\n3,495\n2,900\n1,386\n2,955\n2,195\n\n\nNova Lima\nMG\n2,270\n3,072\n2,118\n2,792\n0,073\n\n\nRibeirão das Neves\nMG\n3,282\n2,320\n4,007\n2,358\n-0,944\n\n\nUberaba\nMG\n3,304\n3,326\n0,995\n3,208\n2,426\n\n\nUberlândia\nMG\n4,003\n3,907\n1,752\n3,795\n2,332\n\n\nSerra\nES\n3,040\n2,923\n2,726\n2,891\n0,284\n\n\nRio de Janeiro\nRJ\n3,591\n3,414\n2,301\n3,286\n1,393\n\n\nGuarulhos\nSP\n3,055\n2,881\n2,875\n2,802\n0,155\n\n\nSão Paulo\nSP\n4,578\n4,338\n3,294\n4,267\n2,022\n\n\nAraucária\nPR\n2,360\n3,294\n2,508\n2,752\n0,310\n\n\nCascavel\nPR\n3,108\n3,016\n1,810\n2,989\n2,310\n\n\nCuritiba\nPR\n4,576\n4,554\n5,174\n4,384\n0,210\n\n\nFoz do Iguaçu\nPR\n2,930\n3,212\n2,899\n2,666\n0,660\n\n\nLondrina\nPR\n3,531\n3,376\n2,550\n3,365\n1,995\n\n\nMaringá\nPR\n3,255\n3,306\n3,514\n3,330\n0,361\n\n\nPonta Grossa\nPR\n3,135\n3,183\n1,859\n2,982\n2,286\n\n\nSão José dos Pinhais\nPR\n3,058\n3,385\n2,585\n3,081\n1,249\n\n\nFlorianópolis\nSC\n3,322\n3,046\n2,472\n3,052\n1,339\n\n\nItajaí\nSC\n2,731\n3,372\n2,587\n3,097\n0,268\n\n\nJoinville\nSC\n3,436\n3,443\n2,155\n3,134\n1,988\n\n\nCanoas\nRS\n3,136\n3,231\n3,730\n2,942\n-0,689\n\n\nCaxias do Sul\nRS\n3,370\n3,492\n1,901\n3,237\n1,694\n\n\nPelotas\nRS\n3,083\n2,722\n1,637\n2,718\n1,669\n\n\nPorto Alegre\nRS\n4,230\n4,316\n3,739\n4,169\n0,561\n\n\nCampo Grande\nMS\n4,131\n3,470\n2,785\n3,555\n1,112\n\n\nCuiabá\nMT\n3,653\n2,967\n3,573\n3,093\n0,040\n\n\nAnápolis\nGO\n3,070\n3,060\n2,746\n3,038\n0,151\n\n\nAparecida de Goiânia\nGO\n3,297\n3,039\n3,874\n3,094\n-0,966\n\n\nGoiânia\nGO\n4,110\n3,976\n3,908\n4,062\n-0,081"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-expectativa/index.html",
    "href": "posts/general-posts/2023-11-wz-expectativa/index.html",
    "title": "Expectativa de Vida em São Paulo",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(tidypod)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(readxl)\nlibrary(here)\nlibrary(sf)\nlibrary(patchwork)\n\nfont_add_google(\"Roboto Mono\", \"Roboto Mono\")\nshowtext_auto()\n\n# Import districts shapefile\ndstr &lt;- tidypod::districts\ndstr &lt;- filter(dstr, code_muni == 36)\n\n# Import Mapa Desigualdade Data\n# https://www.nossasaopaulo.org.br/campanhas/#13\n\nurl &lt;- \"https://www.nossasaopaulo.org.br/wp-content/uploads/2023/11/mapa_da_desigualdade_2023_dados.xlsx\"\ntf &lt;- tempfile(fileext = \"xlsx\")\ndownload.file(url, tf, quiet = TRUE)\n\nsp &lt;- read_excel(\n  tf,\n  sheet = 4,\n  .name_repair = janitor::make_clean_names\n)\n\nsp &lt;- sp |&gt; \n  rename(name_district = distritos) |&gt; \n  mutate(name_district = if_else(name_district == \"Moóca\", \"Mooca\", name_district))\n\n\nmapasp &lt;- dstr |&gt; \n  left_join(sp, by = \"name_district\")\n\npmap &lt;- ggplot(mapasp, aes(fill = idade_media_ao_morrer)) +\n  geom_sf() +\n  scale_fill_fermenter(\n    name = \"\",\n    palette = \"RdBu\",\n    breaks = seq(55, 85, 5),\n    direction = 1) +\n  labs(\n    title = \"Expectativa de Vida\",\n    subtitle = \"Idade média ao morrer por distrito em São Paulo\",\n    caption = \"Fonte: Nossa São Paulo, Mapa da Desigualdade (2023)\"\n    ) +\n  ggthemes::theme_map(base_family = \"Roboto Mono\") +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.width = unit(1.25, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 28, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n  )\n\ncount_group &lt;- mapasp |&gt; \n  st_drop_geometry() |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = findInterval(idade_media_ao_morrer, seq(55, 85, 5))\n  ) |&gt; \n  summarise(\n    count = n(),\n    min = min(idade_media_ao_morrer),\n    pop = sum(populacao_total),\n    .by = \"group\"\n  ) |&gt; \n  mutate(share = pop / sum(pop) * 100) |&gt; \n  arrange(group)\n\npcol &lt;- ggplot(count_group, aes(x = group, y = share, fill = as.factor(group))) +\n  geom_col() +\n  geom_text(\n    family = \"Roboto Mono\",\n    size = 5,\n    aes(x = group, y = share + 2, label = round(share, 1))\n    ) +\n  labs(title = stringr::str_wrap(\"Percentual da população dentro de cada grupo\", 25)) +\n  scale_fill_brewer(palette = \"RdBu\") +\n  guides(fill = 'none') +\n  theme_void(base_family = \"Roboto Mono\") +\n  theme(plot.title = element_text(size = 10, hjust = 0))\n\nmap_expectativa_de_vida &lt;- \n  pmap + inset_element(pcol, left = 0.55, bottom = 0.05, right = 1, top = 0.4)\n\n\n\nExpectativa de vida\nSão Paulo é uma cidada marcada por desigualdades. Talvez uma das mais surpreendentes seja a da expectativa de vida. Do Jardim Paulista até Anhanguera são 23 anos de diferença na idade média ao morrer. Enquanto no Jardim Paulista espera-se viver até 82 anos, em Anhanguera a expectativa de vida não chega aos 60 anos.\n\n\n\n\n\n\nDados: Nossa São Paulo (Mapa da Desigualdade, 2023)\nPaleta: RdBu (ColorBrewer)\nTipografia: Roboto Mono"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-sp-college/index.html",
    "href": "posts/general-posts/2023-11-wz-sp-college/index.html",
    "title": "Ensino Superior em São Paulo",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(tidypod)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(sf)\nlibrary(stringr)\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nshowtext_opts(dpi = 72)\nshowtext_auto()\n\n# Import POD data\npod &lt;- tidypod::import_pod_tables(geo = TRUE)\npod &lt;- filter(pod, code_muni == 36)\n\n\n# Ensino Superior ---------------------------------------------------------\n\njenks &lt;- BAMMtools::getJenksBreaks(pod$share_educ_superior, 7)\n\nmap_ensino_superior &lt;- ggplot(pod) +\n  geom_sf(aes(fill = share_educ_superior), lwd = 0.1, color = \"white\") +\n  scale_fill_fermenter(\n    name = \"\",\n    palette = \"BrBG\",\n    direction = 1,\n    breaks = jenks,\n    labels = round(jenks, 1)) +\n  coord_sf(ylim = c(-24, -23.38), xlim = c(-46.81, -46.38)) +\n  ggtitle(\"Percentual com Ensino Superior (%)\") +\n  labs(\n    subtitle = \"Percentual da população adulta com diploma de ensino superior por Zona OD.\",\n    caption = \"Fonte: Pesquisa Origem e Destino (2017). @viniciusoike.\") +\n  ggthemes::theme_map(base_family = \"Playfair Display\") +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.75, \"cm\"),\n    legend.text = element_text(size = 16),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 28, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n  )\n\n\n\nEnsino Superior\nO acesso ao ensino superior no Brasil historicamente apresenta grandes desigualdades. Na cidade de São Paulo não é diferente. No mapa abaixo, apresenta-se o percentual da população adulta com ensino superior por Zona OD. Vê-se nos dados o conhecido padrão da cidade: indicadores de ensino superior altos dentro do Centro Expandido da cidade, que vão decaindo gradativamente à medida que se aproxima da periferia.\n\n\n\n\n\n\nDados: Metrô (Pesquisa Origem e Destino, 2017)\nTipografia: Playfair Display\nPaleta: BrBG (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2023-11-wz-sp-carros/index.html",
    "href": "posts/general-posts/2023-11-wz-sp-carros/index.html",
    "title": "Carros em São Paulo",
    "section": "",
    "text": "Posse de carros em São Paulo\nA posse de automóveis em São Paulo, grosso modo, é positivamente correlacionada com a renda familiar. O mapa abaixo mostra as zonas da cidade com maiores e menores taxas de automóveis (número de carros por domicílio). Bairros centrais como Pacaembu, Alto de Pinheiros, Morumbi e Jardim Europa têm as taxas mais elevadas. Bairros periféricos como Morro do Índio, Cocaia e Bororé têm as taxas mais baixas. Vale notar que o Centro Antigo da cidade (República, Santa Ifigênia, etc.) também têm bairros com taxas bastante baixas. Na maior parte da cidade, as Zonas têm entre 0,4 e 0,8 carros por domicílio.\n\n\n\n\n\n\nDados: Metrô (Pesquisa Origem e Destino, 2017)\nTipografia: Helvetica Neue\nPaleta: PuBuGn (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-sp-renda/index.html",
    "href": "posts/general-posts/2024-02-wz-sp-renda/index.html",
    "title": "Distribuição de Renda em São Paulo",
    "section": "",
    "text": "Renda Familiar em São Paulo\nO mapa abaixo mostra o rendimento domiciliar em grupos de decis em São Paulo. Divide-se toda a população, do mais pobre ao mais rico, em dez grupos de igual tamanho; assim, os hexágonos em azul estão no top 10% da distribuição de renda (10% mais ricos); já os hexágonos em vermelho estão no bottom 10% da distribuição (10% mais pobres).\nO grid hexagonal segue o padrão H3, em resolução 9. Os dados de renda provém do Censo Demográfico do IBGE a nível setor censitário. Usa-se uma técnica de interpolação espacial para converter os dados para o padrão hexagonal.\nApesar dos dados brutos do Censo estarem defasados, é improvável que a sua distribuição espacial tenha se alterado significativamente na última década. Grosso modo, as maiores rendas se concentram no Centro Expandido e no Quadrante Sudoeste da cidade. As duas exceções são a região de Santana-Tucuruvi e o eixo Tatuapé - Jardim Anália Franco.\n\n\n\n\n\n\nDados: IPEA (Acesso a Oportunidades) acessado via aopdata\nTipografia: Lato\nPaleta: Spectral (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-sp-idh-atlas/index.html",
    "href": "posts/general-posts/2024-02-wz-sp-idh-atlas/index.html",
    "title": "IDH por região em São Paulo",
    "section": "",
    "text": "IDH em São Paulo\nO mapa abaixo mostra o Índice de Desenvolvimento Humano (IDH) das regiões de São Paulo. As estimativas provêm do projeto Atlas Brasil, que computa o IDH a nível submunicipal nas chamadas Unidades de Desenvolvimento Humano (UDHs). Como se vê no desenho do mapa, as UDHs tem formatos bastante flexíveis. De forma geral, as UDHs seguem divisões administrativas pré-existentes mas também ressaltam áreas de vulnerabilidade social que estão imersas dentro de outras regiões.\nO gráfico de colunas mostra o percentual da população que vive em cada grupo de IDH. Este gráfico ajuda a reduzir a distorção visual causada pelo mapa. Visualmente, tende-se a associar o tamanho das áreas com a sua representatividade. Isto acontece tanto no extremo sul da cidade, que é esparsamente povoado, como também no Centro Expandido. De maneira geral, a maior parte da população vive na faixa entre 0,650 a 0,800 um IDH médio. Cerca de um terço da população vive em regiões com IDH acima de 0,800, um IDH alto ou altíssimo.\n\n\n\n\n\n\nDados: Atlas Brasil\nTipografia: Raleway\nPaleta: YlGnBu (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-filter/index.html",
    "href": "posts/general-posts/2024-01-tidyverse-filter/index.html",
    "title": "O novo tidyverse: filter",
    "section": "",
    "text": "O tidyverse é uma coleção poderosa de pacotes, voltados para a manipulação e limpeza de dados. Num outro post, discuti alguns aspectos gerais da filosofia destes pacotes que incluem a sua consistência sintática e o uso de pipes. A filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes” que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático.\nO tidyverse está em constante expansão, novas funcionalidades são criadas para melhorar a performance e capabilidade de suas funções. Assim, é importante atualizar nosso conhecimento destes pacotes periodicamente. Nesta série de posts vou focar nas funções principais dos pacotes dplyr e tidyr, voltados para a limpeza de dados."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-filter/index.html#o-básico",
    "href": "posts/general-posts/2024-01-tidyverse-filter/index.html#o-básico",
    "title": "O novo tidyverse: filter",
    "section": "O básico",
    "text": "O básico\nOs pacotes utilizados neste tutorial são listados abaixo.\n\nlibrary(dplyr)\nlibrary(readr)\n\nPara praticar as funções vamos utilizar uma tabela que traz informações sobre as cidades do Brasil.\n\ndat &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/cities_brazil.csv\"\n  )\n\n# dat &lt;- select(dat, 1:7, population, population_growth, pib)\n\nA função filter é talvez uma das que menos mudou ao longo do desenvolvimento do pacote dplyr. A função serve para filtrar as linhas de um data.frame segundo alguma condição lógica.\n\nfiltered_dat &lt;- filter(dat, population_growth &lt; 0)\n\nnrow(filtered_dat)\n\n[1] 2399\n\n\nOs principais operadores lógicos no R:\n\n“Maior que”, “Menor que”: &gt;, &lt;, &gt;=, &lt;=\nE/ou: &, |\n“Negação”: !\n“Igual a”: ==\n“Dentro de”: %in%\n\nAs funções is_* também são bastante importantes; em particular a função is.na() é útil para encontrar ou remover observações ausentes.\nO exemplo abaixo mostra como filtrar linhas baseado num string. Note que quando se usa múltiplos strings é preciso usar o %in%.\n\nfilter(dat, name_muni == \"São Paulo\")\nfilter(dat, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\"))\n\ncities &lt;- c(\"São Paulo\", \"Rio de Janeiro\")\nfilter(dat, name_muni %in% cities)\n\n\nfilter(dat, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\")) |&gt; \n  print_table()\n\nPara negar a igualdade, basta usar o operador !. No caso do operador %in% há duas maneiras válidas de negá-lo: pode-se colocar o ! no começo da expressão ou colocar a expressão inteira dentro de um parêntesis. Eu tendo a preferir a segunda sintaxe.\n\n#&gt; Remove todas as cidades da região Sudeste\nfilter(dat, name_region != \"Sudeste\")\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(dat, !name_region %in% c(\"Sudeste\", \"Norte\"))\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(dat, !(name_region %in% c(\"Sudeste\", \"Norte\")))\n\nEm geral, pode-se omitir o operador E (&), já que se pode concatenar várias condições lógicas dentro uma mesma chamada para a função filter, separando as condições por vírgulas. Esta sintaxe costuma ser preferida pois ela é mais eficiente do que chamar a função a função filter múltiplas vezes. Além disso, a escrita do código fica mais limpa, pois é fácil separar as condições em linhas distintas. As três versões do código abaixo geram o mesmo resultado.\n\n# Mais eficiente e mais fácil de ler\nd1 &lt;- dat |&gt; \n  filter(\n    name_region == \"Nordeste\",\n    !(name_state %in% c(\"Pernambuco\", \"Piauí\")),\n    !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n  )\n# Igualmente eficiente, leitura fica um pouco pior\nd2 &lt;- dat |&gt; \n  filter(\n    name_region == \"Nordeste\" & \n      !(name_state %in% c(\"Pernambuco\", \"Piauí\")) & \n      !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n    )\n\n# Menos eficiente\nd3 &lt;- dat |&gt; \n  filter(name_region == \"Nordeste\") |&gt; \n  filter(!(name_state %in% c(\"Pernambuco\", \"Piauí\"))) |&gt; \n  filter(!(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\")))\n\nall.equal(d1, d2)\nall.equal(d2, d3)\nall.equal(d3, d1)\n\nRelações de grandeza funcionam naturalmente com números. A tabela abaixo mostra todos os municípios com mais do que um milhão de habitantes.\n\nfilter(dat, population &gt; 1e6) \n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npopulation\npopulation_density\n\n\n\n\nSão Paulo\nSP\n11.451.245\n7.528\n\n\nRio de Janeiro\nRJ\n6.211.423\n5.175\n\n\nBrasília\nDF\n2.817.068\n489\n\n\nFortaleza\nCE\n2.428.678\n7.775\n\n\nSalvador\nBA\n2.418.005\n3.487\n\n\nBelo Horizonte\nMG\n2.315.560\n6.988\n\n\nManaus\nAM\n2.063.547\n181\n\n\nCuritiba\nPR\n1.773.733\n4.079\n\n\nRecife\nPE\n1.488.920\n6.804\n\n\nGoiânia\nGO\n1.437.237\n1.971\n\n\nPorto Alegre\nRS\n1.332.570\n2.690\n\n\nBelém\nPA\n1.303.389\n1.230\n\n\nGuarulhos\nSP\n1.291.784\n4.054\n\n\nCampinas\nSP\n1.138.309\n1.433\n\n\nSão Luís\nMA\n1.037.775\n1.780\n\n\n\n\n\n\n\n\nTambém pode-se usar alguma função que retorne um valor numérico. Nos exemplos abaixo filtra-se apenas os municípios com PIB acima da média e os municípios no top 1% da distribuição do PIB.\n\nfilter(dat, pib &gt; mean(pib))\nfilter(dat, pib &gt; quantile(pib, probs = 0.99))\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npib\npib_share_uf\n\n\n\n\nSão Paulo\nSP\n748.759.007\n31\n\n\nRio de Janeiro\nRJ\n331.279.902\n44\n\n\nBrasília\nDF\n265.847.334\n100\n\n\nBelo Horizonte\nMG\n97.509.893\n14\n\n\nManaus\nAM\n91.768.773\n79\n\n\nCuritiba\nPR\n88.308.728\n18\n\n\nOsasco\nSP\n76.311.814\n3\n\n\nPorto Alegre\nRS\n76.074.563\n16\n\n\nGuarulhos\nSP\n65.849.311\n3\n\n\nCampinas\nSP\n65.419.717\n3\n\n\nFortaleza\nCE\n65.160.893\n39\n\n\nSalvador\nBA\n58.938.115\n19\n\n\nGoiânia\nGO\n51.961.311\n23\n\n\nBarueri\nSP\n51.254.572\n2\n\n\nJundiaí\nSP\n51.235.050\n2\n\n\nRecife\nPE\n50.311.002\n26\n\n\nSão Bernardo do Campo\nSP\n48.614.342\n2\n\n\nDuque de Caxias\nRJ\n47.153.673\n6\n\n\nNiterói\nRJ\n40.949.495\n5\n\n\nSão José dos Campos\nSP\n39.148.012\n2\n\n\nPaulínia\nSP\n38.572.766\n2\n\n\nParauapebas\nPA\n38.014.863\n18\n\n\nUberlândia\nMG\n37.631.537\n6\n\n\nSorocaba\nSP\n36.723.769\n2\n\n\nJoinville\nSC\n36.391.912\n10\n\n\nMaricá\nRJ\n35.618.327\n5\n\n\nRibeirão Preto\nSP\n35.218.869\n1\n\n\nItajaí\nSC\n33.084.145\n9\n\n\nSão Luís\nMA\n33.074.010\n31\n\n\nBelém\nPA\n30.835.763\n14\n\n\nCampo Grande\nMS\n30.121.789\n25\n\n\nContagem\nMG\n29.558.094\n4\n\n\nSanto André\nSP\n29.440.477\n1\n\n\nPiracicaba\nSP\n27.172.817\n1\n\n\nCuiabá\nMT\n26.528.839\n15\n\n\nBetim\nMG\n26.185.005\n4\n\n\nCaxias do Sul\nRS\n25.965.161\n6\n\n\nCamaçari\nBA\n25.697.266\n8\n\n\nVitória\nES\n25.473.898\n18\n\n\nSerra\nES\n25.079.657\n18\n\n\nCampos dos Goytacazes\nRJ\n23.841.837\n3\n\n\nMaceió\nAL\n22.872.756\n36\n\n\nNatal\nRN\n22.729.773\n32\n\n\nCanaã dos Carajás\nPA\n22.522.725\n10\n\n\nSantos\nSP\n22.073.535\n1\n\n\nSão José dos Pinhais\nPR\n21.975.612\n4\n\n\nLondrina\nPR\n21.729.852\n4\n\n\nTeresina\nPI\n21.578.875\n38\n\n\nFlorianópolis\nSC\n21.312.447\n6\n\n\nCajamar\nSP\n20.798.646\n1\n\n\nJoão Pessoa\nPB\n20.766.551\n30\n\n\nMaringá\nPR\n20.005.630\n4\n\n\nAraucária\nPR\n19.724.416\n4\n\n\nPorto Velho\nRO\n19.448.762\n38\n\n\nSão Gonçalo\nRJ\n19.002.883\n3\n\n\nSão José do Rio Preto\nSP\n18.694.213\n1"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-filter/index.html#grupos",
    "href": "posts/general-posts/2024-01-tidyverse-filter/index.html#grupos",
    "title": "O novo tidyverse: filter",
    "section": "Grupos",
    "text": "Grupos\nA função de filtro segue uma regra lógica que é aplicada sobre a tabela como um todo. É possível filtrar dentro de grupos usando o argumento .by = \"nome_do_grupo\".\nNo código abaixo, novamente filtra-se os municípios com PIB acima da média. No segundo exemplo, contudo, este filtro é aplicado dentro de cada região, segundo a coluna/grupo name_region. A regra lógica pib &gt; mean(pib) é aplicada dentro de cada região, isto é, filtra-se todos os municípios que têm PIB superior à média do PIB da sua região.\n\ndat |&gt; filter(pib &gt; mean(pib)) \ndat |&gt; filter(pib &gt; mean(pib), .by = \"name_region\")\n\nVale notar que a a sintaxe .by = \"grupo\" ainda está em fase experimental. Ela oferece um substituto mais sucinto à antiga sintaxe que usava a função group_by() com a vantagem de sempre aplicar a função ungroup() ao final do processo, isto é, o resultado final da função acima será uma tabela sem grupos. O código acima é equivalente ao código abaixo.\n\ndat |&gt; \n  group_by(name_region) |&gt; \n  filter(pib &gt; mean(pib)) |&gt; \n  ungroup()\n\nEste outro exemplo enfatiza como o resultado da função filter muda quando é aplicada em diferentes grupos.\n\ndat |&gt; filter(pib == max(pib))\ndat |&gt; filter(pib == max(pib), .by = \"name_state\")\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npib\npib_share_uf\n\n\n\n\nPorto Velho\nRO\n19.448.762\n38\n\n\nRio Branco\nAC\n9.579.592\n58\n\n\nManaus\nAM\n91.768.773\n79\n\n\nBoa Vista\nRR\n11.826.207\n74\n\n\nParauapebas\nPA\n38.014.863\n18\n\n\nMacapá\nAP\n11.735.557\n64\n\n\nPalmas\nTO\n9.940.091\n23\n\n\nSão Luís\nMA\n33.074.010\n31\n\n\nTeresina\nPI\n21.578.875\n38\n\n\nFortaleza\nCE\n65.160.893\n39\n\n\nNatal\nRN\n22.729.773\n32\n\n\nJoão Pessoa\nPB\n20.766.551\n30\n\n\nRecife\nPE\n50.311.002\n26\n\n\nMaceió\nAL\n22.872.756\n36\n\n\nAracaju\nSE\n16.447.105\n36\n\n\nSalvador\nBA\n58.938.115\n19\n\n\nBelo Horizonte\nMG\n97.509.893\n14\n\n\nVitória\nES\n25.473.898\n18\n\n\nRio de Janeiro\nRJ\n331.279.902\n44\n\n\nSão Paulo\nSP\n748.759.007\n31\n\n\nCuritiba\nPR\n88.308.728\n18\n\n\nJoinville\nSC\n36.391.912\n10\n\n\nPorto Alegre\nRS\n76.074.563\n16\n\n\nCampo Grande\nMS\n30.121.789\n25\n\n\nCuiabá\nMT\n26.528.839\n15\n\n\nGoiânia\nGO\n51.961.311\n23\n\n\nBrasília\nDF\n265.847.334\n100"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-filter/index.html#if_any-e-if_all",
    "href": "posts/general-posts/2024-01-tidyverse-filter/index.html#if_any-e-if_all",
    "title": "O novo tidyverse: filter",
    "section": "if_any e if_all",
    "text": "if_any e if_all\nA função filter não funciona em conjunção com a função across(). Esta função foi desenvolvida para funcionar apenas com mutate e summarise e aplica uma mesma regra/função sobre múltiplas colunas.\nJá a função filter recebeu duas funções auxiliares: if_any e if_all. Elas seguem o mesmo padrão das funções base any e all. Estas funções servem para agregar condições lógicas. A função any, por exemplo, testa múltiplas condições lógicas e retorna um único TRUE se houver ao menos um TRUE entre as condições lógicas. Já a função all retorna um único TRUE se absolutamente todas as condições lógicas testadas também retornaram TRUE.\nA função if_any aplica uma mesma regra em múltiplas colunas e retorna todas as linhas que atendem esta regra. No exemplo abaixo\n\ndat |&gt; filter(if_any(starts_with(\"pib\"), ~ . &gt; 100000))\n\nO exemplo seguinte é mais interessante. Neste caso, todas as variáveis numéricas da tabela são normalizadas (por região) e retorna-se apenas os municípios onde o valor de cada coluna é superior a 1. Como as variáveis estão normalizadas isto é equivalente a retornar os municípios que estão 1 desvio-padrão acima da média da sua região em todos os atributos numéricos considerados.\n\ndat |&gt; \n  select(-contains(\"code\")) |&gt; \n  select(where(~all(.x &gt; 0))) |&gt; \n  mutate(across(where(is.numeric), ~as.numeric(scale(log(.x)))), .by = \"name_region\") |&gt; \n  filter(if_all(everything(), ~ . &gt; 1))\n\n# A tibble: 2 × 15\n  name_muni            name_state abbrev_state name_region population city_area\n  &lt;chr&gt;                &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 Paranaguá            Paraná     PR           Sul               2.32      1.13\n2 São José dos Pinhais Paraná     PR           Sul               3.00      1.28\n  population_density households dwellers_per_household   pib pib_taxes\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1               1.52       2.24                   2.25  2.80      2.81\n2               2.10       2.97                   1.04  3.27      3.31\n  pib_added_value pib_industrial pib_services pib_govmt_services\n            &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n1            2.76           2.36         2.71               2.57\n2            3.18           2.76         2.92               3.17\n\n\nO último exemplo é similar ao anterior. As variáveis numéricas novamente são normalizadas mas desta vez busca-se somente os municípios que estão 3 desvios-padrão, acima da média do seu estado, ou na população ou no PIB.\n\ndat |&gt; \n  select(-contains(\"code\")) |&gt; \n  select(where(~all(.x &gt; 0))) |&gt; \n  mutate(across(where(is.numeric), ~as.numeric(scale(log(.x)))), .by = \"name_state\") |&gt; \n  filter(if_any(c(population, pib), ~ . &gt; 3))\n\n\n\n\n\n\n\n\nname_muni\nabbrev_state\npopulation\npib\npopulation_density\npib_services\ncity_area\n\n\n\n\nPorto Velho\nRO\n3,249\n3,508\n1,129\n2,954\n2,622\n\n\nRio Branco\nAC\n3,254\n3,337\n2,525\n3,112\n0,628\n\n\nManaus\nAM\n5,214\n5,439\n3,420\n5,136\n-0,239\n\n\nBoa Vista\nRR\n3,299\n3,310\n3,059\n3,172\n-0,709\n\n\nBelém\nPA\n4,041\n3,410\n3,122\n3,670\n-0,672\n\n\nCanaã dos Carajás\nPA\n0,950\n3,151\n0,511\n2,260\n0,069\n\n\nParauapebas\nPA\n2,305\n3,582\n0,816\n2,795\n0,602\n\n\nAraguaína\nTO\n3,837\n3,459\n2,184\n3,492\n1,201\n\n\nGurupi\nTO\n3,062\n2,857\n2,255\n3,018\n0,381\n\n\nPalmas\nTO\n4,467\n4,166\n3,269\n4,073\n0,580\n\n\nBalsas\nMA\n2,045\n3,190\n-0,898\n3,117\n2,899\n\n\nImperatriz\nMA\n3,236\n3,599\n2,353\n3,654\n0,373\n\n\nSão José de Ribamar\nMA\n3,103\n2,412\n4,270\n2,681\n-1,894\n\n\nSão Luís\nMA\n4,845\n5,096\n4,541\n4,903\n-0,581\n\n\nParnaíba\nPI\n3,676\n3,413\n3,580\n3,577\n-0,564\n\n\nPicos\nPI\n2,881\n3,018\n2,643\n3,340\n-0,266\n\n\nTeresina\nPI\n5,667\n5,526\n4,092\n5,417\n0,676\n\n\nUruçuí\nPI\n1,464\n3,101\n-1,185\n2,756\n2,604\n\n\nCaucaia\nCE\n3,041\n2,955\n1,882\n2,814\n0,878\n\n\nFortaleza\nCE\n5,212\n4,963\n5,137\n4,922\n-0,641\n\n\nMaracanaú\nCE\n2,569\n3,239\n3,900\n3,146\n-1,851\n\n\nParnamirim\nRN\n3,428\n3,265\n4,138\n3,371\n-0,674\n\n\nMossoró\nRN\n3,475\n3,434\n1,213\n3,512\n2,658\n\n\nNatal\nRN\n4,538\n4,421\n4,968\n4,488\n-0,323\n\n\nCabedelo\nPB\n2,206\n3,090\n3,771\n3,125\n-2,247\n\n\nCampina Grande\nPB\n4,163\n4,287\n2,668\n4,093\n1,407\n\n\nJoão Pessoa\nPB\n4,893\n4,951\n4,328\n4,721\n0,137\n\n\nSanta Rita\nPB\n3,070\n3,029\n1,490\n2,768\n1,645\n\n\nIpojuca\nPE\n1,478\n3,168\n0,696\n2,718\n0,530\n\n\nJaboatão dos Guararapes\nPE\n3,469\n3,148\n2,836\n3,085\n-0,142\n\n\nRecife\nPE\n4,360\n4,261\n3,673\n4,202\n-0,303\n\n\nArapiraca\nAL\n3,021\n2,913\n3,084\n3,187\n0,645\n\n\nMaceió\nAL\n4,588\n4,318\n4,478\n4,537\n1,190\n\n\nAracaju\nSE\n3,638\n3,676\n4,258\n3,886\n-0,094\n\n\nCamaçari\nBA\n3,357\n4,191\n2,547\n3,774\n0,001\n\n\nFeira de Santana\nBA\n4,229\n3,722\n2,744\n3,800\n0,506\n\n\nJuazeiro\nBA\n3,068\n2,623\n0,357\n2,778\n2,135\n\n\nLuís Eduardo Magalhães\nBA\n2,123\n3,040\n0,108\n3,046\n1,628\n\n\nSalvador\nBA\n5,882\n4,927\n4,579\n5,007\n-0,122\n\n\nSão Francisco do Conde\nBA\n0,853\n3,510\n1,627\n2,947\n-1,059\n\n\nVitória da Conquista\nBA\n3,615\n3,054\n1,439\n3,230\n1,414\n\n\nBelo Horizonte\nMG\n5,103\n4,613\n5,077\n4,620\n-0,186\n\n\nBetim\nMG\n3,489\n3,638\n3,490\n3,365\n-0,148\n\n\nContagem\nMG\n3,874\n3,728\n4,373\n3,712\n-0,715\n\n\nExtrema\nMG\n1,581\n3,027\n1,961\n3,055\n-0,487\n\n\nGovernador Valadares\nMG\n3,049\n2,630\n1,342\n2,768\n1,768\n\n\nIpatinga\nMG\n2,935\n3,004\n3,619\n2,862\n-0,882\n\n\nJuiz de Fora\nMG\n3,744\n3,312\n2,450\n3,412\n1,280\n\n\nMontes Claros\nMG\n3,495\n2,900\n1,386\n2,955\n2,195\n\n\nNova Lima\nMG\n2,270\n3,072\n2,118\n2,792\n0,073\n\n\nRibeirão das Neves\nMG\n3,282\n2,320\n4,007\n2,358\n-0,944\n\n\nUberaba\nMG\n3,304\n3,326\n0,995\n3,208\n2,426\n\n\nUberlândia\nMG\n4,003\n3,907\n1,752\n3,795\n2,332\n\n\nSerra\nES\n3,040\n2,923\n2,726\n2,891\n0,284\n\n\nRio de Janeiro\nRJ\n3,591\n3,414\n2,301\n3,286\n1,393\n\n\nGuarulhos\nSP\n3,055\n2,881\n2,875\n2,802\n0,155\n\n\nSão Paulo\nSP\n4,578\n4,338\n3,294\n4,267\n2,022\n\n\nAraucária\nPR\n2,360\n3,294\n2,508\n2,752\n0,310\n\n\nCascavel\nPR\n3,108\n3,016\n1,810\n2,989\n2,310\n\n\nCuritiba\nPR\n4,576\n4,554\n5,174\n4,384\n0,210\n\n\nFoz do Iguaçu\nPR\n2,930\n3,212\n2,899\n2,666\n0,660\n\n\nLondrina\nPR\n3,531\n3,376\n2,550\n3,365\n1,995\n\n\nMaringá\nPR\n3,255\n3,306\n3,514\n3,330\n0,361\n\n\nPonta Grossa\nPR\n3,135\n3,183\n1,859\n2,982\n2,286\n\n\nSão José dos Pinhais\nPR\n3,058\n3,385\n2,585\n3,081\n1,249\n\n\nFlorianópolis\nSC\n3,322\n3,046\n2,472\n3,052\n1,339\n\n\nItajaí\nSC\n2,731\n3,372\n2,587\n3,097\n0,268\n\n\nJoinville\nSC\n3,436\n3,443\n2,155\n3,134\n1,988\n\n\nCanoas\nRS\n3,136\n3,231\n3,730\n2,942\n-0,689\n\n\nCaxias do Sul\nRS\n3,370\n3,492\n1,901\n3,237\n1,694\n\n\nPelotas\nRS\n3,083\n2,722\n1,637\n2,718\n1,669\n\n\nPorto Alegre\nRS\n4,230\n4,316\n3,739\n4,169\n0,561\n\n\nCampo Grande\nMS\n4,131\n3,470\n2,785\n3,555\n1,112\n\n\nCuiabá\nMT\n3,653\n2,967\n3,573\n3,093\n0,040\n\n\nAnápolis\nGO\n3,070\n3,060\n2,746\n3,038\n0,151\n\n\nAparecida de Goiânia\nGO\n3,297\n3,039\n3,874\n3,094\n-0,966\n\n\nGoiânia\nGO\n4,110\n3,976\n3,908\n4,062\n-0,081"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "",
    "text": "Neste post, vou mapear a acessibilidade a hospitais e leitos em São Paulo. Para avaliar quantitativamente o nível de acessibilidade vou montar uma métrica bastante simples: o tempo mínimo necessário que se leva para chegar no hospital/leito mais próximo, considerando um deslocamento de bicicleta1.\nPara tornar o problema tratável, divido a cidade em hexágonos, no padrão H3, em resolução 9. Esta resolução tem um tamanho aproximado de 1km2 e estratifica a cidade em cerca de 15 mil subáreas. Seria possível reduzir o número de hexágonos cruzando este grid com dados de população do Censo. Contudo, como os dados do Censo a nível de setor censitário estão consideravelmente defasados, utilizo o grid inteiro.\nPara cada hospital/leito, contruo isócronas de 10, 15, 20, 25 e 30 minutos, considerando um deslocamento de bicicleta. A partir desta métrica simples, pode-se construir refinamentos como ponderar o número de leitos acessíveis em relação a população que reside em cada região. Evidentemente, pode-se considerar também outros modos de transporte e intervalos de tempo."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#dados",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#dados",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Dados",
    "text": "Dados\nAs bases de dados são de livre acesso no site dados.gov.br. Especificamente, vamos usar a base de Cadastros de CNES e de Hospitais e Leitos. Normalmente, eu importaria estes dados usando webscrapping, mas neste caso acho mais simples baixar os arquivos manualmente. Como são apenas duas tabelas que precisam ser baixadas, acho que seria um exagero fazer um rotina para importar os dados via scraping. Além disso, olhando o código-fonte da página, o download é feito via uma URL dinâmica, o que exigiria o uso do RSublime, ou, alternativamente, o uso do BeautifulSoup em Python2.\n\ncnes = fread(\"cnes_estabelecimentos.csv\")\nleitos = fread(\"Leitos_2024.csv\", encoding = \"Latin-1\")"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#limpeza",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#limpeza",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Limpeza",
    "text": "Limpeza\nOs códigos abaixo mostram a rotina de limpeza necessária. Como os dados brutos são relativamente bem estruturados não é necessário muito esforço neste passo. O primeiro código abaixo seleciona apenas as colunas necessárias da base de Cadastro de CNES para São Paulo.\n\n# Simplifica e limpa o nome das colunas\ncnes = janitor::clean_names(cnes)\n# Vetor para renomear e selecionar as colunas\nsel_cols &lt;- c(\"code_cnes\", \"lng\", \"lat\")\n# Altera o nome das colunas\nsetnames(cnes, c(\"co_cnes\", \"nu_longitude\", \"nu_latitude\"), sel_cols)\n# Seleciona as colunas apenas para Sao Paulo\ncnes &lt;- cnes[co_ibge == 355030, ..sel_cols]\n\nO segundo código abaixo mostra os passos para limpar a base de hospitais e leitos. Por fim, as duas bases são combinadas via o identificador code_cnes.\n\n# Vetor para renomear as colunas\n\n# Remove o prefixo do nome das colunas\nnew_names &lt;- str_remove_all(names(leitos), \"^[A-Z]{2}_\")\n# Cria novos nomes paras as colunas\nnew_names &lt;- janitor::make_clean_names(new_names)\nsetnames(leitos, names(leitos), new_names)\nsetnames(leitos, c(\"cnes\", \"endereco\"), c(\"code_cnes\", \"numero\"))\n\n# Vetor para selecionar colunas\nsel_cols &lt;- c(\n  \"code_cnes\", \"nome_estabelecimento\", \"logradouro\", \"numero\", \"bairro\", \"cep\",\n  \"leitos_existentes\"\n  )\n\nleitos &lt;- leitos[uf == \"SP\" & municipio == \"SAO PAULO\", ..sel_cols]\n\ndat &lt;- merge(leitos, cnes, by = \"code_cnes\")\n\n\nGeocoding\nHá poucos casos que não foram identificados no cruzamento dos dados. Especificamente, apenas três hospitais não foram identificados. Note que a função unique remove as linhas duplicadas pois a base de leitos não unicamente identificada por estabelecimento.\n\nmissing_leitos &lt;- dat[is.na(lng) | is.na(lat)]\nmissing_leitos &lt;- unique(missing_leitos[, .(code_cnes, logradouro, numero, bairro)])\n\nprint(missing_leitos)\n\nKey: &lt;code_cnes&gt;\n   code_cnes                logradouro numero        bairro\n       &lt;int&gt;                    &lt;char&gt; &lt;char&gt;        &lt;char&gt;\n1:    955736 RUA GENERAL CHAGAS SANTOS    314 VILA DA SAUDE\n2:   2688603                  PAULISTA    200    BELA VISTA\n3:   7252455                 VERGUEIRO    235     LIBERDADE\n\n\nPara georeferenciar estas observações uso o tidygeocoder.\n\nmissing_leitos[,\n  endereco := glue::glue_data(.SD, \"{numero}, {logradouro}, {bairro}, São Paulo, SP\")\n  ]\n\ngeo_leitos &lt;- geocode(missing_leitos, address = endereco)\n\nPelo mapa abaixo, fica claro que o processo teve sucesso.\n\ntest = st_as_sf(geo_leitos, coords = c(\"long\", \"lat\"), crs = 4326)\nmapview(test)\n\n\n\n\n\n\nPor fim, o código abaixo remove as linhas da base original, que tinham lat/long ausente, e substitui elas pelas novas linhas encontradas acima. Algum cuidado é necessário já que a base original não é identificada por estabelecimento. Por fim, faço um teste simples para verificar que todas as observações tem lat/lng.\n\nsetnames(geo_leitos, \"long\", \"lng\")\nsetDT(geo_leitos)\ngeo_leitos &lt;- geo_leitos[, .(code_cnes, lng, lat)]\n\nfix_dat &lt;- dat[is.na(lng) | is.na(lat)]\nset(fix_dat, j = c(\"lng\", \"lat\"), value = NULL)\nfix_dat &lt;- merge(fix_dat, geo_leitos, all.x = TRUE, by = \"code_cnes\")\n\ndat &lt;- dat[!(is.na(lng) | is.na(lat))]\ndat &lt;- rbind(dat, fix_dat)\n\n# Verifica se há lat/lngs faltando\nnrow(dat[is.na(lng) | is.na(lat)])\n\n[1] 0"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#isócronas",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#isócronas",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Isócronas",
    "text": "Isócronas\nIsócronas são polígonos que representam áreas de alcance dentro de um período de tempo pré-determinado. Isto é, para um determinado ponto no espaço \\(s_{i}\\) a isócrona \\(I(s_{i}, m, t)\\) representa todos os pontos que se pode chegar, partindo de \\(s_{i}\\), usando o modo de transporte \\(m\\) em \\(t\\) minutos. Para este exemplo considero apenas deslocamentos de bicicleta em intervalos de 10, 15, 20, 25, e 30 minutos.\nO código abaixo exemplifica a construção de uma isócrona em torno de um hospital.\n\niso_test = osrmIsochrone(\n  dat[1, .(lng, lat)],\n  breaks = seq(from = 10, to = 30, by = 5),\n  osrm.profile = \"bike\"\n  )\n\nmapview(iso_test, zcol = \"isomax\")\n\n\n\n\n\n\nAo todo, temos 224 pontos que representam um estabelecimento único. Ao todo, calcula-se 1140 isócronas.\n\nlocations &lt;- unique(dat[, .(code_cnes, lng, lat)])\nnrow(locations)\n\n[1] 224\n\n\n\ngeo_locations &lt;- st_as_sf(\n  locations[, .(lng, lat)],\n  coords = c(\"lng\", \"lat\"),\n  crs = 4326\n  )\n\nmapview(geo_locations)\n\n\n\n\n\n\nO código abaixo importa as isócronas para todos os pontos. Note que este código leva um tempo considerável para rodar.\n\nget_isochrone = function(dat) {\n  \n  iso = osrm::osrmIsochrone(\n    dat[, .(lng, lat)],\n    breaks = seq(from = 10, to = 30, by = 5),\n    osrm.profile = \"bike\"\n  )\n  \n  return(iso)\n  \n}\n\nlocations &lt;- split(locations, by = \"code_cnes\")\n\ngeo_locations &lt;- lapply(locations, get_isochrone)"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#grid",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#grid",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Grid",
    "text": "Grid\nPara montar o grid, uso o pacote h3jsr que implementa as funções da biblioteca H3 da Uber dentro do R (via JavaScript). Importo o shapefile da cidade de São Paulo do IBGE via pacote geobr.\nVale notar que é possível conseguir um grid H3 em resolução 9 diretamente do pacote aopdata::read_grid(), que oferece os dados do projeto de Acesso a Oportunidades do IPEA. Este atalho, contudo, está disponível apenas para as cidades que entraram no estudo. Assim, o código abaixo é mais geral, pois funciona para qualquer cidade do Brasil.\n\nlibrary(h3jsr)\nlibrary(geobr)\n\nspo = read_municipality(3550308, simplified = FALSE, showProgress = FALSE)\ngrid = polygon_to_cells(spo, res = 9, simple = FALSE)\n\npolyfill = polygon_to_cells(spo, res = 9, simple = FALSE)\nindex_h3 = unlist(str_split(unlist(polyfill$h3_addresses), \", \"))\n\ngrid = data.frame(\n  id_hex = index_h3\n)\n\nh3grid = cell_to_polygon(grid)\nh3grid = st_as_sf(h3grid, crs = 4326)\nh3grid$id_hex = index_h3\n\nFazendo a interseção espacial entre o grid H3 e as isócronas, calcula-se para cada hexágono o tempo mínimo necessário para chegar no hospital/leito mais próximo.\n\nisocronas &lt;- dplyr::bind_rows(geo_locations, .id = \"code_cnes\")\n\n\nmatch_h3_destination = function(dest) {\n  \n  iso = isocronas |&gt; \n    dplyr::filter(code_cnes == dest)\n\n  hex_codes = lapply(1:5, \\(i) {\n    \n    idhex = dplyr::filter(iso, id == i) |&gt; \n      h3jsr::polygon_to_cells(res = 9) |&gt; \n      unlist()\n    \n    return(data.frame(id_hex = idhex))\n    \n  })\n  \n  names(hex_codes) = as.character(c(10, 15, 20, 25, 30))\n  \n  h3iso = rbindlist(hex_codes, idcol = \"isomax\")\n  \n}\n\ncnes_codes &lt;- unique(isocronas$code_cnes)\n\nod_table &lt;- parallel::mclapply(cnes_codes, match_h3_destination)\n\nAgora, para cada hexágono, encontra-se o leito/hospital mais próximo\n\n# Consolida tabela que faz o match das origens e destinos (hospitais)\nnames(od_table) &lt;- cnes_codes\nod &lt;- rbindlist(od_table, idcol = \"code_cnes\")\n# Encontra o hospital mais próximo para cada ponto\nod[, isomax := as.numeric(isomax)]\nmin_table &lt;- od[, .SD[which.min(isomax)], by = \"id_hex\"]\n\n# Grid H3\ndtgrid &lt;- setDT(st_drop_geometry(h3grid))\ntimetable &lt;- merge(dtgrid, min_table, by = \"id_hex\", all.x = TRUE)\n\n# Junta os dados com o grid\nacesso_saude &lt;- dplyr::left_join(h3grid, timetable, by = \"id_hex\")\n# Troca NAs por &gt; 30\nacesso_saude &lt;- acesso_saude |&gt; \n  dplyr::mutate(\n    isomax = ifelse(is.na(isomax), 30, isomax)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#resultado",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#resultado",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Resultado",
    "text": "Resultado\nO mapa abaixo mostra o resultado final. De maneira geral, a área central da cidade, o Centro Expandido, está relativamente bem atendido de hospitais numa distância de 10 a 15 minutos até o hospital mais próximo. As áreas periféricas da cidade tem indicadores bastante piores, contudo, vale notar que estas áreas potencialmente podem ser atendidas por hospitais em municípios vizinhos, que fazem conurbação com São Paulo.\nComo comentado inicialmente, esta métrica é bastante rudimentar. Uma melhoria interessante seria ponderar o número total de leitos disponíveis em relação a população atendida em cada região.\n\n\nCode\nggplot(acesso_saude) +\n  geom_sf(aes(fill = isomax, color = isomax)) +\n  scale_fill_distiller(\n    name = \"\",\n    palette = \"RdBu\",\n    labels = c(\"Até 10 min.\", \"15 min.\", \"20 min\", \"25 min\", \"30 min. ou mais\")) +\n  scale_color_distiller(\n    name = \"\",\n    palette = \"RdBu\",\n    labels = c(\"Até 10 min.\", \"15 min.\", \"20 min\", \"25 min\", \"30 min. ou mais\")) +\n  labs(\n    title = \"Acesso a Hospitais\",\n    subtitle = \"Tempo mínimo necessário para chegar ao hospital/leito mais próximo.\",\n    caption = \"Fonte: Open Source Routing Machine (osrm), Dados Abertos (Ministério da Saúde).\"\n    ) +\n  ggthemes::theme_map() +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.75, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 18, hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-acesso-saude/index.html#footnotes",
    "href": "posts/general-posts/2023-12-wz-acesso-saude/index.html#footnotes",
    "title": "Acesso a Hospitais e Leitos em São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSeria possível também fazer o deslocamento de carro, mas, na minha experiência, o deslocamento de bicicleta aproxima bem o deslocamento de carro com trânsito. As isócronas de deslocamento de carro em São Paulo - sem ajustes - costumam ser bastante “otimistas” com relação às possibilidades de deslocamento.↩︎\nCaso fosse necessário seria possível combinar a rotina de importação em Python com a rotina de análise em R usando reticulate.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "O tópico de housing affordability entrou em pauta nos últimos anos à medida que o preço dos imóveis cresceu rapidamente na última década em boa parte do mundo desenvolvido. Como mostrei num post anterior, este não é o caso do Brasil: os preços dos imóveis em termos reais “andaram de lado” nos últimos 13 anos. Em termos reais, os preços dos imóveis no final de 2023 estão um pouco abaixo do que estavam em 2010.\nDiversos fatores impactam o preço de imóveis no longo prazo. Num post anterior discuti o fator demográfico, e como o crescimento populacional é pouco correlacionado com os aumentos de preços observados nos últimos anos. O aumento de preços é mais relacionado com a manutenção prolongada de uma baixa taxa de juros na década de 2010 em conjunção com a baixa construção de novas unidades de moradia.\n\n\n\n\n\nMesmo com preços reais sob controle, a acessibilidade à moradia no Brasil é bastante ruim. A Fundação João Pinheiro reporta aumento do déficit habitacional, ano a ano. Em São Paulo, um levantamento recente registrou mais de 50 mil pessoas sem moradia. Em 2022, o Censo da População em Situação de Rua indicou que havia mais de 30 mil pessoas morando nas ruas da cidade.\nA acessibilidade à moradia é função de três fatores centrais:\n\nPreços dos imóveis.\nRenda das famílias.\nCondições de financiamento.\n\nO preço real dos imóveis diminuiu na última década. A renda real das famílias aumentou pouco, mas teve um crescimento na faixa de 20-25% em termos reais, segundo dados da PNADC de 2012 a 2023.\n\n\nO gráfico abaixo mostra a evolução do preço dos imóveis (IVG-R) e da renda média das famílias em comparação com a inflação (nível geral de preços) no Brasil. As três séries mostram a evolução nominal das variáveis e os valores estão indexados em 2010 para facilitar a comparação. Os valores de renda interpolam as séries de PNAD, PNADC e Censo para chegar numa estimativa suavizada da renda familiar habitual mensal.\nEm linhas gerais, o preço dos imóveis para de crescer em 2014-15 com a Crise Econômica e fica estagnado - em termos nominais - nos próximos anos. Isto é, a partir de 2015 o preço dos imóveis começa a cair em termos reais. A renda média, em contrapartida, tem uma tendência de crescimento estável e consistentemente consegue acompanhar a inflação no período observado. Assim, pode-se concluir - ainda que superficialmente - que o affordability deve ter melhorado, visto que os preços dos imóveis cresceram menos do que renda média.\n\n\n\n\n\n\n\n\nO cenário em São Paulo é similar. A série de preços mais extensa que se tem disponível é a do FipeZap, indicada em vermelho no gráfico abaixo. Novamente, as variáveis estão em termos nominais e indexadas em seus valores médios em 2010. A dinâmica de preços em São Paulo é muito similar à dinâmica nacional; a diferença é que os preços sobem mais durante o “boom” e caem menos durante o período de estagnação. A renda das famílias cresce acima da inflação média, indicando um ganho real de renda.\nO gráfico abaixo faz parte do artigo Acesso à moradia em São Paulo escrito por mim em conjunto com outros colegas, publicado na LARES 2021. Como se vê, no longo prazo, a renda conseguiu alcançar o preço dos imóveis, indicando um certo equilíbrio entre renda e preço.\n\n\n\n\n\n\n\n\nHá dois indicadores básicos, que mensuram a acessibilidade financeira à moradia: (1) o price-income ratio (PIR); e (2) o Housing Affordability Index (HAI). Abaixo apresento formalmente os dois indicadores.\n\n\nO PIR é uma razão simples entre o preço médio/mediano dos imóveis e a renda média/mediana anual das famílias:\n\\[\n\\text{PIR} = \\frac{\\text{Precos}}{\\text{Renda Anual}}\n\\]\nO PIR indica, grosso modo, a quantidade de “anos de trabalho” que uma família típica precisa investir de renda para comprar um imóvel típico. A principal vantagem do PIR é a sua simplicidade de cálculo, o que facilita comparações entre diferentes regiões. A principal desvantagem do PIR é de ignorar as condições de financiamento disponíveis para a população. Supondo que a renda anual média das famílias seja \\(R\\$30.000\\) e que o preço médio dos imóveis seja \\(R\\$270.000\\). Então o valor do PIR seria:\n\\[\n\\text{PIR} = \\frac{R\\$270.000}{R\\$30.000} = 9\n\\]\n\n\n\nO HAI é um indicador que possui diversas definições. Uma definição simples é a razão abaixo:\n\\[\n\\text{HAI} = \\frac{\\text{Parcela Máxima}}{\\text{Parcela Típica}}\\times100\n\\]\nonde a “parcela máxima de financiamento” é o valor teórico máximo que uma família típica estaria elegível num financiamento típico. Em outras palavras, suponha que a renda média familiar seja de \\(R\\$4.000\\). Considerando um financiamento a uma taxa de juros \\(r\\) num prazo de financiamento de \\(\\tau\\) períodos e um comprometimento máximo de renda \\(\\gamma\\) chega-se num valor \\(p_{max}\\) que representa o imóvel mais caro que o banco estaria disposto a financiar para esta família. Associado a este \\(p_{max}\\) existe um \\(\\text{pmt}_{max}\\) que é a parcela mais cara que a família poderia “aguentar”.\nComo regra de bolso, considera-se que a parcela de financiamento do imóvel não deve superar 30% da renda bruta familiar mensal. Assim, a parcela máxima para uma família que recebe \\(R\\$4.000\\) deveria ser de \\(R\\$1.200\\). Num típico financiamento SAC1, um imóvel de \\(R\\$270.000\\) teria uma parcela inicial próxima de \\(R\\$2.000\\). Assim, o valor do HAI seria:\n\\[\n\\text{HAI} = \\frac{1.200}{2.000}\\times100 = 60\n\\]\nO HAI mensura o valor da parcela de financiamento do imóvel típico em relação à renda mensal típica. Em outras palavras, este indicador verifica se o financiamento do imóvel típico de uma região “cabe no bolso” da família típica. Um valor próximo de 100 indica que a família com renda média consegue comprar o imóvel com preço médio."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#quantificando",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#quantificando",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "Há dois indicadores básicos, que mensuram a acessibilidade financeira à moradia: (1) o price-income ratio (PIR); e (2) o Housing Affordability Index (HAI). Abaixo apresento formalmente os dois indicadores.\n\n\nO PIR é uma razão simples entre o preço médio/mediano dos imóveis e a renda média/mediana anual das famílias:\n\\[\n\\text{PIR} = \\frac{\\text{Precos}}{\\text{Renda Anual}}\n\\]\nO PIR indica, grosso modo, a quantidade de “anos de trabalho” que uma família típica precisa investir de renda para comprar um imóvel típico. A principal vantagem do PIR é a sua simplicidade de cálculo, o que facilita comparações entre diferentes regiões. A principal desvantagem do PIR é de ignorar as condições de financiamento disponíveis para a população. Supondo que a renda anual média das famílias seja \\(R\\$30.000\\) e que o preço médio dos imóveis seja \\(R\\$270.000\\). Então o valor do PIR seria:\n\\[\n\\text{PIR} = \\frac{R\\$270.000}{R\\$30.000} = 9\n\\]\n\n\n\nO HAI é um indicador que possui diversas definições. Uma definição simples é a razão abaixo:\n\\[\n\\text{HAI} = \\frac{\\text{Parcela Máxima}}{\\text{Parcela Típica}}\\times100\n\\]\nonde a “parcela máxima de financiamento” é o valor teórico máximo que uma família típica estaria elegível num financiamento típico. Em outras palavras, suponha que a renda média familiar seja de \\(R\\$4.000\\). Considerando um financiamento a uma taxa de juros \\(r\\) num prazo de financiamento de \\(\\tau\\) períodos e um comprometimento máximo de renda \\(\\gamma\\) chega-se num valor \\(p_{max}\\) que representa o imóvel mais caro que o banco estaria disposto a financiar para esta família. Associado a este \\(p_{max}\\) existe um \\(\\text{pmt}_{max}\\) que é a parcela mais cara que a família poderia “aguentar”.\nComo regra de bolso, considera-se que a parcela de financiamento do imóvel não deve superar 30% da renda bruta familiar mensal. Assim, a parcela máxima para uma família que recebe \\(R\\$4.000\\) deveria ser de \\(R\\$1.200\\). Num típico financiamento SAC1, um imóvel de \\(R\\$270.000\\) teria uma parcela inicial próxima de \\(R\\$2.000\\). Assim, o valor do HAI seria:\n\\[\n\\text{HAI} = \\frac{1.200}{2.000}\\times100 = 60\n\\]\nO HAI mensura o valor da parcela de financiamento do imóvel típico em relação à renda mensal típica. Em outras palavras, este indicador verifica se o financiamento do imóvel típico de uma região “cabe no bolso” da família típica. Um valor próximo de 100 indica que a família com renda média consegue comprar o imóvel com preço médio."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#footnotes",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#footnotes",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConsiderou-se um financiamento SAC de um imóvel de 270.000 a uma taxa de juros de 10% a.a. financiada em 360 meses com LTV de 70% (i.e., entrada de 81.000). O valor da primeira parcela é de 2032,11.↩︎\nA multiplicação por 12 é uma aproximação. Pode-se também considerar a multiplicação por 13, considerando valor do décimo-terceiro. Idealmente, usa-se a renda familiar anual bruta.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#quadro-geral",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#quadro-geral",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "O gráfico abaixo mostra a evolução do preço dos imóveis (IVG-R) e da renda média das famílias em comparação com a inflação (nível geral de preços) no Brasil. As três séries mostram a evolução nominal das variáveis e os valores estão indexados em 2010 para facilitar a comparação. Os valores de renda interpolam as séries de PNAD, PNADC e Censo para chegar numa estimativa suavizada da renda familiar habitual mensal.\nEm linhas gerais, o preço dos imóveis para de crescer em 2014-15 com a Crise Econômica e fica estagnado - em termos nominais - nos próximos anos. Isto é, a partir de 2015 o preço dos imóveis começa a cair em termos reais. A renda média, em contrapartida, tem uma tendência de crescimento estável e consistentemente consegue acompanhar a inflação no período observado. Assim, pode-se concluir - ainda que superficialmente - que o affordability deve ter melhorado, visto que os preços dos imóveis cresceram menos do que renda média."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#pir-1",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#pir-1",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "PIR",
    "text": "PIR\nNa literatura internacional, valores de PIR na faixa de 5-10 indicam baixa acessibilidade financeira. Estudos feitos em países em desenvolvimento e subdesenvolvidos, contudo, apontam valores de PIR muito mais elevados do que este intervalo, devido tanto à baixa renda média da população como também às condições limitadas de acesso ao crédito. Neste sentido, talvez uma comparação mais justa seja com as megacidades chinesas que tem PIR na faixa de 10-15.\n\n\n\n\n\nO Jardim Europa foi a região menos acessível de São Paulo, nesta análise. O preço mediano do metro quadrado na região está em torno de 14 mil. Considerando um imóvel de 100 m2 temos um preço mediano na faixa 1,4 milhão. A renda mínima necessária para estar elegível a um financiamento deste imóvel seria em torno de 35 mil, considerando um comprometimento de 30% num sistema SAC e uma taxa de 10%. Evidentemente, este exemplo é bastante artificial quando se considera que esta região possui imóveis com metragem consideravelmente mais elevadas do que esta.\nPode-se calcular indiretamente qual seria um valor do PIR máximo para cada região. Suponha que a renda mensal típica considerada seja em torno de \\(R\\$4.500\\). Então para um típico financiamento no sistema SAC a 10% de juros e 360 meses, o imóvel mais caro que esta família pode financiar teria preço próximo a 180 mil. Este valor resulta num PIR próximo de 32.\n\\[\n\\text{PIR}_{max} = \\frac{180000}{12\\times 4500} = 3.33\n\\]\nGrosso modo, isto significa que qualquer imóvel que tenha preço 3 vezes superior à renda anual bruta familiar é inacessível. Para São Paulo, isto indica que a cidade inteira é inacessível para uma família que recebe em torno de 54 mil por ano.\nPara exemplificar o impacto do crédito sobre a acessibilidade, vale notar que uma redução dos juros para 5% e uma extensão do prazo para 35 anos (420 meses) aumentaria o preço máximo do imóvel para 300 mil. Isto aumentaria o PIR máximo para 5.5, valor próximo ao threshold de países desenvolvidos."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#hai-1",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#hai-1",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "HAI",
    "text": "HAI\nO HAI é um indicador mais interessante do que o PIR pois ele incorpora as condições de financiamento diretamente à métrica de acessibilidade. Assim como o PIR, o HAI aponta um grave problema de acessibilidade à moradia em São Paulo."
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#são-paulo",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#são-paulo",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "",
    "text": "O cenário em São Paulo é similar. A série de preços mais extensa que se tem disponível é a do FipeZap, indicada em vermelho no gráfico abaixo. Novamente, as variáveis estão em termos nominais e indexadas em seus valores médios em 2010. A dinâmica de preços em São Paulo é muito similar à dinâmica nacional; a diferença é que os preços sobem mais durante o “boom” e caem menos durante o período de estagnação. A renda das famílias cresce acima da inflação média, indicando um ganho real de renda.\nO gráfico abaixo faz parte do artigo Acesso à moradia em São Paulo escrito por mim em conjunto com outros colegas, publicado na LARES 2021. Como se vê, no longo prazo, a renda conseguiu alcançar o preço dos imóveis, indicando um certo equilíbrio entre renda e preço."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html",
    "title": "O novo tidyverse: select",
    "section": "",
    "text": "O tidyverse é uma coleção poderosa de pacotes, voltados para a manipulação e limpeza de dados. Num outro post, discuti alguns aspectos gerais da filosofia destes pacotes que incluem a sua consistência sintática e o uso de pipes. A filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes” que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático.\nO tidyverse está em constante expansão, novas funcionalidades são criadas para melhorar a performance e capabilidade de suas funções. Assim, é importante atualizar nosso conhecimento destes pacotes periodicamente. Nesta série de posts vou focar nas funções principais dos pacotes dplyr e tidyr, voltados para a limpeza de dados."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#o-básico",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#o-básico",
    "title": "O novo tidyverse: select",
    "section": "O básico",
    "text": "O básico\nOs pacotes utilizados neste tutorial são listados abaixo.\n\nlibrary(dplyr)\nlibrary(readr)\n\nPara praticar as funções vamos utilizar uma tabela que traz informações sobre as cidades do Brasil.\n\ntbl &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/cities_brazil.csv\"\n  )\n\nA função select serve para selecionar colunas. Adicionalmente, ela também pode renomear as colunas selecionadas. A sintaxe da função é a seguinte\nselect(dados, coluna1, coluna2, nome_coluna = coluna3)\nO código abaixo seleciona três colunas: name_muni, population, pib.\n\nselect(tbl, name_muni, population, pib)\n\n# A tibble: 5,570 × 3\n   name_muni             population     pib\n   &lt;chr&gt;                      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alta Floresta D'Oeste      21495  570272\n 2 Ariquemes                  96833 2818049\n 3 Cabixi                      5363  167190\n 4 Cacoal                     86895 2519353\n 5 Cerejeiras                 15890  600670\n 6 Colorado do Oeste          15663  366931\n 7 Corumbiara                  7519  268381\n 8 Costa Marques              12627  261978\n 9 Espigão D'Oeste            29397  666331\n10 Guajará-Mirim              39386  984586\n# ℹ 5,560 more rows\n\n\nPode-se selecionar colunas de três maneiras gerais: (1) como expressões (escrevendo o nome delas como se elas fossem objetos); (2) strings; (3) índices (que indicam a sua posição na tabela).\n\n# Selecionando colunas de modo geral\nselect(tbl, name_muni, population, pib)\n\n# Selecionando colunas usando strings\nselect(tbl, \"name_muni\", \"population\", \"pib\")\n# Selecionando colunas usando vetor de texto\nsel_cols &lt;- c(\"name_muni\", \"population\", \"pib\")\nselect(tbl, sel_cols)\n\n# Selecionando colunas usando índices\n\n# Usando índices explicitamente\nselect(tbl, 2, 8, 15)\n# Usando um vetor numérico\n\n# Encontra a posição de todas as colunas que começam com 'pib'\ninds &lt;- grep(\"^pib_\", names(tbl))\nselect(tbl, inds)\n\nPara remover uma coluna usa-se o sinal de menos (-) ou o operador lógico de negação (!)1.\n\nselect(tbl, !name_muni)\nselect(tbl, -name_muni)\n\nDe modo geral, para facilitar a seleção de colunas, pode-se usar os operadores lógicos convencionais (&, |, !). Por fim, existe também o operador : que serve para selecionar colunas contíguas.\n\nselect(tbl, code_muni:name_region)"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#tidyselectors",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#tidyselectors",
    "title": "O novo tidyverse: select",
    "section": "Tidyselectors",
    "text": "Tidyselectors\n\nBásico\nExiste um conjunto de funções auxiliares que facilita a seleção de colunas. Estas funções retornam índices a partir de alguma regra. Isto é, elas permitem selecionar colunas com base em algum padrão. O caso mais geral é da função matches, que seleciona colunas com base em um regex.\n\n# Seleciona as colunas que começam com 'pib_'\nselect(tbl, matches(\"^pib_\"))\n# Seleciona as colunas que terminam com 'muni'\nselect(tbl, matches(\"muni$\"))\n# Seleciona as colunas que contém o termo '_share'\nselect(tbl, matches(\"_share\"))\n\nA função matches retorna colunas a partir de algum padrão de texto no nome da coluna. Na linha da filosofia do tidyverse, de transformar tarefas rotineiras em funções específicas e com nomes “intuitivos” há uma série de funções auxiliares que imitam a função matches:\n\nstarts_with() - seleciona colunas que começam com algum string\nends_with() - seleciona colunas que terminam com algum string\ncontains() - seleciona colunas que contêm algum string\n\nIsto é, podemos reescrever os códigos acima da seguinte maneira\n\n# Seleciona as colunas que começam com 'pib_'\nselect(tbl, starts_with(\"pib_\"))\n# Seleciona as colunas que terminam com 'muni'\nselect(tbl, ends_with(\"muni\"))\n# Seleciona as colunas que contém o termo '_share'\nselect(tbl, contains(\"_share\"))\n\n\n\nall_of e any_of\nComo visto acima, pode-se selecionar colunas com base em um vetor de texto. Nos casos em que é necessário maior controle sobre a seleção, há duas funções auxiliares: any_of e all_of. A primeira função faz o match entre o vetor de texto e o nome das colunas e retorna todos os casos positivos; já a segunda função faz o match entre o vetor de texto e o nome das colunas e retorna um resultado somente no caso de todos os matches terem sucesso.\nEstas funções também são úteis para evitar potenciais ambiguidades entre o nome de objetos criados com o nome de colunas.\nA diferença entre as funções fica mais evidente num exemplo. Considere o caso em que colocamos uma coluna adicional pib_per_capita que não existe na base de dados. Quando se usa a função all_of retorna-se todas as colunas onde o match teve sucesso.\n\nsel_cols &lt;- c(\"code_muni\", \"name_muni\", \"population\", \"pib\", \"pib_per_capita\")\n\nselect(tbl, any_of(sel_cols))\n\n# A tibble: 5,570 × 4\n   code_muni name_muni             population     pib\n       &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;   &lt;dbl&gt;\n 1   1100015 Alta Floresta D'Oeste      21495  570272\n 2   1100023 Ariquemes                  96833 2818049\n 3   1100031 Cabixi                      5363  167190\n 4   1100049 Cacoal                     86895 2519353\n 5   1100056 Cerejeiras                 15890  600670\n 6   1100064 Colorado do Oeste          15663  366931\n 7   1100072 Corumbiara                  7519  268381\n 8   1100080 Costa Marques              12627  261978\n 9   1100098 Espigão D'Oeste            29397  666331\n10   1100106 Guajará-Mirim              39386  984586\n# ℹ 5,560 more rows\n\n\nA função all_of é mais exigente e retorna um erro neste caso.\n\nselect(tbl, all_of(sel_cols))\n\nError in `all_of()`:\n! Can't subset columns that don't exist.\n✖ Column `pib_per_capita` doesn't exist.\n\n\nNote que, neste caso, isto é equivalente a simplesmente usar o vetor. Esta sintaxe, contudo, não é recomendado e futuramente será descontinuada.\n\nselect(tbl, sel_cols)\n\nError in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `pib_per_capita` doesn't exist.\n\n\n\n\nConflitos de nomes\nA função select faz um bom trabalho em resolver situações onde há alguma ambiguidade sobre qual o environment em que se deve avaliar uma expressão. O uso de all_of e any_of é recomendado justamente para evitar potenciais ambiguidades. Vale tirar um tempo para entender os exemplos abaixo.\nNote que no primeiro caso, a função ncol é aplicada sobre x dentro da função select. A função ncol é avaliada no environment geral, isto é, ela considera x como o data.frame criado no espaço geral e não como uma coluna específica.\nNo segundo caso, a expressão y dentro da função select é interpretada como uma data-expression, isto é, como uma expressão que se refere ao nome de coluna do data.frame x.\nNo último caso, a função all_of indica que a expressão y deve ser avaliada como uma env-expression, isto é, como uma variável no environment geral, como o vetor de texto criado anteriormente.\n\nx &lt;- data.frame(x = 1, y = 2)\ny &lt;- c(\"x\", \"y\")\n\n# Retorna as duas colunas, 'x', 'y'\nselect(x, 1:ncol(x))\n# Retorna a segunda coluna, 'y'\nselect(x, y)\n# Retorna as duas colunas, 'x', 'y'\nselect(x, all_of(y))\n\n\n\nHelpers de posição\nHá também algumas funções auxilares mais gerais:\n\neverything() - seleciona todas as colunas\nlast_col() - seleciona a última coluna\ngroup_cols() - seleciona todas as colunas que compõem o group.\n\nA função everything() tem um comportamento particular quando combinada com outras colunas. A função seleciona todas as colunas, exceto as que foram explicitamente chamadas. Isto facilita bastante o trabalho de rearranjar as colunas dentro de uma mesma base de dados.\n\n# Coloca as colunas pib e pib_industrial na frente das demais colunas\nselect(tbl, pib, pib_industrial, everything())\n# Dropa todas as variáveis\nselect(tbl, -everything())\n\n\n\nHelpers de tipo\nPor fim, pode-se selecionar as colunas pela sua classe. Vale lembrar que num data.frame cada coluna tem uma classe específica. Os exemplos abaixo mostram os casos de aplicação mais simples.\n\n# Seleciona todas as colunas numéricas\nselect(tbl, where(is.numeric))\n# Seleciona todas as colunas tipo character\nselect(tbl, where(is.character))\n# Seleciona todas as colunas tipo factor\nselect(tbl, where(is.factor))\n# Selciona todas as colunas lógicas (i.e. TRUE, FALSE, NA)\nselect(tbl, where(is.logical))\n# Seleciona todas as colunas\nselect(tbl, where(is.Date))\n\nEssencialmente, o que o código acima faz é aplicar a função selecionada em cada uma das colunas e retornar os casos positivos. É possível criar condições lógicas mais complexas com auxílio do operador ~ (tilde). Os exemplos abaixo mostram como dropar todas as colunas que contém somente NA e selecionar as colunas com datas (Date).\n\n# eval: true\ndat &lt;- tibble(\n  lgl = NA,\n  missing = NA,\n  lglT = sample(c(TRUE, FALSE), size = 10, replace = TRUE),\n  dia_mes = seq(as.Date(\"2000-01-01\"), by = \"month\", length.out = 10),\n  val = rnorm(10)\n)\n\n# \"Remove\" as colunas que contêm somente NA\nselect(dat, !where(~all(is.na(.x))))\n# Seleciona apenas as colunas de data\nselect(dat, where(~all(inherits(.x, \"Date\"))))"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#grupos",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#grupos",
    "title": "O novo tidyverse: select",
    "section": "Grupos",
    "text": "Grupos\nA expressão mutate sempre é aplicada dentro de grupos. No caso em que não existe um grupo, a expressão é aplicada para todos os dados disponíveis. O código abaixo, por exemplo, encontra a participação percentual do PIB de cada município no PIB brasileiro.\n\ntbl |&gt; \n  select(name_muni, abbrev_state, pib) |&gt; \n  mutate(pib_share = pib / sum(pib) * 100) |&gt; \n  arrange(desc(pib_share))\n\n# A tibble: 5,570 × 4\n   name_muni      abbrev_state       pib pib_share\n   &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 São Paulo      SP           748759007     9.84 \n 2 Rio de Janeiro RJ           331279902     4.35 \n 3 Brasília       DF           265847334     3.49 \n 4 Belo Horizonte MG            97509893     1.28 \n 5 Manaus         AM            91768773     1.21 \n 6 Curitiba       PR            88308728     1.16 \n 7 Osasco         SP            76311814     1.00 \n 8 Porto Alegre   RS            76074563     1.00 \n 9 Guarulhos      SP            65849311     0.865\n10 Campinas       SP            65419717     0.860\n# ℹ 5,560 more rows\n\n\nJá este segundo código encontra a participação percentual do PIB de cada município dentro do seu respectivo estado.\n\ntbl |&gt; \n  select(name_muni, abbrev_state, pib) |&gt; \n  mutate(pib_share = pib / sum(pib) * 100, .by = \"abbrev_state\") |&gt; \n  arrange(desc(pib_share))\n\n# A tibble: 5,570 × 4\n   name_muni      abbrev_state       pib pib_share\n   &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 Brasília       DF           265847334     100  \n 2 Manaus         AM            91768773      79.1\n 3 Boa Vista      RR            11826207      73.8\n 4 Macapá         AP            11735557      63.5\n 5 Rio Branco     AC             9579592      58.1\n 6 Rio de Janeiro RJ           331279902      43.9\n 7 Fortaleza      CE            65160893      39.0\n 8 Teresina       PI            21578875      38.3\n 9 Porto Velho    RO            19448762      37.7\n10 Aracaju        SE            16447105      36.2\n# ℹ 5,560 more rows\n\n\nVale notar que a sintaxe .by = \"coluna\" é nova e ainda está em fase experimental. Ela substitui a sintaxe mais antiga do group_by. O código acima é equivalente ao código abaixo.\n\ntbl |&gt; \n  select(name_muni, abbrev_state, pib) |&gt; \n  group_by(abbre_state) |&gt; \n  mutate(pib_share = pib / sum(pib) * 100) |&gt; \n  ungroup()\n\nUma das vantagens de usar .by é que não é necessário usar ungroup já que os dados são desagrupados automaticamente."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#transformando-múltiplas-colunas",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#transformando-múltiplas-colunas",
    "title": "O novo tidyverse: select",
    "section": "Transformando múltiplas colunas",
    "text": "Transformando múltiplas colunas\nA função mutate tem um par importante na função across, que permite aplicar uma mesma função a múltiplas colunas com facilidade. Imagine o seguinte caso, onde quer-se aplicar a função scale, que serve para “normalizar” vetores numéricos, em todas as colunas de uma base. Tipicamente, seria necessário escrever e nomear cada coluna\n\ntbl |&gt; \n  mutate(\n    scaled_pib = scale(pib),\n    scaled_pop = scale(population),\n    scaled_agriculture = scale(pib_agriculture),\n    scaled_industrial = scale(pib_industrial),\n    ...\n  )\n\nEm linhas gerais, o resultado do código acima pode ser replicado simplesmente com:\n\ntbl |&gt; \n  mutate(\n    across(where(is.numeric), scale)\n  )\n\nA função across serve para aplicar uma função sobre um subconjunto de colunas seguindo: across(colunas, funcao). Ela funciona com os tidyselectors, facilitando a seleção de colunas a ser transformadas. Funções mais complexas podem ser utilizadas via função anônima usando o operador ~.\nO primeiro exemplo abaixo mostra como aplicar a função log em todas as colunas cujo nome começa com pib. Já o segundo exemplo mostra como converter todas as colunas do tipo character para factor. O terceiro exemplo mostra como converter as colunas de factor para numeric utilizando o operador ~. Os últimos dois exemplos mostram outras aplicações do mesmo operador.\n\n#&gt; Aplica uma transformação log em todas as colunas que começam com pib\nmutate(tbl, across(starts_with(\"pib\"), log))\n#&gt; Converte todas as colunas de strings para factors\nmutate(tbl, across(where(is.character), as.factor))\n#&gt; Converte as colunas de factors para numeric\nmutate(tbl, across(where(is.factor), ~ as.numeric(as.character(.x))))\n#&gt; Divide por pib e multiplica por 100 todas as colunas entre pib_taxes e\n#&gt; pib_govmt_services\nmutate(tbl, across(pib_taxes:pib_govmt_services, ~.x / pib * 100))\n#&gt; Normaliza todas as colunas numéricas\nmutate(tbl, across(where(is.numeric), ~ as.numeric(scale(.x))))\n\nPor fim, existe um argumento opcional .names que permite renomear as novas colunas usando uma sintaxe estilo glue1. Esta sintaxe tem dois tipos especiais importantes: {.col}, que faz referência ao nome original da coluna, e {.fn}, que faz referência ao nome da função utilizada. O exemplo abaixo refina o primeiro caso que vimos acima. Agora aplica-se a função as.numeric(scale(x)) sobre cada uma das colunas numéricas. As novas colunas têm o nome \"scaled_NomeOriginalDaColuna\".\n\ntbl |&gt; \n  mutate(\n    across(\n      where(is.numeric),\n      ~ as.numeric(scale(.x)),\n      .names = \"scaled_{.col}\"\n      )\n  )\n\nO tipo especial {.fn} é bastante útil com a função summarise, que permite aplicar uma lista de múltiplas funções simultaneamente. Ainda assim, é possível utilizá-lo com a função mutate. A sintaxe tem de ser adaptada, pois {.fn} espera que a função tenha sido passada como uma lista com nomes. No exemplo abaixo, aplica-se a função log sobre todas as colunas númericas e as colunas resultantes são renomeadas. Vale notar que, na maioria dos casos, não vale a pena utilizar {.fn} no contexto do mutate.\n\ntbl |&gt; \n  mutate(\n    across(\n      where(is.numeric),\n      list(\"ln\" = log),\n      .names = \"{.fn}_{.col}\"\n      )\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#outros-argumentos",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#outros-argumentos",
    "title": "O novo tidyverse: select",
    "section": "Outros argumentos",
    "text": "Outros argumentos\nA função mutate tem alguns outros argumentos, de uso diverso. Os argumentos .before e .after permitem selecionar a posição das novas colunas. O padrão da função é de sempre adicionar as novas colunas ao final do data.frame. Estes argumentos aceitam o nome de alguma das colunas ou mesmo funções tidyselect. No caso abaixo, cria-se a coluna lpib que é posta no início do data.frame.\n\ntbl |&gt; \n  mutate(\n    lpib = log(pib),\n    .before = everything()\n  ) |&gt; \n  select(1:5)\n\n# A tibble: 5,570 × 5\n    lpib code_muni name_muni             code_state name_state\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;     \n 1  13.3   1100015 Alta Floresta D'Oeste         11 Rondônia  \n 2  14.9   1100023 Ariquemes                     11 Rondônia  \n 3  12.0   1100031 Cabixi                        11 Rondônia  \n 4  14.7   1100049 Cacoal                        11 Rondônia  \n 5  13.3   1100056 Cerejeiras                    11 Rondônia  \n 6  12.8   1100064 Colorado do Oeste             11 Rondônia  \n 7  12.5   1100072 Corumbiara                    11 Rondônia  \n 8  12.5   1100080 Costa Marques                 11 Rondônia  \n 9  13.4   1100098 Espigão D'Oeste               11 Rondônia  \n10  13.8   1100106 Guajará-Mirim                 11 Rondônia  \n# ℹ 5,560 more rows\n\n\nO outro argumento opcional é o .keep que permite controlar quais colunas devem ser preservadas após a aplicação da função mutate. O padrão da função, naturalmente, é de preservar todas as colunas, isto é, .keep = \"all\". Contudo, pode-se usar .keep = \"used\" para manter somente as colunas que foram utilizadas.\n\ntbl |&gt; \n  mutate(\n    code_muni = as.character(code_muni),\n    lpib = log(pib),\n    .keep = \"used\"\n    )\n\n# A tibble: 5,570 × 3\n   code_muni     pib  lpib\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 1100015    570272  13.3\n 2 1100023   2818049  14.9\n 3 1100031    167190  12.0\n 4 1100049   2519353  14.7\n 5 1100056    600670  13.3\n 6 1100064    366931  12.8\n 7 1100072    268381  12.5\n 8 1100080    261978  12.5\n 9 1100098    666331  13.4\n10 1100106    984586  13.8\n# ℹ 5,560 more rows\n\n\nVale notar que .keep = \"used\" sempre preserva as colunas “agrupadoras”.\n\ntbl |&gt; \n  mutate(\n    code_muni = as.character(code_muni),\n    lpib = log(pib),\n    .by = \"code_state\",\n    .keep = \"used\"\n    )\n\n# A tibble: 5,570 × 4\n   code_muni code_state     pib  lpib\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 1100015           11  570272  13.3\n 2 1100023           11 2818049  14.9\n 3 1100031           11  167190  12.0\n 4 1100049           11 2519353  14.7\n 5 1100056           11  600670  13.3\n 6 1100064           11  366931  12.8\n 7 1100072           11  268381  12.5\n 8 1100080           11  261978  12.5\n 9 1100098           11  666331  13.4\n10 1100106           11  984586  13.8\n# ℹ 5,560 more rows"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#funções-úteis",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#funções-úteis",
    "title": "O novo tidyverse: select",
    "section": "Funções úteis",
    "text": "Funções úteis\nAbaixo segue uma lista de funções úteis.\n\ntbl |&gt; \n  mutate(\n    #&gt; Cria um ranking da variável\n    rank_pib = rank(pib),\n    #&gt; Cria um rakning (em percentil) da variável\n    rank_perc_pib = percent_rank(pib),\n    #&gt; Agrupa em decis \n    group_decile_pib = ntile(pib, 10),\n    #&gt; Cria um id\n    id = row_number(),\n    #&gt; Aplica uma transformação condicional a uma condição lógica\n    lpib = ifelse(pib &gt; 0, log(pib), 1),\n    #&gt; Aplica uma transformação condicional a múltiplas condições lógicas\n    type = case_when(\n      code_state %in% c(11, 12, 13) ~ \"grupo_1\",\n      code_state %in% c(14, 15, 16) ~ \"grupo_2\",\n      TRUE ~ \"outros\"\n    ),\n    #&gt; Soma cumulativa\n    spib = cumsum(pib),\n    #&gt; Diferença percentual usando o valor imediatamente anterior\n    diff_pib = pib / lag(pib) - 1,\n    #&gt; Participação relativa da variável\n    share_pib = pib / sum(pib, na.rm = TRUE) * 100,\n    #&gt; Normalizar variável\n    scaled_pib = as.numeric(scale(pib))\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#footnotes",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#footnotes",
    "title": "O novo tidyverse: select",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApesar de ambas as opções serem válidas, recomenda-se utilizar os operadores booleanos ao invés do opreadores de conjuntos. Isto é, deve-se usar ! ao invés de -.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Os mercados de aluguel e de venda de imóveis são interligados. Em tese, a decisão de comprar ou alugar um imóvel passa pelo preço. Se o aluguel estiver suficientemente “barato”, mais pessoas vão optar pelo aluguel, reduzindo a taxa de vacância e aumentando o preço do aluguel. O preço do aluguel sobe a tal ponto que se torna mais interessante comprar um imóvel do que alugar, incentivando pessoas que alugam a migrar para um apartamento próprio.\nVale notar que o mercado de aluguel é bastante lento. A “migração” do aluguel para um imóvel proprietário não é rápida nem simples. Além do custo monetário há diversos custos de transação envolvidos no processo. De fato, em muitos casos, não é possível fazer uma migração entre um e outro por motivos contratuais.\nPor fim, a literatura empírica do tema não é unânime sobre a relação entre os mercados de venda e de aluguel. Um estudo sobre o mercado de Cingapura encontrou evidência de cointegração apenas em regiões específicas e não conseguiu estabelecer a presença de cointegração a nível nacional.\n\n\n\nAs fontes de dados para verificar a dinâmica destes mercados é limitada no Brasil. O FipeZap oferece a série de aluguel e de venda mais extensa. Este índice coleta todos os anúncios online de imóveis publicados no sistema ZapImóveis e constroi um índice de mediana. Os dados são estratificados por número de dormitórios e territorialmente, usando regiões que se baseam nas áreas de ponderação do IBGE.\nO gráfico abaixo mostra as séries de aluguel e de venda (residenciais) a nível nacional, agregadas por tipologia. Como se vê, o comportamento das séries é muito similar no périodo 2008-2015. Durante a Crise Econômica, o mercado de aluguel parece sofrer mais do que o mercado de venda. Durante e após a pandemia do Covid-19 ambos os mercados se recuperam com força, mas o mercado de aluguel parece crescer mais.\n\n\n\n\n\n\n\n\n\nNa variação acumulada do período, vê-se que o mercado de venda acumulou uma alta significativamente maior do que do mercado de aluguel. A divergência parece ter começado em 2009. A diferença entre as séries fica estável durante a Crise Econômica e o mercado de aluguel parece começar a reduzir esta distância no período da pandemia.\n\n\n\n\n\n\n\n\n\n\n\nOlhando a variação contemporânea nos dois mercados, vê-se que parece existir uma correlação entre as variáveis. O gráfico abaixo mostra o ajuste da equação de um modelo aditivo generalizado e as cores dos pontos indicam ciclos econômicos. Grosso modo, define-se que o ciclo de commodities vai até a metade de 2012; a recessão Dilma começa na metade de 2014 e vai até o final de 2016; o período Covid começa em maio de 2020 e agrupa também o período pós-Covid até o presente.\n\n\n\n\n\n\n\n\n\nPode-se também buscar quebras de tendência entre os períodos. Agora, o gráfico ajusta linhas de regressão linear em cada um dos períodos. Visualmente, apenas o período Covid + Pós-Covid parece apresentar uma quebra na tendência, indicando variações maiores nos preços de aluguel para uma dada variação nos preços de venda. Os demais ciclos se distinguem por mudanças de média/nível e variância, mas preservam uma relação similar entre mudanças nos preços de aluguel e nos preços de venda.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsando uma abordagem mais rigorosa, podemos buscar algum tipo de cointegração entre as séries. Pelos testes de raiz unitária usais, é fácil concluir que ambas as séries são I(1)1. Contudo, tanto o teste de Johanssen como o teste Engle-Granger apontam que não há relação de cointegração entre as séries2.\nVale notar que estas séries passam por dois períodos de “exceção”: a Crise Econômica, do governo Dilma, e a Pandemia do Covid-19. Assim, há motivo para se crer que possa haver quebras estruturais na série. Estas quebras podem estar ocultando alguma relação de cointegração entre as séries. Por outro lado, como o horizonte temporal das séries não é muito longo, seria difícil sustentar uma tese de convergência de equilíbrio de longo prazo se houver mais do que uma quebra estrutural nas séries.\nUsando testes de detecção e verificação de quebras estruturais3, chega-se na conclusão de que há uma quebra na série do aluguel (em outubro de 2020) e 3 quebras na série de preços de venda (em julho de 2010, novembro de 2012 e dezembro de 2018). O gráfico abaixo destaca as quebras nas séries de preço de aluguel e de venda.\nIntuitivamente, pode-se associar a quebra da série de aluguel às mudanças no período da Pandemia e pós-Pandemia. As mudanças na série de vendas são mais opacas. A quebra na série de vendas na metade de 2010 pode estar refletindo o bom momento da economia brasileira na época, que ainda sentia os estímulos da política contracíclica de 2009, como também o recente lançamento do programa MCMV. A quebra no final de 2012 pode estar refletindo tanto o final do ciclo de commodities, como também o início do ciclo (forçado) de queda da taxa SELIC. A quebra no final de 2018 pode estar refletindo mudanças institucionais (reforma previdência, reforma dos distratos, etc.), a taxa de inflação estável e a baixa taxa SELIC.\n\n\n\n\n\n\n\n\n\nPara manter a simplicidade da análise, vamos considerar apenas a quebra na série de aluguel. O gráfico abaixo destaca apenas a quebra na série do aluguel. É interessante notar que o período “normal” apresenta uma correlação positiva entre a variação do preço do aluguel e do preço de venda, sugerindo que os mercados andam lado a lado. Já no período pós-Covid a correlação é inexistente. O que se verifica é que os preços de venda variam consideravelmente, de -0.2% a quase 2% em cada mês, enquanto os preços de aluguem variam muito menos. Neste sentido, os mercados parecem se comportam de maneira quase independente no período denominado “Pós-Covid”.\n\n\n\n\n\n\n\n\n\n\n\n\nPela análise dos dados, parece que o mercado de aluguel entrou num forte ciclo de alto, descolado do mercado de vendas. Isto pode ser resultado de diversos fatores, incluindo (1) o aumento da taxa de juros real da economia; (2) mudanças nos padrões de moradia no período pós-pandemia; ou, vale mencionar; (3) erros de mensuração. Vou tratar mais especificamente do último ponto.\nO Índice FipeZap captura a mediana dos preços de anúncios mês a mês e utiliza uma metodologia relativamente simples para converter este valor num número índice4. O preço do anúncio de um imóvel certamente é correlacionado com o preço final de venda/aluguel, mas esta relação pode ter se alterado - ainda que temporariamente - durante o período da pandemia. O Índice de Variação de Aluguel Residencial (IVAR), desenvolvido pela Fundação Getúlio Vargas (FGV), é uma alternativa ao FipeZap. O IVAR é um índice de alugueis repetidos, metodologia adaptada do famoso Índice Case-Shiller, que utiliza somente a informação de contratos de alugueis efetivamente firmados5.\nEm particular, vale notar que o FipeZap divergiu consideravelmente em relação ao IVAR durante o período da pandemia. Enquanto o IVAR registrou quedas nominais, indicando a tendência do mercado de ceder descontos durante o período de maior incerteza da Pandemia, o FipeZap registrou um longo período de estagnação entrecortado por quedas pontuais. A partir de 2022, ambos os índices registram aumento, mas a alta do IVAR é consideravelmente menor.\n\n\n\n\n\n\n\n\n\nNão é fácil afirmar que um ou outro índice seja mais correto. Contudo, vale afirmar que a metodologia do IVAR é considerada superior à metodologia do FipeZap segundos as melhores práticas internacionais. Ainda assim, é importante notar que a base territorial do FipeZap é consideravelmente superior: na sua versão mais recente, o índice abarca mais de 50 cidades; o IVAR, por outro lado, está disponível somente para quatro cidades (Belo Horizonte, Porto Alegre, Rio de Janeiro e São Paulo). De qualquer maneira, a série histórica do IVAR é muito recente; o índice remonta apenas até 2019, dificultando qualquer tipo de análise de cointegração.\nNum post futuro vou entrar em maiores detalhes sobre as séries do FipeZap e sobre os índices de preços imobiliários em geral. Também devo discutir em maiores detalhes os procedimentos de cointegração e de quebras estruturais em séries de tempo em posts futuros."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#os-mercados",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#os-mercados",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Os mercados de aluguel e de venda de imóveis são interligados. Em tese, a decisão de comprar ou alugar um imóvel passa pelo preço. Se o aluguel estiver suficientemente “barato”, mais pessoas vão optar pelo aluguel, reduzindo a taxa de vacância e aumentando o preço do aluguel. O preço do aluguel sobe a tal ponto que se torna mais interessante comprar um imóvel do que alugar, incentivando pessoas que alugam a migrar para um apartamento próprio.\nVale notar que o mercado de aluguel é bastante lento. A “migração” do aluguel para um imóvel proprietário não é rápida nem simples. Além do custo monetário há diversos custos de transação envolvidos no processo. De fato, em muitos casos, não é possível fazer uma migração entre um e outro por motivos contratuais.\nPor fim, a literatura empírica do tema não é unânime sobre a relação entre os mercados de venda e de aluguel. Um estudo sobre o mercado de Cingapura encontrou evidência de cointegração apenas em regiões específicas e não conseguiu estabelecer a presença de cointegração a nível nacional."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#os-dados",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#os-dados",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "As fontes de dados para verificar a dinâmica destes mercados é limitada no Brasil. O FipeZap oferece a série de aluguel e de venda mais extensa. Este índice coleta todos os anúncios online de imóveis publicados no sistema ZapImóveis e constroi um índice de mediana. Os dados são estratificados por número de dormitórios e territorialmente, usando regiões que se baseam nas áreas de ponderação do IBGE.\nO gráfico abaixo mostra as séries de aluguel e de venda (residenciais) a nível nacional, agregadas por tipologia. Como se vê, o comportamento das séries é muito similar no périodo 2008-2015. Durante a Crise Econômica, o mercado de aluguel parece sofrer mais do que o mercado de venda. Durante e após a pandemia do Covid-19 ambos os mercados se recuperam com força, mas o mercado de aluguel parece crescer mais.\n\n\n\n\n\n\n\n\n\nNa variação acumulada do período, vê-se que o mercado de venda acumulou uma alta significativamente maior do que do mercado de aluguel. A divergência parece ter começado em 2009. A diferença entre as séries fica estável durante a Crise Econômica e o mercado de aluguel parece começar a reduzir esta distância no período da pandemia.\n\n\n\n\n\n\n\n\n\n\n\nOlhando a variação contemporânea nos dois mercados, vê-se que parece existir uma correlação entre as variáveis. O gráfico abaixo mostra o ajuste da equação de um modelo aditivo generalizado e as cores dos pontos indicam ciclos econômicos. Grosso modo, define-se que o ciclo de commodities vai até a metade de 2012; a recessão Dilma começa na metade de 2014 e vai até o final de 2016; o período Covid começa em maio de 2020 e agrupa também o período pós-Covid até o presente.\n\n\n\n\n\n\n\n\n\nPode-se também buscar quebras de tendência entre os períodos. Agora, o gráfico ajusta linhas de regressão linear em cada um dos períodos. Visualmente, apenas o período Covid + Pós-Covid parece apresentar uma quebra na tendência, indicando variações maiores nos preços de aluguel para uma dada variação nos preços de venda. Os demais ciclos se distinguem por mudanças de média/nível e variância, mas preservam uma relação similar entre mudanças nos preços de aluguel e nos preços de venda."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#um-novo-ciclo-de-aluguel",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#um-novo-ciclo-de-aluguel",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Usando uma abordagem mais rigorosa, podemos buscar algum tipo de cointegração entre as séries. Pelos testes de raiz unitária usais, é fácil concluir que ambas as séries são I(1)1. Contudo, tanto o teste de Johanssen como o teste Engle-Granger apontam que não há relação de cointegração entre as séries2.\nVale notar que estas séries passam por dois períodos de “exceção”: a Crise Econômica, do governo Dilma, e a Pandemia do Covid-19. Assim, há motivo para se crer que possa haver quebras estruturais na série. Estas quebras podem estar ocultando alguma relação de cointegração entre as séries. Por outro lado, como o horizonte temporal das séries não é muito longo, seria difícil sustentar uma tese de convergência de equilíbrio de longo prazo se houver mais do que uma quebra estrutural nas séries.\nUsando testes de detecção e verificação de quebras estruturais3, chega-se na conclusão de que há uma quebra na série do aluguel (em outubro de 2020) e 3 quebras na série de preços de venda (em julho de 2010, novembro de 2012 e dezembro de 2018). O gráfico abaixo destaca as quebras nas séries de preço de aluguel e de venda.\nIntuitivamente, pode-se associar a quebra da série de aluguel às mudanças no período da Pandemia e pós-Pandemia. As mudanças na série de vendas são mais opacas. A quebra na série de vendas na metade de 2010 pode estar refletindo o bom momento da economia brasileira na época, que ainda sentia os estímulos da política contracíclica de 2009, como também o recente lançamento do programa MCMV. A quebra no final de 2012 pode estar refletindo tanto o final do ciclo de commodities, como também o início do ciclo (forçado) de queda da taxa SELIC. A quebra no final de 2018 pode estar refletindo mudanças institucionais (reforma previdência, reforma dos distratos, etc.), a taxa de inflação estável e a baixa taxa SELIC.\n\n\n\n\n\n\n\n\n\nPara manter a simplicidade da análise, vamos considerar apenas a quebra na série de aluguel. O gráfico abaixo destaca apenas a quebra na série do aluguel. É interessante notar que o período “normal” apresenta uma correlação positiva entre a variação do preço do aluguel e do preço de venda, sugerindo que os mercados andam lado a lado. Já no período pós-Covid a correlação é inexistente. O que se verifica é que os preços de venda variam consideravelmente, de -0.2% a quase 2% em cada mês, enquanto os preços de aluguem variam muito menos. Neste sentido, os mercados parecem se comportam de maneira quase independente no período denominado “Pós-Covid”."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#considerações-importantes",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#considerações-importantes",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "",
    "text": "Pela análise dos dados, parece que o mercado de aluguel entrou num forte ciclo de alto, descolado do mercado de vendas. Isto pode ser resultado de diversos fatores, incluindo (1) o aumento da taxa de juros real da economia; (2) mudanças nos padrões de moradia no período pós-pandemia; ou, vale mencionar; (3) erros de mensuração. Vou tratar mais especificamente do último ponto.\nO Índice FipeZap captura a mediana dos preços de anúncios mês a mês e utiliza uma metodologia relativamente simples para converter este valor num número índice4. O preço do anúncio de um imóvel certamente é correlacionado com o preço final de venda/aluguel, mas esta relação pode ter se alterado - ainda que temporariamente - durante o período da pandemia. O Índice de Variação de Aluguel Residencial (IVAR), desenvolvido pela Fundação Getúlio Vargas (FGV), é uma alternativa ao FipeZap. O IVAR é um índice de alugueis repetidos, metodologia adaptada do famoso Índice Case-Shiller, que utiliza somente a informação de contratos de alugueis efetivamente firmados5.\nEm particular, vale notar que o FipeZap divergiu consideravelmente em relação ao IVAR durante o período da pandemia. Enquanto o IVAR registrou quedas nominais, indicando a tendência do mercado de ceder descontos durante o período de maior incerteza da Pandemia, o FipeZap registrou um longo período de estagnação entrecortado por quedas pontuais. A partir de 2022, ambos os índices registram aumento, mas a alta do IVAR é consideravelmente menor.\n\n\n\n\n\n\n\n\n\nNão é fácil afirmar que um ou outro índice seja mais correto. Contudo, vale afirmar que a metodologia do IVAR é considerada superior à metodologia do FipeZap segundos as melhores práticas internacionais. Ainda assim, é importante notar que a base territorial do FipeZap é consideravelmente superior: na sua versão mais recente, o índice abarca mais de 50 cidades; o IVAR, por outro lado, está disponível somente para quatro cidades (Belo Horizonte, Porto Alegre, Rio de Janeiro e São Paulo). De qualquer maneira, a série histórica do IVAR é muito recente; o índice remonta apenas até 2019, dificultando qualquer tipo de análise de cointegração.\nNum post futuro vou entrar em maiores detalhes sobre as séries do FipeZap e sobre os índices de preços imobiliários em geral. Também devo discutir em maiores detalhes os procedimentos de cointegração e de quebras estruturais em séries de tempo em posts futuros."
  },
  {
    "objectID": "posts/general-posts/2024-02-wz-rent-house/index.html#footnotes",
    "href": "posts/general-posts/2024-02-wz-rent-house/index.html#footnotes",
    "title": "Preços de Aluguel e de Venda de Imóveis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsou-se o teste de Philips-Perron (ur.pp) e o teste Dickey Fuller Aumentado (ur.df) do pacote urca. Na série de venda incluiu-se uma constante e uma tendência linear. Na série de aluguel não se incluiu nem constante e nem tendência linear.↩︎\nEm ambos os casos não se rejeita a hipótese nula de \\(r = 0\\), isto é, de que há 0 relações de cointegração entre as variáveis. O teste foi realizado com e sem constante e com e sem tendência.↩︎\nEspecificamente, testou-se a presença de quebras na série usando um teste F (Fstats) e um teste generalizado (empirical fluctuation process), considerando um processo SARMA(1,0,0)(1,0,0)[12]. O teste utilizado foi o efp(type = \"OLS-CUSUM\"). O número ótimo de quebras na série foi detectado usando o método de Bai e Perron (2003) via breakspoints. Todas as funções listadas são do pacote strucchange.↩︎\nPara mais detalhes sobre a metodologia do Índice FipeZap veja Fipe (2011) e a atualização Fipe (2020)↩︎\nPara mais detalhes sobre a metodologia do IVAR veja FGV (2021).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "",
    "text": "Um índice de preço convencional mensura a evolução do preço de um bem ou de uma cesta de bens ao longo do tempo. No caso mais simples, acompanha-se a dinâmica de um único bem, cujo preço no período \\(t\\) é dado por \\(p_{t}\\). Assim, temos um índice \\(I_t\\) dado por:\n\\[\nI_{t} = \\frac{p_{t}}{p_{t-1}}\\times 100\n\\]\nTipicamente, nosso interesse é de acompanhar uma cesta de bens. No caso do IPCA, indicador oficial da inflação, por exemplo, acomapanha-se um conjunto de bens que representa o “consumidor típico”. Assim temos \\(i\\) bens diferentes cada um com um preço \\(p_{i, t}\\). Neste post, vamos ignorar o problema (bastante complexo) de como agregar estes diferentes bens num índice único. Para todos efeitos vamos imaginar que existam pesos \\(w_i\\) imutáveis que representam o quão importante cada bem é para o “consumidor típico”. Assim temos: \\[\nI_{t} = \\frac{\\sum_{i \\in I}w_{i}p_{i,t}}{\\sum_{i \\in I}w_i p_{i, t-1}}\n\\]\nO índice assim é chamado de Índice de Laspeyres e é um dos índices mais comuns. O supracitado IPCA é um índice de Laspeyres, por exemplo. Apesar de simples, ele serve como base para se pensar índices de preços imobiliários1. Um índice de preços imobiliário mensura a tendência dos preços no mercado de imóveis. Em geral, costuma-se distinguir os mercados residencial e comercial devido às idissincracias de cada mercado. Neste post vamos tratar somente do mercado residencial.\nHá alguns desafios gerais relacionados à criação de um índice de preços imobiliário residencial (RPPI2). Os pontos gerais são listados abaixo\n\nImóveis são bens heterogêneos\nImóveis não são transacionados com frequência\nImóveis operam em submercados específicos\n\nAbaixo entro em mais detalhes sobre estes pontos. Uma revisão geral sobre a literatura de índices de preços imobiliários no contexto brasileiro está disponível no texto do IPEA, Índice de Preços para Imóveis. A referência geral e completa é o manual da Eurostat (2013), Handbook on Residential Property Price Indices.\n\n\nEm economia, há bens homogêneos (ou commodities) e bens heterogêneos. Pode-se pensar em bens homogêneos como bens em que é fácil encontrar um substituto; já bens heterogêneos são aqueles em que é muito difícil encontrar um substituto. Imóveis são um caso limite de bens heterogêneos. Quando se considera a localização como parte de seus atributos, cada imóvel é literalmente único, não existe nenhum outro equivalente no mundo. Mesmo quando se constroi apartamentos com plantas idênticas num mesmo prédio, pode-se ainda encontrar diferenças entre as unidades (seja o andar, a posição solar, etc.).\nO preço de bens heterogêneos reflete diferenças de qualidade. No caso de imóveis, pode-se pensar nos seus atributos estruturais (metragem, número de dormitórios, etc.) e locacionais (rua, bairro, proximidade a pontos de interesse, etc.). Faz sentido que a maior ou menor disponibilidade destes atributos influencie o preço do imóvel.\nPara entender como isto dificulta a estimação de um índice considere o caso de um imóvel abandonado que passa por uma significativa reforma. Naturalmente, o preço de venda deste imóvel aumenta várias vezes. Um índice de preços que identifica isto como um movimento dos preços em geral será enviesado.\nUma maneira de contornar isto é usando a modelagem hedônica. A modelagem ou precificação hedônica decompõe o preço de um imóvel em função de duas características observáveis. Assim, atribui-se um “preço” para cada uma das características do imóvel. Isto permite que se atribua corretamente qual parte do movimento dos preços é, de fato, um movimento de mercado.\n\n\n\nDescobrir o preço do arroz é uma tarefa tão simples como de olhar o valor etiquetado sobre a sua embalagem no mercado. No caso de imóveis isto é muito mais complicado. Todo imóvel tem um preço, mas apenas uma minúscula fração do estoque total de imóveis está “no mercado”, disponível para ser transacionado. Isto dificulta a aferição de preços.\nAlém disso, torna-se difícil comparar “laranjas com laranjas”. Considere o caso de um índice mensal. Dificilmente, os mesmos imóveis serão vendidos todos mês; pelo contrário, o caso mais comum seria observar, a cada novo mês, uma lista completamente nova de imóveis vendidos. Isto significa que o Índice vai estar comparando a evolução de preços de imóveis diferentes a cada mês.\nUma maneira de contornar isto é usando modelagem hedônica para precificar os imóveis que não estão à venda. Assim, seria possível prever o preço que um imóvel teria, caso ele tivesse sido vendido num determinado mês, o que permite fazer comparações adequadas. A qualidade desta precificação, por sua vez, depende da qualidade de dados disponíveis.\n\n\n\nIsto é outra maneira de dizer que nem todos os imóveis competem entre si. O mercado de imóveis se comporta similarmente a um mercado de mathcing onde as demandas dos clientes limitam a oferta relevante. Uma família de quatro pessoas dificilmente vai estar interessada em comprar um studio ou apartamento de 1 dormitório, qualquer que seja o seu preço. Na prática, isto significa que há vários mercados imobiliários; é possível que o mercado de casas de 4 dormitórios esteja em alta enquanto o mercado de apartamentos de 1d esteja em baixa. Similarmente, o mercado imobiliário da capital pode seguir um processo distinto do mercado imobiliário do interior.\nNovamente, uma estratégia para resolver isto é usar a modelagem hedônica. Outra estratégia é estratificar o índice para criar subíndices. Esta é uma prática bastante comum. Tipicamente, cria-se subíndices segundo algum critério estrutural (número de dormitórios ou tipologia) ou regional (capital x região metropolitana, interior).\n\n\n\n\nExiste uma literatura considerável sobre a criação de índices de preços. Desde a Grande Crise Financeira de 2008, há um interesse renovado no monitoramento do preço dos imóveis3.\nO atual consenso da literatura é de que um RPPI deve controlar pela qualidade dos imóveis observados (quality-adjusted index). Idealmente, a melhor metodologia é construir um índice hedônico com alguma estratégia de estratificação e imputação dupla. O único motivo para não usar um índice hedônico é quando não há informação suficiente para estimá-lo.\nA estratégia de imputação dupla permite maior flexibilidade na escolha da forma funcional do modelo hedônico. Escolhas comuns incluem: (1) regressão linear; (2) modelos de econometria espacial (SAR, SEM, etc.); e (3) modelos aditivos generalizados. Mais recentemente, alguns pesquisadores têm experimentado com modelos de aprendizado de máquina como de redes neurais. Em 2023, a Zillow revisou a metodologia do Zillow Home Value Index (ZHVI); o índice agora utiliza um modelo de redes neurais.\nEnquanto um Índice de Fisher (ou Tornqvist) apresenta propriedades ideais, nem sempre existe informação disponível para calculá-los. Assim, é natural utilizar um índice de Laspeyres junto com informação censitária para ajustar os pesos. Infelizmente, o Brasil não dispõe de um Censo de Imóveis, assim é necessário adaptar os pesos a partir do Censo Demográfico."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#a-teoria",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#a-teoria",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "",
    "text": "Um índice de preço convencional mensura a evolução do preço de um bem ou de uma cesta de bens ao longo do tempo. No caso mais simples, acompanha-se a dinâmica de um único bem, cujo preço no período \\(t\\) é dado por \\(p_{t}\\). Assim, temos um índice \\(I_t\\) dado por:\n\\[\nI_{t} = \\frac{p_{t}}{p_{t-1}}\\times 100\n\\]\nTipicamente, nosso interesse é de acompanhar uma cesta de bens. No caso do IPCA, indicador oficial da inflação, por exemplo, acomapanha-se um conjunto de bens que representa o “consumidor típico”. Assim temos \\(i\\) bens diferentes cada um com um preço \\(p_{i, t}\\). Neste post, vamos ignorar o problema (bastante complexo) de como agregar estes diferentes bens num índice único. Para todos efeitos vamos imaginar que existam pesos \\(w_i\\) imutáveis que representam o quão importante cada bem é para o “consumidor típico”. Assim temos: \\[\nI_{t} = \\frac{\\sum_{i \\in I}w_{i}p_{i,t}}{\\sum_{i \\in I}w_i p_{i, t-1}}\n\\]\nO índice assim é chamado de Índice de Laspeyres e é um dos índices mais comuns. O supracitado IPCA é um índice de Laspeyres, por exemplo. Apesar de simples, ele serve como base para se pensar índices de preços imobiliários1. Um índice de preços imobiliário mensura a tendência dos preços no mercado de imóveis. Em geral, costuma-se distinguir os mercados residencial e comercial devido às idissincracias de cada mercado. Neste post vamos tratar somente do mercado residencial.\nHá alguns desafios gerais relacionados à criação de um índice de preços imobiliário residencial (RPPI2). Os pontos gerais são listados abaixo\n\nImóveis são bens heterogêneos\nImóveis não são transacionados com frequência\nImóveis operam em submercados específicos\n\nAbaixo entro em mais detalhes sobre estes pontos. Uma revisão geral sobre a literatura de índices de preços imobiliários no contexto brasileiro está disponível no texto do IPEA, Índice de Preços para Imóveis. A referência geral e completa é o manual da Eurostat (2013), Handbook on Residential Property Price Indices.\n\n\nEm economia, há bens homogêneos (ou commodities) e bens heterogêneos. Pode-se pensar em bens homogêneos como bens em que é fácil encontrar um substituto; já bens heterogêneos são aqueles em que é muito difícil encontrar um substituto. Imóveis são um caso limite de bens heterogêneos. Quando se considera a localização como parte de seus atributos, cada imóvel é literalmente único, não existe nenhum outro equivalente no mundo. Mesmo quando se constroi apartamentos com plantas idênticas num mesmo prédio, pode-se ainda encontrar diferenças entre as unidades (seja o andar, a posição solar, etc.).\nO preço de bens heterogêneos reflete diferenças de qualidade. No caso de imóveis, pode-se pensar nos seus atributos estruturais (metragem, número de dormitórios, etc.) e locacionais (rua, bairro, proximidade a pontos de interesse, etc.). Faz sentido que a maior ou menor disponibilidade destes atributos influencie o preço do imóvel.\nPara entender como isto dificulta a estimação de um índice considere o caso de um imóvel abandonado que passa por uma significativa reforma. Naturalmente, o preço de venda deste imóvel aumenta várias vezes. Um índice de preços que identifica isto como um movimento dos preços em geral será enviesado.\nUma maneira de contornar isto é usando a modelagem hedônica. A modelagem ou precificação hedônica decompõe o preço de um imóvel em função de duas características observáveis. Assim, atribui-se um “preço” para cada uma das características do imóvel. Isto permite que se atribua corretamente qual parte do movimento dos preços é, de fato, um movimento de mercado.\n\n\n\nDescobrir o preço do arroz é uma tarefa tão simples como de olhar o valor etiquetado sobre a sua embalagem no mercado. No caso de imóveis isto é muito mais complicado. Todo imóvel tem um preço, mas apenas uma minúscula fração do estoque total de imóveis está “no mercado”, disponível para ser transacionado. Isto dificulta a aferição de preços.\nAlém disso, torna-se difícil comparar “laranjas com laranjas”. Considere o caso de um índice mensal. Dificilmente, os mesmos imóveis serão vendidos todos mês; pelo contrário, o caso mais comum seria observar, a cada novo mês, uma lista completamente nova de imóveis vendidos. Isto significa que o Índice vai estar comparando a evolução de preços de imóveis diferentes a cada mês.\nUma maneira de contornar isto é usando modelagem hedônica para precificar os imóveis que não estão à venda. Assim, seria possível prever o preço que um imóvel teria, caso ele tivesse sido vendido num determinado mês, o que permite fazer comparações adequadas. A qualidade desta precificação, por sua vez, depende da qualidade de dados disponíveis.\n\n\n\nIsto é outra maneira de dizer que nem todos os imóveis competem entre si. O mercado de imóveis se comporta similarmente a um mercado de mathcing onde as demandas dos clientes limitam a oferta relevante. Uma família de quatro pessoas dificilmente vai estar interessada em comprar um studio ou apartamento de 1 dormitório, qualquer que seja o seu preço. Na prática, isto significa que há vários mercados imobiliários; é possível que o mercado de casas de 4 dormitórios esteja em alta enquanto o mercado de apartamentos de 1d esteja em baixa. Similarmente, o mercado imobiliário da capital pode seguir um processo distinto do mercado imobiliário do interior.\nNovamente, uma estratégia para resolver isto é usar a modelagem hedônica. Outra estratégia é estratificar o índice para criar subíndices. Esta é uma prática bastante comum. Tipicamente, cria-se subíndices segundo algum critério estrutural (número de dormitórios ou tipologia) ou regional (capital x região metropolitana, interior)."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#resumo",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#resumo",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "",
    "text": "Existe uma literatura considerável sobre a criação de índices de preços. Desde a Grande Crise Financeira de 2008, há um interesse renovado no monitoramento do preço dos imóveis3.\nO atual consenso da literatura é de que um RPPI deve controlar pela qualidade dos imóveis observados (quality-adjusted index). Idealmente, a melhor metodologia é construir um índice hedônico com alguma estratégia de estratificação e imputação dupla. O único motivo para não usar um índice hedônico é quando não há informação suficiente para estimá-lo.\nA estratégia de imputação dupla permite maior flexibilidade na escolha da forma funcional do modelo hedônico. Escolhas comuns incluem: (1) regressão linear; (2) modelos de econometria espacial (SAR, SEM, etc.); e (3) modelos aditivos generalizados. Mais recentemente, alguns pesquisadores têm experimentado com modelos de aprendizado de máquina como de redes neurais. Em 2023, a Zillow revisou a metodologia do Zillow Home Value Index (ZHVI); o índice agora utiliza um modelo de redes neurais.\nEnquanto um Índice de Fisher (ou Tornqvist) apresenta propriedades ideais, nem sempre existe informação disponível para calculá-los. Assim, é natural utilizar um índice de Laspeyres junto com informação censitária para ajustar os pesos. Infelizmente, o Brasil não dispõe de um Censo de Imóveis, assim é necessário adaptar os pesos a partir do Censo Demográfico."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#ivg-r",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#ivg-r",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "IVG-R",
    "text": "IVG-R\nO IVG-R foi desenvolvido pelo Banco Central do Brasil a partir de laudos que são feitos durante o processo de financiamento de imóveis para pessoas físicas. Neste sentido, a base do IVG-R é muito rica pois abarca a totalidade dos imóveis financiados pelos sistema financeiro nacional. Evidentemente, isto implica que os imóveis comprados à vista não estão incluidos neste índice. Territorialmente, o índice considera as mesmas regiões metropolitanas do IPCA, o que garante boa representatividade nacional.\nEm termos metodológicos o IVGR é um índice de mediana estratificado. Essencialmente, verifica-se o preço mediano do imóvel financiado mês a mês e encadeia-se este valor de maneira a gerar um índice. A interpretação do índice é bastante simples e usa-se a mediana, ao invés da média, pois o preço de imóveis costuma ser fortemente assimétrico à direita. Na prática, alguns cuidados adicionais são feitos para reduzir a volatilidade do indicador. As estimativas mensais são geradas em janelas móveis de três meses (right-aligned), considerando o preço do mês atual e dos dois meses anteriores. A série final é suavizada pelo filtro HP e as séries regionais são agregadas considerando pesos do Censo do IBGE (2010).\nComo resultado, o IVG-R é um índice bastante suave. O IVG-R é o mais próximo que existe de um índice “oficial” de preços de imóveis no Brasil. Ele é o índice utilizado pelo BIS na compilação de índices de preços imobiliários."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#fipezap",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#fipezap",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "FipeZap",
    "text": "FipeZap\nO Índice FipeZap é um índice de mediana estratificado construído a partir de anúncios online de imóveis. Atualmente, há bastante apoio teórico e empírico para a construção de indicadores baseados em anúncios:\n\nO preço do anúncio é fortemente correlacionado com o preço de venda;\nA difusão da internet e a digitalização do mercado imobiliário garante que bases de anúncios tenham uma abrangência boa e uma temporalidade quase instantânea;\nO custo de construção de um índice de anúncios é baixíssimo quando comparado com outros métodos de coleta de dados."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#igmi-r",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#igmi-r",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "IGMI-R",
    "text": "IGMI-R\nO Índice Geral do Mercado Imobiliário Residencial é o primeiro índice de preços imobiliário hedônico do Brasil. A base de dados do índice é similar a do IVG-R mas o seu tratamento é mais sofisticado. Mais de 40 variáveis são utilizadas no modelo hedônico que ajuda a decompor o preço dos imóveis.\nO IGMI-R foi desenvolvido pelo economista Paulo Pichetti, numa parceria entre a FGV e a Abecip. Alguns detalhes sobre a metodologia do índice podem ser vistos nesta apresentação institucional. Atualmente, o IGMI-R é o índice mais preciso sobre o mercado de vendas no Brasil.\nA principal fragilidade do IGMI-R é o seu histórico curto. Além disso, o timing de criação do índice foi bastante infortuito, pois a série inicia em 2014. Nos seus breves 10 anos de história, o IGMI-R passou pela maior recessão econômica da história recente do país, uma pandemia global, dois ciclos de alta de juros e a menor taxa de juros da história."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#comparando-os-índices",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#comparando-os-índices",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "Comparando os índices",
    "text": "Comparando os índices\n\nQuadro Geral\nA tabela abaixo mostra a variação anual acumulada dos três. Para facilitar a contextualização, coloca-se também a variação do IPCA e do INCC.\n\n\n\n\n\n\n  \n    \n      Ano\n      IGMI-R\n      IVG-R\n      FipeZap\n      IPCA\n      INCC\n    \n  \n  \n    2003\n—\n14.17\n—\n9.30\n14.42\n    2004\n—\n11.24\n—\n7.60\n11.04\n    2005\n—\n8.64\n—\n5.69\n6.83\n    2006\n—\n13.66\n—\n3.14\n5.04\n    2007\n—\n19.43\n—\n4.46\n6.16\n    2008\n—\n23.73\n14.67\n5.90\n11.86\n    2009\n—\n26.00\n21.13\n4.31\n3.25\n    2010\n—\n23.40\n26.86\n5.91\n7.77\n    2011\n—\n16.28\n26.32\n6.50\n7.48\n    2012\n—\n10.08\n13.03\n5.84\n7.12\n    2013\n—\n8.71\n13.74\n5.91\n8.09\n    2014\n1.09\n5.02\n6.70\n6.41\n6.94\n    2015\n−0.20\n−1.57\n1.32\n10.67\n7.49\n    2016\n−2.26\n−2.83\n0.57\n6.29\n6.10\n    2017\n−0.60\n−1.11\n−0.53\n2.95\n4.25\n    2018\n0.64\n1.47\n−0.21\n3.75\n3.83\n    2019\n4.11\n5.00\n0.00\n4.31\n4.14\n    2020\n10.28\n9.08\n3.67\n4.52\n8.81\n    2021\n16.25\n6.34\n5.29\n10.06\n13.84\n    2022\n15.06\n1.17\n6.16\n5.78\n9.27\n    2023\n7.97\n1.43\n5.13\n4.62\n3.49\n  \n  \n  \n\n\n\n\n\n\nGráficos\nPara fazer uma comparação visual entre as séries é preciso definir um período comum para servir de base. No gráfico abaixo, ajusto as três séries em torno de seus valores médios em 2019. Vê-se como o comportamento das séries é muito similar nas séries históricas. Há uma divergência - cada vez maior - a partir de 2021 no período da pandemia. O IGMI-R registra um aumento significativo dos preços enquanto o IVG-R fica estagnado. Já o FipeZap registra uma alta muito mais tímida do que o IGMI-R.\n\n\n\n\n\n\n\n\n\nO gráfico abaixo encadeia todas as séries a partir de 2014. Este gráfico facilita a comparação dos índices no período mais recente.\n\n\n\n\n\n\n\n\n\nO próximo gráfico contrasta os índices de preços imobiliárias com a taxa de inflação geral da economia.\n\n\n\n\n\n\n\n\n\nPor fim, compara-se a evolução dos índices com relação aos custos de construção. Novamente, os preços finais dos imóveis não acompanham o aumento nos custos da construção civil, ao menos dentro desta janela de análise.\n\n\n\n\n\n\n\n\n\n\n\nCorrelação e Cointegração\nEm termos mais técnicos, pode-se ver que as séries naturalmente são correlacionadas, visto que elas estão tentando mensurar a mesma variável. Assim, não deve ser surpreendente que as séries tenham autocorrelação cruzada e que sejam cointegradas. O gráfico abaixo mostra a função de autocorrelação entre as séries após se tomar uma diferença simples e uma diferença sazonal.\n\n\n\n\n\n\n\n\n\nMais a título de curiosidade reporta-se as estimativas dos coeficientes dos termos de ajustamento do modelo de correção de erros, normalizadas segundo a série do IGMI.\n\n\n\n\n\n\n  \n    \n      \n      ect1\n      ect2\n    \n  \n  \n    IGMI.R.l3\n1.0000\n0.0000\n    IVG.R.l3\n0.0000\n1.0000\n    FipeZap.l3\n−3.4145\n−0.8616\n    trend.l3\n0.1856\n0.1271\n  \n  \n  \n\n\n\n\n\n\nLongo Prazo\nA maior janela de análise possível é comparar o IVG-R com os demais índices de preço da economia. Este gráfico é interessante, pois mostra como o preço dos imóveis no Brasil cresceu acima tanto da inflação como dos custos quando se olha o horizonte mais longo, desde 2001. Estes resultados constrastam com o verificado nos últimos 14 anos, desde 2010, como apontei em outro post. Uma discussão interessante sobre estes três indicadores está disponível em Lima (2022) num texto de discussão do Núcleo de Real Estate da Poli-USP.\nA dinâmica de preços de imóveis no Brasil, em partes reflete a própria economia brasileira. O ciclo de crescimento de preços é interrompido em 2014 e só volta a crescer em 2018, ganhando força em 2020 com a baixa taxa de juros."
  },
  {
    "objectID": "posts/general-posts/2024-02-house-prices-br/index.html#footnotes",
    "href": "posts/general-posts/2024-02-house-prices-br/index.html#footnotes",
    "title": "Índices de Preços Imobiliários no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUma discussão mais aprofundada da teoria de números-índice está disponível em Hill & Melser (2008), especificamente no contexto de imóveis. Os autores concluem que os índices de Fisher e de Törnqvist apresentam as melhores propriedades. A escolha entre um ou outro depende da forma funcional da equação de regressão utilizada no modelo hedônico. Num modelo semilog (onde a variável dependente, i.e., preço do imóvel, está em log) o melhor índice é o de Törnqvist.↩︎\nDo inglês Residential Property Price Index.↩︎\nUma lista de justificativas e usos de índices de preços imobiliários pode ser vista em …↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-03-carry-over/index.html",
    "href": "posts/general-posts/2024-03-carry-over/index.html",
    "title": "Ano de 2024 começa mais difícil",
    "section": "",
    "text": "O carry-over estatístico de 2024 está no nível mais baixo desde a Crise Econômica de 2015-17. Isto aponta um desafio ainda maior para as metas de crescimento.\n\n\n\n\n\n\n\n\n\nOlhando os dados históricos, é o pior carry-over desde 1995 quando se exclui os anos de recessão.\n\n\n\n\n\n\n  \n    \n      Ano\n      Carry-over\n    \n  \n  \n    1996 / 1997\n1.45\n    1997 / 1998\n1.44\n    1998 / 1999\n−0.40\n    1999 / 2000\n1.18\n    2000 / 2001\n1.86\n    2001 / 2002\n−0.66\n    2002 / 2003\n1.44\n    2003 / 2004\n0.99\n    2004 / 2005\n1.89\n    2005 / 2006\n0.99\n    2006 / 2007\n1.86\n    2007 / 2008\n2.05\n    2008 / 2009\n−1.59\n    2009 / 2010\n3.49\n    2010 / 2011\n1.78\n    2011 / 2012\n0.81\n    2012 / 2013\n1.33\n    2013 / 2014\n0.70\n    2014 / 2015\n−0.06\n    2015 / 2016\n−2.08\n    2016 / 2017\n−0.26\n    2017 / 2018\n0.78\n    2018 / 2019\n0.27\n    2019 / 2020\n0.72\n    2020 / 2021\n4.16\n    2021 / 2022\n0.68\n    2022 / 2023\n0.92\n    2023 / 2024\n0.20\n  \n  \n  \n\n\n\n\n\n\nO carry-over ou carry-over effect mensura qual seria a variação anual do PIB caso a economia estagnasse. No caso acima, toma-se o valor do PIB no último trimestre do ano e faz-se uma simulação de crescimento zero. O PIB no último trimstre de 2023 registrou o valor (indexado) de 183,88. Supondo que o país não crescesse em 2024, o valor do PIB no último trimestre de 2024 seria também de 183,88. Comparando este valor com o PIB médio de 2023 chega-se no valor de 0,2% apontado no gráfico acima.\nO carry-over é uma consequência direta da maneira como o PIB ou mais especificamente, o crescimento do PIB, costuma ser mensurado. Tipicamente, as agências de estatísticas reportam uma estimativa trimestral do PIB, que é dessazonalizada e comparada com o crescimento dos últimos quatro trimestres. Isto é, cria-se uma espeície de “ano móvel” para verificar a direção da economia. Isto significa que o crescimento anual do PIB depende da dinâmica dos quatro trimestres do ano anterior."
  },
  {
    "objectID": "posts/general-posts/2024-03-carry-over/index.html#o-que-é-o-carry-over",
    "href": "posts/general-posts/2024-03-carry-over/index.html#o-que-é-o-carry-over",
    "title": "Ano de 2024 começa mais difícil",
    "section": "",
    "text": "O carry-over ou carry-over effect mensura qual seria a variação anual do PIB caso a economia estagnasse. No caso acima, toma-se o valor do PIB no último trimestre do ano e faz-se uma simulação de crescimento zero. O PIB no último trimstre de 2023 registrou o valor (indexado) de 183,88. Supondo que o país não crescesse em 2024, o valor do PIB no último trimestre de 2024 seria também de 183,88. Comparando este valor com o PIB médio de 2023 chega-se no valor de 0,2% apontado no gráfico acima.\nO carry-over é uma consequência direta da maneira como o PIB ou mais especificamente, o crescimento do PIB, costuma ser mensurado. Tipicamente, as agências de estatísticas reportam uma estimativa trimestral do PIB, que é dessazonalizada e comparada com o crescimento dos últimos quatro trimestres. Isto é, cria-se uma espeície de “ano móvel” para verificar a direção da economia. Isto significa que o crescimento anual do PIB depende da dinâmica dos quatro trimestres do ano anterior."
  },
  {
    "objectID": "posts/general-posts/2024-03-ciclos-economicos/index.html",
    "href": "posts/general-posts/2024-03-ciclos-economicos/index.html",
    "title": "Recessões no Brasil",
    "section": "",
    "text": "Ciclos econômicos são flutuações recorrentes observadas empiricamente na atividade econômica de países. Estes ciclos são caracterizados por variações numa série de índices de atividade, emprego, investimento, etc. Estas flutuações geralmente seguem um padrão de expansão seguido por contração/recessão, formando o que é conhecido como um ciclo. Os ciclos econômicos são influenciados por uma variedade de fatores, incluindo mudanças na demanda do consumidor, políticas governamentais, inovações tecnológicas e choques externos (e.g. quebra de safra, pandemia, etc.).\nNo Brasil, a datação dos ciclos econômicos é feita pela CODACE1. Desde 1996, houve 6 recessões. A maioria das recessões foi relativamente curta, durando de 2 a 5 trimestres. A recessão mais longa foi a da Crise Econômica no segundo governo Dilma; oficialmente, a recessão começou no segundo trimestre de 2014 e foi até o quarto trimestre de 2016.\nO gráfico abaixo mostra a série trimestral do PIB, dessazonalizada, em número índice. A base do índice é a média de 1996. Nota-se que os períodos de expansão são mais frequentes do que os períodos de recessão: no período da amostra houve 87 trimestres de expansão contra 25 trimestres de recessão. Grosso modo, 1 a cada quatro trimestres apresentou uma recessão.\n\n\n\n\n\nO gráfico abaixo normaliza o valor do PIB em cada uma das recessões ao valor pré-crise, isto é, ao valor observado no trimestre imediatamente anterior ao do início da recessão. Comparativamente, a maior queda relativa registrada aconteceu na Crise do Covid, na primeira metade de 2020. Ao todo a queda foi de quase 11%.\n\n\n\n\n\nO período de recessão termina, grosso modo, quando a economia para de diminuir. Isto é, o final da recessão não implica que a economia já está operando no mesmo nível que estava antes da crise. É preciso alguns trimestres a mais de crescimento simplesmente para recuperar o que foi perdido, em termos de PIB.\nO segundo gráfico mostra justamente o esforço necessário para voltar ao patamar pré-crise. Na maioria dos casos, foi necessário mais 2-3 trimestres para que a economia recuperasse o que foi perdido na recessão. A exceção notável foi a crise de 2014-16: foi necessário mais de 5 anos, após o final da recessão, para que a economia voltasse ao mesmo nível que estava antes da crise. Isto é, apenas no segundo trimestre de 2022 a economia brasileira apresentou PIB acima do observado no primeiro trimestre de 2014.\n\n\n\n\n\nA tabela final abaixo resume alguns fatos sobre as recessões econômicas mais recentes da história econômica brasileira.\n\n\n\n\n\n\n  \n    \n      Período\n      Nome\n      Início\n      Fim\n      Recuperação\n      Duração Recessão\n      Tempo até Recuperação*\n      Crescimento Médio**\n      Perda Total\n    \n  \n  \n    1998Q1-1999Q1\nFHC-1\nQ1 1998\nQ1 1999\nQ4 1999\n5\n8\n−1.02%\n−1.31%\n    2001Q2-Q4\nFHC-2\nQ2 2001\nQ4 2001\nQ1 2002\n3\n4\n−1.93%\n−1.29%\n    2003Q1-2003Q2\nLULA\nQ1 2003\nQ2 2003\nQ4 2003\n2\n4\n−2.67%\n−1.15%\n    2008Q4-2009Q1\nGFR\nQ4 2008\nQ1 2009\nQ4 2009\n2\n5\n−14.10%\n−4.99%\n    2014Q2-2016Q4\nDILMA\nQ2 2014\nQ4 2016\nQ2 2022\n11\n33\n−4.94%\n−8.00%\n    2020Q1-2020Q2\nCOVID\nQ1 2020\nQ2 2020\nQ1 2021\n2\n5\n−30.72%\n−10.78%\n  \n  \n  \n    \n       (*) - Número de trimestres até a economia voltar ao patamar pré-crise\n    \n    \n       (**) - Crescimento trimestral médio anualizado"
  },
  {
    "objectID": "posts/general-posts/2024-03-ciclos-economicos/index.html#footnotes",
    "href": "posts/general-posts/2024-03-ciclos-economicos/index.html#footnotes",
    "title": "Recessões no Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nO Comitê de Datação de Ciclos Econômicos (CODACE) organizado pela Fundação Getúlio Vargas (FGV) se reúne periodicamente para datar os ciclos econômicos brasileiros. Para mais informações veja o site.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#resumindo",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#resumindo",
    "title": "O novo tidyverse: select",
    "section": "Resumindo",
    "text": "Resumindo\nEm resumo, temos quatro grupos gerais de tidyselectors.\n\nSeleção com base num padrão de texto. (matches, starts_with, ends_with, contains)\nSeleção com base num vetor de texto. (all_of, any_of)\nSeleção com base na “posição”. (last_col, everything, group_cols)\nSeleção com base na “classe”, i.e., numa função que retorna um valor lógico. (where)\n\nEstas funções auxiliares são muito importantes pois elas funcionam não somente com o select mas também com outras funções do pacote dplyr como mutate, rename, summarise e outras. Estas funções são relativamente recentes e marcam uma mudança considerável em relação às versões &lt;1 do dplyr, que utilizam sufixos (all, if, at) para diferenciar as funções como select_if, ou mutate_at."
  },
  {
    "objectID": "posts/general-posts/2024-01-tidyverse-select/index.html#veja-também",
    "href": "posts/general-posts/2024-01-tidyverse-select/index.html#veja-também",
    "title": "O novo tidyverse: select",
    "section": "Veja também",
    "text": "Veja também\n\nTidyselect\nTidyselect help"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#referências",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#referências",
    "title": "Pipes",
    "section": "Referências",
    "text": "Referências\n\nGuia de estilo do tidyverse\nDifferences between the base R and magrittr pipes\nChanges in R 4.0-4.1\nChanges in R 4.2.0\nChanges in R 4.3.0"
  },
  {
    "objectID": "posts/general-posts/2024-02-affordability-mapa/index.html#referências",
    "href": "posts/general-posts/2024-02-affordability-mapa/index.html#referências",
    "title": "Acessibilidade financeira à moradia em São Paulo",
    "section": "Referências",
    "text": "Referências\n\nVinícius Oike Reginatto & Fernando F.A. Souza & Lucas Hernandes Porto & Rafael Butt Ferna Farias, 2021. “Acesso à moradia em São Paulo: visão geral e mensuração,” LARES lares-2021-4drb, Latin American Real Estate Society (LARES).\nLi, Y. S., Li, A. H., Wang, Z. F., & Wu, Q. (2019). Analysis on housing affordability of urban residents in Mainland China based on multiple indexes: taking 35 cities as examples. Annals of Data Science, 6, 305-319.\nQian, H., Ma, X., Wang, Q., & Liu, C. (2015). Temporal and Spatial Variation of Housing Affordability in China. In Proceedings of the 19th International Symposium on Advancement of Construction Management and Real Estate(pp. 595-605). Springer Berlin Heidelberg.\nAcolin, A., & Green, R. K. (2017). Measuring housing affordability in São Paulo metropolitan region: Incorporating location. Cities, 62, 41-49."
  },
  {
    "objectID": "posts/general-posts/2024-03-affordability-sp/index.html",
    "href": "posts/general-posts/2024-03-affordability-sp/index.html",
    "title": "Housing Affordability em São Paulo",
    "section": "",
    "text": "O mapa abaixo apresenta o Price-Income-Ratio (PIR) por Zona OD na cidade de São Paulo. Os dados são de 2021 e provêm do artigo Acesso à moradia em São Paulo: visão geral e mensuração, publicado na LARES 2021. O PIR é um indicador simples que mensura a acessibilidade financeira à moradia numa região: sua interpretação é do tipo “quanto maior, pior”, isto é, quanto maior for o valor do PIR, pior é a acessibilidade financeira à moradia. O mapa abaixo compara o preço médio dos imóveis disponíveis1 nestas regiões com a renda média familiar bruta de São Paulo. Regiões centrais como Jardins e Alto de Pinheiros apresentam os piores indicadores de acessibilidade.\n\n\n\n\n\n\nDados: Acesso à moradia em São Paulo: visão geral e mensuração\nTipografia: Playfair Display\nPaleta: Reds (ColorBrewer)\n\n\n\nO PIR é uma razão simples entre o preço médio/mediano dos imóveis e a renda média/mediana anual das famílias:\n\\[\n\\text{PIR} = \\frac{\\text{Precos}}{\\text{Renda Anual}}\n\\]\nO PIR indica, grosso modo, a quantidade de “anos de trabalho” que uma família típica precisa investir de renda para comprar um imóvel típico. A principal vantagem do PIR é a sua simplicidade de cálculo, o que facilita comparações entre diferentes regiões. A principal desvantagem do PIR é de ignorar as condições de financiamento disponíveis para a população. Supondo que a renda anual média das famílias seja \\(R\\$30.000\\) e que o preço médio dos imóveis seja \\(R\\$270.000\\). Então o valor do PIR seria:\n\\[\n\\text{PIR} = \\frac{R\\$270.000}{R\\$30.000} = 9\n\\]\nNa literatura internacional, valores de PIR na faixa de 5-10 indicam baixa acessibilidade financeira. Estudos feitos em países em desenvolvimento e subdesenvolvidos, contudo, apontam valores de PIR muito mais elevados do que este intervalo, devido tanto à baixa renda média da população como também às condições limitadas de acesso ao crédito. Neste sentido, talvez uma comparação mais justa seja com as megacidades chinesas que tem PIR na faixa de 10-15. No artigo Acesso à moradia em São Paulo: visão geral e mensuração, escrito por mim em parceria com outros colegas, estima-se que o PIR de São Paulo em 2021 era próximo de 19-20. Mais a título de curiosidade, o site Numbeo apresenta o PIR para um amplo grupo de cidades no mundo2. Em 2021, segundo o site, o PIR de São Paulo era de 17,8."
  },
  {
    "objectID": "posts/general-posts/2024-03-affordability-sp/index.html#o-que-é-o-pir",
    "href": "posts/general-posts/2024-03-affordability-sp/index.html#o-que-é-o-pir",
    "title": "Housing Affordability em São Paulo",
    "section": "",
    "text": "O PIR é uma razão simples entre o preço médio/mediano dos imóveis e a renda média/mediana anual das famílias:\n\\[\n\\text{PIR} = \\frac{\\text{Precos}}{\\text{Renda Anual}}\n\\]\nO PIR indica, grosso modo, a quantidade de “anos de trabalho” que uma família típica precisa investir de renda para comprar um imóvel típico. A principal vantagem do PIR é a sua simplicidade de cálculo, o que facilita comparações entre diferentes regiões. A principal desvantagem do PIR é de ignorar as condições de financiamento disponíveis para a população. Supondo que a renda anual média das famílias seja \\(R\\$30.000\\) e que o preço médio dos imóveis seja \\(R\\$270.000\\). Então o valor do PIR seria:\n\\[\n\\text{PIR} = \\frac{R\\$270.000}{R\\$30.000} = 9\n\\]\nNa literatura internacional, valores de PIR na faixa de 5-10 indicam baixa acessibilidade financeira. Estudos feitos em países em desenvolvimento e subdesenvolvidos, contudo, apontam valores de PIR muito mais elevados do que este intervalo, devido tanto à baixa renda média da população como também às condições limitadas de acesso ao crédito. Neste sentido, talvez uma comparação mais justa seja com as megacidades chinesas que tem PIR na faixa de 10-15. No artigo Acesso à moradia em São Paulo: visão geral e mensuração, escrito por mim em parceria com outros colegas, estima-se que o PIR de São Paulo em 2021 era próximo de 19-20. Mais a título de curiosidade, o site Numbeo apresenta o PIR para um amplo grupo de cidades no mundo2. Em 2021, segundo o site, o PIR de São Paulo era de 17,8."
  },
  {
    "objectID": "posts/general-posts/2024-03-affordability-sp/index.html#footnotes",
    "href": "posts/general-posts/2024-03-affordability-sp/index.html#footnotes",
    "title": "Housing Affordability em São Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOs dados provêm de um webscrape dos principais portais de anúncios.↩︎\nÉ difícil apurar a qualidade dos dados utilizados no site.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "Agregando todos os acidentes de trânsito não-fatais vê-se que há dois principais clusters de acidentes: o maior deles começa na região do Centro Histórico e se estende até a região da Paulista e mais ao oeste chegando até o Itaim Bibi; o segundo cluster, menos intenso, aparece entre a região de Santo Amaro e Campo Limpo. No geral, os focos de acidente, seguem as principais de vias de tráfego.\nOs dados se referem a todos os acidentes de trânsito não-fatais registrados dentro do município de São Paulo em 2023.\n\n\n\n\n\n\n\n\nO painel de mapas abaixo subdivide os dados entre acidentes que ocorreram no final de semana x dias de trabalho e pelo período do dia. Os dados são normalizados dentro de cada célula para facilitar a leitura do padrão: em termos absolutos, a maior parte dos acidentes ocorre durante a manhã e a tarde nos dias úteis.\nÉ interessante notar como o padrão de acidentes se torna muito mais disperso nos finais de semana em relação aos dias úteis. Durante a semana, os acidentes à tarde estão quase que totalmente concentrados no Centro Histórico da cidade; já no final de semana, durante o mesmo período, os acidentes estão espacialmente dispersos por todas as zonas da cidade.\n\n\n\n\n\n\n\n\nAs horas do dia com maior número de acidente são, grosso modo, os horários de pico, quando há maior volume de veículos trafegando.\n\n\n\n\n\n* Dados: Painel de Dados SigaSP"
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#panorama",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#panorama",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "Agregando todos os acidentes de trânsito não-fatais vê-se que há dois principais clusters de acidentes: o maior deles começa na região do Centro Histórico e se estende até a região da Paulista e mais ao oeste chegando até o Itaim Bibi; o segundo cluster, menos intenso, aparece entre a região de Santo Amaro e Campo Limpo. No geral, os focos de acidente, seguem as principais de vias de tráfego.\nOs dados se referem a todos os acidentes de trânsito não-fatais registrados dentro do município de São Paulo em 2023."
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#hora-do-dia",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#hora-do-dia",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "As horas do dia com maior número de acidente são, grosso modo, os horários de pico, quando há maior volume de veículos trafegando.\n\n\n\n\n\n* Dados: Painel de Dados SigaSP"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "Brazilian geographical hierarchy comprises several fundamental concepts. From an administrative standpoint, the smallest delineations are either zip code areas or neighborhoods. While zip codes offer greater precision, their accessibility in terms of shapefiles is limited. Therefore, our focus primarily lies on neighborhoods.\nMoving up the hierarchy, we encounter cities (also referred to as municipalities), metropolitan regions, states (including the Distrito Federal), regional divisions, and the entirety of the country. As of 2022, Brazil is composed of 5,568 cities, 77 metropolitan regions plus 1 RIDE, 26 states, and 5 regional divisions.\nFrom a statistical perspective, the Brazilian Institute of Geography and Statistics (IBGE) delineates three other crucial spatial units: census tracts (setores censitários), weighting areas (áreas de ponderação), and the statistical grid. Finally, there are also other sets of cities, that are more specially defined than typical metropolitan regions:\n\nPopulation arrangements (arranjos populacionais): a grouping of two or more municipalities where there is a strong population integration due to commuting for work or study, or due to contiguity between the main urbanized areas.\nUrban Concentration areas (concentracoes urbanas): isolated cities or population arrangements with over 100,000 inhabitants.\nMeso Regions and Micro Regions:\nIntermediate Regions and Imediate Regions:\n\nAdditionally, there are other noteworthy spatial delineations, such as the Unidades de Desenvolvimento Humano, introduced by the Institute for Applied Economic Research (IPEA) in a 2013 study, and the Origin-Destination Zones, utilized by Metro to segment the São Paulo metropolitan region.\nIn summary,\nAdministrative Divsions\n\nMacro Regions\nStates\nMetropolitan Regions\nCities\nNeighborhood\nDistricts and subdistricts\nZip-code area\n\nStatistical Division\n\nWeighting Areas\nCensus Tracts\nStatistical Grid\n\n\n\nTo illustrate the above shapes consider the city of Curitiba. Curitiba is the capital city of the state of Paraná and the 8th most populous capital in Brazil and also the 8th smallest capital which helps keep the maps smaller. The state of Paraná is also relatively small.\nImporting these shapefiles into R is made very easy thanks to the excellent geobr package.\n\n\nCode\nlibrary(geobr)\nlibrary(censobr)\nlibrary(sidrar)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(MetBrewer)\nlibrary(h3)\nlibrary(areal)\n\ncode_cur &lt;- 4106902\n\n# Region borders\nregion &lt;- read_region(showProgress = FALSE)\n# State borders\nstate &lt;- read_state(showProgress = FALSE)\n# Metropolitan regions borders\nmetro &lt;- read_metro_area(showProgress = FALSE)\n# City border\nborder &lt;- read_municipality(code_cur, simplified = FALSE, showProgress = FALSE)\n# Neighborhoods\nnb &lt;- read_neighborhood(showProgress = FALSE)\n\n# Weighting areas \nwa &lt;- read_weighting_area(code_cur, showProgress = FALSE, simplified = FALSE)\n# Census tracts\nct &lt;- read_census_tract(code_cur, showProgress = FALSE, simplified = FALSE)\n\n\n\n\nThe map below shows where Paraná is in Brazil. Paraná is in the South region. As mentioned, there are 5 regions in Brazil: North, Northeast, Midwest, Southeast, and South.\n\n\nCode\n#&gt; Define a factor variable to signal the South region\nregion &lt;- region |&gt; \n  mutate(south = factor(if_else(name_region == \"Sul\", 1L, 0L)))\n\n#&gt; Get the centroid of the city of Curitiba\ncuritiba &lt;- st_centroid(border)\n\ntranslate &lt;- c(\n  \"Norte\" = \"North\",\n  \"Nordeste\" = \"Northeast\",\n  \"Centro Oeste\" = \"Midwest\",\n  \"Sul\" = \"South\",\n  \"Sudeste\" = \"Southeast\"\n  )\n\nregion_label &lt;- region %&gt;%\n  st_centroid() %&gt;%\n  mutate(label = stringr::str_replace_all(name_region, translate))\n\nggplot() +\n  geom_sf(data = region, aes(fill = name_region), color = \"white\", lwd = 0.15) +\n  geom_sf_label(\n    data = region_label,\n    aes(label = label),\n    family = \"Lato\",\n    label.padding = unit(0.15, \"lines\")\n    ) +\n  geom_sf(data = curitiba) +\n  scale_fill_met_d(\"Hokusai1\") +\n  labs(title = \"Regions in Brazil\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 26 states (equivalent to provinces in some countries). Distrito Federal is an importatn exception. Although it is equivalent to a state, it doesn’t contain cities but administrative regions (regiões administrativas). The capital of Brazil, Brasília, is located within the Distrito Federal.\n\n\nCode\nstate &lt;- state |&gt; \n  mutate(parana = factor(if_else(code_state == 41, 1L, 0L)))\n\nparana_label &lt;- state |&gt; \n  filter(code_state == 41) |&gt; \n  st_centroid()\n\nggplot() +\n  geom_sf(\n    data = state,\n    aes(fill = parana),\n    color = \"white\"\n    ) +\n  geom_sf(data = curitiba) +\n  geom_sf_label(\n    data = parana_label,\n    aes(label = name_state),\n    family = \"Lato\",\n    nudge_x = -1,\n    size = 3\n    ) +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  guides(fill = \"none\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown above, there exist several groupings of cities in Brazil, each with a distinct goal. To keep this post simple, I will focus only on metropolitan regions. As of 2018, the metropolitan region of Curitiba contained 29 cities.\n\n\nCode\nggplot() +\n  geom_sf(\n    data = cities_metro,\n    aes(fill = curitiba),\n    alpha = 0.85,\n    color = \"white\") +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  # scale_fill_manual(values = pal[c(2, 4)]) +\n  labs(title = \"Curitiba Metro Region\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nThe second map below highlights the name of each city.\n\n\nCode\ncities_metro &lt;- cities_metro %&gt;%\n  mutate(\n    name_label = stringr::str_wrap(name_muni, width = 8)\n  )\n\nggplot() +\n  geom_sf(data = cities_metro, aes(fill = name_muni)) +\n  geom_sf_label(\n    data = cities_metro,\n    aes(label = name_label),\n    size = 4) +\n  scale_fill_manual(values = MetBrewer::met.brewer(\"Hokusai1\", nrow(cities_metro))) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThe map below shows only the city of Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = border) +\n  labs(title = \"The City of Curitiba\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, there is an official definition of neighborhoods provided by IBGE for only some cities. Most notably, the city of São Paulo does not have an official definition of its neighborhoods. There are also districts and subdistricts but we ignore both of these for the purpose of this exposition.\nThere are 78 neighborhoods in Curitiba. The map below highlights all of them.\n\n\nCode\nnb &lt;- nb |&gt; \n  filter(code_muni == code_cur)\n\nggplot() +\n  geom_sf(data = nb, lwd = 0.15, aes(fill = name_neighborhood), color = \"white\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", nrow(nb))) +\n  labs(title = \"Neighborhoods of Curitiba\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncur_sg &lt;- qs::qread(\n  here::here(\"static/data/statistical_grid_cur.qs\")\n)\n\n\n\n\nThe smallest statistical subdivision available is the statistical grid. This is a varying size squared grid that offers a 200 x 200 m resolution in urban areas. It contains minimal information and is most useful as an intermediate shape in dasymetric interpolations. More specifically, it contains a population and household counts. This grid splits Curitiba into 11259 equally sized quadrants.\nThe map below shows the population counts at the statistical grid level. Since the area of all quadrants is the same, this data can be interpreted as the population density at the 200 x 200 m level.\n\n\nCode\nggplot(cur_sg) +\n  geom_sf(aes(fill = sqrt(POP), color = sqrt(POP))) +\n  scale_fill_distiller(palette = \"BuPu\", direction = 1) +\n  scale_color_distiller(palette = \"BuPu\", direction = 1) +\n  labs(title = \"Population count\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nCensus tracts are the strata used by IBGE in their decennial Census. The shape of each census tract usually respects administrative borders, land barriers, public spaces (parks, beaches, etc.), and follows the shape of the city blocks. Census tracts also exhibit relatively homogeneous socioeconomic and demographic characteristics. This makes census tracts a very useful statistical tool in regression analysis and classification.\nThe map below shows the 2395 census tracts in Curitiba.\n\n\nCode\nggplot(ct) +\n  geom_sf(lwd = 0.15) +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nIt is important to note that these shapes are not temporally consistent, meaning that they changed from 1991 to 2010 and then again from 2010 to 2020. Academics have devised methods to make the census tracts compatible over time and IBGE has also announced that they will facilitate backwards compatibility.\nAll census tracts contain basic information on population and households (age, race, sex, family configuration, household type, income group) and some information of the infrastructure of the region (garbage, sewage, trees, etc.). Their shape tries to create areas that share similar socioeconomic and demographic characteristics.\n\n\n\nThe weighting areas are an aggregation of census tracts for which there are much more detailed information. For instance, one can estimate the number of 3-bedroom apartments that are rented in a specific weighting area. Information on income, education, and housing quality are also available.\nThere are 55 weighting areas in Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = wa) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nTo illustrate how one can aggregate census tracts into weighting areas consider the map below that overlaps both. For this map I only take a subset of weighting areas near the city’s CBD.\n\n\nCode\n#&gt; Create a bounding box around the city's Central Business District (CBD)\ncbd_bbox &lt;- st_bbox(\n  c(ymin = -25.442283, ymax = -25.421747, xmin = -49.285610, xmax = -49.256937),\n  crs = 4326)\n#&gt; Create an identifier to filter after joins\ncbd_bbox &lt;- st_as_sfc(cbd_bbox)\ncbd_bbox &lt;- st_as_sf(cbd_bbox)\ncbd_bbox$gid &lt;- 1L\n\ncenter &lt;- st_coordinates(st_centroid(cbd_bbox))\n\ncbd_wa &lt;- wa |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nct_inside &lt;- ct %&gt;%\n  st_centroid() %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_join(cbd_wa) %&gt;%\n  filter(!is.na(code_weighting)) |&gt; \n  pull(code_tract) |&gt; \n  unique()\n\ncbd_ct &lt;- filter(ct, code_tract %in% ct_inside)\n\nm1 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\nm2 &lt;- ggplot() +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\nm3 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Overlap\") +\n  theme_vini\n\nlibrary(patchwork)\n\nm1 | m2 | m3\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many possible ways to bin the world into equal geometric shapes. Hexagons, squares, and triangles are the most commonly used. Below I show Uber’s H3 hexagons and a native st_make_grid() together with the official statistical grid shown previously.\n\n\nCode\n#&gt; Join statistical grid with CBD\ncbd_sg &lt;- cur_sg |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nhex_id &lt;- polyfill(cbd_bbox, res = 9)\nh3_grid &lt;- h3_to_geo_boundary_sf(hex_id)\n\ngrid_500 &lt;- cbd_bbox |&gt; \n  st_transform(crs = 32722) |&gt; \n  st_make_grid(500, square = FALSE, flat_topped = FALSE) |&gt; \n  st_as_sf() |&gt; \n  st_transform(crs = 4326)\n\nm1 &lt;- ggplot(cbd_sg) +\n  geom_sf() +\n  ggtitle(\"Statistical Grid\") +\n  theme_vini\n\nm2 &lt;- ggplot(h3_grid) +\n  geom_sf() +\n  ggtitle(\"Uber H3 (res. 9)\") +\n  theme_vini\n\nm3 &lt;- ggplot(grid_500) +\n  geom_sf() +\n  ggtitle(\"st_make_grid()\") +\n  theme_vini\n\nm1 | m2 | m3\n\n\n\n\n\n\n\n\n\nThe interactive map below illustrates the size of each hexagon in relation to the city. For simplicity I show only hexagons near the city center.\n\n\nCode\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(data = h3_grid, weight = 1) %&gt;%\n  addProviderTiles(providers$CartoDB)"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#administrative-divisions",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#administrative-divisions",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "To illustrate the above shapes consider the city of Curitiba. Curitiba is the capital city of the state of Paraná and the 8th most populous capital in Brazil and also the 8th smallest capital which helps keep the maps smaller. The state of Paraná is also relatively small.\nImporting these shapefiles into R is made very easy thanks to the excellent geobr package.\n\n\nCode\nlibrary(geobr)\nlibrary(censobr)\nlibrary(sidrar)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(MetBrewer)\nlibrary(h3)\nlibrary(areal)\n\ncode_cur &lt;- 4106902\n\n# Region borders\nregion &lt;- read_region(showProgress = FALSE)\n# State borders\nstate &lt;- read_state(showProgress = FALSE)\n# Metropolitan regions borders\nmetro &lt;- read_metro_area(showProgress = FALSE)\n# City border\nborder &lt;- read_municipality(code_cur, simplified = FALSE, showProgress = FALSE)\n# Neighborhoods\nnb &lt;- read_neighborhood(showProgress = FALSE)\n\n# Weighting areas \nwa &lt;- read_weighting_area(code_cur, showProgress = FALSE, simplified = FALSE)\n# Census tracts\nct &lt;- read_census_tract(code_cur, showProgress = FALSE, simplified = FALSE)\n\n\n\n\nThe map below shows where Paraná is in Brazil. Paraná is in the South region. As mentioned, there are 5 regions in Brazil: North, Northeast, Midwest, Southeast, and South.\n\n\nCode\n#&gt; Define a factor variable to signal the South region\nregion &lt;- region |&gt; \n  mutate(south = factor(if_else(name_region == \"Sul\", 1L, 0L)))\n\n#&gt; Get the centroid of the city of Curitiba\ncuritiba &lt;- st_centroid(border)\n\ntranslate &lt;- c(\n  \"Norte\" = \"North\",\n  \"Nordeste\" = \"Northeast\",\n  \"Centro Oeste\" = \"Midwest\",\n  \"Sul\" = \"South\",\n  \"Sudeste\" = \"Southeast\"\n  )\n\nregion_label &lt;- region %&gt;%\n  st_centroid() %&gt;%\n  mutate(label = stringr::str_replace_all(name_region, translate))\n\nggplot() +\n  geom_sf(data = region, aes(fill = name_region), color = \"white\", lwd = 0.15) +\n  geom_sf_label(\n    data = region_label,\n    aes(label = label),\n    family = \"Lato\",\n    label.padding = unit(0.15, \"lines\")\n    ) +\n  geom_sf(data = curitiba) +\n  scale_fill_met_d(\"Hokusai1\") +\n  labs(title = \"Regions in Brazil\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 26 states (equivalent to provinces in some countries). Distrito Federal is an importatn exception. Although it is equivalent to a state, it doesn’t contain cities but administrative regions (regiões administrativas). The capital of Brazil, Brasília, is located within the Distrito Federal.\n\n\nCode\nstate &lt;- state |&gt; \n  mutate(parana = factor(if_else(code_state == 41, 1L, 0L)))\n\nparana_label &lt;- state |&gt; \n  filter(code_state == 41) |&gt; \n  st_centroid()\n\nggplot() +\n  geom_sf(\n    data = state,\n    aes(fill = parana),\n    color = \"white\"\n    ) +\n  geom_sf(data = curitiba) +\n  geom_sf_label(\n    data = parana_label,\n    aes(label = name_state),\n    family = \"Lato\",\n    nudge_x = -1,\n    size = 3\n    ) +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  guides(fill = \"none\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nAs shown above, there exist several groupings of cities in Brazil, each with a distinct goal. To keep this post simple, I will focus only on metropolitan regions. As of 2018, the metropolitan region of Curitiba contained 29 cities.\n\n\nCode\nggplot() +\n  geom_sf(\n    data = cities_metro,\n    aes(fill = curitiba),\n    alpha = 0.85,\n    color = \"white\") +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", 3)[c(2, 3)]) +\n  # scale_fill_manual(values = pal[c(2, 4)]) +\n  labs(title = \"Curitiba Metro Region\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nThe second map below highlights the name of each city.\n\n\nCode\ncities_metro &lt;- cities_metro %&gt;%\n  mutate(\n    name_label = stringr::str_wrap(name_muni, width = 8)\n  )\n\nggplot() +\n  geom_sf(data = cities_metro, aes(fill = name_muni)) +\n  geom_sf_label(\n    data = cities_metro,\n    aes(label = name_label),\n    size = 4) +\n  scale_fill_manual(values = MetBrewer::met.brewer(\"Hokusai1\", nrow(cities_metro))) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nThe map below shows only the city of Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = border) +\n  labs(title = \"The City of Curitiba\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, there is an official definition of neighborhoods provided by IBGE for only some cities. Most notably, the city of São Paulo does not have an official definition of its neighborhoods. There are also districts and subdistricts but we ignore both of these for the purpose of this exposition.\nThere are 78 neighborhoods in Curitiba. The map below highlights all of them.\n\n\nCode\nnb &lt;- nb |&gt; \n  filter(code_muni == code_cur)\n\nggplot() +\n  geom_sf(data = nb, lwd = 0.15, aes(fill = name_neighborhood), color = \"white\") +\n  scale_fill_manual(values = met.brewer(\"Hokusai1\", nrow(nb))) +\n  labs(title = \"Neighborhoods of Curitiba\") +\n  theme_vini"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#statistical-subdivisions",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#statistical-subdivisions",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "Code\ncur_sg &lt;- qs::qread(\n  here::here(\"static/data/statistical_grid_cur.qs\")\n)\n\n\n\n\nThe smallest statistical subdivision available is the statistical grid. This is a varying size squared grid that offers a 200 x 200 m resolution in urban areas. It contains minimal information and is most useful as an intermediate shape in dasymetric interpolations. More specifically, it contains a population and household counts. This grid splits Curitiba into 11259 equally sized quadrants.\nThe map below shows the population counts at the statistical grid level. Since the area of all quadrants is the same, this data can be interpreted as the population density at the 200 x 200 m level.\n\n\nCode\nggplot(cur_sg) +\n  geom_sf(aes(fill = sqrt(POP), color = sqrt(POP))) +\n  scale_fill_distiller(palette = \"BuPu\", direction = 1) +\n  scale_color_distiller(palette = \"BuPu\", direction = 1) +\n  labs(title = \"Population count\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\n\nCensus tracts are the strata used by IBGE in their decennial Census. The shape of each census tract usually respects administrative borders, land barriers, public spaces (parks, beaches, etc.), and follows the shape of the city blocks. Census tracts also exhibit relatively homogeneous socioeconomic and demographic characteristics. This makes census tracts a very useful statistical tool in regression analysis and classification.\nThe map below shows the 2395 census tracts in Curitiba.\n\n\nCode\nggplot(ct) +\n  geom_sf(lwd = 0.15) +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nIt is important to note that these shapes are not temporally consistent, meaning that they changed from 1991 to 2010 and then again from 2010 to 2020. Academics have devised methods to make the census tracts compatible over time and IBGE has also announced that they will facilitate backwards compatibility.\nAll census tracts contain basic information on population and households (age, race, sex, family configuration, household type, income group) and some information of the infrastructure of the region (garbage, sewage, trees, etc.). Their shape tries to create areas that share similar socioeconomic and demographic characteristics.\n\n\n\nThe weighting areas are an aggregation of census tracts for which there are much more detailed information. For instance, one can estimate the number of 3-bedroom apartments that are rented in a specific weighting area. Information on income, education, and housing quality are also available.\nThere are 55 weighting areas in Curitiba.\n\n\nCode\nggplot() +\n  geom_sf(data = wa) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\n\n\n\n\n\n\n\n\nTo illustrate how one can aggregate census tracts into weighting areas consider the map below that overlaps both. For this map I only take a subset of weighting areas near the city’s CBD.\n\n\nCode\n#&gt; Create a bounding box around the city's Central Business District (CBD)\ncbd_bbox &lt;- st_bbox(\n  c(ymin = -25.442283, ymax = -25.421747, xmin = -49.285610, xmax = -49.256937),\n  crs = 4326)\n#&gt; Create an identifier to filter after joins\ncbd_bbox &lt;- st_as_sfc(cbd_bbox)\ncbd_bbox &lt;- st_as_sf(cbd_bbox)\ncbd_bbox$gid &lt;- 1L\n\ncenter &lt;- st_coordinates(st_centroid(cbd_bbox))\n\ncbd_wa &lt;- wa |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nct_inside &lt;- ct %&gt;%\n  st_centroid() %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_join(cbd_wa) %&gt;%\n  filter(!is.na(code_weighting)) |&gt; \n  pull(code_tract) |&gt; \n  unique()\n\ncbd_ct &lt;- filter(ct, code_tract %in% ct_inside)\n\nm1 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  labs(title = \"Weighting Areas\") +\n  theme_vini\n\nm2 &lt;- ggplot() +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Census Tracts\") +\n  theme_vini\n\nm3 &lt;- ggplot() +\n  geom_sf(data = cbd_wa, fill = NA, color = \"#35978f\", lwd = 1) +\n  geom_sf(data = cbd_ct, fill = NA, color = \"#bf812d\") +\n  labs(title = \"Overlap\") +\n  theme_vini\n\nlibrary(patchwork)\n\nm1 | m2 | m3"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#other-statistical-subdivisions",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#other-statistical-subdivisions",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "",
    "text": "There are many possible ways to bin the world into equal geometric shapes. Hexagons, squares, and triangles are the most commonly used. Below I show Uber’s H3 hexagons and a native st_make_grid() together with the official statistical grid shown previously.\n\n\nCode\n#&gt; Join statistical grid with CBD\ncbd_sg &lt;- cur_sg |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(cbd_bbox) |&gt; \n  filter(!is.na(gid))\n\nhex_id &lt;- polyfill(cbd_bbox, res = 9)\nh3_grid &lt;- h3_to_geo_boundary_sf(hex_id)\n\ngrid_500 &lt;- cbd_bbox |&gt; \n  st_transform(crs = 32722) |&gt; \n  st_make_grid(500, square = FALSE, flat_topped = FALSE) |&gt; \n  st_as_sf() |&gt; \n  st_transform(crs = 4326)\n\nm1 &lt;- ggplot(cbd_sg) +\n  geom_sf() +\n  ggtitle(\"Statistical Grid\") +\n  theme_vini\n\nm2 &lt;- ggplot(h3_grid) +\n  geom_sf() +\n  ggtitle(\"Uber H3 (res. 9)\") +\n  theme_vini\n\nm3 &lt;- ggplot(grid_500) +\n  geom_sf() +\n  ggtitle(\"st_make_grid()\") +\n  theme_vini\n\nm1 | m2 | m3\n\n\n\n\n\n\n\n\n\nThe interactive map below illustrates the size of each hexagon in relation to the city. For simplicity I show only hexagons near the city center.\n\n\nCode\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(data = h3_grid, weight = 1) %&gt;%\n  addProviderTiles(providers$CartoDB)"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#neighborhoods-by-population-density",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#neighborhoods-by-population-density",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "Neighborhoods by population density",
    "text": "Neighborhoods by population density\nWe will make a simple map that shows the population and population density of each neighborhood in Curitiba. The first step is to get the population data from SIDRA using the sidrar package.\n\n\nCode\n# Import population table from SIDRA\npop_nb &lt;- sidrar::get_sidra(\n  x = 1378,\n  variable = 93,\n  classific = \"c2\",\n  geo = \"Neighborhood\",\n  geo.filter = list(\"City\" = code_cur)\n)\n\n# Clean table and make it wide\npop_nb &lt;- pop_nb |&gt; \n  as_tibble() |&gt; \n  janitor::clean_names() |&gt; \n  select(code_neighborhood = bairro_codigo, sex = sexo, count = valor) |&gt; \n  mutate(\n    sex = str_replace(sex, \"Homens\", \"Male\"),\n    sex = str_replace(sex, \"Mulheres\", \"Female\")\n    ) |&gt; \n  pivot_wider(\n    id_cols = \"code_neighborhood\",\n    names_from = \"sex\",\n    values_from = \"count\"\n    )\n\n# Make neighborhood codes compatible\npop_nb &lt;- pop_nb |&gt; \n  mutate(code_neighborhood = str_c(\n    str_sub(code_neighborhood, 1, 7), \"05\", str_sub(code_neighborhood, 8, 10))\n    )\n# Join census table with shapefile\ncur_nb &lt;- left_join(nb, pop_nb, by = \"code_neighborhood\")\n\n# Calculate population density\ncur_nb &lt;- cur_nb %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  mutate(\n    area = st_area(.),\n    area = as.numeric(area) / 1e5,\n    pop_dens = Total / area,\n    pop_ntile = ntile(pop_dens, 5)\n    )\n# Convert back to 4326 for leaflet\ncur_nb &lt;- st_transform(cur_nb, crs = 4326)\n\n\nAfter cleaning and merging the datasets we can make an interactive map using leaflet.\n\n\nCode\n# Color palette and bins\nbins &lt;- quantile(cur_nb$pop_dens, probs = seq(0.2, 0.8, 0.2))\nbins &lt;- c(0, bins, max(cur_nb$pop_dens))\npal &lt;- colorBin(\n  palette = as.character(met.brewer(\"Hokusai2\", 5)),\n  domain = cur_nb$pop_dens,\n  bins = bins)\n\n# Labels\nlabels &lt;- sprintf(\n  \"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt; %s people &lt;br/&gt; %g people / ha&lt;sup&gt;2&lt;/sup&gt;\",\n  cur_nb$name_neighborhood,\n  format(cur_nb$Total, big.mark = \".\"),\n  round(cur_nb$pop_dens, 1)\n  )\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\n# Center of the map for zoom\ncenter &lt;- st_coordinates(st_centroid(border))\n\nleaflet(cur_nb) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    color = \"white\",\n    fillColor = ~pal(pop_dens),\n    fillOpacity = 0.8,\n    highlightOptions = highlightOptions(\n      color = \"gray20\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~pop_dens,\n    title = \"Densidade Pop.\",\n    position = \"bottomright\"\n  ) %&gt;%\n  addProviderTiles(providers$CartoDB) %&gt;%\n  setView(lng = center[1], lat = center[2], zoom = 11)"
  },
  {
    "objectID": "posts/general-posts/2024-04-brazil-shapes/index.html#interpolation",
    "href": "posts/general-posts/2024-04-brazil-shapes/index.html#interpolation",
    "title": "Administrative and Statistical Divisions in Brazil",
    "section": "Interpolation",
    "text": "Interpolation\nWhen using different shapefiles it’s necessary to interpolate data from the “source” shapefile to the desired “target” shapefile. For instance, we might have the total number of apartments by census tract (source) but desire to have the same number by H3 hexagons (target). In this example, the number of apartments would be our target variable that should be interpolated from one shape to another.\nFor simplicity sake, I show how to make a simple areal interpolation from Curitiba’s census tracts to a hexagonal H3 grid.\n\nImport data\nI import census tract level socioeconomic data using censobr. I retrieve the total number of houses, apartments, and some information on housing ownership (i.e. rented, owned, etc.).\n\n\nCode\nlibrary(censobr)\n\nhh &lt;- read_tracts(dataset = \"Domicilio\", showProgress = FALSE)\n\ncur_domicilios &lt;- hh |&gt; \n  filter(code_muni == code_cur) |&gt; \n  select(code_tract, domicilio01_V002:domicilio01_V011) |&gt; \n  collect()\n\ncol_names &lt;- c(\n  \"hh_total\", \"hh_house\", \"hh_cndm\", \"hh_apt\",\n  \"hh_owned1\", \"hh_owned2\", \"hh_rented\", \"hh_cedido1\", \"hh_cedido2\", \"hh_other\"\n)\n\nnames(cur_domicilios)[-1] &lt;- col_names\n\ncur_census &lt;- left_join(ct, cur_domicilios, by = \"code_tract\")\n\n\nThe map below shows the total number of apartments in Curitiba by census tract.\n\n\nCode\nggplot(cur_census) +\n  geom_sf(aes(fill = sqrt(hh_apt), color = sqrt(hh_apt))) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\n\nInterpolation\nTo interpolate we use the areal package. It requires the shapefile to be inputed in a planar projection.\n\n\nCode\nindex &lt;- h3::polyfill(border, res = 9)\nh3_grid &lt;- h3::h3_to_geo_boundary_sf(index)\n\nh3_grid &lt;- st_transform(h3_grid, crs = 31984)\ncur_census &lt;- st_transform(cur_census, crs = 31984)\n\n#&gt; Compute areal interpolation\ninterp &lt;- aw_interpolate(\n  h3_grid,\n  tid = h3_index,\n  source = cur_census,\n  sid = \"code_tract\",\n  weight = \"sum\",\n  extensive = \"hh_apt\"\n)\n\ninterp &lt;- st_transform(interp, crs = 4326)\n\n\n\n\nResults\nFinally, the panel below shows the result of the interpolation.\n\n\nCode\nm1 &lt;- ggplot(cur_census) +\n  geom_sf(aes(fill = sqrt(hh_apt), color = sqrt(hh_apt))) +\n  ggtitle(\"Census Tract\") +\n  scale_fill_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  scale_color_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  ggthemes::theme_map(base_family = \"Lato\")\n\nm2 &lt;- ggplot(h3_grid) +\n  ggtitle(\"H3 (res. 9)\") +\n  geom_sf(lwd = 0.05) +\n  theme_vini\n\nm3 &lt;- ggplot(interp) +\n  geom_sf(aes(fill = sqrt(hh_apt), color = sqrt(hh_apt))) +\n  scale_fill_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  scale_color_distiller(\n    name = \"Apartaments (total)\",\n    palette = \"BuPu\",\n    breaks = c(5, 10, 15, 20, 25),\n    labels = c(5, 10, 15, 20, 25)^2,\n    direction = 1) +\n  ggtitle(\"Interpolated\") +\n  ggthemes::theme_map(base_family = \"Lato\")\n\npanel &lt;- m1 | m2 | m3\n\npanel + plot_layout(guides = \"collect\") &\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = 0.5,\n    legend.key.size = unit(0.75, \"cm\"),\n    legend.key.width = unit(1, \"cm\"),\n    legend.text = element_text(size = 12),\n    text = element_text(family = \"Lato\")\n    )"
  },
  {
    "objectID": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#finais-de-semana",
    "href": "posts/general-posts/2024-03-wz-acidentes-carro/index.html#finais-de-semana",
    "title": "Acidentes de Trânsito em São Paulo",
    "section": "",
    "text": "O painel de mapas abaixo subdivide os dados entre acidentes que ocorreram no final de semana x dias de trabalho e pelo período do dia. Os dados são normalizados dentro de cada célula para facilitar a leitura do padrão: em termos absolutos, a maior parte dos acidentes ocorre durante a manhã e a tarde nos dias úteis.\nÉ interessante notar como o padrão de acidentes se torna muito mais disperso nos finais de semana em relação aos dias úteis. Durante a semana, os acidentes à tarde estão quase que totalmente concentrados no Centro Histórico da cidade; já no final de semana, durante o mesmo período, os acidentes estão espacialmente dispersos por todas as zonas da cidade."
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html",
    "title": "Casas e Apartamentos",
    "section": "",
    "text": "A vasta maioria dos domicílios residenciais no Brasil são casas1. De fato, em apenas 3, das mais de 5000 cidades brasileiras, o número de apartamentos é superior ao número de casas. Isto é, apenas em Santos, Balneário Camboriú e São Caetano do Sul, existe uma proporção maior de apartamentos do que de casas. Embora São Paulo seja famosa por seus prédios, os apartamentos representam apenas um terço das moradias na cidade.\nA tabela abaixo resume os dados para as vinte cidade mais “verticalizadas” do Brasil.\n\n\n\n\n\n\n  \n    \n      Nome Cidade\n      UF\n      Casa\n      Apartamento\n      Outros\n      Total Domicílios\n    \n  \n  \n    Santos\nSP\n31,9%\n67,1%\n1,0%\n167.478\n    Balneário Camboriú\nSC\n36,4%\n63,3%\n0,3%\n57.862\n    São Caetano do Sul\nSP\n46,3%\n52,5%\n1,2%\n61.995\n    Porto Alegre\nRS\n50,3%\n49,6%\n0,2%\n558.252\n    Vitória\nES\n50,4%\n49,0%\n0,6%\n128.617\n    São José\nSC\n53,9%\n45,9%\n0,2%\n105.580\n    Viçosa\nMG\n54,8%\n45,0%\n0,2%\n29.055\n    Florianópolis\nSC\n55,7%\n44,0%\n0,3%\n219.739\n    Niterói\nRJ\n55,8%\n44,0%\n0,2%\n194.492\n    Itapema\nSC\n56,2%\n43,5%\n0,3%\n27.423\n    João Pessoa\nPB\n58,5%\n41,3%\n0,2%\n296.249\n    Vila Velha\nES\n58,8%\n40,8%\n0,4%\n177.116\n    Rio de Janeiro\nRJ\n59,7%\n39,5%\n0,8%\n2.436.971\n    Valparaíso de Goiás\nGO\n60,2%\n38,9%\n0,9%\n71.169\n    Belo Horizonte\nMG\n60,2%\n38,9%\n0,9%\n889.584\n    Bento Gonçalves\nRS\n62,3%\n37,7%\n0,0%\n48.813\n    Juiz de Fora\nMG\n62,7%\n37,1%\n0,2%\n210.953\n    Castelo\nES\n64,7%\n35,2%\n0,1%\n14.229\n    Santo André\nSP\n64,7%\n34,7%\n0,6%\n280.389\n    Brasília\nDF\n64,9%\n34,2%\n0,9%\n988.191\n  \n  \n  \n\n\n\n\n\n\nPode-se imaginar que cidades maiores são mais verticalizadas, na média, do que cidades menores. Isto não parece ser o caso no Brasil. O gráfico abaixo mostra a relação entre o número total de domicílios e o share de apartamentos nas maiores cidades brasileiras. Considerou-se somente cidades com pelo menos 50 mil domicílios (grosso modo, 150 mil habitantes).\nOs dados apresentam uma correlação muito fraca e grande variabilidade. De fato, não parece haver relação alguma entre os dados.\n\n\n\n\n\n\n\n\n\nA tabela abaixo ilustra este fato. A tabela mostra todas as cidades na faixa de 150 a 200 mil domicílios. A cidade com maior percentual de apartamentos é Santos: 67% dos seus 167 mil domicílios são apartamentos; um contraste enorme com São João de Meriti: menos de 5% dos seus 168 domicílios são apartamentos.\n\n\n\n\n\n\n  \n    \n      Nome Cidade\n      UF\n      Apartamento\n      Total Domicílios\n    \n  \n  \n    Santos\nSP\n67,1%\n167.478\n    Niterói\nRJ\n44,0%\n194.492\n    Vila Velha\nES\n40,8%\n177.116\n    Caxias do Sul\nRS\n33,9%\n184.843\n    Jundiaí\nSP\n30,6%\n163.219\n    Maringá\nPR\n29,1%\n156.517\n    Piracicaba\nSP\n23,5%\n155.428\n    Mogi das Cruzes\nSP\n22,3%\n158.693\n    São José do Rio Preto\nSP\n20,9%\n185.633\n    Serra\nES\n20,0%\n191.936\n    Porto Velho\nRO\n16,2%\n151.905\n    Campos dos Goytacazes\nRJ\n13,3%\n175.744\n    Ananindeua\nPA\n12,9%\n154.891\n    Mauá\nSP\n12,2%\n152.619\n    Aparecida de Goiânia\nGO\n10,5%\n186.221\n    Belford Roxo\nRJ\n7,3%\n180.893\n    São João de Meriti\nRJ\n4,3%\n168.771\n  \n  \n  \n\n\n\n\nComparando o share de apartamentos com o tamanho do município2, chega-se na mesma conclusão. O gráfico abaixo mostra o percentual de domicílios contra a área total do município. Seria intuitivo que municípios menores, onde há menos oferta de terra, exibissem maiores percentuais de apartamentos. Contudo, não parece haver relação entre as variáveis.\n\n\n\n\n\n\n\n\n\nNa faixa de 450 a 500 hectares, temos 20 municípios, a maioria com menos de 5% de apartamentos. Neste contexto, Balneário Camboriú é um outlier notável com mais de 60% de apartamentos.\n\n\n\n\n\n\n  \n    \n      Nome Cidade\n      UF\n      Apartamento\n      Total Domicílios\n      Área (ha.)\n    \n  \n  \n    Balneário Camboriú\nSC\n63,3%\n57.862\n475\n    Lindóia\nSP\n23,7%\n2.591\n495\n    Cachoeirinha\nRS\n14,6%\n52.484\n453\n    São José da Lapa\nMG\n10,4%\n8.973\n480\n    Harmonia\nRS\n9,0%\n2.022\n462\n    Vale Real\nRS\n8,5%\n2.263\n466\n    São Sebastião de Lagoa de Roça\nPB\n6,9%\n3.764\n500\n    Bandeira do Sul\nMG\n3,5%\n2.232\n478\n    Francisco Morato\nSP\n3,4%\n57.668\n498\n    Santana do São Francisco\nSE\n1,6%\n2.366\n453\n    Mato Leitão\nRS\n1,3%\n1.883\n476\n    Bom Jesus\nPB\n0,5%\n805\n475\n    Ribeirão Vermelho\nMG\n0,4%\n1.470\n496\n    Nova Floresta\nPB\n0,4%\n3.577\n477\n    Chã de Alegria\nPE\n0,3%\n4.384\n487\n    Carmópolis\nSE\n0,2%\n4.689\n461\n    Vila Flor\nRN\n0,1%\n987\n477\n    Belém\nAL\n0,1%\n1.676\n489\n    Palestina\nAL\n0,0%\n1.321\n490\n    Telha\nSE\n0,0%\n1.117\n487\n  \n  \n  \n\n\n\n\n\n\n\nA lista completa dos dados pode ser acessada na tabela abaixo."
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html#tamanho-do-município",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html#tamanho-do-município",
    "title": "Casas e Apartamentos",
    "section": "",
    "text": "Pode-se imaginar que cidades maiores são mais verticalizadas, na média, do que cidades menores. Isto não parece ser o caso no Brasil. O gráfico abaixo mostra a relação entre o número total de domicílios e o share de apartamentos nas maiores cidades brasileiras. Considerou-se somente cidades com pelo menos 50 mil domicílios (grosso modo, 150 mil habitantes).\nOs dados apresentam uma correlação muito fraca e grande variabilidade. De fato, não parece haver relação alguma entre os dados.\n\n\n\n\n\n\n\n\n\nA tabela abaixo ilustra este fato. A tabela mostra todas as cidades na faixa de 150 a 200 mil domicílios. A cidade com maior percentual de apartamentos é Santos: 67% dos seus 167 mil domicílios são apartamentos; um contraste enorme com São João de Meriti: menos de 5% dos seus 168 domicílios são apartamentos.\n\n\n\n\n\n\n  \n    \n      Nome Cidade\n      UF\n      Apartamento\n      Total Domicílios\n    \n  \n  \n    Santos\nSP\n67,1%\n167.478\n    Niterói\nRJ\n44,0%\n194.492\n    Vila Velha\nES\n40,8%\n177.116\n    Caxias do Sul\nRS\n33,9%\n184.843\n    Jundiaí\nSP\n30,6%\n163.219\n    Maringá\nPR\n29,1%\n156.517\n    Piracicaba\nSP\n23,5%\n155.428\n    Mogi das Cruzes\nSP\n22,3%\n158.693\n    São José do Rio Preto\nSP\n20,9%\n185.633\n    Serra\nES\n20,0%\n191.936\n    Porto Velho\nRO\n16,2%\n151.905\n    Campos dos Goytacazes\nRJ\n13,3%\n175.744\n    Ananindeua\nPA\n12,9%\n154.891\n    Mauá\nSP\n12,2%\n152.619\n    Aparecida de Goiânia\nGO\n10,5%\n186.221\n    Belford Roxo\nRJ\n7,3%\n180.893\n    São João de Meriti\nRJ\n4,3%\n168.771\n  \n  \n  \n\n\n\n\nComparando o share de apartamentos com o tamanho do município2, chega-se na mesma conclusão. O gráfico abaixo mostra o percentual de domicílios contra a área total do município. Seria intuitivo que municípios menores, onde há menos oferta de terra, exibissem maiores percentuais de apartamentos. Contudo, não parece haver relação entre as variáveis.\n\n\n\n\n\n\n\n\n\nNa faixa de 450 a 500 hectares, temos 20 municípios, a maioria com menos de 5% de apartamentos. Neste contexto, Balneário Camboriú é um outlier notável com mais de 60% de apartamentos.\n\n\n\n\n\n\n  \n    \n      Nome Cidade\n      UF\n      Apartamento\n      Total Domicílios\n      Área (ha.)\n    \n  \n  \n    Balneário Camboriú\nSC\n63,3%\n57.862\n475\n    Lindóia\nSP\n23,7%\n2.591\n495\n    Cachoeirinha\nRS\n14,6%\n52.484\n453\n    São José da Lapa\nMG\n10,4%\n8.973\n480\n    Harmonia\nRS\n9,0%\n2.022\n462\n    Vale Real\nRS\n8,5%\n2.263\n466\n    São Sebastião de Lagoa de Roça\nPB\n6,9%\n3.764\n500\n    Bandeira do Sul\nMG\n3,5%\n2.232\n478\n    Francisco Morato\nSP\n3,4%\n57.668\n498\n    Santana do São Francisco\nSE\n1,6%\n2.366\n453\n    Mato Leitão\nRS\n1,3%\n1.883\n476\n    Bom Jesus\nPB\n0,5%\n805\n475\n    Ribeirão Vermelho\nMG\n0,4%\n1.470\n496\n    Nova Floresta\nPB\n0,4%\n3.577\n477\n    Chã de Alegria\nPE\n0,3%\n4.384\n487\n    Carmópolis\nSE\n0,2%\n4.689\n461\n    Vila Flor\nRN\n0,1%\n987\n477\n    Belém\nAL\n0,1%\n1.676\n489\n    Palestina\nAL\n0,0%\n1.321\n490\n    Telha\nSE\n0,0%\n1.117\n487"
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html#tabela-completa",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html#tabela-completa",
    "title": "Casas e Apartamentos",
    "section": "",
    "text": "A lista completa dos dados pode ser acessada na tabela abaixo."
  },
  {
    "objectID": "posts/general-posts/2024-03-censo-apartamentos/index.html#footnotes",
    "href": "posts/general-posts/2024-03-censo-apartamentos/index.html#footnotes",
    "title": "Casas e Apartamentos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale notar que os dados refletem o número total de domicílios e não o número total de pessoas morando em domicílios (por tipo de domicílio).↩︎\nExiste uma leve distorção no cálculo da área do município. Calculou-se a área total do perímetro municipal, o que penaliza cidades com muitas áreas verdes. Cidades como Campo Grande e Manaus, por exemplo, tem uma área total muito superior à área urbana. Conseguir o shapefile somente da área urbana da cidade, contudo, não é tarefa simples.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/economist.html",
    "href": "posts/general-posts/2023-12-wz-inflation/economist.html",
    "title": "Brazilian Inflation",
    "section": "",
    "text": "Brazil has a long and painful story with inflation. Though technically Brazilian inflation only classified as a hyperinflation during a brief period in the early 1990s1, few countries had to endure such a long-standing period of prolonged and relentless inflation. The chart below shows the monthly percentage change in Brazil’s official consumer price index IPCA. I highlight some important economic and institutional events. Chiefs of the executive are also shaded in through different colors. It’s important to note that Figueiredo was not a constitutionally elected president, but rather the last leader of the Military Regime.\nThis visualization is heavily inspired by the graphic from the book Saga Brasileira: a longa luta de um povo por sua moeda2, by Miriam Leitão. Total inflation during the “hyperinflation” period surpassed 13 trillion percent! In July of 1994 the ingeniously designed Plano Real stabilizes Brazil’s economy and ends a long period of persistently high inflation. The Brazil post-Plano Real represents a paradigm shift, yet it is not entirely immune to inflation. In the nearly 30 years since the implementation of Plano Real, Brazil has witnessed a cumulative inflation of 560%.\nA nuance that is not captured by the graphic is the frequency of currency changes during this era3. Nearly every new economic plan brought about either a change in currency or a price/wage freeze. The dips in the plot coincide with theses moments, marking brief lapses of time when inflation seemed to magically subside, only to return with renewed vigor.\nOBS: to better see the plot: right-click &gt; “Open image in new tab”.\n\n\n\n\n\n\n\nEven after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Two significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the subsequent decade, the looming threat of inflation resurfaced. Facing a slowing economy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices were increased drastically causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer assumed office.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence came in early 2021. The Covid-19 inflation surge was tackled with inordinary high interest rates: in fact, for many months Brazil had the highest real interest rate in the world! After nearly a year of tight monetary policy, the Brazilian Central Bank started to cut interest rates as inflation converged to its long-run target."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/economist.html#post-plano-real",
    "href": "posts/general-posts/2023-12-wz-inflation/economist.html#post-plano-real",
    "title": "Brazilian Inflation",
    "section": "",
    "text": "Even after the ingenious Plano Real, it took some more effort to bring down inflation. The second chart highlights the post-Real Brazil and now presents the year-on-year percent changes in consumer prices. Since Plano Real was implemented in July of 1994, I exclude the first twelve months to avoid contaminating the series with previous hyperinflation.\n\n\n\n\n\nBrazil’s CPI index dropped steadily after Plano Real and reached its lowest point in decades at the end of the 1990s. The series of economic crisis that began in Asia, spread across other emerging economies and eventually forced the Brazilian Central Bank to devalue the Real and abandon its fixed exchange rate to the USD. Two significant institutional changes were introduced in the late 1990s. First, the adoption of a formal inflation target: starting at 8% and decreasing to 4% in 2001, with a 2% interval of tolerance. Second, the implementation of a new fiscal regime, known as the Law of Fiscal Responsibility, sought to streamline public finance administration and limit spending.\nIn the 2000s, inflation in Brazil was “higher than comfort”. Despite fiscal surpluses and high interest rates, the country struggled to keep inflation within its revised 4.5% target. The overall outlook of the economy was much more positive. The commodity boom resulted in bigger commercial surpluses, and the country’s fiscal stability and solid institutions made it an attractive destination for foreign investment. Even during the 2008 financial crisis, Brazil remained relatively unscathed.\nIn the subsequent decade, the looming threat of inflation resurfaced. Facing a slowing economy, the Brazilian government launched an aggressive pro-growth agenda. Government spending rose (with little to no accountability), interest rates were cut, and the Real devalued significantly. New subsidized credit lines and tax reliefs aimed to improve business conditions and foster growth. To contain the rising inflation that resulted from this massive injection of stimulus, arbitrary price controls were implemented. Bus fares, electric energy, and even oil prices were tinkered with to help curb rising prices. In 2015, Brazil was forced to come back to terms with reality: administered prices were increased drastically causing a surge in inflation and a further deterioration of macroeconomic conditions. In August 2016, president Dilma Roussef is impeached and vice-president Michel Temer assumed office.\nIn the period that followed, inflation was much lower and important fiscal measures such as the Expenditure Ceiling and Pensions Reform were passed. Central Bank independence came in early 2021. The Covid-19 inflation surge was tackled with inordinary high interest rates: in fact, for many months Brazil had the highest real interest rate in the world! After nearly a year of tight monetary policy, the Brazilian Central Bank started to cut interest rates as inflation converged to its long-run target."
  },
  {
    "objectID": "posts/general-posts/2023-12-wz-inflation/economist.html#footnotes",
    "href": "posts/general-posts/2023-12-wz-inflation/economist.html#footnotes",
    "title": "Brazilian Inflation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Hyperinflation_in_Brazil↩︎\nLoosely translated: “The Brazilian Saga: the long struggle of a people for its currency”.↩︎\nBefore 1986, Brazil’s currency was the Cruzeiro, though it had been “updated” several times to accommodate inflation. After the Cruzado Plan, the official currency became the “Cruzado”. In 1989, it transitioned to the “Cruzado Novo” (New Cruzado), following the Verão Plan. With the Collor Plan, in 1990, came a new currency that borrowed the old “Cruzeiro” name. Ultimately, in the lead-up to the Real, Brazil introduced a “virtual currency” named URV and the Cruzeiro was renamed as “Cruzeiro Real”.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-03-starbucks-scrape/index.html",
    "href": "posts/general-posts/2024-03-starbucks-scrape/index.html",
    "title": "Encontrando todos os Starbucks do Brasil",
    "section": "",
    "text": "O processo de web scraping consiste em extrair informação de uma página na internet. A dificuldade ou facilidade em extrair esta informação depende da qualidade de construção da página. Em alguns casos mais complexos, a informação pode estar atrás de um captcha ou num painel interativo que depende de outros comandos do usuário.\nNeste exemplo simples vou mostrar como conseguir encontrar a localização de todas as unidades da Starbucks no Brasil. A lista completa das lojas em atividade da Starbucks pode ser encontrada no site do Starbucks Brasil. Como de praxe, vamos utilizar o tidyverse em conjunto com os pacotes rvest e xml2.\n\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tidyverse)\n\n\n\nA lista completa das lojas em atividade da Starbucks pode ser encontrada no site do Starbucks Brasil. Para ler a página usa-se read_html.\n\nurl = \"https://starbucks.com.br/lojas\"\n\npage = xml2::read_html(url)\n\nO “xpath” mostra o caminho até um determinado elemento na página. Para encontrar o logo do Starbucks, no canto superior-esquerdo da página, por exemplo podemos usar o código abaixo.\n\npage %&gt;%\n  html_element(xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\")\n\n{html_node}\n&lt;img alt=\"Starbucks Logo\" src=\"/public/img/icons/starbucks-nav-logo.svg\"&gt;\n\n\nPara ver mais sobre xpaths consulte este cheatsheet.\nEm geral, em páginas bem construídas, o nome dos elementos será bastante auto-explicativo. No caso acima, o atributo “alt” já indica que é objeto é o logo da starbucks e o “src” direciona para um arquivo em formato svg (imagem) chamado starbucks-nav-logo. Infelizmente, isto nem sempre será o caso. Em algumas páginas os elementos podem ser bastante confusos.\nPara puxar um atributo específico usamos a função html_attr.\n\npage %&gt;%\n  html_element(\n    xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\"\n    ) %&gt;%\n  html_attr(\"src\")\n\n[1] \"/public/img/icons/starbucks-nav-logo.svg\"\n\n\nSe você combinar este último link a “www.starbucks.com.br” você deve chegar numa imagem com o logo da empresa1.\n\n\n\n\n\nPara encontrar a grande lista de lojas no painel da esquerda vamos aproveitar o fato de que o div que guarda esta lista tem uma classe única chamada “place-list”. É fácil verificar isto no próprio navegador. Se você usar o Chrome, por exemplo, basta clicar com o botão direito sobre o painel e clicar em Inspect.\n\n\n\n\n\n\n\n\n\n\nComo comentei acima, nem sempre as coisas estarão bem organizadas. Note que como queremos puxar múltiplos elementos e múltiplos (todos) os atributos usamamos as variantes: html_elements e html_attrs.\n\nlist_attr &lt;- page %&gt;%\n  html_elements(xpath = '//div[@class=\"place-list\"]/div') %&gt;%\n  html_attrs()\n\nO objeto extraído é uma lista onde cada elemento é um vetor de texto que contém as seguintes informações. Temos o nome da loja, a latitude/longitude, e o endereço.\n\npluck(list_attr, 1)\n\n                  class           data-latitude          data-longitude \n\"place-item r-place-15\"           \"-23.5658059\"           \"-46.6508012\" \n              data-name             data-street              data-index \n  \"Shopping Top Center\" \"Avenida Paulista, 854\"                     \"0\" \n\n\nA esta altura, o processo de webscrapping já terminou. Novamente, o processo foi fácil, pois os dados estão muito bem estruturados na página da Starbucks. Agora, precisamos apenas limpar os dados.\n\n\n\n\nNão vou me alongar muito nos detalhes. Basicamente precisamos converter cada elemento da lista em um data.frame, empilhar os resultados e aí converter os tipos de cada coluna.\n\n# Convert os elementos em data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Empilha os resultados\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Renomeia as colunas\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Seleciona as colunas de interesse\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng para numérico\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nclean_dat\n\n# A tibble: 142 × 5\n   index name                            \n   &lt;dbl&gt; &lt;chr&gt;                           \n 1     0 Shopping Top Center             \n 2     1 Shopping Cidade São Paulo       \n 3     2 Paulista Trianon                \n 4     3 Paulista 500                    \n 5     4 Jardim Pamplona Shopping        \n 6     5 Shopping Patio Paulista         \n 7     6 Shopping Center 3               \n 8     7 Hospital Beneficência Portuguesa\n 9     8 Eliseu Guilherme                \n10     9 Haddock Lobo                    \n   street                                    lat   lng\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt;\n 1 Avenida Paulista, 854                   -23.6 -46.7\n 2 Avenida Paulista, 1154                  -23.6 -46.7\n 3 Avenida Paulista, 1499                  -23.6 -46.7\n 4 Avenida Paulista, 500                   -23.6 -46.6\n 5 Rua Pamplona, 1704                      -23.6 -46.7\n 6 Rua Treze de Maio, 1933                 -23.6 -46.6\n 7 Avenida Paulista, 2064                  -23.6 -46.7\n 8 Rua Maestro Cardim, 769                 -23.6 -46.6\n 9 Rua Desembargador Eliseu Guilherme, 200 -23.6 -46.6\n10 Rua Haddock Lobo, 608                   -23.6 -46.7\n# ℹ 132 more rows\n\n\n\n\n\nA tabela acima já está em um formato bastante satisfatório. Podemos verificar os dados construindo um mapa simples.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nVale notar que dados extraídos a partir de webscraping quase sempre apresentam algum ruído. Neste caso, os dados parecem relativamente limpos após um pouco de limpeza. Os endereços nem sempre são muito instrutivos, como no caso “Rodovia Hélio Smidt, S/N”, mas isto acontece porque muitas unidades estão dentro de hospitais, shoppings ou aeroportos.\nCom estes dados já podemos fazer análises interessantes. Podemos descobrir, por exemplo, que há 5 unidades da Starbucks apenas na Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n3     2 Paulista Trianon          Avenida Paulista, 1499 -23.6 -46.7\n4     3 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n5     6 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2  (-46.65438 -23.5631)\n3  (-46.6558 -23.56226)\n4 (-46.64809 -23.56784)\n5 (-46.65895 -23.55785)\n\n\nPodemos também contar o número de unidades em cada um dos aeroportos. Aparentemente, há 8 unidades no aeroporto de Guarulhos, o que me parece um número muito alto.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 GRU               8\n2 Brasília          3\n3 Florianópolis     3\n4 Confins           2\n5 Galeão            2\n6 Viracopos         2\n7 Congonhas         1\n8 Curitiba          1\n9 Santos            1\n\n\nPor fim, podemos notar que muitas das unidades do Starbucks se localizam dentro de shoppings. Uma conta simples mostra que cerca de 75 unidades estão localizadas dentro de shoppings, perto de 50% das unidades2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 75\n\n\n\n\n\nA partir destes dados podemos acrescentar mais informação. A partir do geobr podemos identificar em quais cidades as unidades se encontram.\n\ndim_city = geobr::read_municipality(showProgress = FALSE)\ndim_city = st_transform(dim_city, crs = 4326)\nsf::sf_use_s2(FALSE)\n\nstarbucks = starbucks %&gt;%\n  st_join(dim_city) %&gt;%\n  relocate(c(name_muni, abbrev_state), .before = lat)\n\nAgora podemos ver quais cidades tem mais Starbucks.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name_muni, abbrev_state, sort = TRUE) \n\n# A tibble: 43 × 3\n   name_muni      abbrev_state     n\n   &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;\n 1 São Paulo      SP              45\n 2 Rio De Janeiro RJ              11\n 3 Guarulhos      SP               9\n 4 Curitiba       PR               8\n 5 Brasília       DF               6\n 6 Campinas       SP               6\n 7 Florianópolis  SC               5\n 8 Jundiaí        SP               4\n 9 Porto Alegre   RS               4\n10 Ribeirão Preto SP               3\n# ℹ 33 more rows\n\n\nOu seja, há mais Starbucks somente na Paulista do que em quase todas as demais cidades do Brasil."
  },
  {
    "objectID": "posts/general-posts/2024-03-starbucks-scrape/index.html#web-scraping",
    "href": "posts/general-posts/2024-03-starbucks-scrape/index.html#web-scraping",
    "title": "Encontrando todos os Starbucks do Brasil",
    "section": "",
    "text": "O processo de web scraping consiste em extrair informação de uma página na internet. A dificuldade ou facilidade em extrair esta informação depende da qualidade de construção da página. Em alguns casos mais complexos, a informação pode estar atrás de um captcha ou num painel interativo que depende de outros comandos do usuário.\nNeste exemplo simples vou mostrar como conseguir encontrar a localização de todas as unidades da Starbucks no Brasil. A lista completa das lojas em atividade da Starbucks pode ser encontrada no site do Starbucks Brasil. Como de praxe, vamos utilizar o tidyverse em conjunto com os pacotes rvest e xml2.\n\nlibrary(rvest)\nlibrary(xml2)\nlibrary(tidyverse)\n\n\n\nA lista completa das lojas em atividade da Starbucks pode ser encontrada no site do Starbucks Brasil. Para ler a página usa-se read_html.\n\nurl = \"https://starbucks.com.br/lojas\"\n\npage = xml2::read_html(url)\n\nO “xpath” mostra o caminho até um determinado elemento na página. Para encontrar o logo do Starbucks, no canto superior-esquerdo da página, por exemplo podemos usar o código abaixo.\n\npage %&gt;%\n  html_element(xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\")\n\n{html_node}\n&lt;img alt=\"Starbucks Logo\" src=\"/public/img/icons/starbucks-nav-logo.svg\"&gt;\n\n\nPara ver mais sobre xpaths consulte este cheatsheet.\nEm geral, em páginas bem construídas, o nome dos elementos será bastante auto-explicativo. No caso acima, o atributo “alt” já indica que é objeto é o logo da starbucks e o “src” direciona para um arquivo em formato svg (imagem) chamado starbucks-nav-logo. Infelizmente, isto nem sempre será o caso. Em algumas páginas os elementos podem ser bastante confusos.\nPara puxar um atributo específico usamos a função html_attr.\n\npage %&gt;%\n  html_element(\n    xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\"\n    ) %&gt;%\n  html_attr(\"src\")\n\n[1] \"/public/img/icons/starbucks-nav-logo.svg\"\n\n\nSe você combinar este último link a “www.starbucks.com.br” você deve chegar numa imagem com o logo da empresa1.\n\n\n\n\n\nPara encontrar a grande lista de lojas no painel da esquerda vamos aproveitar o fato de que o div que guarda esta lista tem uma classe única chamada “place-list”. É fácil verificar isto no próprio navegador. Se você usar o Chrome, por exemplo, basta clicar com o botão direito sobre o painel e clicar em Inspect.\n\n\n\n\n\n\n\n\n\n\nComo comentei acima, nem sempre as coisas estarão bem organizadas. Note que como queremos puxar múltiplos elementos e múltiplos (todos) os atributos usamamos as variantes: html_elements e html_attrs.\n\nlist_attr &lt;- page %&gt;%\n  html_elements(xpath = '//div[@class=\"place-list\"]/div') %&gt;%\n  html_attrs()\n\nO objeto extraído é uma lista onde cada elemento é um vetor de texto que contém as seguintes informações. Temos o nome da loja, a latitude/longitude, e o endereço.\n\npluck(list_attr, 1)\n\n                  class           data-latitude          data-longitude \n\"place-item r-place-15\"           \"-23.5658059\"           \"-46.6508012\" \n              data-name             data-street              data-index \n  \"Shopping Top Center\" \"Avenida Paulista, 854\"                     \"0\" \n\n\nA esta altura, o processo de webscrapping já terminou. Novamente, o processo foi fácil, pois os dados estão muito bem estruturados na página da Starbucks. Agora, precisamos apenas limpar os dados."
  },
  {
    "objectID": "posts/general-posts/2024-03-starbucks-scrape/index.html#limpeza-de-dados",
    "href": "posts/general-posts/2024-03-starbucks-scrape/index.html#limpeza-de-dados",
    "title": "Encontrando todos os Starbucks do Brasil",
    "section": "",
    "text": "Não vou me alongar muito nos detalhes. Basicamente precisamos converter cada elemento da lista em um data.frame, empilhar os resultados e aí converter os tipos de cada coluna.\n\n# Convert os elementos em data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Empilha os resultados\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Renomeia as colunas\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Seleciona as colunas de interesse\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng para numérico\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nclean_dat\n\n# A tibble: 142 × 5\n   index name                            \n   &lt;dbl&gt; &lt;chr&gt;                           \n 1     0 Shopping Top Center             \n 2     1 Shopping Cidade São Paulo       \n 3     2 Paulista Trianon                \n 4     3 Paulista 500                    \n 5     4 Jardim Pamplona Shopping        \n 6     5 Shopping Patio Paulista         \n 7     6 Shopping Center 3               \n 8     7 Hospital Beneficência Portuguesa\n 9     8 Eliseu Guilherme                \n10     9 Haddock Lobo                    \n   street                                    lat   lng\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt;\n 1 Avenida Paulista, 854                   -23.6 -46.7\n 2 Avenida Paulista, 1154                  -23.6 -46.7\n 3 Avenida Paulista, 1499                  -23.6 -46.7\n 4 Avenida Paulista, 500                   -23.6 -46.6\n 5 Rua Pamplona, 1704                      -23.6 -46.7\n 6 Rua Treze de Maio, 1933                 -23.6 -46.6\n 7 Avenida Paulista, 2064                  -23.6 -46.7\n 8 Rua Maestro Cardim, 769                 -23.6 -46.6\n 9 Rua Desembargador Eliseu Guilherme, 200 -23.6 -46.6\n10 Rua Haddock Lobo, 608                   -23.6 -46.7\n# ℹ 132 more rows"
  },
  {
    "objectID": "posts/general-posts/2024-03-starbucks-scrape/index.html#mapa",
    "href": "posts/general-posts/2024-03-starbucks-scrape/index.html#mapa",
    "title": "Encontrando todos os Starbucks do Brasil",
    "section": "",
    "text": "A tabela acima já está em um formato bastante satisfatório. Podemos verificar os dados construindo um mapa simples.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nVale notar que dados extraídos a partir de webscraping quase sempre apresentam algum ruído. Neste caso, os dados parecem relativamente limpos após um pouco de limpeza. Os endereços nem sempre são muito instrutivos, como no caso “Rodovia Hélio Smidt, S/N”, mas isto acontece porque muitas unidades estão dentro de hospitais, shoppings ou aeroportos.\nCom estes dados já podemos fazer análises interessantes. Podemos descobrir, por exemplo, que há 5 unidades da Starbucks apenas na Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n3     2 Paulista Trianon          Avenida Paulista, 1499 -23.6 -46.7\n4     3 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n5     6 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2  (-46.65438 -23.5631)\n3  (-46.6558 -23.56226)\n4 (-46.64809 -23.56784)\n5 (-46.65895 -23.55785)\n\n\nPodemos também contar o número de unidades em cada um dos aeroportos. Aparentemente, há 8 unidades no aeroporto de Guarulhos, o que me parece um número muito alto.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 GRU               8\n2 Brasília          3\n3 Florianópolis     3\n4 Confins           2\n5 Galeão            2\n6 Viracopos         2\n7 Congonhas         1\n8 Curitiba          1\n9 Santos            1\n\n\nPor fim, podemos notar que muitas das unidades do Starbucks se localizam dentro de shoppings. Uma conta simples mostra que cerca de 75 unidades estão localizadas dentro de shoppings, perto de 50% das unidades2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 75"
  },
  {
    "objectID": "posts/general-posts/2024-03-starbucks-scrape/index.html#construindo",
    "href": "posts/general-posts/2024-03-starbucks-scrape/index.html#construindo",
    "title": "Encontrando todos os Starbucks do Brasil",
    "section": "",
    "text": "A partir destes dados podemos acrescentar mais informação. A partir do geobr podemos identificar em quais cidades as unidades se encontram.\n\ndim_city = geobr::read_municipality(showProgress = FALSE)\ndim_city = st_transform(dim_city, crs = 4326)\nsf::sf_use_s2(FALSE)\n\nstarbucks = starbucks %&gt;%\n  st_join(dim_city) %&gt;%\n  relocate(c(name_muni, abbrev_state), .before = lat)\n\nAgora podemos ver quais cidades tem mais Starbucks.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name_muni, abbrev_state, sort = TRUE) \n\n# A tibble: 43 × 3\n   name_muni      abbrev_state     n\n   &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;\n 1 São Paulo      SP              45\n 2 Rio De Janeiro RJ              11\n 3 Guarulhos      SP               9\n 4 Curitiba       PR               8\n 5 Brasília       DF               6\n 6 Campinas       SP               6\n 7 Florianópolis  SC               5\n 8 Jundiaí        SP               4\n 9 Porto Alegre   RS               4\n10 Ribeirão Preto SP               3\n# ℹ 33 more rows\n\n\nOu seja, há mais Starbucks somente na Paulista do que em quase todas as demais cidades do Brasil."
  },
  {
    "objectID": "posts/general-posts/2024-03-starbucks-scrape/index.html#footnotes",
    "href": "posts/general-posts/2024-03-starbucks-scrape/index.html#footnotes",
    "title": "Encontrando todos os Starbucks do Brasil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://starbucks.com.br/public/img/icons/starbucks-nav-logo.svg↩︎\nAqui, estamos assumindo que o a tag “name” sempre inclui a palavra shopping se a unidade estiver dentro de um shopping. Eventualmente, este número pode estar subestimado se houver unidades dentro de shoppings que não tem a palavra “shopping” no seu nome. A rigor, também não verificamos se, de fato, a tag shopping sempre está associada a um shopping em atividade.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html",
    "href": "posts/general-posts/2024-03-google-places/index.html",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "O Google Places API permite acessar os dados do Google Maps. O pacote googleway integra estes dados dentro de R já em formato tidy. Neste post vou mostrar como importar dados desta API com foco no Places, que encontra informações sobre estabelecimentos ou pontos de interesse, em geral. Há diversos outros usos desta API, como de encontrar rotas, estimar a elevação, recuperar fotos do Street View, entre outros. Para uma lista completa veja a documentação do pacote.\n\nlibrary(sf)\nlibrary(googleway)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(leaflet)\n\n\n\nPara usar o pacote é preciso registrar uma API. Se você não tem uma key é preciso criar uma. Um tutorial do Google está disponível aqui.\n\ngoogleway::set_key(\"sua_chave\")\n\n\n\n\nPara exemplificar o uso da API vamos começar com um exemplo simples. Via de regra, é preciso definir apenas dois parâmetros para começar a busca:\n\nUm termo de busca\nUm ponto\n\nNo exemplo abaixo buscamos o termo “starbucks” a partir da coordenada do Museu de Arte de São Paulo (MASP).\n\nponto = c(-23.561462, -46.655937)\n\nsearch_sbux = google_places(\n  search_string = \"starbucks\",\n  location = ponto\n)\n\nO objeto guarda vários resultados da query.\n\n# Resultados da query em formato data.frame\nplaces$results\n# 'token' para continuar a busca\nplaces$next_page_token\n# Status da query\nplaces$status\n\nA query de busca encontra apenas 20 resultados (no máximo). Para encontrar mais resultados é preciso fazer uma nova busca, usando $next_page_token para encontrar mais resultados.\n\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_2 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux$next_page_token\n)\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_3 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux_2$next_page_token\n)\n# Agrega todos os resultados encontrados\nres &lt;- bind_rows(\n  search_sbux$results,\n  search_sbux_2$results,\n  search_sbux_3$results\n)\n\nAo todo, a query encontrou 50 resultados que estão agregados no objeto res acima. A query retorna várias das informações que normalmente se encontram numa busca via Google Maps: a nota média do estabelecimento, algumas fotos, o endereço, a posição geográfica (lat/lng), o tipo do estabelecimento, etc.\nFormalmentem, as informações estão estruturadas num “nested” data.frame, que é um data.frame comum que possui outros data.frame ou list como colunas. Isto pode causar algum estranhamento, mas não é nada demais. Para selecionar uma coluna acaba sendo necessário fazer df$col1$col2 ou usar os comandos tidyr::unnest.\n\nsubres &lt;- res %&gt;%\n  unnest(cols = \"geometry\") %&gt;%\n  unnest(cols = \"location\") %&gt;%\n  select(\n    business_status, name, formatted_address, rating, user_ratings_total, types,\n    lat, lng\n  )\n\nsubres &lt;- st_as_sf(subres, coords = c(\"lng\", \"lat\"), crs = 4326)\n\n# Mapa iterativo simples\nleaflet(subres) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(label = ~name) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\nA coluna types é uma lista que contém cinco strings que categorizam o estabelecimento.\n\nsubres$types[[1]]\n\nOs dados do Google Maps não costumam ser 100% limpos. Não é incomum encontrar estabelecimentos fantasmas, ou mal-construídos. Um jeito fácil de encontrar estes casos é filtrar pelo número de reviews ou pelo nome. Na tabela abaixo há dois estabelecimentos suspeitos: o STARBUCKS CO e o Starbucks Coffee: ambos têm um baixo número de reviews e têm uma nota muito baixa (2 e 2.1, respectivamente).\n\nsubres |&gt; \n  arrange(user_ratings_total) |&gt; \n  head(10)\n\n\n\n\nVou montar uma busca simples para retornar todos os Starbucks do Brasil. Seria muito demorado fazer uma busca completa, no país inteiro, então vou usar como ponto de partida os pares de coordenadas que encontrei via webscrape.\nA função abaixo procura pelo termo “starbucks” em todos os pontos que forneço. Para simplificar, a função devolve apenas algumas das colunas.\n\n# Function to grap starbucks info\nget_starbucks_info &lt;- function(lat, lng) {\n  \n  places = google_places(\n    search_string = \"starbucks\",\n    location = c(lat, lng)\n  )\n  \n  sel_cols = c(\n    \"name\", \"formatted_address\", \"lat\", \"lng\", \"rating\", \"user_ratings_total\",\n    \"business_status\")\n  \n  places$results %&gt;%\n    tidyr::unnest(\"geometry\") %&gt;%\n    tidyr::unnest(\"location\") %&gt;%\n    dplyr::select(dplyr::all_of(sel_cols))\n  \n}\n\nO código abaixo roda a função acima em todos os 142 estabelecimentos, encontrados na página oficial do Starbucks Brasil.\n\n# Remove geometry and keep only coordinates\ncoords_starbucks &lt;- starbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  select(index, name, lat, lng)\n\nstarbucks_info = purrr::map2(\n  coords_starbucks$lat,\n  coords_starbucks$lng,\n  get_starbucks_info\n  )\n\ndat &lt;- starbucks_info %&gt;%\n  bind_rows(.id = \"search_id\") %&gt;%\n  distinct()\n\nNovamente, é preciso limpar os resultados. Olhando para a coluna de nome vemos que há vários estabelecimentos errados.\n\nunique(dat$name)\n\n [1] \"Starbucks - Top Center\"                                         \n [2] \"Starbucks\"                                                      \n [3] \"Starbucks Coffee\"                                               \n [4] \"Starbucks Shopping Higienópolis\"                                \n [5] \"STARBUCKS CO\"                                                   \n [6] \"Starbucks Shopping Plaza Sul\"                                   \n [7] \"Starbucks Einstein - Maternidade\"                               \n [8] \"Starbucks Interlagos\"                                           \n [9] \"Starbucks Grand Plaza Shopping\"                                 \n[10] \"Starbucks Shopping Tamboré\"                                     \n[11] \"STARBUCKS COFFEE\"                                               \n[12] \"Starbucks T3 Desembarque\"                                       \n[13] \"Starbucks - Mogi Shopping\"                                      \n[14] \"Lago Azul Restaurant - North\"                                   \n[15] \"Starbucks Litoral Plaza Shopping\"                               \n[16] \"Starbucks - Parque Dom Pedro\"                                   \n[17] \"Starbucks Livraria Leitura - Parque Dom Pedro\"                  \n[18] \"Starbuks\"                                                       \n[19] \"Starbucks Ribeirão Shopping\"                                    \n[20] \"Starbucks Coffee - Shopping Mueller\"                            \n[21] \"Starbucks - Palladium\"                                          \n[22] \"Starbucks (Sala de Embarque) Aeroporto Curitiba Afonso Pena\"    \n[23] \"Starbucks Shopping Jockey\"                                      \n[24] \"Starbucks Shopping Curitiba\"                                    \n[25] \"Casa Bauducco\"                                                  \n[26] \"Havanna Café Palladium Curitiba\"                                \n[27] \"Season Coffee & Co.\"                                            \n[28] \"Starbucks - Santos Dumont\"                                      \n[29] \"DarkCoffee Centro Rio\"                                          \n[30] \"Starbucks Garten Shopping\"                                      \n[31] \"Starbucks Norte Shopping\"                                       \n[32] \"Starbucks Itaú Power Shopping\"                                  \n[33] \"Starbucks | Beiramar Shopping\"                                  \n[34] \"Starbucks Boulevard Shopping\"                                   \n[35] \"Starbucks Airport Gate 115\"                                     \n[36] \"Starbucks - Barra Shopping Sul\"                                 \n[37] \"Starbucks Portão 8 Aeroporto Internacional de Brasília Lago Sul\"\n[38] \"Caffetteria\"                                                    \n[39] \"Black Coffee\"                                                   \n[40] \"Duckbill Cookies & Coffee Taguatinga Shopping - Brasília/DF\"    \n[41] \"Caffetteria WPS Santa Lúcia\"                                    \n\n\nPara fazer a limpeza, vou manter apenas as unidades ativas com contêm “Starbucks” no seu nome. Além disso, vou parear os dados com a minha base de webscape usando st_nearest_feature(x, y). Esta função encontra o ponto mais próximo em y para cada ponto de x.\n\ndat &lt;- dat |&gt; \n  filter(str_detect(name, \"Starbucks\"), business_status == \"OPERATIONAL\") |&gt; \n  arrange(formatted_address)\n\ngoogle_data &lt;- dat %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326)\n\ninds &lt;- st_nearest_feature(google_data, starbucks)\n\nmetadata &lt;- starbucks %&gt;%\n  slice(inds) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble()\n\ngoogle_data &lt;- google_data |&gt; \n  rename(google_name = name, google_address = formatted_address) |&gt; \n  bind_cols(metadata)\n\n\n\nO mapa interativo abaixo mostra todos os Starbucks de São Paulo. A cor de cada círculo representa a sua nota e o tamanho do círculo, o número de avaliações. As unidades no corredor da Av. Paulista, por exemplo, têm notas médias elevadas e grande número de avaliações. Uma das piores unidades parece ser a da Uv. Mackenzie, que tem nota 2,1 e 15 avaliações. Na Zona Leste, a unidade no Shopping Aricanduva também tem nota um pouco inferior, 3,9 com 158 avaliações.\n\nsp &lt;- filter(google_data, name_muni == \"São Paulo\")\n\nsp &lt;- sp |&gt; \n  mutate(\n    rad = findInterval(user_ratings_total, c(25, 100, 1000, 2500, 5000))*2 + 5\n  )\n\npal &lt;- colorNumeric(\"RdBu\", domain = sp$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {sp$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {sp$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {sp$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(sp) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~rad,\n    color = ~pal(rating),\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt; \n  addLegend(pal = pal, values = ~rating) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\n\n\n\nTecnicamente, o Starbucks mais bem avaliado de São Paulo é o Starbucks da Ala Consultório do Hospital Albert Einstein, com nota 4,8. Contudo, considerando as unidades com maior número de avaliações, podemos montar um mapa dos “melhores” Starbucks de São Paulo: as unidades com nota igual ou superior a 4,4 e com mais de 500 avaliações.\nDe maneira geral, o mapa mostra as unidades dentro do Centro Expandido, no eixo da Paulista e do Itaim Bibi. Nas bordas do Centro Expandido temos quase exclusivamente, unidades dentro de shoppings.\n\nmelhores_starbucks &lt;- sp |&gt; \n  filter(rating &gt;= 4.4 & user_ratings_total &gt; 500)\n\npal &lt;- colorNumeric(\"Blues\", domain = melhores_starbucks$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {melhores_starbucks$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {melhores_starbucks$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {melhores_starbucks$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(melhores_starbucks) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~sqrt(user_ratings_total / 10),\n    color = \"#045a8d\",\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt;\n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\n\n\n\n\n\nEncontrando todos os Starbucks do Brasil\nTutorial Leaflet: mapas interativos com leaflet"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html#starbucks",
    "href": "posts/general-posts/2024-03-google-places/index.html#starbucks",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "Para exemplificar o uso da API vamos começar com um exemplo simples. Via de regra, é preciso definir apenas dois parâmetros para começar a busca:\n\nUm termo de busca\nUm ponto\n\nNo exemplo abaixo buscamos o termo “starbucks” a partir da coordenada do Museu de Arte de São Paulo (MASP).\n\nponto = c(-23.561462, -46.655937)\n\nsearch_sbux = google_places(\n  search_string = \"starbucks\",\n  location = ponto\n)\n\nO objeto guarda vários resultados da query.\n\n# Resultados da query em formato data.frame\nplaces$results\n# 'token' para continuar a busca\nplaces$next_page_token\n# Status da query\nplaces$status\n\nA query de busca encontra apenas 20 resultados (no máximo). Para encontrar mais resultados é preciso fazer uma nova busca, usando $next_page_token para encontrar mais resultados.\n\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_2 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux$next_page_token\n)\n# busca mais resultados para a mesma pesquisa\nsearch_sbux_3 = google_places(\n  search_string = \"starbucks\",\n  location = ponto,\n  page_token = search_sbux_2$next_page_token\n)\n# Agrega todos os resultados encontrados\nres &lt;- bind_rows(\n  search_sbux$results,\n  search_sbux_2$results,\n  search_sbux_3$results\n)\n\nAo todo, a query encontrou 50 resultados que estão agregados no objeto res acima. A query retorna várias das informações que normalmente se encontram numa busca via Google Maps: a nota média do estabelecimento, algumas fotos, o endereço, a posição geográfica (lat/lng), o tipo do estabelecimento, etc.\nFormalmentem, as informações estão estruturadas num “nested” data.frame, que é um data.frame comum que possui outros data.frame ou list como colunas. Isto pode causar algum estranhamento, mas não é nada demais. Para selecionar uma coluna acaba sendo necessário fazer df$col1$col2 ou usar os comandos tidyr::unnest.\n\nsubres &lt;- res %&gt;%\n  unnest(cols = \"geometry\") %&gt;%\n  unnest(cols = \"location\") %&gt;%\n  select(\n    business_status, name, formatted_address, rating, user_ratings_total, types,\n    lat, lng\n  )\n\nsubres &lt;- st_as_sf(subres, coords = c(\"lng\", \"lat\"), crs = 4326)\n\n# Mapa iterativo simples\nleaflet(subres) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(label = ~name) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\nA coluna types é uma lista que contém cinco strings que categorizam o estabelecimento.\n\nsubres$types[[1]]\n\nOs dados do Google Maps não costumam ser 100% limpos. Não é incomum encontrar estabelecimentos fantasmas, ou mal-construídos. Um jeito fácil de encontrar estes casos é filtrar pelo número de reviews ou pelo nome. Na tabela abaixo há dois estabelecimentos suspeitos: o STARBUCKS CO e o Starbucks Coffee: ambos têm um baixo número de reviews e têm uma nota muito baixa (2 e 2.1, respectivamente).\n\nsubres |&gt; \n  arrange(user_ratings_total) |&gt; \n  head(10)"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html#busca-estruturada",
    "href": "posts/general-posts/2024-03-google-places/index.html#busca-estruturada",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "Vou montar uma busca simples para retornar todos os Starbucks do Brasil. Seria muito demorado fazer uma busca completa, no país inteiro, então vou usar como ponto de partida os pares de coordenadas que encontrei via webscrape.\nA função abaixo procura pelo termo “starbucks” em todos os pontos que forneço. Para simplificar, a função devolve apenas algumas das colunas.\n\n# Function to grap starbucks info\nget_starbucks_info &lt;- function(lat, lng) {\n  \n  places = google_places(\n    search_string = \"starbucks\",\n    location = c(lat, lng)\n  )\n  \n  sel_cols = c(\n    \"name\", \"formatted_address\", \"lat\", \"lng\", \"rating\", \"user_ratings_total\",\n    \"business_status\")\n  \n  places$results %&gt;%\n    tidyr::unnest(\"geometry\") %&gt;%\n    tidyr::unnest(\"location\") %&gt;%\n    dplyr::select(dplyr::all_of(sel_cols))\n  \n}\n\nO código abaixo roda a função acima em todos os 142 estabelecimentos, encontrados na página oficial do Starbucks Brasil.\n\n# Remove geometry and keep only coordinates\ncoords_starbucks &lt;- starbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  select(index, name, lat, lng)\n\nstarbucks_info = purrr::map2(\n  coords_starbucks$lat,\n  coords_starbucks$lng,\n  get_starbucks_info\n  )\n\ndat &lt;- starbucks_info %&gt;%\n  bind_rows(.id = \"search_id\") %&gt;%\n  distinct()\n\nNovamente, é preciso limpar os resultados. Olhando para a coluna de nome vemos que há vários estabelecimentos errados.\n\nunique(dat$name)\n\n [1] \"Starbucks - Top Center\"                                         \n [2] \"Starbucks\"                                                      \n [3] \"Starbucks Coffee\"                                               \n [4] \"Starbucks Shopping Higienópolis\"                                \n [5] \"STARBUCKS CO\"                                                   \n [6] \"Starbucks Shopping Plaza Sul\"                                   \n [7] \"Starbucks Einstein - Maternidade\"                               \n [8] \"Starbucks Interlagos\"                                           \n [9] \"Starbucks Grand Plaza Shopping\"                                 \n[10] \"Starbucks Shopping Tamboré\"                                     \n[11] \"STARBUCKS COFFEE\"                                               \n[12] \"Starbucks T3 Desembarque\"                                       \n[13] \"Starbucks - Mogi Shopping\"                                      \n[14] \"Lago Azul Restaurant - North\"                                   \n[15] \"Starbucks Litoral Plaza Shopping\"                               \n[16] \"Starbucks - Parque Dom Pedro\"                                   \n[17] \"Starbucks Livraria Leitura - Parque Dom Pedro\"                  \n[18] \"Starbuks\"                                                       \n[19] \"Starbucks Ribeirão Shopping\"                                    \n[20] \"Starbucks Coffee - Shopping Mueller\"                            \n[21] \"Starbucks - Palladium\"                                          \n[22] \"Starbucks (Sala de Embarque) Aeroporto Curitiba Afonso Pena\"    \n[23] \"Starbucks Shopping Jockey\"                                      \n[24] \"Starbucks Shopping Curitiba\"                                    \n[25] \"Casa Bauducco\"                                                  \n[26] \"Havanna Café Palladium Curitiba\"                                \n[27] \"Season Coffee & Co.\"                                            \n[28] \"Starbucks - Santos Dumont\"                                      \n[29] \"DarkCoffee Centro Rio\"                                          \n[30] \"Starbucks Garten Shopping\"                                      \n[31] \"Starbucks Norte Shopping\"                                       \n[32] \"Starbucks Itaú Power Shopping\"                                  \n[33] \"Starbucks | Beiramar Shopping\"                                  \n[34] \"Starbucks Boulevard Shopping\"                                   \n[35] \"Starbucks Airport Gate 115\"                                     \n[36] \"Starbucks - Barra Shopping Sul\"                                 \n[37] \"Starbucks Portão 8 Aeroporto Internacional de Brasília Lago Sul\"\n[38] \"Caffetteria\"                                                    \n[39] \"Black Coffee\"                                                   \n[40] \"Duckbill Cookies & Coffee Taguatinga Shopping - Brasília/DF\"    \n[41] \"Caffetteria WPS Santa Lúcia\"                                    \n\n\nPara fazer a limpeza, vou manter apenas as unidades ativas com contêm “Starbucks” no seu nome. Além disso, vou parear os dados com a minha base de webscape usando st_nearest_feature(x, y). Esta função encontra o ponto mais próximo em y para cada ponto de x.\n\ndat &lt;- dat |&gt; \n  filter(str_detect(name, \"Starbucks\"), business_status == \"OPERATIONAL\") |&gt; \n  arrange(formatted_address)\n\ngoogle_data &lt;- dat %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326)\n\ninds &lt;- st_nearest_feature(google_data, starbucks)\n\nmetadata &lt;- starbucks %&gt;%\n  slice(inds) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble()\n\ngoogle_data &lt;- google_data |&gt; \n  rename(google_name = name, google_address = formatted_address) |&gt; \n  bind_cols(metadata)\n\n\n\nO mapa interativo abaixo mostra todos os Starbucks de São Paulo. A cor de cada círculo representa a sua nota e o tamanho do círculo, o número de avaliações. As unidades no corredor da Av. Paulista, por exemplo, têm notas médias elevadas e grande número de avaliações. Uma das piores unidades parece ser a da Uv. Mackenzie, que tem nota 2,1 e 15 avaliações. Na Zona Leste, a unidade no Shopping Aricanduva também tem nota um pouco inferior, 3,9 com 158 avaliações.\n\nsp &lt;- filter(google_data, name_muni == \"São Paulo\")\n\nsp &lt;- sp |&gt; \n  mutate(\n    rad = findInterval(user_ratings_total, c(25, 100, 1000, 2500, 5000))*2 + 5\n  )\n\npal &lt;- colorNumeric(\"RdBu\", domain = sp$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {sp$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {sp$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {sp$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(sp) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~rad,\n    color = ~pal(rating),\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt; \n  addLegend(pal = pal, values = ~rating) |&gt; \n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\n\n\n\nTecnicamente, o Starbucks mais bem avaliado de São Paulo é o Starbucks da Ala Consultório do Hospital Albert Einstein, com nota 4,8. Contudo, considerando as unidades com maior número de avaliações, podemos montar um mapa dos “melhores” Starbucks de São Paulo: as unidades com nota igual ou superior a 4,4 e com mais de 500 avaliações.\nDe maneira geral, o mapa mostra as unidades dentro do Centro Expandido, no eixo da Paulista e do Itaim Bibi. Nas bordas do Centro Expandido temos quase exclusivamente, unidades dentro de shoppings.\n\nmelhores_starbucks &lt;- sp |&gt; \n  filter(rating &gt;= 4.4 & user_ratings_total &gt; 500)\n\npal &lt;- colorNumeric(\"Blues\", domain = melhores_starbucks$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {melhores_starbucks$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {melhores_starbucks$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {melhores_starbucks$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(melhores_starbucks) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~sqrt(user_ratings_total / 10),\n    color = \"#045a8d\",\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt;\n  addProviderTiles(\"CartoDB\")"
  },
  {
    "objectID": "posts/general-posts/2024-03-google-places/index.html#posts-relacionados",
    "href": "posts/general-posts/2024-03-google-places/index.html#posts-relacionados",
    "title": "Enriquecendo e coletando do Google Maps",
    "section": "",
    "text": "Encontrando todos os Starbucks do Brasil\nTutorial Leaflet: mapas interativos com leaflet"
  },
  {
    "objectID": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html",
    "href": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html",
    "title": "Mapas Interativos com Leaflet e R",
    "section": "",
    "text": "Mapas interativos permitem uma visualização mais interessante e exploratória dos dados. Nos últimos tempos, este tipo de visualização se tornou mais popular por permitir que usuário encontre os dados que procura de maneira mais simples e intuitiva. Neste post vou mostrar como fazer mapas interativos usando o pacote leaflet no R."
  },
  {
    "objectID": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#básico",
    "href": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#básico",
    "title": "Mapas Interativos com Leaflet e R",
    "section": "Básico",
    "text": "Básico\n\n# Pacotes necessários para este tutorial\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(sidrar)\n# Pactotes que utilizamos apenas uma função\n# library(janitor)\n# library(mapview)\n# library(sidrar)\n# library(MetBrewer)\n\nCriar um mapa iterativo no leaflet é bastante simples. O código abaixo cria um mapa de São Paulo. Note que o mapa é construindo como uma composição de funções usando o operador pipe. Assim, a construção de mapas no leaflet fica similar ao ggplot2 onde soma-se várias funções a uma mesma visualização. O pacote leaflet carrega automaticamente o pipe do magritrr, mas, naturalmente, é possível usar o pipe |&gt; nativo do R. Para mais informações sobre o operador pipe no R vale consultar meu post sobre o assunto.\nA função setView define o ponto central do mapa e o nível do zoom. Defino o ponto central como as coordenadas do Museu de Arte de São Paulo (MASP).\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -46.655837, lat = -23.561387, zoom = 12)\n\nm\n\n\n\n\n\nPode-se trocar o basemap do mapa alterando o provider.\n\nm %&gt;%\n  addProviderTiles(provider = \"CartoDB\")\n\n\n\n\n\nPara adicionar “shapes” ao mapa usa-se as funções add* como addMarkers.\n\npontos &lt;- tibble(\n  lng = -46.655837, lat = -23.561387\n)\n\npontos &lt;- st_as_sf(pontos, coords = c(\"lng\", \"lat\"), crs = 4326)\n\nm %&gt;%\n  addMarkers(data = pontos) %&gt;%\n  addProviderTiles(\"CartoDB\")"
  },
  {
    "objectID": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#markers",
    "href": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#markers",
    "title": "Mapas Interativos com Leaflet e R",
    "section": "Markers",
    "text": "Markers\nO leaflet tem algumas opções para mapear pontos. A mais simples delas é a addMarkers, vista acima. No mapa abaixo, mostro todos os Starbucks de São Paulo. Os dados provêm de um webscrape, que fiz em outro post.\n\nsp_starbucks &lt;- dplyr::filter(starbucks, code_muni == 3550308)\n  \nm %&gt;%\n  addMarkers(data = sp_starbucks, label = ~as.character(name)) %&gt;%\n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\nQuando temos muitos pontos podemos agregá-los usando markerClusterOptions().\n\nm %&gt;%\n  addMarkers(\n    data = sp_starbucks,\n    label = ~as.character(name),\n    clusterOptions = markerClusterOptions()) %&gt;%\n  addProviderTiles(\"CartoDB\")\n\n\n\n\n\nÉ possível fazer outros tipos de marcadores e inclusive usar cores para representar diferentes valores ou categorias. Comparado com outras alternativas, como tmap ou mapview, contudo, o leaflet é um pouco mais trabalhoso. O código abaixo categoriza as unidades do Starbucks em 4 tipos distintos:\n\nShopping - se a unidade estiver dentro de um shopping.\nAeroporto - se a unidade estiver dentro de um aeroporto.\nB2B - se a unidade estiver inserida dentro de outro negócio (e.g. hospital, torre corporativa, etc.)\nOn-street - se a unidade for uma loja de rua.\n\nEu mapeio uma cor distinta para cada uma das categorias. Como se vê, é necessário criar um objeto que mapeia os valores para as cores (usando colorFactor). Vale notar que esta função aceita paletas de cores do RColorBrewer. A legenda de cores não é adicionada automaticamente e é preciso configurá-la também.\n\nsp_starbucks &lt;- sp_starbucks %&gt;%\n  mutate(\n    type = case_when(\n      str_detect(name, \"Shopping|shopping\") ~ \"Shopping\",\n      str_detect(name, \"Aeroporto\") ~ \"Aeroporto\",\n      str_detect(name, \"Hospital|Corporate\") ~ \"B2B\",\n      TRUE ~ \"On-street\"\n    )\n  )\n\npal &lt;- colorFactor(\"Set1\", domain = unique(sp_starbucks$type))\n\nm %&gt;%\n  addCircleMarkers(\n    data = sp_starbucks,\n    color = ~pal(type),\n    stroke = FALSE,\n    fillOpacity = 0.5,\n    label = ~as.character(name)) %&gt;%\n  addProviderTiles(provider = \"CartoDB\") %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal,\n    values = unique(sp_starbucks$type))\n\n\n\n\n\nA título de exemplo, note como é possível chegar numa mapa muito similar usando somente uma linha de código.\n\nmapview::mapview(sp_starbucks, zcol = \"type\")\n\n\n\n\n\n\nO exemplo acima ilustra bem o trade-off do leaflet: a flexibilidade do pacote vem ao custo de maior complexidade."
  },
  {
    "objectID": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#domicílios-por-tipo-de-ocupação",
    "href": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#domicílios-por-tipo-de-ocupação",
    "title": "Mapas Interativos com Leaflet e R",
    "section": "Domicílios por tipo de Ocupação",
    "text": "Domicílios por tipo de Ocupação\n\nDados\nPara este exemplo vamos importar os dados da PNADC/A sobre condição de ocupação dos imóveis. Importo os dados usando o pacote sidrar. Para mais informações sobre como usar o pacote veja o meu post sobre o assunto.\nQueremos montar um mapa que mostre o percentual de domicílio alugados em cada estado. Além disso, o mapa deve mostrar os outros tipos de ocupação (próprio, etc.)\n\npnad &lt;- sidrar::get_sidra(\n  6821,\n  period = \"2022\",\n  variable = 162,\n  geo = \"State\"\n)\n\nApós importar os dados, precisamos fazer uma limpeza básica e agregar um pouco as categorias do IBGE.\n\ntab_houses &lt;- pnad |&gt; \n  as_tibble() |&gt; \n  janitor::clean_names() |&gt; \n  select(\n    code_state = unidade_da_federacao_codigo,\n    prop_type = condicao_de_ocupacao_do_domicilio,\n    count = valor\n    )\n\ntab_houses &lt;- tab_houses |&gt; \n  filter(prop_type != \"Total\") |&gt; \n  mutate(\n    prop_type_grouped = case_when(\n      str_detect(prop_type, \"Próprio\") ~ \"Owned\",\n      prop_type == \"Alugado\" ~ \"Rented\",\n      TRUE ~ \"Other\"\n    )\n  ) |&gt; \n  summarise(\n    total = sum(count, na.rm = TRUE),\n    .by = c(\"code_state\", \"prop_type_grouped\")\n    ) |&gt; \n  mutate(share = total / sum(total) * 100, .by = \"code_state\")\n\nPor fim, precisamos transformar os dados em “wide” para fazer o join com o shapefile.\n\nhouses_prop &lt;- tab_houses |&gt; \n  pivot_wider(\n    id_cols = \"code_state\",\n    names_from = \"prop_type_grouped\",\n    values_from = \"share\"\n  ) |&gt; \n  mutate(code_state = as.numeric(code_state))\n\nhouses_prop\n\n# A tibble: 27 × 4\n   code_state Owned Rented Other\n        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1         11  65.1   22.2 12.8 \n 2         12  78.0   13.8  8.16\n 3         13  78.1   14.5  7.43\n 4         14  64.6   23.4 12   \n 5         15  78.4   12.8  8.82\n 6         16  80     12.7  7.35\n 7         17  65.7   23.3 11.0 \n 8         21  81.7   11.5  6.86\n 9         22  81.7   10.3  8.06\n10         23  71.3   19.4  9.34\n# ℹ 17 more rows\n\n\n\n\nShape das UFs\nConseguir o shape dos estados é bastante simples, usando o pacote geobr. O leaflet é feito para funcionar com a projeção WGS84. Na prática, é possível usar outras (como a 4674, por exemplo), mas a função vai sempre retornar um aviso: Need '+proj=longlat +datum=WGS84'.\nApós importar o shape, basta juntar os dados com o shapefile usando left_join.\n\n# Importa o shapefile de todos os estados do Brasil\nstate_border &lt;- geobr::read_state(showProgress = FALSE)\nstate_border &lt;- sf::st_transform(state_border, crs = 4326)\n# Junta o shapefile com os dados da tabela\nstate_prop &lt;- left_join(state_border, houses_prop, by = \"code_state\")\n\n\n\nO mapa\nO código abaixo mostra como fazer a primeira versão do mapa. Este mapa é quase equivalente a um mapa estático: mostra a proporção domicílios alugados numa escala de cor simples. A função colorNumeric serve para mapear os valores numéricos na coluna Rented em cores. O primeiro argumento pode ser tanto um vetor com cores, como o nome de uma paleta; felizmente, as paletas do ColorBrewer são aceitas.\n\npal = colorNumeric(\"Greens\", domain = c(min(state_prop$Rented), max(state_prop$Rented)))\n\nleaflet(state_prop) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    color = \"white\",\n    fillColor = ~pal(Rented),\n    fillOpacity = 0.9\n    ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~Rented,\n    title = \"Rented (%)\",\n    position = \"bottomright\"\n  ) %&gt;%\n  addProviderTiles(\"CartoDB\") \n\n\n\n\n\nPara adicionar labels, é preciso formatar o texto em html. Este é o texto que vai aparecer para o usuário quando o cursor do mouse estiver sobre o polígono. Além disso, para melhor formatar os números, uso as funções round e format.\n\nlabels &lt;- sprintf(\n  \"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt; Owned: %s &lt;br/&gt; Rented: %s &lt;br/&gt; Other: %s\",\n  state_prop$name_state,\n  format(round(state_prop$Owned, 1), decimal.mark = \",\"),\n  format(round(state_prop$Rented, 1), decimal.mark = \",\"),\n  format(round(state_prop$Other, 1), decimal.mark = \",\")\n  )\n\n# Exemplifica o label\nlabels[1]\n\n[1] \"&lt;strong&gt;Rondônia&lt;/strong&gt;&lt;br/&gt; Owned: 65,1 &lt;br/&gt; Rented: 22,2 &lt;br/&gt; Other: 12,8\"\n\n# Converte para html\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nAlém de acrescentar este “popup” de informação, também podemos dar um destaque para o estado que o usuário está analisando com highlightOptions.\n\nleaflet(state_prop) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    color = \"white\",\n    fillColor = ~pal(Rented),\n    fillOpacity = 0.9,\n    label = ~labels,\n    highlightOptions = highlightOptions(\n      color = \"orange\",\n      weight = 4,\n      bringToFront = TRUE)\n    ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~Rented,\n    title = \"Rented (%)\",\n    position = \"bottomright\"\n  ) %&gt;%\n  addProviderTiles(\"CartoDB\")"
  },
  {
    "objectID": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#população-bairros-de-curitiba",
    "href": "posts/general-posts/2024-03-mapas-interativos-leaflet/index.html#população-bairros-de-curitiba",
    "title": "Mapas Interativos com Leaflet e R",
    "section": "População Bairros de Curitiba",
    "text": "População Bairros de Curitiba\nComo segundo exemplo vamos montar um mapa com a densidade populacional dos bairros de Curitiba. Novamente, os dados são importados do SIDRA via sidrar e o shapefile via geobr.\n\nDados\n\n# Neighborhoods\nnb &lt;- geobr::read_neighborhood(showProgress = FALSE)\nnb &lt;- nb |&gt; \n  filter(code_muni == 4106902)\n\n# Import population table from SIDRA\npop_nb &lt;- sidrar::get_sidra(\n  x = 1378,\n  variable = 93,\n  classific = \"c2\",\n  geo = \"Neighborhood\",\n  geo.filter = list(\"City\" = 4106902)\n)\n\n# Clean table and make it wide\npop_nb &lt;- pop_nb |&gt; \n  as_tibble() |&gt; \n  janitor::clean_names() |&gt; \n  select(code_neighborhood = bairro_codigo, sex = sexo, count = valor) |&gt; \n  mutate(\n    sex = str_replace(sex, \"Homens\", \"Male\"),\n    sex = str_replace(sex, \"Mulheres\", \"Female\")\n    ) |&gt; \n  pivot_wider(\n    id_cols = \"code_neighborhood\",\n    names_from = \"sex\",\n    values_from = \"count\"\n    )\n\n# Make neighborhood codes compatible\npop_nb &lt;- pop_nb |&gt; \n  mutate(code_neighborhood = str_c(\n    str_sub(code_neighborhood, 1, 7), \"05\", str_sub(code_neighborhood, 8, 10))\n    )\n\n# Join census table with shapefile\ncur_nb &lt;- left_join(nb, pop_nb, by = \"code_neighborhood\")\n\n# Calculate population density\ncur_nb &lt;- cur_nb %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  mutate(\n    area = st_area(.),\n    area = as.numeric(area) / 1e5,\n    pop_dens = Total / area,\n    pop_ntile = ntile(pop_dens, 5)\n    )\n# Convert back to 4326 for leaflet\ncur_nb &lt;- st_transform(cur_nb, crs = 4326)\n\n\n\nO mapa\nNeste mapa, vamos discretizar os dados usando quintis. Infelizmente, o leaflet não tem uma função que facilita esta classificação então temos que fazê-la passo a passo. Poderíamos ter optado por outra classificação, usando decis ou quebras naturais. Para algumas outras opções vale ver o meu post sobre mapas em ggplot2. O código abaixo usa a função colorBin para mapear as cores nos respectivos valores. Também uso uma paleta de cores diferente, usando o pacote MetBrewer.\n\n# Color palette and bins\nbins &lt;- quantile(cur_nb$pop_dens, probs = seq(0.2, 0.8, 0.2))\nbins &lt;- c(0, bins, max(cur_nb$pop_dens))\npal &lt;- colorBin(\n  palette = as.character(MetBrewer::met.brewer(\"Hokusai2\", 5)),\n  domain = cur_nb$pop_dens,\n  bins = bins)\n\nNovamente para montar os labels precisamos escrever em html.\n\n# Labels\nlabels &lt;- sprintf(\n  \"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt; %s people &lt;br/&gt; %g people / ha&lt;sup&gt;2&lt;/sup&gt;\",\n  cur_nb$name_neighborhood,\n  format(cur_nb$Total, big.mark = \".\"),\n  round(cur_nb$pop_dens, 1)\n  )\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nAgora, com todos os elementos podemos montar o mapa.\n\n# Center of the map for zoom\nborder &lt;- geobr::read_municipality(4106902, showProgress = FALSE)\ncenter &lt;- st_coordinates(st_centroid(border))\n\nleaflet(cur_nb) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    color = \"white\",\n    fillColor = ~pal(pop_dens),\n    fillOpacity = 0.8,\n    highlightOptions = highlightOptions(\n      color = \"gray20\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~pop_dens,\n    title = \"Densidade Pop.\",\n    position = \"bottomright\"\n  ) %&gt;%\n  addProviderTiles(providers$CartoDB) %&gt;%\n  setView(lng = center[1], lat = center[2], zoom = 11)"
  },
  {
    "objectID": "posts/general-posts/2024-04-sp-grid-houses/index.html",
    "href": "posts/general-posts/2024-04-sp-grid-houses/index.html",
    "title": "Domicilios em Sao Paulo",
    "section": "",
    "text": "Code\n# Setup ---------------------------------------------------------\n\n# Libraries\nlibrary(ggplot2)\nlibrary(geobr)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(MetBrewer)\n# https://github.com/viniciusoike/tidypod\nlibrary(tidypod)\n\n# Fonts\nfont_add(\"HelveticaNeue\", \"HelveticaNeue.ttc\")\nshowtext_auto()\n\n# Data ----------------------------------------------------------\n\n# Import Sao Paulo shapefile\nsp_border &lt;- geobr::read_municipality(3550308, showProgress = FALSE)\n\n# Import locally Census data\ncenso22 &lt;- data.table::fread(\"/Volumes/T7 Touch/github/tidyibge/data-raw/35.csv\")\n\n# Convert column names to lowercase and filter only capital\ncenso22 &lt;- censo22 |&gt; \n  rename_with(tolower) |&gt; \n  filter(cod_mun == 3550308)\n\n# Variable dictionary\ndictionary_censo &lt;- tribble(\n  ~cod_especie, ~label_especie,\n  1, \"Domicílio particular\",\n  2, \"Domicílio coletivo\",\n  3, \"Estabelecimento agropecuário\",\n  4, \"Estabelecimento de ensino\",\n  5, \"Estabelecimento de saúde\",\n  6, \"Estabelecimento de outras finalidades\",\n  7, \"Edificação em construção\",\n  8, \"Estabelecimento religioso\"\n)\n\n# Join data with dictionary -------------------------------------\ncenso22 &lt;- left_join(censo22, dictionary_censo, by = \"cod_especie\")\n\n\n# Spatial data --------------------------------------------------\n\n# Import districts shapefile\ndstr &lt;- tidypod::districts\ndstr &lt;- filter(dstr, code_muni == 36)\n\ndomicilios &lt;- st_as_sf(\n  censo22[cod_especie == 1],\n  coords = c(\"longitude\", \"latitude\"),\n  crs = 4674\n  )\n\nsp_grid_rectangular &lt;- sp_border |&gt; \n  st_transform(crs = 32722) |&gt; \n  st_make_grid(cellsize = 100) |&gt; \n  st_as_sf() |&gt; \n  st_transform(crs = 4326) |&gt; \n  mutate(gid = row_number())\n\ndomi_grid &lt;- domicilios |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_join(sp_grid_rectangular) |&gt; \n  filter(!is.na(gid))\n\ndomi_grid_count &lt;- domi_grid |&gt; \n  st_drop_geometry() |&gt; \n  summarise(total = n(), .by = \"gid\")\n\ngrid_count &lt;- left_join(sp_grid_rectangular, domi_grid_count, by = \"gid\")\n\n# Plot ----------------------------------------------------------\n\nbreaks &lt;- BAMMtools::getJenksBreaks(na.omit(grid_count$total), 7)\nlabels &lt;- format(round(breaks, -1), big.mark = \".\")\n\nmap_sp &lt;- ggplot() +\n  geom_sf(data = sp_border, lwd = 0.05, fill = NA) +\n  geom_sf(\n    data = filter(grid_count, !is.na(total)),\n    aes(fill = total, color = total),\n    alpha = 0.8\n    ) +\n  scale_fill_fermenter(\n    name = \"\",\n    na.value = \"gray50\",\n    direction = 1,\n    palette = \"BuPu\",\n    breaks = breaks,\n    labels = labels\n    ) +\n  scale_color_fermenter(\n    name = \"\",\n    na.value = \"gray50\",\n    direction = 1,\n    palette = \"BuPu\",\n    breaks = breaks,\n    labels = labels\n    ) +\n  coord_sf() +\n  labs(\n    title = \"Domicílios em São Paulo\",\n    subtitle = \"Concentração de domicílios em grid 100x100m em São Paulo\") +\n  ggthemes::theme_map(base_family = \"HelveticaNeue\") +\n  theme(\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(0.85, \"cm\"),\n    legend.key.width = unit(1.5, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 24, hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5)\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-2",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-2",
    "title": "Replicando gráficos",
    "section": "Gráfico 2",
    "text": "Gráfico 2\nO gráfico mostra o percentual de carteiras de habilitação por faixa etária e por sexo, no Brasio, em janeiro de 2024. Os dados são do Renach (Registro Nacional de Condutores habilitados). Link para a matéria original.\n\nGráfico Original\n\n\n\n\n\n\n\nDados\nJá que nosso foco é somente em replicar a visualização, vamos copiar os dados diretamente da tabela. Haverá um pequeno erro de precisão no processo.\n\n\nCode\n# Copia os dados do gráfico\ntabela &lt;- tribble(\n  ~age_label, ~masc, ~femi,\n  \"18 a 30\", 0.627, 0.373,\n  \"31 a 40\", 0.622, 0.378,\n  \"41 a 50\", 0.641, 0.359,\n  \"51 a 60\", 0.674, 0.326,\n  \"61 a 70\", 0.709, 0.291,\n  \"71 ou +\", 0.774, 0.291\n)\n\n# Formata os dados\ndados &lt;- tabela |&gt; \n  pivot_longer(cols = -\"age_label\", names_to = \"sex\", values_to = \"share\") |&gt; \n  mutate(\n    # Converte as variáveis categóricas para factor\n    sex = factor(sex,\n                 levels = c(\"femi\", \"masc\"),\n                 labels = c(\"Feminino\", \"Masculino\")\n                 ),\n    age_min = as.numeric(str_sub(age_label, 1, 2)),\n    age_label = factor(age_label),\n    age_label = fct_reorder(age_label, -age_min),\n    # Cria o label de percentual para facilitar\n    share_label = paste0(format(share * 100, decimal.mark = \",\"), \"%\")\n  )\n\n\n\n\nReplicando o Gráfico\n\n\nBásico\nEssencialmente, a visualização do Nexo é um gráfico de colunas deitado; o valor percentual de cada coluna é impresso nos cantos do gráfico e as cores representam os diferentes sexos. O código abaixo captura o essencial do gráfico.\n\n\nCode\nggplot(dados, aes(age_label, share, fill = sex)) +\n  geom_col(position = \"fill\") +\n  geom_hline(yintercept = 0, lwd = 1) +\n  geom_hline(yintercept = 1, lwd = 1) +\n  geom_hline(yintercept = 0.5, linetype = 2, lwd = 0.25) +\n  geom_text(\n    data = filter(dados, sex == \"Masculino\"),\n    aes(age_label, 0.075, label = share_label),\n    color = \"white\"\n  ) +\n  geom_text(\n    data = filter(dados, sex == \"Feminino\"),\n    aes(age_label, 0.925, label = share_label),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#40c2cf\", \"#22678c\")) +\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nCompleto\nO gráfico completo faz diversas alterações temáticas no gráfico. O portal Nexo utiliza a fonte Gotham Rounded em algumas variantes. Para usar esta fonte é preciso tê-la instalada. Para evitar problemas, o código abaixo verifica se as fontes necessárias estão instaladas; caso contrário, usa-se a fonte Montserrat, do Google Fonts.\nO código abaixo replica o gráfico sem a legenda de cores e sem ajustar o alinhamento do título.\n\n\nCode\nggplot(dados, aes(age_label, share, fill = sex)) +\n  geom_col(position = \"fill\") +\n  geom_hline(yintercept = 0, lwd = 1) +\n  geom_hline(yintercept = 1, lwd = 1) +\n  geom_hline(yintercept = 0.5, linetype = 2, lwd = 0.25) +\n  geom_text(\n    data = filter(dados, sex == \"Masculino\"),\n    aes(age_label, 0.1, label = share_label),\n    color = \"white\",\n    family = \"Gotham Rounded Bold\",\n    size = 5\n  ) +\n  geom_text(\n    data = filter(dados, sex == \"Feminino\"),\n    aes(age_label, 0.9, label = share_label),\n    color = \"white\",\n    family = \"Gotham Rounded Bold\",\n    size = 5\n  ) +\n  scale_y_continuous(\n    breaks = c(0, 0.5, 1),\n    labels = c(\"0%\", \"50%\", \"100%\"),\n    position = \"right\"\n    ) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#40c2cf\", \"#22678c\")) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Percentual de carteiras de habilitação\",\n    subtitle = \"POR FAIXA ETÁRIA, EM JANEIRO DE 2024\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(\n      size = 14,\n      color = \"#000000\",\n      family = \"Gotham Rounded Bold\"\n      ),\n    axis.text.x = element_text(\n      size = 14,\n      color = \"#747474\",\n      family = \"Gotham Rounded Light\"\n    ),\n    plot.title = element_text(\n      family = \"Crimson Text\",\n      size = 20,\n      face = \"bold\",\n      hjust = -0.25),\n    plot.subtitle = element_text(\n      family = \"Gotham Rounded Medium\",\n      size = 10,\n      hjust = -0.2\n    )\n  )\n\n\n\n\n\n\n\n\n\nHá várias maneiras de replicar a legenda do gráfico original. Talvez a mais simples seja usar algum software de edição de imagem. Uma solução usando apenas ggplot2 é criar um gráfico auxiliar, que contém a legenda e então compor os gráficos usando patchwork.\n\n\nCode\n# Gráfico com a legenda --------------------------------------------\n\n# data.frame auxiliar para montar a legenda\ndf_aux &lt;- tibble(\n  x = 1,\n  y = c(0.5, 0.5),\n  ypos = c(0.25, 0.75),\n  label = c(\"Masculino\", \"Feminino\")\n)\n\n# Gráfico com a legenda de cores\np_legend &lt;- ggplot(df_aux, aes(x, y, fill = label)) +\n  geom_col() +\n  geom_text(\n    aes(y = ypos, label = label),\n    color = \"white\",\n    family = \"Gotham Rounded Bold\",\n    size = 3) +\n  scale_fill_manual(values = c(\"#40c2cf\", \"#22678c\")) +\n  guides(fill = 'none') +\n  coord_flip() +\n  theme_void()\n\n# Gráfico principal --------------------------------------------\n\n# Gráfico principal (sem título e sem subtítulo)\np_main &lt;- ggplot(dados, aes(age_label, share, fill = sex)) +\n  geom_col(position = \"fill\") +\n  geom_hline(yintercept = 0, lwd = 1) +\n  geom_hline(yintercept = 1, lwd = 1) +\n  geom_hline(yintercept = 0.5, linetype = 2, lwd = 0.25) +\n  geom_text(\n    data = filter(dados, sex == \"Masculino\"),\n    aes(age_label, 0.075, label = share_label),\n    color = \"white\",\n    family = \"Gotham Rounded Bold\",\n    size = 5\n  ) +\n  geom_text(\n    data = filter(dados, sex == \"Feminino\"),\n    aes(age_label, 0.925, label = share_label),\n    color = \"white\",\n    family = \"Gotham Rounded Bold\",\n    size = 5\n  ) +\n  scale_y_continuous(\n    breaks = c(0, 0.5, 1),\n    labels = c(\"0%\", \"50%\", \"100%\"),\n    position = \"right\"\n  ) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#40c2cf\", \"#22678c\")) +\n  guides(fill = \"none\") +\n  labs(x = NULL,y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(\n      size = 14,\n      color = \"#000000\",\n      family = \"Gotham Rounded Bold\"\n    ),\n    axis.text.x = element_text(\n      size = 14,\n      color = \"#747474\",\n      family = \"Gotham Rounded Light\"\n    )\n  )\n\n# Compor os gráficos -----------------------------------------------\n\n# Coloca um espaço vazio acima do gráfico principal\npanel &lt;- plot_spacer() / p_main + plot_layout(heights = c(0.1, 0.9))\n\n# Gráfico final\npanel +\n  # Insere a legenda no espaço vazio acima do gráfico principal\n  inset_element(p_legend, left = -0.125, bottom = 1.1, right = 0.5, top = 1.2) +\n  # Adiciona título e subtítulo\n  plot_annotation(\n    title = \"Percentual de carteiras de habilitação\",\n    subtitle = \"POR FAIXA ETÁRIA, EM JANEIRO DE 2024\",\n  ) &\n  # Ajusta fonte, tamanho e posição\n  theme(\n    plot.title = element_text(\n      family = \"Crimson Text\",\n      size = 20,\n      face = \"bold\",\n      hjust = 0.05),\n    plot.subtitle = element_text(\n      family = \"Gotham Rounded Medium\",\n      size = 10,\n      hjust = 0.040\n    )\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#dados-5",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#dados-5",
    "title": "Replicando gráficos",
    "section": "Dados",
    "text": "Dados\nOs dados originais estão disponíveis on GitHub da The Economist mas eu não consegui encontrar o código que gera o gráfico acima. Como resultado, vou tentar adivinhar quais colunas, de fato, são utilizadas no gráfico. Além disso, como a fonte da The Economist é proprietária vou utilizar a Lato, da Google Fonts.\n\n\nCode\ndat &lt;- readr::read_csv(here::here(\"static/data/gdp_over_hours_worked_with_estimated_hours_worked.csv\"))\n\ncountries_sel &lt;- c(\"Norway\", \"Belgium\", \"Austria\", \"United States\", \"Germany\")\n\nmeasures &lt;- c(\"gdp_over_pop\", \"gdp_ppp_over_pop\", \"gdp_ppp_over_k_hours_worked\")\n\nsub &lt;- dat |&gt; \n  select(country, year, all_of(measures)) |&gt; \n  na.omit()\n\nranking &lt;- sub |&gt; \n  filter(year == max(year)) |&gt; \n  pivot_longer(cols = -c(country, year), names_to = \"measure\") |&gt; \n  mutate(rank = rank(-value), .by = \"measure\")\n\nranking &lt;- ranking |&gt; \n  mutate(\n    highlight = if_else(country %in% countries_sel, country, \"\"),\n    highlight = factor(highlight, levels = c(countries_sel, \"\")),\n    is_highlight = factor(if_else(country %in% countries_sel, 1L, 0L)),\n    rank_labels = if_else(rank %in% c(1, 5, 10, 15, 20), rank, NA),\n    rank_labels = stringr::str_replace(rank_labels, \"^1$\", \"1st\"),\n    measure = factor(measure, levels = measures)\n    )\n\ncores &lt;- c(\"#101010\", \"#f7443e\", \"#8db0cc\", \"#fa9494\", \"#225d9f\", \"#c7c7c7\")\n\ndf_gdp &lt;- tibble(\n  measure = measures,\n  measure_label = c(\n    \"GDP per person at market rates\",\n    \"Adjusted for cost differences*\",\n    \"Adjusted for costs and hours worked\"\n  ),\n  position = -1.5\n)\n\ndf_gdp &lt;- df_gdp |&gt; \n  mutate(\n    measure = factor(measure, levels = measures),\n    measure_label = stringr::str_wrap(measure_label, width = 12),\n    measure_label = paste0(\"  \", measure_label)\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-5",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#replicando-o-gráfico-5",
    "title": "Replicando gráficos",
    "section": "Replicando o gráfico",
    "text": "Replicando o gráfico"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#básico-5",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#básico-5",
    "title": "Replicando gráficos",
    "section": "Básico",
    "text": "Básico\nA versão simplificada do gráfico está resumida no código abaixo. Vale notar o uso da coord_cartesian para “cortar o gráfico” sem perder informação. Não é muito usual utilizar linewidth como um elemento estético dentro de aes mas pode-se ver como isto é bastante simples e como isto economiza algumas linhas de código, quando comparado com o gráfico anterior.\n\nlibrary(ggbump)\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]]),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2))"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#completo-5",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#completo-5",
    "title": "Replicando gráficos",
    "section": "Completo",
    "text": "Completo\nO objetivo deste post é de sempre fazer o máximo possível usando ggplot2 mas, na prática, as caixas de texto acima do gráfico podem ser feitas num software externo. Não é muito fácil usar caracteres especiais (neste caso, flechas) e a própria fonte (Lato) não inclui flechas em unicode. Pode-se melhorar a ordem da sobreposição das linhas usando geom_bump duas vezes como fizemos no gráfico dos imóveis, mas isto exigiria várias linhas adicionais de código.\n\n\nCode\nfont_add_google(\"Lato\", \"Lato\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight != 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight == 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\",\n    size = 3\n  ) +\n  geom_text(\n    data = filter(ranking, measure == measures[[1]]),\n    aes(x = measure, y = rank, label = rank_labels),\n    nudge_x = -0.15,\n    hjust = 0,\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = df_gdp,\n    aes(x = measure, y = position, label = measure_label),\n    inherit.aes = FALSE,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\",\n    size = 3\n  ) +\n  annotate(\"text\", x = 1, y = -2.5, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 2, y = -2.5, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 3, y = -2.5, label = expression(\"\\u2193\")) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2)) +\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-3",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-3",
    "title": "Replicando gráficos",
    "section": "Gráfico",
    "text": "Gráfico\n\nDados\nOs dados podem ser baixados diretamente do site da OWID ou do meu GitHub.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nÉ preciso fazer apenas alguns ajustes nos dados. Em particular precisamos filtrar o ano e juntar a tabela de países com a de continentes.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")\n\n\n\n\nReplicando o gráfico\n\n\nBásico\nA visualização, fundamentalmente, é um gráfico de dispersão em que as cores representam o continente de cada país e em que o tamanho do círculo é proporcional à população de cada país. É tipo de gráfico também é conhecido como “bubble plot”.\n\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    )\n\n\n\n\n\n\nCompleto\nA versão finalizada do gráfico exige bastante trabalho manual, já que é necessário destacar o nome dos países. Fora isso, é preciso ajustar o tamanho dos círculos, trocar a paleta de cores, inserir as informações textuais e ajustar a legenda de cores. Para este gráfico uso as fontes Lato e Playfair Display. Assim como no gráfico do G1, é preciso usar override.aes para modificar a legenda de cores.\n\n\nCode\nfont_add_google(\"Lato\", \"Lato\")\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\n# Tema do gráfico\ntheme_owid &lt;- theme_minimal() +\n  theme(\n    # Linhas de grade\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(linetype = 3, color = \"#DDDDDD\"),\n    \n    # Título, subtítulo e nota de rodapé\n    text = element_text(family = \"Lato\"),\n    title = element_text(family = \"Lato\"),\n    \n    plot.caption = element_text(color = \"#777777\", hjust = 0, size = 8),\n    plot.title = element_text(\n      color = \"#444444\",\n      family = \"Playfair Display\",\n      size = 18),\n    plot.subtitle = element_text(color = \"#666666\", size = 11),\n    # Textos nos eixos\n    axis.title = element_text(color = \"#000000\", size = 9),\n    axis.text = element_text(color = \"#666666\", size = 12),\n    # Legenda de cores\n    legend.key.size = unit(5, \"pt\"),\n    legend.position = \"right\",\n    legend.text = element_text(size = 10),\n    # Margens do gráfico\n    plot.margin = margin(rep(10, 4))\n  )\n\n#&gt; Countries to highlight\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\n#&gt; Caption\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n#&gt; Subtitle\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\n#&gt; x-axis labels\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\n#&gt; Colors\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\n# Gráfico final\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_label_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3,\n    force = 5,\n    family = \"Lato\",\n    label.padding = unit(0.05, \"lines\"),\n    label.size = NA\n    ) +\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  scale_y_continuous(breaks = 3:7) +\n  scale_size_continuous(range = c(1, 15)) +\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  guides(\n    color = \"none\",\n    size = \"none\",\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    ) +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    ) +\n  theme_owid"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-1-1",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-1-1",
    "title": "Replicando gráficos",
    "section": "Gráfico 1",
    "text": "Gráfico 1\nEsta visualização da The Economist mostra um ranking da riqueza entre países. O artigo original discute diferentes maneiras de mensurar e de comparar a riqueza entre países. Vamos nos focar apenas no gráfico abaixo. Este gráfico mostra um ranking dos países mais ricos do mundo segundo três métricas.\nNa primeira “coluna”, temos o PIB per capita a preços correntes convencional. Esta é a medida mais crua de riqueza disponível. Na segunda coluna temos o PIB per capita em paridade de poder de compra (PPC), que ajusta a medida do PIB per capita pelo “custo de vida” de cada país. Por fim, a última coluna ajusta o PIB per capita em PPC pela número médio de horas trabalhado em cada país. Note como esta medida eleva consideravelmente a posição de países europeus como Bélgica, Alemanha, Áustria e Dinamarca, enquanto derruba alguns países como EUA e Singapura.\nNum post anterior, eu descrevi em maiores detalhes este tipo de gráfico.\n\n\n\n\n\n\nDados\nOs dados originais estão disponíveis on GitHub da The Economist mas eu não consegui encontrar o código que gera o gráfico acima. Como resultado, vou tentar adivinhar quais colunas, de fato, são utilizadas no gráfico. Além disso, como a fonte da The Economist é proprietária vou utilizar a Lato, da Google Fonts.\n\n\nCode\ndat &lt;- readr::read_csv(\"/Users/viniciusoike/Documents/GitHub/restateinsight/static/data/gdp_over_hours_worked_with_estimated_hours_worked.csv\")\n\ncountries_sel &lt;- c(\"Norway\", \"Belgium\", \"Austria\", \"United States\", \"Germany\")\n\nmeasures &lt;- c(\"gdp_over_pop\", \"gdp_ppp_over_pop\", \"gdp_ppp_over_k_hours_worked\")\n\nsub &lt;- dat |&gt; \n  select(country, year, all_of(measures)) |&gt; \n  na.omit()\n\nranking &lt;- sub |&gt; \n  filter(year == max(year)) |&gt; \n  pivot_longer(cols = -c(country, year), names_to = \"measure\") |&gt; \n  mutate(rank = rank(-value), .by = \"measure\")\n\nranking &lt;- ranking |&gt; \n  mutate(\n    highlight = if_else(country %in% countries_sel, country, \"\"),\n    highlight = factor(highlight, levels = c(countries_sel, \"\")),\n    is_highlight = factor(if_else(country %in% countries_sel, 1L, 0L)),\n    rank_labels = if_else(rank %in% c(1, 5, 10, 15, 20), rank, NA),\n    rank_labels = stringr::str_replace(rank_labels, \"^1$\", \"1st\"),\n    measure = factor(measure, levels = measures)\n    )\n\ncores &lt;- c(\"#101010\", \"#f7443e\", \"#8db0cc\", \"#fa9494\", \"#225d9f\", \"#c7c7c7\")\n\ndf_gdp &lt;- tibble(\n  measure = measures,\n  measure_label = c(\n    \"GDP per person at market rates\",\n    \"Adjusted for cost differences*\",\n    \"Adjusted for costs and hours worked\"\n  ),\n  position = -1.5\n)\n\ndf_gdp &lt;- df_gdp |&gt; \n  mutate(\n    measure = factor(measure, levels = measures),\n    measure_label = stringr::str_wrap(measure_label, width = 12),\n    measure_label = paste0(\"  \", measure_label)\n    )\n\n\n\n\nReplicando o gráfico\n\n\nBásico\nA versão simplificada do gráfico está resumida no código abaixo. Vale notar o uso da coord_cartesian para “cortar o gráfico” sem perder informação. Não é muito usual utilizar linewidth como um elemento estético dentro de aes mas pode-se ver como isto é bastante simples e como isto economiza algumas linhas de código, quando comparado com o gráfico anterior.\n\nlibrary(ggbump)\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]]),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\"\n  ) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2))\n\n\n\n\n\n\n\n\n\n\nCompleto\nO objetivo deste post é de sempre fazer o máximo possível usando ggplot2 mas, na prática, as caixas de texto acima do gráfico podem ser feitas num software externo. Não é muito fácil usar caracteres especiais (neste caso, flechas) e a própria fonte (Lato) não inclui flechas em unicode. Pode-se melhorar a ordem da sobreposição das linhas usando geom_bump duas vezes como fizemos no gráfico dos imóveis, mas isto exigiria várias linhas adicionais de código.\n\n\nCode\nfont_add_google(\"Lato\", \"Lato\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\nggplot(ranking, aes(measure, rank, group = country)) +\n  geom_bump(aes(color = highlight, linewidth = is_highlight)) +\n  geom_point(shape = 21, color = \"white\", aes(fill = highlight), size = 3) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight != 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = filter(ranking, measure == measures[[3]], is_highlight == 1L),\n    aes(x = measure, y = rank, label = country),\n    nudge_x = 0.05,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\",\n    size = 3\n  ) +\n  geom_text(\n    data = filter(ranking, measure == measures[[1]]),\n    aes(x = measure, y = rank, label = rank_labels),\n    nudge_x = -0.15,\n    hjust = 0,\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = df_gdp,\n    aes(x = measure, y = position, label = measure_label),\n    inherit.aes = FALSE,\n    hjust = 0,\n    family = \"Lato\",\n    fontface = \"bold\",\n    size = 3\n  ) +\n  annotate(\"text\", x = 1, y = -2.5, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 2, y = -2.5, label = expression(\"\\u2193\")) +\n  annotate(\"text\", x = 3, y = -2.5, label = expression(\"\\u2193\")) +\n  coord_cartesian(ylim = c(21, -2)) +\n  scale_color_manual(values = cores) +\n  scale_fill_manual(values = cores) +\n  scale_linewidth_manual(values = c(0.5, 1.2)) +\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    axis.text = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-2-1",
    "href": "posts/general-posts/2023-11-replicating-plots/index.html#gráfico-2-1",
    "title": "Replicando gráficos",
    "section": "Gráfico 2",
    "text": "Gráfico 2\nUma recente edição da revista inglesa The Economist exibe uma série de listras coloridas em sua capa. Elas formam um degradê que vai de um azul escuro até um vermelho intenso. Cada listra representa a temperatura de um ano e a linha do tempo vai desde o 1850 até o presente. A mensagem é bastante clara: o planeta esta cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Esta imagem é creditada a Ed Hawkings, editor do Climate Lab Book.\n\n\n\n\n\n\nDados\nPara ser preciso, a imagem não plota a temperatura de cada ano, mas sim o quanto cada ano se desvia da temperatura média do período 1971-2000. Isto é, anos acima dessa média têm um valor positivo, valores abaixo dessa média, valores negativos.\nExiste uma base gtemp_both do pacote astsa que mede estas anomalias climáticas no período 1850-2023. Contudo, esta série usa o período 1991-2020 como referência. Assim, os valores são um pouco diferentes. Ainda assim, acho que vale a pena usar esta base pela sua conveniência.\n\nlibrary(astsa)\n\n# Converte o objeto para data.frame\ntemperature &lt;- tibble(\n  year = as.numeric(time(gtemp_both)),\n  temp = as.numeric(gtemp_both)\n)\n\n\n\nReplicando o gráfico\n\n\nBásico\nÉ possível reproduzir o essencial deste gráfico com poucas linhas de código. Aqui, utilizo geom_tile para fazer as listras e scale_fill_gradient2 para construir uma paleta de cores divergente.\n\n# Monta o gráfico\nggplot(temperature, aes(x = year, y = 0, fill = temp)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#104e8b\", high = \"#ff0000\") +\n  guides(fill = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nCompleto\nO gráfico original usa a paleta de cores do ColorBrewer: especificamente, ela combina as cores mais saturadas das paletas “Blues” e “Reds”. A fonte do texto é Georgia. Eu adapto um pouco os anos já que os dados estão mais atualizados.\n\n\nCode\nfont_add(\"Georgia\", \"Georgia.ttf\")\nshowtext_auto()\n\n# Paleta de cores\nblues &lt;- RColorBrewer::brewer.pal(9, \"Blues\")[2:9]\nreds &lt;- RColorBrewer::brewer.pal(9, \"Reds\")[2:9]\n\npalette &lt;- c(rev(blues), reds)\n\n# Data.frames auxiliares para plotar as anotações de texto\ndf_aux_title &lt;- tibble(x = 1940, y = 0, label = \"The Climate Issue\")\ndf_aux_anos &lt;- tibble(\n  label = seq(1860, 2020, 40),\n  x = c(1865, 1900, 1940, 1980, 2010)\n)\n\nggplot() +\n  geom_tile(data = temperature, aes(x = year, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 6,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = x, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 11,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_gradientn(colors = palette) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n  )\n\n\n\n\n\n\n\n\n\nO resultado acima é bastante satisfatório, mesmo quando consideramos que a base de dados é um pouco diferente e que a série inclui dados mais atuais. Vale notar que a visualização abaixo não é a mais precisa, pois alguns valores positivos estão sendo mapeados em tons de azul-claro.\nPara chegar numa visualização mais precisa é preciso ajustar a paleta de cores.\n\n\nCode\nblues &lt;- RColorBrewer::brewer.pal(9, \"Blues\")[3:6]\nreds &lt;- RColorBrewer::brewer.pal(9, \"Reds\")[2:9]\npalette &lt;- c(rev(blues), reds)\n\nggplot(temperature) +\n  geom_tile(aes(x = year, y = 0, fill = temp), height = 3) +\n  geom_line(aes(year, temp)) +\n  geom_point(aes(year, temp)) +\n  labs(x = NULL, y = NULL) +\n  coord_cartesian(ylim = c(-1, NA)) +\n  scale_fill_stepsn(\n    name = \"\",\n    colors = palette,\n    breaks = round(seq(-0.6, 1.2, 0.2), 1)\n    ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.key.size = unit(1.5, \"cm\")\n    )\n\n\n\n\n\n\n\n\n\nO resultado final (atualizado) fica assim.\n\n\nCode\nggplot() +\n  geom_tile(data = temperature, aes(x = year, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 5,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = x, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 10,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_stepsn(colors = palette, breaks = round(seq(-0.6, 1.2, 0.2), 1)) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-sp-grid-houses/index.html#footnotes",
    "href": "posts/general-posts/2024-04-sp-grid-houses/index.html#footnotes",
    "title": "Domicilios em Sao Paulo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDefinidos como domicílios particulares permanentes, segundo o Censo do IBGE (2022).↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-importando-pdf/index.html",
    "href": "posts/general-posts/2024-04-importando-pdf/index.html",
    "title": "Importando dados em PDF no R",
    "section": "",
    "text": "Trabalhar com dados e visualizar dados, muitas vezes, são objetivos que não se conciliam facilmente. Uma tabela, otimizada para leitura humana, dificilmente será a mais apropriada para análise de dados. De maneira geral, a maior parte dos dados que se encontra são sujos, num sentido amplo. Isto não significa que estes dados estejam errados, ou com algum tipo de imprecisão; quer dizer que os dados não estão num formato tabular, apropriado para a análise de dados.\nTalvez um dos formatos mais frustrantes para se consumir dados seja o PDF. Arquivos PDF não são um típico formato de armazenamento de dados, como CSV, XLSX, SAV, etc. PDFs são relatórios que combinam texto, imagens, tabelas, fórmulas, etc. Ainda assim, não é incomum receber tabelas de dados salvas dentro de arquivos PDF.\n\n\n\n\n\nNeste post vou mostrar uma solução para importar dados em formato PDF de maneira fácil e prática usando R.\n\n\nO pacote tabulizer oferece funções simples para importar tabelas de dados que estão salvas em formato PDF. Como o formato PDF é muito flexível e diverso, não é possível garantir que o tabulizer funcione sempre. Além disso, como veremos adiante, o processo de limpeza dos dados é bastante artesanal, variando caso a caso.\n\n\nComo exemplo, vamos importar dados sobre a composição religiosa de cada país. A tabela provém do Pew Research Center e pode ser baixada no seguinte link. Mais recentemente, foi disponibilizada uma versão interativa destes dados, com uma opção mais simples de consumo. Mas, para seguir os objetivos deste post, vamos seguir com o arquivo PDF.\nA imagem abaixo mostra o começo da tabela em PDF. A tabela é dividida em 6 páginas e tem dez colunas.\n\n\n\n\n\n\n\n\nAbaixo listo os pacotes necessários para este tutorial.\n\nlibrary(tabulizer)\nlibrary(dplyr)\nlibrary(tidyr, include.only = \"pivot_longer\")\nlibrary(janitor)\nlibrary(stringr)\nlibrary(countries, include.only = \"country_name\")\n\nO primeiro passo é baixar os dados e importar a tabela. Note que seria possível fazer isto de maneira mais automatizada usando download.file, criando um arquivo temporário, etc. Contudo, como trata-se de um dado estático, que será consumido uma única vez, acaba sendo mais simples baixá-lo manualmente. Para ler as tabelas do PDF usa-se a função extract_tables(). No código abaixo eu utilizo o pacote here, que não é essencial, mas é recomendável. Para mais sobre o pacote veja ‘Escrevendo paths relativos com here’.\n\n# path fake, use o seu path\npath = here::here(\"project/data/globalReligion-tables.pdf\")\ntables = extract_tables(path)\n\nO resultado é uma lista de seis elementos (um para cada página). Pode-se ver também que houve algum problema na hora de importar os dados. Como comentei, o padrão PDF não é apropriado para compatilhamento de dados; inevitavelmente, o processo de importação e limpeza será caso a caso.\n\nstr(tables)\n\nList of 6\n $ : chr [1:40, 1:10] \"\" \"COUNTRY\" \"Afghanistan\" \"Albania\" ...\n $ : chr [1:40, 1:10] \"ayman Islands\" \"entral African Republic\" \"had\" \"hannel Islands\" ...\n $ : chr [1:40, 1:10] \"reece\" \"reenland\" \"renada\" \"uadeloupe\" ...\n $ : chr [1:40, 1:10] \"uxembourg\" \"acau\" \"adagascar\" \"alawi\" ...\n $ : chr [1:40, 1:10] \"apua New Guinea\" \"araguay\" \"eru\" \"hilippines\" ...\n $ : chr [1:41, 1:10] \"weden\" \"witzerland\" \"yria\" \"aiwan\" ...\n\n\n\n\n\nOlhando as primeiras linhas do primeiro elemento, vemos que o cabeçalho foi importado em duas linhas distintas.\n\ntables[[1]][1:5, ]\n\n     [,1]          [,2]         [,3]        [,4]      [,5]       [,6]     \n[1,] \"\"            \"COUNTRY\"    \"PERCENT\"   \"PERCENT\" \"PERCENT\"  \"PERCENT\"\n[2,] \"COUNTRY\"     \"POPULATION\" \"CHRISTIAN\" \"MUSLIM\"  \"UNAFFIL.\" \"HINDU\"  \n[3,] \"Afghanistan\" \"31,410,000\" \"0.1 %\"     \"99.7 %\"  \"&lt; 0.1 %\"  \"&lt; 0.1 %\"\n[4,] \"Albania\"     \"3,200,000\"  \"18.0\"      \"80.3\"    \"1.4\"      \"&lt; 0.1\"  \n[5,] \"Algeria\"     \"35,470,000\" \"0.2\"       \"97.9\"    \"1.8\"      \"&lt; 0.1\"  \n     [,7]       [,8]       [,9]       [,10]    \n[1,] \"PERCENT\"  \"FOLK\"     \"OTHER\"    \"PERCENT\"\n[2,] \"BUDDHIST\" \"RELIGION\" \"RELIGION\" \"JEWISH\" \n[3,] \"&lt; 0.1 %\"  \"&lt; 0.1 %\"  \"&lt; 0.1 %\"  \"&lt; 0.1 %\"\n[4,] \"&lt; 0.1\"    \"&lt; 0.1\"    \"0.2\"      \"&lt; 0.1\"  \n[5,] \"&lt; 0.1\"    \"&lt; 0.1\"    \"&lt; 0.1\"    \"&lt; 0.1\"  \n\n\nVamos primeiro separar estas linhas e montar o cabeçalho (nome das colunas) da nossa tabela. Queremos (1) juntar as informações; e (2) renomear as colunas. Assim, queremos juntar \"PERCENT\" com \"CHRISTIAN\" e depois tornar o nome limpo, i.e., \"percent_christian\".\n\n# Seleciona apenas as duas primeiras linhas do primeiro elemento\nheader = tables[[1]][1:2, ]\n\n# Junta as colunas e 'limpa' os nomes\ncol_names = apply(header, 2, \\(x) make_clean_names(str_c(x, collapse = \"_\")))\n\ncol_names\n\n [1] \"country\"            \"country_population\" \"percent_christian\" \n [4] \"percent_muslim\"     \"percent_unaffil\"    \"percent_hindu\"     \n [7] \"percent_buddhist\"   \"folk_religion\"      \"other_religion\"    \n[10] \"percent_jewish\"    \n\n\nAgora vamos empilhar os dados e usar o cabeçalho. Além disso, vamos inspecionar as primeiras linhas.\n\ntable_data = bind_rows(lapply(tables, as.data.frame))\ntable_data = table_data[3:nrow(table_data), ]\nnames(table_data) = col_names\n\nhead(table_data)\n\n         country country_population percent_christian percent_muslim\n3    Afghanistan         31,410,000             0.1 %         99.7 %\n4        Albania          3,200,000              18.0           80.3\n5        Algeria         35,470,000               0.2           97.9\n6 American Samoa             70,000              98.3          &lt; 0.1\n7        Andorra             80,000              89.5            0.8\n8         Angola         19,080,000              90.5            0.2\n  percent_unaffil percent_hindu percent_buddhist folk_religion other_religion\n3         &lt; 0.1 %       &lt; 0.1 %          &lt; 0.1 %       &lt; 0.1 %        &lt; 0.1 %\n4             1.4         &lt; 0.1            &lt; 0.1         &lt; 0.1            0.2\n5             1.8         &lt; 0.1            &lt; 0.1         &lt; 0.1          &lt; 0.1\n6             0.7         &lt; 0.1              0.3           0.4            0.3\n7             8.8           0.5            &lt; 0.1         &lt; 0.1            0.1\n8             5.1         &lt; 0.1            &lt; 0.1           4.2          &lt; 0.1\n  percent_jewish\n3        &lt; 0.1 %\n4          &lt; 0.1\n5          &lt; 0.1\n6          &lt; 0.1\n7            0.3\n8          &lt; 0.1\n\n\nA primeira coluna de nossa tabela é de strings, enquanto as demais são todas numéricas. Para facilitar a leitura humana, os números foram formatados, com separador de milhar, sinal de porcentagem, etc. O próximo passo é formatar os números: o código abaixo remove este símbolos e converte para numérico.\n\ntable_data = table_data |&gt; \n  as_tibble() |&gt; \n  mutate(across(2:last_col(), ~as.numeric(str_remove_all(.x, \"[%&lt;&gt;,]\"))))\n\nPor fim, se olharmos para as últimas linhas da tabela, veremos que as linhas representam regiões ao invés de países. Isto é muito frequente em tabelas: é costumeiro acrescentar “totais” ou “agregados” nas últimas linhas de uma tabela para facilitar a interpretação dos dados.\nEm termos de análise, contudo, é importante que cada linha representa a mesma unidade. Isto é, cada linha na tabela principal deve representar um país.\n\nslice_tail(table_data, n = 7)\n\n# A tibble: 7 × 10\n  country                 country_population percent_christian percent_muslim\n  &lt;chr&gt;                                &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1 sia-Pacific                     4054990000               7.1           24.3\n2 urope                            742550000              75.2            5.9\n3 atin America-Caribbean           590080000              90              0.1\n4 iddle East-North Africa          341020000               3.7           93  \n5 orth America                     344530000              77.4            1  \n6 ub-Saharan Africa                822720000              62.9           30.2\n7 orld                            6895890000              31.5           23.2\n  percent_unaffil percent_hindu percent_buddhist folk_religion other_religion\n            &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1            21.2          25.3             11.9           9              1.3\n2            18.2           0.2              0.2           0.1            0.1\n3             7.7           0.1              0.1           1.7            0.2\n4             0.6           0.5              0.1           0.3            0.1\n5            17.1           0.7              1.1           0.3            0.6\n6             3.2           0.2              0.1           3.3            0.2\n7            16.3          15                7.1           5.9            0.8\n  percent_jewish\n           &lt;dbl&gt;\n1              0\n2              0\n3              0\n4              1\n5              1\n6              0\n7              0\n\n\nVamos guardar esta informação num objeto secundário chamado table_regions.\n\ntable_regions &lt;- slice_tail(table_data, n = 7)\ntable_data &lt;- slice(table_data, 1:(nrow(table_data) - 7))\n\nPor fim, temos um problema específico. O nome de muitos países saiu cortado, faltando a primeira letra. Para resolver isto vamos usar o pacote countries que oferece um fuzzy matching para o nome de países.\nPara melhor compreender o uso do pacote vamos analisar um exemplo. Olhando a linha 51, vemos que o país importado foi “yprus”. O correto seria “Cyprus”.\n\ntest_name = table_data[51, ]$country\ntest_name\n\n[1] \"yprus\"\n\n\nA função country_name consegue identificar o nome corretamente.\n\ncountry_name(test_name, fuzzy_match = TRUE, to = \"name_en\")\n\n[1] \"Cyprus\"\n\n\nPode-se aplicar esta função em todos os nomes dos países. Além disso, é importante verificar se houve erros. O código busca qualquer entrada com NA em name ou iso3c.\n\ntest = table_data |&gt; \n  mutate(\n    name = country_name(country, to = \"name_en\", fuzzy_match = TRUE),\n    iso3c = country_name(country, to = \"ISO3\", fuzzy_match = TRUE)\n  )\n\nerrors = test |&gt; \n  filter(if_any(name:iso3c, ~is.na(.))) |&gt; \n  pull(country)\n\nerrors\n\n[1] \"hannel Islands\"         \"osovo\"                  \"etherlands Antilles\"   \n[4] \"orthern Mariana Is.\"    \"alestinian territories\" \"ruguay\"                \n\n\nComo se vê, o algoritmo falha em identificar alguns países, que serão corrigidos manualmente. O foco do post não é de como usar o algortimo de fuzzy matching então não vou entrar em muitos detalhes. Vale notar que é possível acrescentar o argumento verbose = TRUE na função country_name para receber um retorno mais informativo.\nAlém dos valores ausentes, vou verificar também os matchings duplicados. O código abaixo faz uma correção manual destes casos. Vale notar que há um caso “impossível”: Gambia e Zambia, pois ambos os países estão com o nome “ambia” na tabela importada. Neste caso, uso a informação da população para discriminar os casos.\n\nerrors = test |&gt;\n  get_dupes(name) |&gt; \n  select(name, country)\n  \ncorrection = c(\n  \"c\", \"k\", \"n\", \"n\", \"p\", \"u\", \"m\", \"m\", \"p\", \"\", \"m\", \"\", \"l\", \"e\", \"p\", \"f\",\n  \"i\", \"n\", \"z\", \"g\", \"u\", \"i\", \"o\", \"r\", \"r\", \"p\", \"s\", \"g\", \"u\", \"f\", \"u\"\n  )\n\nfix_table = errors |&gt; \n  mutate(country_fixed = str_c(str_to_upper(correction), country)) |&gt; \n  select(country, country_fixed)\n\ntable_data = table_data |&gt; \n  left_join(fix_table, by = 'country', relationship = \"many-to-many\") |&gt; \n  mutate(\n    temp_name = case_when(\n      country == \"ambia\" & country_population == 1730000 ~ \"Gambia\",\n      country == \"ambia\" & country_population &gt; 1730000 ~ \"Zambia\",\n      is.na(country_fixed) ~ country,\n      TRUE ~ country_fixed\n    ),\n    # faz o matching do nomes dos países\n    name = country_name(temp_name, to = \"name_en\", fuzzy_match = TRUE),\n    # resolve alguns casos extremos \"Channel Islands\"\n    name = if_else(is.na(name), temp_name, name),\n    # encontra o ISO3 code de cada país\n    iso3c = country_name(name, to = \"ISO3\", fuzzy_match = FALSE)\n  )\n\nOs erros finais acontecem porque alguns dos países listados não possuem ISO3, como \"Netherlands Antilles\"\n\n\n\nA tabela final é apresentada abaixo. Vale notar que alguns dos valores estão truncados, o que gera uma pequena imprecisão, como é o caso das entradas que eram listadas como “&lt; 0.1”.\n\ntable_data = table_data %&gt;%\n  select(name, iso3c, country_population:percent_jewish)\n\n\nDT::datatable(table_data)\n\n\n\n\n\n\n\n\n\nPor fim, vale notar que podemos melhorar ainda mais os dados acima. Na tabela acima, muitas das colunas são valores da variável “religião”; então podemos, transformar os dados em “tidy” convertendo-os em longitudinais da seguinte forma.\n\ntab_religion = table_data |&gt; \n  pivot_longer(\n    cols = percent_christian:percent_jewish,\n    names_to = \"religion\",\n    values_to = \"share\"\n    ) |&gt; \n  mutate(religion = str_remove(religion, \"(percent_)|(_religion)\"))\n\ntab_religion\n\n# A tibble: 1,872 × 5\n   name        iso3c country_population religion  share\n   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Afghanistan AFG             31410000 christian   0.1\n 2 Afghanistan AFG             31410000 muslim     99.7\n 3 Afghanistan AFG             31410000 unaffil     0.1\n 4 Afghanistan AFG             31410000 hindu       0.1\n 5 Afghanistan AFG             31410000 buddhist    0.1\n 6 Afghanistan AFG             31410000 folk        0.1\n 7 Afghanistan AFG             31410000 other       0.1\n 8 Afghanistan AFG             31410000 jewish      0.1\n 9 Albania     ALB              3200000 christian  18  \n10 Albania     ALB              3200000 muslim     80.3\n# ℹ 1,862 more rows\n\n\nAgora temos uma coluna religion que identifica cada uma das religiões consideradas na pesquisa e uma coluna share que mostra a representatividade de cada religão em cada país. Como comentado anteriormente, existem algumas impreciões pois substituímos os valores “&lt; 0.1” simplesmente por “0.1”, então alguns shares vão somar valores um pouco maiores do que 100.\nCom os dados neste formato fica fácil responder perguntas como: qual a religão dominante em cada país? Olhando os dados vemos que há 149 países em que a religião “dominante”, definida simplesmente como a religão que tem o maior share de convertidos, é o cristianismo. Há um gap considerável entre a religão muçulmana, com 47 países.\n\ntab_religion |&gt; \n  filter(share == max(share), .by = c(\"name\", \"iso3c\")) |&gt; \n  count(religion, sort = TRUE)\n\n# A tibble: 7 × 2\n  religion      n\n  &lt;chr&gt;     &lt;int&gt;\n1 christian   160\n2 muslim       50\n3 buddhist      8\n4 unaffil       7\n5 folk          3\n6 hindu         3\n7 jewish        1\n\n\nE qual o resultado quando se olha para o total da população religiosa? A religião cristã continua em primeiro lugar, com quase 1.6 bilhão de convertidos. Já a religião hindu, apesar de ser dominante em apenas 3 países, aparece com quase 1 bilhão de seguidores. Isto acontece porque esta é a religão dominante da Índia, que à época da pesquisa tinha 1.22 bilhão de habitantes.\n\ntab_religion |&gt; \n  filter(share == max(share), .by = c(\"name\", \"iso3c\")) |&gt; \n  mutate(total = country_population * share / 100) |&gt; \n  summarise(total_relig = sum(total), .by = \"religion\") |&gt; \n  arrange(desc(total_relig))\n\n# A tibble: 7 × 2\n  religion  total_relig\n  &lt;chr&gt;           &lt;dbl&gt;\n1 christian  1984890470\n2 muslim     1174670350\n3 hindu       998475870\n4 unaffil     824792400\n5 buddhist    138877020\n6 folk         50377350\n7 jewish        5565000\n\n\n\ntab_religion |&gt; \n  filter(share == max(share), .by = c(\"name\", \"iso3c\")) |&gt; \n  filter(religion == \"hindu\")\n\n# A tibble: 3 × 5\n  name      iso3c country_population religion share\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 India     IND           1224610000 hindu     79.5\n2 Mauritius MUS              1300000 hindu     56.4\n3 Nepal     NPL             29960000 hindu     80.7\n\n\n\n\n\n\nO código abaixo resume os passos da importação e limpeza dos dados.\n\nlibrary(tabulizer)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(tidyr, include.only = \"pivot_longer\")\nlibrary(countries, include.only = \"country_name\")\n\n\n# (opcional: baixa os dados)\nurl = \"https://pewresearch ...\"\ndownload.file(url, destfile = tempfile(fileext = \"pdf\"))\n\n# Importa a tabela\npath = here::here(\"project/data/globalReligion-tables.pdf\")\ntables = extract_tables(path)\n\n# Limpeza -------------------------------------------------------\n\n# Nome das colunas\n\nheader = tables[[1]][1:2, ]\n# Junta as colunas e 'limpa' os nomes\ncol_names = apply(header, 2, \\(x) make_clean_names(str_c(x, collapse = \"_\")))\n# Empilha os dados e define nome das colunas\ntable_data = bind_rows(lapply(tables, as.data.frame))\ntable_data = table_data[3:nrow(table_data), ]\nnames(table_data) = col_names\n\n# Converte colunas para numérico\ntable_data = table_data |&gt; \n  as_tibble() |&gt; \n  mutate(across(2:last_col(), ~as.numeric(str_remove_all(.x, \"[%&lt;&gt;,]\"))))\n\n# Remove as últimas sete linhas\ntable_regions &lt;- slice_tail(table_data, n = 7)\ntable_data &lt;- slice(table_data, 1:(nrow(table_data) - 7))\n\n# Nome dos países\n\n# Correção manual\ntest = table_data |&gt; \n  mutate(\n    name = country_name(country, to = \"name_en\", fuzzy_match = TRUE),\n    iso3c = country_name(country, to = \"ISO3\", fuzzy_match = TRUE)\n  )\n\nerrors = test |&gt;\n  get_dupes(name) |&gt; \n  select(name, country)\n  \ncorrection = c(\n  \"c\", \"k\", \"n\", \"n\", \"p\", \"u\", \"m\", \"m\", \"p\", \"\", \"m\", \"\", \"l\", \"e\", \"p\", \"f\",\n  \"i\", \"n\", \"z\", \"g\", \"u\", \"i\", \"o\", \"r\", \"r\", \"p\", \"s\", \"g\", \"u\", \"f\", \"u\"\n  )\n\nfix_table = errors |&gt; \n  mutate(country_fixed = str_c(str_to_upper(correction), country)) |&gt; \n  select(country, country_fixed)\n\n# Matching dos nomes\ntable_data = table_data |&gt; \n  left_join(fix_table, by = 'country', relationship = \"many-to-many\") |&gt; \n  mutate(\n    temp_name = case_when(\n      country == \"ambia\" & country_population == 1730000 ~ \"Gambia\",\n      country == \"ambia\" & country_population &gt; 1730000 ~ \"Zambia\",\n      is.na(country_fixed) ~ country,\n      TRUE ~ country_fixed\n    ),\n    # faz o matching do nomes dos países\n    name = country_name(temp_name, to = \"name_en\", fuzzy_match = TRUE),\n    # resolve alguns casos extremos \"Channel Islands\"\n    name = if_else(is.na(name), temp_name, name),\n    # encontra o ISO3 code de cada país\n    iso3c = country_name(name, to = \"ISO3\", fuzzy_match = FALSE)\n  ) |&gt; \n  select(name, iso3c, country_population:percent_jewish)\n\n# Tidy\n\n# Converte os dados para long\ntab_religion = table_data |&gt; \n  pivot_longer(\n    cols = percent_christian:percent_jewish,\n    names_to = \"religion\",\n    values_to = \"share\"\n    ) |&gt; \n  mutate(religion = str_remove(religion, \"(percent_)|(_religion)\"))"
  },
  {
    "objectID": "posts/general-posts/2024-04-importando-pdf/index.html#tabulizer",
    "href": "posts/general-posts/2024-04-importando-pdf/index.html#tabulizer",
    "title": "Importando dados em PDF no R",
    "section": "",
    "text": "O pacote tabulizer oferece funções simples para importar tabelas de dados que estão salvas em formato PDF. Como o formato PDF é muito flexível e diverso, não é possível garantir que o tabulizer funcione sempre. Além disso, como veremos adiante, o processo de limpeza dos dados é bastante artesanal, variando caso a caso.\n\n\nComo exemplo, vamos importar dados sobre a composição religiosa de cada país. A tabela provém do Pew Research Center e pode ser baixada no seguinte link. Mais recentemente, foi disponibilizada uma versão interativa destes dados, com uma opção mais simples de consumo. Mas, para seguir os objetivos deste post, vamos seguir com o arquivo PDF.\nA imagem abaixo mostra o começo da tabela em PDF. A tabela é dividida em 6 páginas e tem dez colunas.\n\n\n\n\n\n\n\n\nAbaixo listo os pacotes necessários para este tutorial.\n\nlibrary(tabulizer)\nlibrary(dplyr)\nlibrary(tidyr, include.only = \"pivot_longer\")\nlibrary(janitor)\nlibrary(stringr)\nlibrary(countries, include.only = \"country_name\")\n\nO primeiro passo é baixar os dados e importar a tabela. Note que seria possível fazer isto de maneira mais automatizada usando download.file, criando um arquivo temporário, etc. Contudo, como trata-se de um dado estático, que será consumido uma única vez, acaba sendo mais simples baixá-lo manualmente. Para ler as tabelas do PDF usa-se a função extract_tables(). No código abaixo eu utilizo o pacote here, que não é essencial, mas é recomendável. Para mais sobre o pacote veja ‘Escrevendo paths relativos com here’.\n\n# path fake, use o seu path\npath = here::here(\"project/data/globalReligion-tables.pdf\")\ntables = extract_tables(path)\n\nO resultado é uma lista de seis elementos (um para cada página). Pode-se ver também que houve algum problema na hora de importar os dados. Como comentei, o padrão PDF não é apropriado para compatilhamento de dados; inevitavelmente, o processo de importação e limpeza será caso a caso.\n\nstr(tables)\n\nList of 6\n $ : chr [1:40, 1:10] \"\" \"COUNTRY\" \"Afghanistan\" \"Albania\" ...\n $ : chr [1:40, 1:10] \"ayman Islands\" \"entral African Republic\" \"had\" \"hannel Islands\" ...\n $ : chr [1:40, 1:10] \"reece\" \"reenland\" \"renada\" \"uadeloupe\" ...\n $ : chr [1:40, 1:10] \"uxembourg\" \"acau\" \"adagascar\" \"alawi\" ...\n $ : chr [1:40, 1:10] \"apua New Guinea\" \"araguay\" \"eru\" \"hilippines\" ...\n $ : chr [1:41, 1:10] \"weden\" \"witzerland\" \"yria\" \"aiwan\" ...\n\n\n\n\n\nOlhando as primeiras linhas do primeiro elemento, vemos que o cabeçalho foi importado em duas linhas distintas.\n\ntables[[1]][1:5, ]\n\n     [,1]          [,2]         [,3]        [,4]      [,5]       [,6]     \n[1,] \"\"            \"COUNTRY\"    \"PERCENT\"   \"PERCENT\" \"PERCENT\"  \"PERCENT\"\n[2,] \"COUNTRY\"     \"POPULATION\" \"CHRISTIAN\" \"MUSLIM\"  \"UNAFFIL.\" \"HINDU\"  \n[3,] \"Afghanistan\" \"31,410,000\" \"0.1 %\"     \"99.7 %\"  \"&lt; 0.1 %\"  \"&lt; 0.1 %\"\n[4,] \"Albania\"     \"3,200,000\"  \"18.0\"      \"80.3\"    \"1.4\"      \"&lt; 0.1\"  \n[5,] \"Algeria\"     \"35,470,000\" \"0.2\"       \"97.9\"    \"1.8\"      \"&lt; 0.1\"  \n     [,7]       [,8]       [,9]       [,10]    \n[1,] \"PERCENT\"  \"FOLK\"     \"OTHER\"    \"PERCENT\"\n[2,] \"BUDDHIST\" \"RELIGION\" \"RELIGION\" \"JEWISH\" \n[3,] \"&lt; 0.1 %\"  \"&lt; 0.1 %\"  \"&lt; 0.1 %\"  \"&lt; 0.1 %\"\n[4,] \"&lt; 0.1\"    \"&lt; 0.1\"    \"0.2\"      \"&lt; 0.1\"  \n[5,] \"&lt; 0.1\"    \"&lt; 0.1\"    \"&lt; 0.1\"    \"&lt; 0.1\"  \n\n\nVamos primeiro separar estas linhas e montar o cabeçalho (nome das colunas) da nossa tabela. Queremos (1) juntar as informações; e (2) renomear as colunas. Assim, queremos juntar \"PERCENT\" com \"CHRISTIAN\" e depois tornar o nome limpo, i.e., \"percent_christian\".\n\n# Seleciona apenas as duas primeiras linhas do primeiro elemento\nheader = tables[[1]][1:2, ]\n\n# Junta as colunas e 'limpa' os nomes\ncol_names = apply(header, 2, \\(x) make_clean_names(str_c(x, collapse = \"_\")))\n\ncol_names\n\n [1] \"country\"            \"country_population\" \"percent_christian\" \n [4] \"percent_muslim\"     \"percent_unaffil\"    \"percent_hindu\"     \n [7] \"percent_buddhist\"   \"folk_religion\"      \"other_religion\"    \n[10] \"percent_jewish\"    \n\n\nAgora vamos empilhar os dados e usar o cabeçalho. Além disso, vamos inspecionar as primeiras linhas.\n\ntable_data = bind_rows(lapply(tables, as.data.frame))\ntable_data = table_data[3:nrow(table_data), ]\nnames(table_data) = col_names\n\nhead(table_data)\n\n         country country_population percent_christian percent_muslim\n3    Afghanistan         31,410,000             0.1 %         99.7 %\n4        Albania          3,200,000              18.0           80.3\n5        Algeria         35,470,000               0.2           97.9\n6 American Samoa             70,000              98.3          &lt; 0.1\n7        Andorra             80,000              89.5            0.8\n8         Angola         19,080,000              90.5            0.2\n  percent_unaffil percent_hindu percent_buddhist folk_religion other_religion\n3         &lt; 0.1 %       &lt; 0.1 %          &lt; 0.1 %       &lt; 0.1 %        &lt; 0.1 %\n4             1.4         &lt; 0.1            &lt; 0.1         &lt; 0.1            0.2\n5             1.8         &lt; 0.1            &lt; 0.1         &lt; 0.1          &lt; 0.1\n6             0.7         &lt; 0.1              0.3           0.4            0.3\n7             8.8           0.5            &lt; 0.1         &lt; 0.1            0.1\n8             5.1         &lt; 0.1            &lt; 0.1           4.2          &lt; 0.1\n  percent_jewish\n3        &lt; 0.1 %\n4          &lt; 0.1\n5          &lt; 0.1\n6          &lt; 0.1\n7            0.3\n8          &lt; 0.1\n\n\nA primeira coluna de nossa tabela é de strings, enquanto as demais são todas numéricas. Para facilitar a leitura humana, os números foram formatados, com separador de milhar, sinal de porcentagem, etc. O próximo passo é formatar os números: o código abaixo remove este símbolos e converte para numérico.\n\ntable_data = table_data |&gt; \n  as_tibble() |&gt; \n  mutate(across(2:last_col(), ~as.numeric(str_remove_all(.x, \"[%&lt;&gt;,]\"))))\n\nPor fim, se olharmos para as últimas linhas da tabela, veremos que as linhas representam regiões ao invés de países. Isto é muito frequente em tabelas: é costumeiro acrescentar “totais” ou “agregados” nas últimas linhas de uma tabela para facilitar a interpretação dos dados.\nEm termos de análise, contudo, é importante que cada linha representa a mesma unidade. Isto é, cada linha na tabela principal deve representar um país.\n\nslice_tail(table_data, n = 7)\n\n# A tibble: 7 × 10\n  country                 country_population percent_christian percent_muslim\n  &lt;chr&gt;                                &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1 sia-Pacific                     4054990000               7.1           24.3\n2 urope                            742550000              75.2            5.9\n3 atin America-Caribbean           590080000              90              0.1\n4 iddle East-North Africa          341020000               3.7           93  \n5 orth America                     344530000              77.4            1  \n6 ub-Saharan Africa                822720000              62.9           30.2\n7 orld                            6895890000              31.5           23.2\n  percent_unaffil percent_hindu percent_buddhist folk_religion other_religion\n            &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1            21.2          25.3             11.9           9              1.3\n2            18.2           0.2              0.2           0.1            0.1\n3             7.7           0.1              0.1           1.7            0.2\n4             0.6           0.5              0.1           0.3            0.1\n5            17.1           0.7              1.1           0.3            0.6\n6             3.2           0.2              0.1           3.3            0.2\n7            16.3          15                7.1           5.9            0.8\n  percent_jewish\n           &lt;dbl&gt;\n1              0\n2              0\n3              0\n4              1\n5              1\n6              0\n7              0\n\n\nVamos guardar esta informação num objeto secundário chamado table_regions.\n\ntable_regions &lt;- slice_tail(table_data, n = 7)\ntable_data &lt;- slice(table_data, 1:(nrow(table_data) - 7))\n\nPor fim, temos um problema específico. O nome de muitos países saiu cortado, faltando a primeira letra. Para resolver isto vamos usar o pacote countries que oferece um fuzzy matching para o nome de países.\nPara melhor compreender o uso do pacote vamos analisar um exemplo. Olhando a linha 51, vemos que o país importado foi “yprus”. O correto seria “Cyprus”.\n\ntest_name = table_data[51, ]$country\ntest_name\n\n[1] \"yprus\"\n\n\nA função country_name consegue identificar o nome corretamente.\n\ncountry_name(test_name, fuzzy_match = TRUE, to = \"name_en\")\n\n[1] \"Cyprus\"\n\n\nPode-se aplicar esta função em todos os nomes dos países. Além disso, é importante verificar se houve erros. O código busca qualquer entrada com NA em name ou iso3c.\n\ntest = table_data |&gt; \n  mutate(\n    name = country_name(country, to = \"name_en\", fuzzy_match = TRUE),\n    iso3c = country_name(country, to = \"ISO3\", fuzzy_match = TRUE)\n  )\n\nerrors = test |&gt; \n  filter(if_any(name:iso3c, ~is.na(.))) |&gt; \n  pull(country)\n\nerrors\n\n[1] \"hannel Islands\"         \"osovo\"                  \"etherlands Antilles\"   \n[4] \"orthern Mariana Is.\"    \"alestinian territories\" \"ruguay\"                \n\n\nComo se vê, o algoritmo falha em identificar alguns países, que serão corrigidos manualmente. O foco do post não é de como usar o algortimo de fuzzy matching então não vou entrar em muitos detalhes. Vale notar que é possível acrescentar o argumento verbose = TRUE na função country_name para receber um retorno mais informativo.\nAlém dos valores ausentes, vou verificar também os matchings duplicados. O código abaixo faz uma correção manual destes casos. Vale notar que há um caso “impossível”: Gambia e Zambia, pois ambos os países estão com o nome “ambia” na tabela importada. Neste caso, uso a informação da população para discriminar os casos.\n\nerrors = test |&gt;\n  get_dupes(name) |&gt; \n  select(name, country)\n  \ncorrection = c(\n  \"c\", \"k\", \"n\", \"n\", \"p\", \"u\", \"m\", \"m\", \"p\", \"\", \"m\", \"\", \"l\", \"e\", \"p\", \"f\",\n  \"i\", \"n\", \"z\", \"g\", \"u\", \"i\", \"o\", \"r\", \"r\", \"p\", \"s\", \"g\", \"u\", \"f\", \"u\"\n  )\n\nfix_table = errors |&gt; \n  mutate(country_fixed = str_c(str_to_upper(correction), country)) |&gt; \n  select(country, country_fixed)\n\ntable_data = table_data |&gt; \n  left_join(fix_table, by = 'country', relationship = \"many-to-many\") |&gt; \n  mutate(\n    temp_name = case_when(\n      country == \"ambia\" & country_population == 1730000 ~ \"Gambia\",\n      country == \"ambia\" & country_population &gt; 1730000 ~ \"Zambia\",\n      is.na(country_fixed) ~ country,\n      TRUE ~ country_fixed\n    ),\n    # faz o matching do nomes dos países\n    name = country_name(temp_name, to = \"name_en\", fuzzy_match = TRUE),\n    # resolve alguns casos extremos \"Channel Islands\"\n    name = if_else(is.na(name), temp_name, name),\n    # encontra o ISO3 code de cada país\n    iso3c = country_name(name, to = \"ISO3\", fuzzy_match = FALSE)\n  )\n\nOs erros finais acontecem porque alguns dos países listados não possuem ISO3, como \"Netherlands Antilles\"\n\n\n\nA tabela final é apresentada abaixo. Vale notar que alguns dos valores estão truncados, o que gera uma pequena imprecisão, como é o caso das entradas que eram listadas como “&lt; 0.1”.\n\ntable_data = table_data %&gt;%\n  select(name, iso3c, country_population:percent_jewish)\n\n\nDT::datatable(table_data)\n\n\n\n\n\n\n\n\n\nPor fim, vale notar que podemos melhorar ainda mais os dados acima. Na tabela acima, muitas das colunas são valores da variável “religião”; então podemos, transformar os dados em “tidy” convertendo-os em longitudinais da seguinte forma.\n\ntab_religion = table_data |&gt; \n  pivot_longer(\n    cols = percent_christian:percent_jewish,\n    names_to = \"religion\",\n    values_to = \"share\"\n    ) |&gt; \n  mutate(religion = str_remove(religion, \"(percent_)|(_religion)\"))\n\ntab_religion\n\n# A tibble: 1,872 × 5\n   name        iso3c country_population religion  share\n   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Afghanistan AFG             31410000 christian   0.1\n 2 Afghanistan AFG             31410000 muslim     99.7\n 3 Afghanistan AFG             31410000 unaffil     0.1\n 4 Afghanistan AFG             31410000 hindu       0.1\n 5 Afghanistan AFG             31410000 buddhist    0.1\n 6 Afghanistan AFG             31410000 folk        0.1\n 7 Afghanistan AFG             31410000 other       0.1\n 8 Afghanistan AFG             31410000 jewish      0.1\n 9 Albania     ALB              3200000 christian  18  \n10 Albania     ALB              3200000 muslim     80.3\n# ℹ 1,862 more rows\n\n\nAgora temos uma coluna religion que identifica cada uma das religiões consideradas na pesquisa e uma coluna share que mostra a representatividade de cada religão em cada país. Como comentado anteriormente, existem algumas impreciões pois substituímos os valores “&lt; 0.1” simplesmente por “0.1”, então alguns shares vão somar valores um pouco maiores do que 100.\nCom os dados neste formato fica fácil responder perguntas como: qual a religão dominante em cada país? Olhando os dados vemos que há 149 países em que a religião “dominante”, definida simplesmente como a religão que tem o maior share de convertidos, é o cristianismo. Há um gap considerável entre a religão muçulmana, com 47 países.\n\ntab_religion |&gt; \n  filter(share == max(share), .by = c(\"name\", \"iso3c\")) |&gt; \n  count(religion, sort = TRUE)\n\n# A tibble: 7 × 2\n  religion      n\n  &lt;chr&gt;     &lt;int&gt;\n1 christian   160\n2 muslim       50\n3 buddhist      8\n4 unaffil       7\n5 folk          3\n6 hindu         3\n7 jewish        1\n\n\nE qual o resultado quando se olha para o total da população religiosa? A religião cristã continua em primeiro lugar, com quase 1.6 bilhão de convertidos. Já a religião hindu, apesar de ser dominante em apenas 3 países, aparece com quase 1 bilhão de seguidores. Isto acontece porque esta é a religão dominante da Índia, que à época da pesquisa tinha 1.22 bilhão de habitantes.\n\ntab_religion |&gt; \n  filter(share == max(share), .by = c(\"name\", \"iso3c\")) |&gt; \n  mutate(total = country_population * share / 100) |&gt; \n  summarise(total_relig = sum(total), .by = \"religion\") |&gt; \n  arrange(desc(total_relig))\n\n# A tibble: 7 × 2\n  religion  total_relig\n  &lt;chr&gt;           &lt;dbl&gt;\n1 christian  1984890470\n2 muslim     1174670350\n3 hindu       998475870\n4 unaffil     824792400\n5 buddhist    138877020\n6 folk         50377350\n7 jewish        5565000\n\n\n\ntab_religion |&gt; \n  filter(share == max(share), .by = c(\"name\", \"iso3c\")) |&gt; \n  filter(religion == \"hindu\")\n\n# A tibble: 3 × 5\n  name      iso3c country_population religion share\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 India     IND           1224610000 hindu     79.5\n2 Mauritius MUS              1300000 hindu     56.4\n3 Nepal     NPL             29960000 hindu     80.7"
  },
  {
    "objectID": "posts/general-posts/2024-04-importando-pdf/index.html#resumo",
    "href": "posts/general-posts/2024-04-importando-pdf/index.html#resumo",
    "title": "Importando dados em PDF no R",
    "section": "",
    "text": "O código abaixo resume os passos da importação e limpeza dos dados.\n\nlibrary(tabulizer)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(tidyr, include.only = \"pivot_longer\")\nlibrary(countries, include.only = \"country_name\")\n\n\n# (opcional: baixa os dados)\nurl = \"https://pewresearch ...\"\ndownload.file(url, destfile = tempfile(fileext = \"pdf\"))\n\n# Importa a tabela\npath = here::here(\"project/data/globalReligion-tables.pdf\")\ntables = extract_tables(path)\n\n# Limpeza -------------------------------------------------------\n\n# Nome das colunas\n\nheader = tables[[1]][1:2, ]\n# Junta as colunas e 'limpa' os nomes\ncol_names = apply(header, 2, \\(x) make_clean_names(str_c(x, collapse = \"_\")))\n# Empilha os dados e define nome das colunas\ntable_data = bind_rows(lapply(tables, as.data.frame))\ntable_data = table_data[3:nrow(table_data), ]\nnames(table_data) = col_names\n\n# Converte colunas para numérico\ntable_data = table_data |&gt; \n  as_tibble() |&gt; \n  mutate(across(2:last_col(), ~as.numeric(str_remove_all(.x, \"[%&lt;&gt;,]\"))))\n\n# Remove as últimas sete linhas\ntable_regions &lt;- slice_tail(table_data, n = 7)\ntable_data &lt;- slice(table_data, 1:(nrow(table_data) - 7))\n\n# Nome dos países\n\n# Correção manual\ntest = table_data |&gt; \n  mutate(\n    name = country_name(country, to = \"name_en\", fuzzy_match = TRUE),\n    iso3c = country_name(country, to = \"ISO3\", fuzzy_match = TRUE)\n  )\n\nerrors = test |&gt;\n  get_dupes(name) |&gt; \n  select(name, country)\n  \ncorrection = c(\n  \"c\", \"k\", \"n\", \"n\", \"p\", \"u\", \"m\", \"m\", \"p\", \"\", \"m\", \"\", \"l\", \"e\", \"p\", \"f\",\n  \"i\", \"n\", \"z\", \"g\", \"u\", \"i\", \"o\", \"r\", \"r\", \"p\", \"s\", \"g\", \"u\", \"f\", \"u\"\n  )\n\nfix_table = errors |&gt; \n  mutate(country_fixed = str_c(str_to_upper(correction), country)) |&gt; \n  select(country, country_fixed)\n\n# Matching dos nomes\ntable_data = table_data |&gt; \n  left_join(fix_table, by = 'country', relationship = \"many-to-many\") |&gt; \n  mutate(\n    temp_name = case_when(\n      country == \"ambia\" & country_population == 1730000 ~ \"Gambia\",\n      country == \"ambia\" & country_population &gt; 1730000 ~ \"Zambia\",\n      is.na(country_fixed) ~ country,\n      TRUE ~ country_fixed\n    ),\n    # faz o matching do nomes dos países\n    name = country_name(temp_name, to = \"name_en\", fuzzy_match = TRUE),\n    # resolve alguns casos extremos \"Channel Islands\"\n    name = if_else(is.na(name), temp_name, name),\n    # encontra o ISO3 code de cada país\n    iso3c = country_name(name, to = \"ISO3\", fuzzy_match = FALSE)\n  ) |&gt; \n  select(name, iso3c, country_population:percent_jewish)\n\n# Tidy\n\n# Converte os dados para long\ntab_religion = table_data |&gt; \n  pivot_longer(\n    cols = percent_christian:percent_jewish,\n    names_to = \"religion\",\n    values_to = \"share\"\n    ) |&gt; \n  mutate(religion = str_remove(religion, \"(percent_)|(_religion)\"))"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html",
    "title": "Os Pecados da Visualização de Dados",
    "section": "",
    "text": "Como parte do curso de Inferência Estatística, oferecido pela Duke University, eu tive de ver um gráfico abominável. O gráfico abaixo faz parte de um press release da WIN-Gallup International de 2012. Ele mostra a relação entre religiosidade, mensurada por um índice que varia de 0 a 100, e o PIB per capita de vários países.\nA imagem abaixo foi retirada diretamente do press release e está em baixa resolução no original.\n\n\n\n\n\n\n\nSão vários os erros da visualização acima. Talvez o mais sério seja a dificuldade de leitura dos dados. O tamanho da fonte utilizado, junto com a baixa resolução da imagem dificultam muito a interpretação dos números.\nO uso excessivo de cores e emojis prejudicam a leitura do gráfico e, eventualmente, até podem levar a interpretações tendenciosas dos dados. Há também um excesso de informação nos eixos do gráfico, com as setas e cifrões. Em resumo,\n\nUso excessivo de cores, potencialmente tendenciosas.\nDados estão obscuros, difícil de interpretar.\nEixos com informação redundante.\nEmojis cafonas."
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#os-pecados-da-visualização-de-dados",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#os-pecados-da-visualização-de-dados",
    "title": "Os Pecados da Visualização de Dados",
    "section": "",
    "text": "São vários os erros da visualização acima. Talvez o mais sério seja a dificuldade de leitura dos dados. O tamanho da fonte utilizado, junto com a baixa resolução da imagem dificultam muito a interpretação dos números.\nO uso excessivo de cores e emojis prejudicam a leitura do gráfico e, eventualmente, até podem levar a interpretações tendenciosas dos dados. Há também um excesso de informação nos eixos do gráfico, com as setas e cifrões. Em resumo,\n\nUso excessivo de cores, potencialmente tendenciosas.\nDados estão obscuros, difícil de interpretar.\nEixos com informação redundante.\nEmojis cafonas."
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#pacotes",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#pacotes",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Pacotes",
    "text": "Pacotes\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(countries)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#dados",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#dados",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Dados",
    "text": "Dados\nPode-se conseguir os dados diretamente do PDF linkado. Os dados brutos estão na tabela 1, página 10. Vale dizer que o press release inteiro é uma aula de como não fazer gráficos e tabelas. Para importar os dados dentro do R vale ver o meu tutorial sobre como importar dados de PDF. Outra opção, ainda mais simples é copiar os números e importar as linhas diretamente de um arquivo de texto.\n\nrelig &lt;- as_tibble(relig)\n\ntab_relig &lt;- relig |&gt; \n  mutate(\n    index = as.numeric(str_extract(value, \"\\\\d+\")),\n    country = str_trim(str_remove(value, \"\\\\d+\")),\n    iso3c = country_name(country)\n    ) |&gt; \n  select(iso3c, index)\n\nPode-se importar os dados de PIB per capita diretamente do Banco Mundial via pacote WDI com o código abaixo. Importa-se o PIB per capita, em paridade de poder de compra (PPP) a dólares constantes de 2017. Como a pesquisa da WIN-Gallup é de 2012, vamos filtrar os dados deste ano.\n\ngdp &lt;- WDI::WDI(indicator = \"NY.GDP.PCAP.PP.KD\")\n\ngdp &lt;- gdp |&gt;\n  as_tibble() |&gt;\n  janitor::clean_names() |&gt;\n  filter(year == 2012) |&gt;\n  rename(gdppc = ny_gdp_pcap_pp_kd)\n\ndat &lt;- left_join(tab_relig, gdp, by = \"iso3c\")"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#gráfico",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#gráfico",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Gráfico",
    "text": "Gráfico\nNa primeira versão do gráfico, usa-se geom_label_repel para plotar o nome dos países. Esta é uma solução para evitar o overplotting. Além disso, quebro alguns nomes maiores em duas linhas\n\nggplot(dat, aes(gdppc, index)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggrepel::geom_label_repel(aes(label = str_wrap(country, 12)), size = 2) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nAjustando as escalas e os eixos é possível chegar num resultado bastante satisfatório.\n\nggplot(dat, aes(gdppc, index)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggrepel::geom_label_repel(aes(label = str_wrap(country, 12)), size = 2) +\n  scale_x_continuous(\n    breaks = seq(10000, 60000, 10000),\n    labels = scales::label_number(big.mark = \",\")\n    ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  labs(\n    title = \"Religiosity Index and National Income\",\n    x = \"GDP per capita (PPP, US$ constant 2017)\",\n    y = \"Index (100 = most religious)\",\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nUma solução alternativa para contornar os problemas de overplotting é de usar as siglas dos países, isto é, os códigos ISO3.\n\nplot_main &lt;- ggplot(dat, aes(gdppc, index)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggrepel::geom_label_repel(aes(label = iso3c), size = 2) +\n  scale_x_continuous(\n    breaks = seq(10000, 60000, 10000),\n    labels = scales::label_number(big.mark = \",\")\n    ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  labs(\n    title = \"Religiosity Index and National Income\",\n    x = \"GDP per capita (PPP, US$ constant 2017)\",\n    y = \"Index (100 = most religious)\",\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\nplot_main\n\n\n\n\n\n\n\n\nPodemos também acrescentar o texto informativo, ao lado do gráfico usando o pacote patchwork. Pode-se ver o resultado final abaixo. O gráfico está mais sucinto e informativo, ainda que muito menos colorido.\n\nlibrary(patchwork)\n\ntxt &lt;- \"The Religiosity Index represents the percentage of the\npopulation who self-describe themselves as a 'religious person' in the question worded as: Irrespective of whether you attend a palce of\nworship or not, would you say you are a religious person, not a\nreligious person or a convinced atheist?\"\n\ndf = tibble(x = 1, y = 1, label = txt)\n\nplot_txt = ggplot(df) +\n  geom_text(aes(x, y, label = str_wrap(label, width = 30)), hjust = 0, size = 2) +\n  theme_void()\n\npanel = plot_main +\n  theme(\n    plot.margin = margin(5, 100, 5, 5)\n  ) +\n  inset_element(plot_txt, left = 0.75, 1, 1.3, 0.4)\n\npanel"
  },
  {
    "objectID": "posts/general-posts/2024-04-plots-sacrilegio/index.html#um-pouco-a-mais",
    "href": "posts/general-posts/2024-04-plots-sacrilegio/index.html#um-pouco-a-mais",
    "title": "Os Pecados da Visualização de Dados",
    "section": "Um pouco a mais",
    "text": "Um pouco a mais\nPara tornar o gráfico mais interessante podemos combinar o índice de religiosidade com as religiões predominantes de cada país. Os dados são da PewResearch Center e foram coletados no meu outro post. Pode-se também fazer o download deles diretamente do meu GitHub.\n\ntab_religion = religion |&gt; \n  filter(share == max(share), .by = \"iso3c\")\n\ndat = left_join(dat, tab_religion, by = \"iso3c\")\n\ndat = dat |&gt; \n  filter(!is.na(religion)) |&gt; \n  mutate(\n    religion = str_replace(religion, \"unaffil\", \"unaffiliated\"),\n    religion = str_to_title(religion),\n    religion = factor(religion)\n    )\n\nPara minha visualização, vou mapear cada religião numa cor diferente. Além disso vou usar uma escala log para reduzir a variância (e distorção visual) dos dados de renda per capita. Por fim, vou mapear o tamanho da população de cada país para o tamanho de cada círculo.\nO resultado final é um gráfico bastante interessante. Vê-se que a maior parte dos países é predominantemente de religão cristã. Há um grupo de países cristãos europeus, de alta renda, e baixa religiosidade; um grupo de países cristãos latino-americanos, de renda média e alta religiosidade; e, finalmente, um grupo de países cristãos africanos, de renda bastante baixa, mas com religiosidade similar à do último grupo.\nEm poucos países, a maior parte da população não segue uma religião. É o caso, por exemplo, de Hong Kong, Japão e China. Vale notar que os principais países de maioria budista não foram contemplados no estudo da WIN-Gallup (e.g. Tailândia, Mianmar, etc.).\nPor fim, é interessante notar como a variabilidade da correlação entre as variáveis aumenta conforme aumenta a renda do país. Na faixa de 20 a 30 mil, por exemplo, temos a Polônia, de maioria cristã, com índice próximo de 80 e a Turquia, de maioria muçulmana, com índice próximo de 20.\n\nggplot(dat, aes(log(gdppc), index, color = religion)) +\n  ggrepel::geom_label_repel(aes(label = iso3c), size = 2) +\n  geom_point(aes(size = sqrt(country_population))) +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(\n    breaks = log(c(5000, 10000, 20000, 30000, 40000, 60000)),\n    labels = format(c(5000, 10000, 20000, 30000, 40000, 60000), big.mark = \",\")\n  ) +\n  scale_y_continuous(breaks = seq(0, 100, 20)) +\n  scale_color_manual(\n    name = \"Main Religion\",\n    values = MetBrewer::met.brewer(\"Hokusai1\", 5)\n  ) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Religiosity Index and National Income\",\n    subtitle = \"Size of each circle is proportional to the country's population.\",\n    x = \"GDP per capita (PPP, US$ constant 2017)\",\n    y = \"Index (100 = most religious)\",\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-rdt-brasil/index.html",
    "href": "posts/general-posts/2024-04-wz-rdt-brasil/index.html",
    "title": "Razão de Dependência no Brasil",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(leaflet)\n\nbrasil = geobr::read_country(showProgress = FALSE)\ncenter = st_coordinates(st_centroid(brasil))\nstate_border = geobr::read_state(showProgress = FALSE)\ndim_state = as_tibble(st_drop_geometry(state_border))\n\ncodes = c(93070, 93084:93098, 49108, 49109, 60040, 60041, 6653)\n\ntab_population = sidrar::get_sidra(\n  9514,\n  variable = 93,\n  geo = \"State\",\n  classific = \"c287\",\n  category = list(codes)\n)\n\ntab_pop &lt;- tab_population |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble() |&gt; \n  filter(sexo == \"Total\", forma_de_declaracao_da_idade == \"Total\") |&gt; \n  select(\n    code_state = unidade_da_federacao_codigo,\n    age_group = idade,\n    count = valor\n  )\n\ntab_pop &lt;- tab_pop |&gt; \n  mutate(\n    code_state = as.numeric(code_state),\n    age_min = as.numeric(stringr::str_extract(age_group, \"\\\\d+\")),\n    age_group = factor(age_group),\n    age_group = forcats::fct_reorder(age_group, age_min),\n    age_ibge = case_when(\n      age_min &lt; 15 ~ \"young\",\n      age_min &gt;= 15 & age_min &lt; 65 ~ \"adult\",\n      age_min &gt;= 65 ~ \"elder\"\n    ),\n    factor(age_ibge, levels = c(\"young\", \"adult\", \"elder\"))\n  )\n\npop_state &lt;- tab_pop %&gt;%\n  summarise(\n    total = sum(count), .by = c(\"age_ibge\", \"code_state\")\n  ) %&gt;%\n  pivot_wider(\n    id_cols = \"code_state\",\n    names_from = \"age_ibge\",\n    values_from = \"total\"\n  ) %&gt;%\n  mutate(\n    dre = elder / adult * 100,\n    dry = young / adult * 100,\n    tdr = dre + dry\n  )\n\ntab_pop_state &lt;- left_join(dim_state, pop_state, by = \"code_state\")\npop &lt;- left_join(state_border, pop_state, by = \"code_state\")\n\nbins &lt;- quantile(pop$tdr, probs = seq(0, 1, 0.2))\nbins &lt;- BAMMtools::getJenksBreaks(pop$tdr, k = 6)\npal &lt;- colorBin(\n  palette = as.character(MetBrewer::met.brewer(\"Hokusai2\", 5)),\n  domain = pop$tdr,\n  bins = bins\n)\n\nlabels &lt;- sprintf(\n  \"&lt;b&gt;RDT&lt;b/&gt;: %s &lt;br&gt;\n   &lt;b&gt;RDJ&lt;b/&gt;: %s &lt;br&gt;\n   &lt;b&gt;RDI&lt;b/&gt;: %s &lt;br&gt;\",\n  format(round(pop$tdr, 1), decimal.mark = \",\"),\n  format(round(pop$dry, 1), decimal.mark = \",\"),\n  format(round(pop$dre, 1), decimal.mark = \",\")\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nmap &lt;- leaflet(pop) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    color = \"white\",\n    fillColor = ~pal(tdr),\n    fillOpacity = 0.9,\n    highlightOptions = highlightOptions(\n      color = \"#e09351\",\n      weight = 10,\n      fillOpacity = 0.8,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", \"font-family\" = \"Fira Code\")\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~tdr,\n    labFormat = labelFormat(digits = 1),\n    title = \"RDT (2022)\",\n    position = \"bottomright\"\n  ) %&gt;%\n  addProviderTiles(providers$CartoDB) %&gt;%\n  setView(lng = center[1], lat = center[2], zoom = 4)"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-rdt-brasil/index.html#sobre-a-razão-de-dependência",
    "href": "posts/general-posts/2024-04-wz-rdt-brasil/index.html#sobre-a-razão-de-dependência",
    "title": "Razão de Dependência no Brasil",
    "section": "Sobre a Razão de Dependência",
    "text": "Sobre a Razão de Dependência\nA razão de dependência mede a proporção da população que depende, num sentido amplo, do trabalho da população economicamente ativa. No Brasil, o IBGE define a Razão de Dependência Total (RDT), como a razão entre o número de jovens e idosos e o número de adultos (em idade de trabalhar). Como critério de corte, define-se jovem como uma pessoa com até 14 anos e idoso como uma pessoa com 65 anos ou mais.Formalmente, define-se a RDT como:\n\\[\\text{RDT} = \\frac{\\text{Idade}\\leq14 \\, \\lor \\, \\text{Idade} \\geq 65}{\\text{Idade} &gt;14 \\, \\land \\, \\text{Idade} &lt; 65}\n\\]\nO denominador da fórmula representa a População Economicamente Ativa (PEA) e é uma proxy para a população trabalhadora.\nAdicionalmente, também pode-se definir a Razão de Dependência Jovem (RDJ) e a Razão de Dependência Idosa (RDI)\n\\[\n\\text{RDJ} = \\frac{\\text{Idade}\\leq14}{\\text{Idade} &gt;14 \\, \\land \\, \\text{Idade} &lt; 65} = \\frac{\\text{Idade}\\leq14}{\\text{PEA}}\n\\]\n\\[\n\\text{RDI} = \\frac{\\text{Idade} \\geq 65}{\\text{Idade} &gt;14 \\, \\land \\, \\text{Idade} &lt; 65} = \\frac{\\text{Idade} \\geq 65}{\\text{PEA}}\n\\]\nVale notar que:\n\\[\n\\text{RDT} = \\text{RDJ} + \\text{RDI}\n\\]"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "O IBGE recentemente atualizou as contagens de nascimento e óbitos no Brasil, no estudo períodico Estatísticas do Registro Civil. Esta base de dados estima o número total de nascidos vivos e de óbitos em cada município a cada ano. Os dados são bastante detalhados e permitem diversos tipos de análise. Já usei estes dados em outro post, onde mostrei como as divergências do Censo 2022 eram surpreendentes, visto que houve “houve cerca de 400 mil mortes a mais do que o projetado e 800 mil nascimentos a menos”.\n\n\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(showtext)\nlibrary(biscale)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nrecords &lt;- qs::qread(\"...\")\nrec &lt;- as_tibble(st_drop_geometry(records))\n\nPode-se importar o shapefile dos estados via geobr.\n\nstate_border &lt;- geobr::read_state(showProgress = FALSE)\ndim_state &lt;- as_tibble(st_drop_geometry(state_border))\n\ndim_state &lt;- dim_state |&gt; \n  mutate(name_state = stringr::str_replace(\n    name_state,\n    \"Espirito Santo\",\n    \"Espírito Santo\")\n    )\n\n# Aggregate data by state\nstate_records &lt;- rec |&gt; \n  summarise(across(population:births, ~sum(.x, na.rm = TRUE)), .by = \"abbrev_state\") |&gt; \n  mutate(cbr = births / population * 1000, cdr = deaths / population * 1000)\n\nstate_pop &lt;- left_join(state_border, state_records, by = \"abbrev_state\")\n\n\n\nQuando se olha para a tendência dentro de cada estado, vê-se uma divisão regional. Os estados do Norte têm taxas de natalidade elevadas e taxas de mortalidade baixas. O Centro-Oeste tem TBN e TBM moderadas, com exceção do Distrito Federal, que tem ambas as taxas baixas. Já o litoral do país apresenta taxas de mortalidade moderadas ou elevadas e baixas taxas de natalidade. Isto é particularmente relevante, visto que a maior parte da população brasileira mora nestes estados.\nÉ importante notar também que o Nordeste começa a exibir padrões demográficos similares a do Sul e do Sudeste, apesar de ter um renda per capita significativamente menor. Isto tende a se traduzir em desafios ainda maiores para o desenvolvimento econômico da região.\n\n\nCode\nrates &lt;- bi_class(state_pop, cdr, cbr, style = \"jenks\", dim = 3)\n\npal &lt;- \"BlueOr\"\n\np_map &lt;- ggplot(rates) +\n  geom_sf(aes(fill = bi_class)) +\n  bi_scale_fill(pal = pal, dim = 3) +\n  guides(fill = \"none\") +\n  coord_sf(xlim = c(NA, -36.25)) +\n  theme_void()\n\np_legend &lt;- bi_legend(\n  pal = pal,\n  dim = 3,\n  xlab = \"Deaths\",\n  ylab = \"Births\",\n  size = 8)\n\nstate_map &lt;- p_map + inset_element(p_legend, 0, 0, 0.4, 0.4)\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nstate_map &lt;- state_map + plot_annotation(\n  title = \"Nascimentos e Mortes\",\n  subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n  theme = theme_map\n  )\n\nstate_map\n\n\n\n\n\n\n\n\n\nDe modo geral, a maior parte dos municípios continua apresentando números maiores de nascimentos do que óbitos. Cerca de 84% dos municípios tem taxas brutas de natalidade (TBN) superiores às suas respectivas taxas brutas de mortalidade (TBM). O gráfico abaixo mostra a dispersão entre a taxa bruta de natalidade, por mil habitantes, contra a taxa bruta de mortalidade, por mil habitantes. Outliers são removidos para facilitar a leitura dos dados.\nOs municípios abaixo da linha laranja apresentam TBM maior do que TBN, isto é, são municípios onde se registram mais óbitos do que nascimentos. O contrário é válido para os municípios acima da linha: nos municípios acima da linha laranja há mais nascimentos do que óbitos. Isto, de fato, reflete o acelerado envelhecimento da população, como mostrei em outro post.\n\n\nCode\nsub &lt;- rec |&gt; \n  filter(cdr &lt;= 20, cbr &lt;= 30)\n\nplot_scatter &lt;- ggplot(sub, aes(cdr, cbr)) +\n  geom_point(\n    aes(size = sqrt(population)),\n    alpha = 0.5,\n    shape = 21,\n    color = \"#023047\"\n    ) +\n  geom_abline(slope = 1, intercept = 0, lwd = 1, color = \"#fb8500\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_text(\n    data = tibble(x = 17.5, y = 3, label = \"Mais Óbitos\\ndo que\\nNascimentos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = tibble(x = 17.5, y = 25.25, label = \"Mais Nasc.\\ndo que\\nÓbitos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Nascimentos e Óbitos\",\n    subtitle = stringr::str_wrap(\"Taxa Bruta de Nascimentos e Óbitos por mil habitantes por município no Brasil (2022). Tamanho do círculo proporcional à população do município.\", 86),\n    x = \"Óbitos (p/ 1.000 habitantes)\",\n    y = \"Nascimentos (p/ 1.000 habitantes)\"\n  ) +\n  theme_minimal(base_family = \"Lato\")\n\nplot_scatter\n\n\n\n\n\n\n\n\n\nA tabela abaixo apresenta os principais municípios com baixas taxas de natalidade e altas taxas de mortalidade. Vê-se que municípios de médio porte, do interior do Rio Grande do Sul, predominam na lista. É o caso de cidade como São Borja, São Gabriel, Bagé e Vacaria.\n\n\n\n\n\n\n  \n    \n      Nome\n      UF\n      \n        Taxa por mil\n      \n      \n        Absoluto\n      \n      População\n    \n    \n      Nascimentos\n      Óbitos\n      Nascimentos\n      Óbitos\n    \n  \n  \n    Cachoeira do Sul\nRS\n11,02\n12,71\n882\n1.018\n80.070\n    Petrópolis\nRJ\n11,28\n11,68\n3.146\n3.258\n278.881\n    São Gabriel\nRS\n11,93\n11,61\n698\n679\n58.487\n    Santos\nSP\n9,09\n11,56\n3.807\n4.838\n418.608\n    Rio Grande\nRS\n10,79\n11,43\n2.070\n2.194\n191.900\n    São Borja\nRS\n11,16\n11,43\n666\n682\n59.676\n    Tupã\nSP\n9,21\n11,42\n589\n730\n63.928\n    Penápolis\nSP\n10,96\n11,30\n676\n697\n61.679\n    Valença\nRJ\n10,43\n11,26\n710\n767\n68.088\n    Vacaria\nRS\n13,89\n11,06\n892\n710\n64.197\n    Bagé\nRS\n11,22\n11,00\n1.323\n1.297\n117.938\n    Camaquã\nRS\n12,09\n10,95\n752\n681\n62.200\n    Alegrete\nRS\n10,26\n10,92\n743\n791\n72.409\n    Peruíbe\nSP\n13,56\n10,91\n927\n746\n68.352\n    Cataguases\nMG\n9,34\n10,90\n619\n722\n66.261\n  \n  \n  \n\n\n\n\nA taxa bruta de natalidade é uma boa proxy para a taxa de fecundidade e a sua redução, de fato, indica menor crescimento demográfico. A tabela abaixo mostra os principais municípios com TBN elevada. Todos os municípios listados estão na região Norte e os estados do Pará e, sobretudo, do Amazonas predominam na lista.\n\n\n\n\n\n\n  \n    \n      Nome\n      UF\n      \n        Taxa por mil\n      \n      \n        Absoluto\n      \n      População\n    \n    \n      Nascimentos\n      Óbitos\n      Nascimentos\n      Óbitos\n    \n  \n  \n    São Gabriel da Cachoeira\nAM\n31,43\n4,13\n1.628\n214\n51.795\n    Portel\nPA\n31,13\n2,14\n1.946\n134\n62.503\n    Breves\nPA\n27,34\n2,52\n2.924\n270\n106.968\n    Tabatinga\nAM\n25,57\n4,46\n1.707\n298\n66.764\n    Maués\nAM\n24,97\n4,02\n1.528\n246\n61.204\n    Coari\nAM\n23,52\n4,84\n1.661\n342\n70.616\n    Tomé-Açu\nPA\n23,26\n3,79\n1.572\n256\n67.585\n    Santana\nAP\n23,11\n4,00\n2.487\n431\n107.618\n    Juruti\nPA\n22,64\n3,38\n1.152\n172\n50.881\n    Tefé\nAM\n21,79\n3,24\n1.605\n239\n73.669\n    Buriticupu\nMA\n21,55\n3,77\n1.196\n209\n55.499\n    Oriximiná\nPA\n21,30\n4,61\n1.455\n315\n68.294\n    Tailândia\nPA\n21,12\n4,39\n1.531\n318\n72.493\n    Grajaú\nMA\n21,12\n5,21\n1.560\n385\n73.872\n    Parintins\nAM\n21,06\n4,70\n2.030\n453\n96.372\n  \n  \n  \n\n\n\n\n\n\n\n\nComo visto, a dinâmica populacional varia de região para região e até de estado para estado. O mapa abaixo normaliza a taxa de crescimento natural para o Rio Grande do Sul. Aqui, define-se que a TCN é a diferença entre a TBN e a TBM, isto é\n\\[\n\\text{TCN} = \\text{TBN} - \\text{TBM} = \\frac{N}{P}\\times1.000 - \\frac{O}{P}\\times1.000 = 1.000 (\\frac{N - O}{P})\n\\] onde \\(N\\) é o número total de nascidos vivos no ano, \\(O\\) é o número total de óbitos registrados no ano e \\(P\\) é a contagem total da população no ano.\nO gráfico abaixo mostra a distribuição da TCN entre os municípios do Rio Grande do Sul em contraste com os demais municípios do Brasil. Nota-se que o formato da distribuição é relativamente similar; contudo, a distribuição da TCN está deslocada à esquerda no gráfico, indicando que os municípios gaúchos tem TCN menores.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a distribuição espacial da TCN no Rio Grande do Sul. Os dados são normalizados e as cores destacam os municípios que estão “nas pontas” da distribuição. A distância é mensurada em termos de desvios-padrão.\nOs municípios em cinza apresentam TCN próximas à média do estado; já os municípios em vermelho apresentam TCN abaixo da média. Nota-se que o Centro-Sul do estado possui praticamente nenhum município com TCN acima da média. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul, indicando TCN positivas.\n\n\n\n\n\n\n\n\n\nPode-se olhar também para a distribuição simulatânea da TBN e da TBM no estado. A função abaixo implementa uma maneira fácil de montar esta visualização para qualquer estado do Brasil.\n\n\nCode\navailable_states &lt;- unique(dim_state$abbrev_state)\navailable_states &lt;- available_states[order(available_states)]\navailable_states &lt;- paste(available_states, collapse = \", \")\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nplot_biscale &lt;- function(\n    state,\n    t = 0.35,\n    r = 0.35,\n    b = 0,\n    l = 0,\n    pal = \"BlueOr\",\n    theme = theme_map\n    ) {\n  \n  if (is.numeric(state) && nchar(state) == 2) {\n    state &lt;- dplyr::filter(records, code_state == state)$abbrev_state\n  }\n  \n  if (length(state) == 0) {\n    stop(\"Argument `state` must be one of: \", available_states, \".\")\n  }\n  \n  sub &lt;- dplyr::filter(records, abbrev_state == state)\n  rates &lt;- bi_class(sub, cdr, cbr, style = \"jenks\", dim = 3)\n\n  p_map &lt;- ggplot(rates) +\n    geom_sf(aes(fill = bi_class)) +\n    bi_scale_fill(pal = pal, dim = 3) +\n    guides(fill = \"none\") +\n    theme_void()\n\n  p_legend &lt;- bi_legend(\n    pal = pal,\n    dim = 3,\n    xlab = \"Deaths\",\n    ylab = \"Births\",\n    size = 8)\n\n  p_map &lt;- p_map + inset_element(p_legend, left = l, bottom = b, right = r, top = t)\n  \n  p_map &lt;- p_map + plot_annotation(\n    title = stringr::str_glue(\"Nascimentos e Mortes ({state})\"),\n    subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n    theme = theme_map\n    )\n  \n  return(p_map)\n  \n}\n\n\n\n\nOlhando para o mapa, vê-se que a região Centro-Sul do estado apresenta taxas de mortalidade moderadas ou elevadas. Aqui, os municípios em laranja são os mais preocupantes, por apresentar simultaneamente mortalidade elevada e natalidade baixa. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul claro e azul escuro, indicando TBN moderada ou elevada e TBM baixa.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNascimentos no Brasil\nEnvelhecimento no Brasil"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html#análise-de-dados",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html#análise-de-dados",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "library(dplyr)\nlibrary(sf)\nlibrary(showtext)\nlibrary(biscale)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nrecords &lt;- qs::qread(\"...\")\nrec &lt;- as_tibble(st_drop_geometry(records))\n\nPode-se importar o shapefile dos estados via geobr.\n\nstate_border &lt;- geobr::read_state(showProgress = FALSE)\ndim_state &lt;- as_tibble(st_drop_geometry(state_border))\n\ndim_state &lt;- dim_state |&gt; \n  mutate(name_state = stringr::str_replace(\n    name_state,\n    \"Espirito Santo\",\n    \"Espírito Santo\")\n    )\n\n# Aggregate data by state\nstate_records &lt;- rec |&gt; \n  summarise(across(population:births, ~sum(.x, na.rm = TRUE)), .by = \"abbrev_state\") |&gt; \n  mutate(cbr = births / population * 1000, cdr = deaths / population * 1000)\n\nstate_pop &lt;- left_join(state_border, state_records, by = \"abbrev_state\")\n\n\n\nQuando se olha para a tendência dentro de cada estado, vê-se uma divisão regional. Os estados do Norte têm taxas de natalidade elevadas e taxas de mortalidade baixas. O Centro-Oeste tem TBN e TBM moderadas, com exceção do Distrito Federal, que tem ambas as taxas baixas. Já o litoral do país apresenta taxas de mortalidade moderadas ou elevadas e baixas taxas de natalidade. Isto é particularmente relevante, visto que a maior parte da população brasileira mora nestes estados.\nÉ importante notar também que o Nordeste começa a exibir padrões demográficos similares a do Sul e do Sudeste, apesar de ter um renda per capita significativamente menor. Isto tende a se traduzir em desafios ainda maiores para o desenvolvimento econômico da região.\n\n\nCode\nrates &lt;- bi_class(state_pop, cdr, cbr, style = \"jenks\", dim = 3)\n\npal &lt;- \"BlueOr\"\n\np_map &lt;- ggplot(rates) +\n  geom_sf(aes(fill = bi_class)) +\n  bi_scale_fill(pal = pal, dim = 3) +\n  guides(fill = \"none\") +\n  coord_sf(xlim = c(NA, -36.25)) +\n  theme_void()\n\np_legend &lt;- bi_legend(\n  pal = pal,\n  dim = 3,\n  xlab = \"Deaths\",\n  ylab = \"Births\",\n  size = 8)\n\nstate_map &lt;- p_map + inset_element(p_legend, 0, 0, 0.4, 0.4)\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nstate_map &lt;- state_map + plot_annotation(\n  title = \"Nascimentos e Mortes\",\n  subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n  theme = theme_map\n  )\n\nstate_map\n\n\n\n\n\n\n\n\n\nDe modo geral, a maior parte dos municípios continua apresentando números maiores de nascimentos do que óbitos. Cerca de 84% dos municípios tem taxas brutas de natalidade (TBN) superiores às suas respectivas taxas brutas de mortalidade (TBM). O gráfico abaixo mostra a dispersão entre a taxa bruta de natalidade, por mil habitantes, contra a taxa bruta de mortalidade, por mil habitantes. Outliers são removidos para facilitar a leitura dos dados.\nOs municípios abaixo da linha laranja apresentam TBM maior do que TBN, isto é, são municípios onde se registram mais óbitos do que nascimentos. O contrário é válido para os municípios acima da linha: nos municípios acima da linha laranja há mais nascimentos do que óbitos. Isto, de fato, reflete o acelerado envelhecimento da população, como mostrei em outro post.\n\n\nCode\nsub &lt;- rec |&gt; \n  filter(cdr &lt;= 20, cbr &lt;= 30)\n\nplot_scatter &lt;- ggplot(sub, aes(cdr, cbr)) +\n  geom_point(\n    aes(size = sqrt(population)),\n    alpha = 0.5,\n    shape = 21,\n    color = \"#023047\"\n    ) +\n  geom_abline(slope = 1, intercept = 0, lwd = 1, color = \"#fb8500\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_text(\n    data = tibble(x = 17.5, y = 3, label = \"Mais Óbitos\\ndo que\\nNascimentos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  geom_text(\n    data = tibble(x = 17.5, y = 25.25, label = \"Mais Nasc.\\ndo que\\nÓbitos\"),\n    aes(x, y, label = label),\n    family = \"Lato\",\n    size = 3\n  ) +\n  guides(size = \"none\") +\n  labs(\n    title = \"Nascimentos e Óbitos\",\n    subtitle = stringr::str_wrap(\"Taxa Bruta de Nascimentos e Óbitos por mil habitantes por município no Brasil (2022). Tamanho do círculo proporcional à população do município.\", 86),\n    x = \"Óbitos (p/ 1.000 habitantes)\",\n    y = \"Nascimentos (p/ 1.000 habitantes)\"\n  ) +\n  theme_minimal(base_family = \"Lato\")\n\nplot_scatter\n\n\n\n\n\n\n\n\n\nA tabela abaixo apresenta os principais municípios com baixas taxas de natalidade e altas taxas de mortalidade. Vê-se que municípios de médio porte, do interior do Rio Grande do Sul, predominam na lista. É o caso de cidade como São Borja, São Gabriel, Bagé e Vacaria.\n\n\n\n\n\n\n  \n    \n      Nome\n      UF\n      \n        Taxa por mil\n      \n      \n        Absoluto\n      \n      População\n    \n    \n      Nascimentos\n      Óbitos\n      Nascimentos\n      Óbitos\n    \n  \n  \n    Cachoeira do Sul\nRS\n11,02\n12,71\n882\n1.018\n80.070\n    Petrópolis\nRJ\n11,28\n11,68\n3.146\n3.258\n278.881\n    São Gabriel\nRS\n11,93\n11,61\n698\n679\n58.487\n    Santos\nSP\n9,09\n11,56\n3.807\n4.838\n418.608\n    Rio Grande\nRS\n10,79\n11,43\n2.070\n2.194\n191.900\n    São Borja\nRS\n11,16\n11,43\n666\n682\n59.676\n    Tupã\nSP\n9,21\n11,42\n589\n730\n63.928\n    Penápolis\nSP\n10,96\n11,30\n676\n697\n61.679\n    Valença\nRJ\n10,43\n11,26\n710\n767\n68.088\n    Vacaria\nRS\n13,89\n11,06\n892\n710\n64.197\n    Bagé\nRS\n11,22\n11,00\n1.323\n1.297\n117.938\n    Camaquã\nRS\n12,09\n10,95\n752\n681\n62.200\n    Alegrete\nRS\n10,26\n10,92\n743\n791\n72.409\n    Peruíbe\nSP\n13,56\n10,91\n927\n746\n68.352\n    Cataguases\nMG\n9,34\n10,90\n619\n722\n66.261\n  \n  \n  \n\n\n\n\nA taxa bruta de natalidade é uma boa proxy para a taxa de fecundidade e a sua redução, de fato, indica menor crescimento demográfico. A tabela abaixo mostra os principais municípios com TBN elevada. Todos os municípios listados estão na região Norte e os estados do Pará e, sobretudo, do Amazonas predominam na lista.\n\n\n\n\n\n\n  \n    \n      Nome\n      UF\n      \n        Taxa por mil\n      \n      \n        Absoluto\n      \n      População\n    \n    \n      Nascimentos\n      Óbitos\n      Nascimentos\n      Óbitos\n    \n  \n  \n    São Gabriel da Cachoeira\nAM\n31,43\n4,13\n1.628\n214\n51.795\n    Portel\nPA\n31,13\n2,14\n1.946\n134\n62.503\n    Breves\nPA\n27,34\n2,52\n2.924\n270\n106.968\n    Tabatinga\nAM\n25,57\n4,46\n1.707\n298\n66.764\n    Maués\nAM\n24,97\n4,02\n1.528\n246\n61.204\n    Coari\nAM\n23,52\n4,84\n1.661\n342\n70.616\n    Tomé-Açu\nPA\n23,26\n3,79\n1.572\n256\n67.585\n    Santana\nAP\n23,11\n4,00\n2.487\n431\n107.618\n    Juruti\nPA\n22,64\n3,38\n1.152\n172\n50.881\n    Tefé\nAM\n21,79\n3,24\n1.605\n239\n73.669\n    Buriticupu\nMA\n21,55\n3,77\n1.196\n209\n55.499\n    Oriximiná\nPA\n21,30\n4,61\n1.455\n315\n68.294\n    Tailândia\nPA\n21,12\n4,39\n1.531\n318\n72.493\n    Grajaú\nMA\n21,12\n5,21\n1.560\n385\n73.872\n    Parintins\nAM\n21,06\n4,70\n2.030\n453\n96.372"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html#posts-relacionados",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html#posts-relacionados",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "Nascimentos no Brasil\nEnvelhecimento no Brasil"
  },
  {
    "objectID": "posts/general-posts/2024-03-maps-birth-deaths/index.html#mapas",
    "href": "posts/general-posts/2024-03-maps-birth-deaths/index.html#mapas",
    "title": "Nascimentos e Óbitos no Brasil",
    "section": "",
    "text": "Como visto, a dinâmica populacional varia de região para região e até de estado para estado. O mapa abaixo normaliza a taxa de crescimento natural para o Rio Grande do Sul. Aqui, define-se que a TCN é a diferença entre a TBN e a TBM, isto é\n\\[\n\\text{TCN} = \\text{TBN} - \\text{TBM} = \\frac{N}{P}\\times1.000 - \\frac{O}{P}\\times1.000 = 1.000 (\\frac{N - O}{P})\n\\] onde \\(N\\) é o número total de nascidos vivos no ano, \\(O\\) é o número total de óbitos registrados no ano e \\(P\\) é a contagem total da população no ano.\nO gráfico abaixo mostra a distribuição da TCN entre os municípios do Rio Grande do Sul em contraste com os demais municípios do Brasil. Nota-se que o formato da distribuição é relativamente similar; contudo, a distribuição da TCN está deslocada à esquerda no gráfico, indicando que os municípios gaúchos tem TCN menores.\n\n\n\n\n\n\n\n\n\nO mapa abaixo mostra a distribuição espacial da TCN no Rio Grande do Sul. Os dados são normalizados e as cores destacam os municípios que estão “nas pontas” da distribuição. A distância é mensurada em termos de desvios-padrão.\nOs municípios em cinza apresentam TCN próximas à média do estado; já os municípios em vermelho apresentam TCN abaixo da média. Nota-se que o Centro-Sul do estado possui praticamente nenhum município com TCN acima da média. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul, indicando TCN positivas.\n\n\n\n\n\n\n\n\n\nPode-se olhar também para a distribuição simulatânea da TBN e da TBM no estado. A função abaixo implementa uma maneira fácil de montar esta visualização para qualquer estado do Brasil.\n\n\nCode\navailable_states &lt;- unique(dim_state$abbrev_state)\navailable_states &lt;- available_states[order(available_states)]\navailable_states &lt;- paste(available_states, collapse = \", \")\n\ntheme_map &lt;- theme_minimal(base_family = \"Lato\") +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n    )\n\nplot_biscale &lt;- function(\n    state,\n    t = 0.35,\n    r = 0.35,\n    b = 0,\n    l = 0,\n    pal = \"BlueOr\",\n    theme = theme_map\n    ) {\n  \n  if (is.numeric(state) && nchar(state) == 2) {\n    state &lt;- dplyr::filter(records, code_state == state)$abbrev_state\n  }\n  \n  if (length(state) == 0) {\n    stop(\"Argument `state` must be one of: \", available_states, \".\")\n  }\n  \n  sub &lt;- dplyr::filter(records, abbrev_state == state)\n  rates &lt;- bi_class(sub, cdr, cbr, style = \"jenks\", dim = 3)\n\n  p_map &lt;- ggplot(rates) +\n    geom_sf(aes(fill = bi_class)) +\n    bi_scale_fill(pal = pal, dim = 3) +\n    guides(fill = \"none\") +\n    theme_void()\n\n  p_legend &lt;- bi_legend(\n    pal = pal,\n    dim = 3,\n    xlab = \"Deaths\",\n    ylab = \"Births\",\n    size = 8)\n\n  p_map &lt;- p_map + inset_element(p_legend, left = l, bottom = b, right = r, top = t)\n  \n  p_map &lt;- p_map + plot_annotation(\n    title = stringr::str_glue(\"Nascimentos e Mortes ({state})\"),\n    subtitle = \"Taxas brutas de nascimento e mortes por mil habitantes\",\n    theme = theme_map\n    )\n  \n  return(p_map)\n  \n}\n\n\n\n\nOlhando para o mapa, vê-se que a região Centro-Sul do estado apresenta taxas de mortalidade moderadas ou elevadas. Aqui, os municípios em laranja são os mais preocupantes, por apresentar simultaneamente mortalidade elevada e natalidade baixa. A região metropolitana de Porto Alegre e boa parte das regiões Norte e Nordeste do estado apresentam vários municípios em azul claro e azul escuro, indicando TBN moderada ou elevada e TBM baixa."
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-age-index/index.html",
    "href": "posts/general-posts/2024-04-wz-age-index/index.html",
    "title": "Índice de Envelhecimento no Brasil",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(showtext)\nlibrary(patchwork)\nfont_add_google(\"Roboto Mono\", \"Roboto Mono\")\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\ncities_age &lt;- readr::read_rds(\n  here::here(\"static/data/census_aging_index_city.rds\")\n)\n\nlabels &lt;- c(\"Less than 25\", \"25 to 50\", \"50 to 75\", \"75 to 100\", \"100 or more\")\n\nmap2022 &lt;- \n  ggplot(cities_age) +\n  geom_sf(aes(fill = age_index_2022, color = age_index_2022)) +\n  scale_fill_fermenter(\n    name = \"\",\n    breaks = seq(0, 125, 25),\n    palette = \"Spectral\"\n  ) +\n  scale_color_fermenter(\n    name = \"\",\n    breaks = seq(0, 125, 25),\n    palette = \"Spectral\"\n  ) +\n  coord_sf(xlim = c(NA, -34.469802)) +\n  labs(\n    title = \"Aging Index in Brazil (2022)\",\n    subtitle = stringr::str_wrap(\"The aging index shows the ratio of the elderly population (65 years or more) per 100 individuals in relation to the young population (14 years of younger).\", 80)\n    ) +\n  ggthemes::theme_map(base_family = \"Roboto Condensed\") +\n  theme(\n    #plot.title = element_text(hjust = 0.5, size = 22),\n    #plot.subtitle = element_text(hjust = 0.5, size = 14),\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    legend.position = \"top\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.5, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    plot.margin = margin(10, 5, 5, 10),\n    plot.title = element_text(size = 38, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5)\n    )\n\ntbl_age_index &lt;- tibble(\n  year = c(1991, 2000, 2010, 2022),\n  age_index = c(13.90, 16.18, 24.31, 55.24)\n)\n\nsparkline &lt;- ggplot(tbl_age_index, aes(year, age_index)) +\n  geom_hline(yintercept = 0) +\n  geom_point(color = \"#02818a\", size = 2) +\n  geom_line(color = \"#02818a\", lwd = 1) +\n  geom_text(\n    aes(label = age_index),\n    nudge_y = c(5, 5, 5, 5),\n    nudge_x = c(0, 0, -2, -1),\n    family = \"Roboto Condensed\",\n    size = 4\n    ) +\n  scale_x_continuous(breaks = c(1991, 2000, 2010, 2022)) +\n  labs(\n    title = \"Brazil is getting older\",\n    subtitle = \"Average Age Index (country)\",\n    x = NULL,\n    y = \"Age Index\"\n  ) +\n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(linetype = 2),\n    axis.text.y = element_blank()\n  )\n\nfinal_map &lt;- map2022 + inset_element(sparkline, 0, 0.05, 0.4, 0.45)\n\n\n\nEnvelhecimento no Brasil\nO mapa abaixo mostra o índice de envelhecimento nas cidades brasileiras. Este índice compara a população idosa (65 anos ou mais) com a população jovem (14 anos ou menos). Os dados são do mais recente Censo do IBGE. Vale lembrar que valores maiores do que 100 indicam que há uma proporção maior de idosos do que jovens numa determinada cidade.\nO futuro demográfico do Brasil, em grande parte, já é conhecido. Assim como no resto do mundo, a combinação de queda de taxa de fecundidade e aumento de expectativa de vida implica no envelhecimento da população. Pelo índice de envelhecimento vê-se como esta tendência já é realidade em boa parte do território brasileiro.\nDe modo geral, as regiões mais envelhecidas são o Sul e Sudeste. O estado de Santa Catarina é uma exceção neste sentido. Algumas partes do Centro Oeste e do Nordeste também já apresentam índices de envelhecimento mais elevados do que a média do país.\n\n\n\n\n\n\nDados: IBGE, Censos Demográficos (1991, 2000, 2010, 2022)\nTipografia: Roboto Condensed\nPaleta: Spectral (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-04-wz-pib-municipios/index.html",
    "href": "posts/general-posts/2024-04-wz-pib-municipios/index.html",
    "title": "GDP in Brazil",
    "section": "",
    "text": "Code\n# Libraries\nlibrary(geobr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(MetBrewer)\nlibrary(showtext)\n\n# Color palette\ncores = met.brewer(\"Hiroshige\", 20)[c(1, 8, 15, 20)]\n# Import Gill Sans locally\nfont_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext_auto()\n\n# Data ------------------------------------------------------------------\n\n# Shapes\n\n# Import city shapefile\nmunis &lt;- read_municipality(year = 2022, showProgress = FALSE)\n# Import state shapefile\ngeostate &lt;- read_state(showProgress = FALSE)\n\n# Import city data from Brazil (see data/raw/R/painel_municipios.R)\npanel &lt;- readr::read_csv(here::here(\"static/data\", \"cities_brazil.csv\"))\n\n## Data cleaning --------------------------------------------------------\n\n# Vector to select GDP columns\npib_cols = c(\"pib_agriculture\", \"pib_industrial\", \"pib_services\",\n             \"pib_govmt_services\")\n\n# Compute GDP share by city\npib_share = panel |&gt; \n  select(all_of(c(\"code_muni\", \"pib\", pib_cols))) |&gt; \n  pivot_longer(cols = 3:last_col(), names_to = \"sector\", values_to = \"total\") |&gt; \n  mutate(\n    share = total / pib * 100,\n    # Improve text labels\n    sector_label = str_remove(sector, \"pib_\"),\n    sector_label = str_to_title(sector_label),\n    sector_label = str_replace(sector_label, \"Govmt_services\", \"Government\"),\n    sector_label = str_replace(sector_label, \"Industrial\", \"Industry\")\n    )\n\n# Convert to wider\ntab_pib_share = pib_share |&gt; \n  pivot_wider(\n    id_cols = \"code_muni\",\n    names_from = \"sector\",\n    values_from = \"share\"\n    )\n\n# Find the most relevant economic sector by city\ntop_pib_share = pib_share |&gt; \n  filter(share == max(share), .by = \"code_muni\") |&gt; \n  select(code_muni, top_sector = sector_label)\n\n# Join tabular data with city shapefile\nmap_share = munis |&gt; \n  left_join(tab_pib_share, by = \"code_muni\") |&gt; \n  left_join(top_pib_share, by = \"code_muni\")\n\n# Map -------------------------------------------------------------------\n\nm1 = ggplot() +\n  # State borders\n  geom_sf(data = geostate, lwd = 0.5, color = \"gray30\", fill = \"white\") +\n  # City colors\n  geom_sf(\n    data = na.omit(map_share),\n    aes(fill = top_sector, color = top_sector),\n    lwd = 0.01\n    ) +\n  # Remove junk from the east coast\n  coord_sf(xlim = c(NA, -35)) +\n  # Use Hiroshige colors\n  scale_fill_manual(name = \"\", values = cores) +\n  scale_color_manual(name = \"\", values = cores) +\n  labs(\n    title = \"Top GDP contribution by City (2021)\",\n    subtitle = \"Colors represent the most relevant economic contributing group in each city\",\n    caption = \"Source: National Accounts, IBGE (2023)\") +\n  ggthemes::theme_map(base_family = \"Gill Sans\") +\n  # Thematic elements\n  theme(\n    # Background\n    panel.background = element_rect(fill = \"#ffffff\", color = NA),\n    plot.background = element_rect(fill = \"#ffffff\", color = NA),\n    # Legend\n    legend.position = \"left\",\n    legend.justification = 0.5,\n    legend.key.size = unit(1, \"cm\"),\n    legend.key.width = unit(1.5, \"cm\"),\n    legend.text = element_text(size = 12),\n    legend.margin = margin(),\n    # Labels (title, subtitle)\n    plot.title = element_text(size = 38, hjust = 0.5),\n    plot.subtitle = element_text(size = 14, hjust = 0.5),\n    # Plot margins\n    plot.margin = margin(10, 5, 5, 10)\n  )\n\n# Histogram -------------------------------------------------------------\n\npib_bars = top_pib_share |&gt; \n  count(top_sector) |&gt; \n  mutate(top_sector = forcats::fct_rev(top_sector))\n\np_hist = ggplot(pib_bars, aes(top_sector, n)) +\n  geom_col(aes(fill = top_sector)) +\n  # Number labels\n  geom_text(\n    aes(y = n, label = n, color = factor(c(0, 0, 0, 1))),\n    size = 4,\n    family = \"Gill Sans\",\n    nudge_y = c(-200, -200, 200, -200)\n    ) +\n  # Text labels\n  geom_text(\n    aes(y = 50, label = top_sector, color = factor(c(0, 0, 0, 1))),\n    family = \"Gill Sans\",\n    size = 4,\n    hjust = 0\n  ) +\n  # Flip axis\n  coord_flip() +\n  scale_fill_manual(values = rev(cores)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  labs(x = NULL, y = NULL, title = \"Number of cities\") +\n  guides(fill = \"none\", color = \"none\") +\n  theme_minimal(base_family = \"Gill Sans\") +\n  theme(\n    plot.title = element_text(size = 14, hjust = 0.5),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.text = element_blank()\n    )\n\n\n\nGDP Contribution by city\n\nThe number of cities where Agriculture is the main driver of the local economy rose from 1049 in 2020 to 1272 in 2021. Agriculture is most prevalent in the Midwest cities.\nPublic Administration is the most prevalent economic activity among cities in the North and Northeast of the country. General services are most prevalent in the Southeast.\nIndustrial activities constitute the primary economic driver for fewer than 8% of cities. The few industrial cities are spatially disperse across the country, exhibiting no clear pattern.\n\n\n\n\n\n\n\nDados: IBGE, Contas Nacionais, Produto Interno Bruto dos Municípios (2023)\nTipografia: Gill Sans\nPaleta: Hiroshige (MetBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html",
    "href": "posts/general-posts/2024-04-radar-plots/index.html",
    "title": "Radar Plots",
    "section": "",
    "text": "Radar plots show a static visualization of the numeric values of several different categories of a same entity. These charts are also known as spider or web plots, since they visually resemble a spider web. The values of each variable are connected by a straight line and the resulting image is a polygon. These charts are commonly used to quickly access a collection of important variables and to make simple comparisons, and are frequently used in dashboards.\n\n\n\n\n\nExample of a radar plot\n\n\n\n\nThe plot above shows 11 characteristics of a car (Datsun 710, from the mtcars dataset). The vertical distance of each dot from the center represents the magnitude of each variable. Since the overall axis is “fixed” this type of plot is ideal to represent scaled or indexed values. In more general cases, this means some data transformation is needed to achieve a better final result."
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#the-data",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#the-data",
    "title": "Radar Plots",
    "section": "The data",
    "text": "The data\nThe Abrainc Radar is a collection of 12 variables, structured into 4 subgroups, that aim to capture the overall condition of the real estate market. These groups are: (1) macroeconomic conditions; (2) real estate credit conditions; (3) real estate demand; (4) real estate conditions.\nVariables include real estate prices, input costs, and interest rates. All variables are scaled in the 0-10 range, where 10 represents more favorable conditions.\nThe code below grabs the most recent data and makes minor improvements to the labels of the variables.\n\n# Download the data\nradar &lt;- get_abrainc_indicators(category = \"radar\")\n\n# Better labels for the plot\nradar_labels &lt;- c(\n  \"Confidence\", \"Activity\", \"Interest Rate\", \"Financing Conditions\",\n  \"Real Concessions\", \"Atractivity\", \"Employment\", \"Wages\",\n  \"Real Estate Investing\", \"Input Costs\", \"New Launches\",\n  \"Real Estate Prices\")\n\nradar &lt;- radar |&gt; \n  mutate(\n    month = lubridate::month(date),\n    variable = factor(variable, levels = unique(variable)),\n    variable_label = factor(\n      variable,\n      levels = unique(variable),\n      labels = radar_labels)\n  )\n\n# Get data only from the most recent period\ncurrent_period &lt;- radar |&gt; \n  filter(date == max(date)) |&gt; \n  pivot_wider(\n    id_cols = \"date\",\n    names_from = \"variable\",\n    values_from = \"value\"\n    )\n\nOur data is two columns and all variables are scaled in the 0-10 range, where higher values indicate more favorable conditions.\n\n\n\n\n\n\n\n\nNAME\nVALUE\n\n\n\n\nMacroeconomic\n\n\nConfidence\n5.97\n\n\nActivity\n9.05\n\n\nInterest Rate\n2.67\n\n\nReal Estate Credit\n\n\nFinancing Conditions\n0.63\n\n\nReal Concessions\n2.53\n\n\nAtractivity\n4.74\n\n\nMarket Demand\n\n\nEmployment\n8.51\n\n\nWages\n9.00\n\n\nReal Estate Investing\n3.08\n\n\nReal Estate\n\n\nInput Costs\n6.38\n\n\nNew Launches\n7.68\n\n\nReal Estate Prices\n4.82\n\n\n\n\n\n\n\nThe Abrainc Radar is a monthly longitudinal panel of data, i.e., a collection of time series. To visualize this data we will select first the most recent period available and compare it to (1) the equivalent period in the previous year; (2) the best equivalent period.\n\nlatest_date &lt;- max(radar$date)\nlast_year &lt;- latest_date - lubridate::years(1)\n\nbest_year &lt;- radar |&gt; \n  filter(month == lubridate::month(latest_date)) |&gt; \n  group_by(date) |&gt; \n  summarise(total = sum(value)) |&gt; \n  slice_max(total, n = 1) |&gt; \n  pull(date)\n\ncompare_best &lt;- radar |&gt; \n  filter(date %in% c(latest_date, best_year)) |&gt; \n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"variable\",\n    values_from = \"value\"\n    )\n\ncompare &lt;- radar |&gt; \n  filter(year &gt;= lubridate::year(latest_date) - 1,\n         month == lubridate::month(latest_date)) |&gt; \n  pivot_wider(\n    id_cols = \"year\",\n    names_from = \"variable\",\n    values_from = \"value\"\n    )"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#ggradar",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#ggradar",
    "title": "Radar Plots",
    "section": "ggradar",
    "text": "ggradar\nUsing default settings, a minimal radar plot requires specifying the scale of the grid and its units. By default, ggradar assumes that all the numeric data is in a 0-100 range. The data.frame input should be a \\(1\\times k\\) table where \\(k\\) is the number of variables. The function ignores non-numerical variables by default.\n\nggradar(\n  current_period,\n  grid.min = 0,\n  grid.mid = 5,\n  grid.max = 10,\n  values.radar = c(\"0\", \"5\", \"10\"),\n  axis.label.size = 3\n  )\n\n\n\n\n\n\n\n\nThere are several arguments to improve the visualization. Since we will make several radar plots it’s convenient to create a function to do this.\n\n\nCode\n# Line break the labels\nradar_labels &lt;- str_wrap(radar_labels, width = 10)\n\nplot_ggradar &lt;- function(.dat, colors, legend = FALSE) {\n  \n  p &lt;- ggradar(\n    .dat,\n    # Define grid limits\n    grid.min = 0,\n    grid.mid = 5,\n    grid.max = 10,\n    # Define grid labels\n    values.radar = c(\"0\", \"5\", \"10\"),\n    grid.label.size = 5,\n    # Shade polygon\n    fill = TRUE,\n    fill.alpha = 0.3,\n    # Axis labels (outside the circle)\n    axis.labels = radar_labels,\n    axis.label.size = 3,\n    font.radar = \"Lato\",\n    # \"Size\" of the plot\n    plot.extent.x.sf = 1.5,\n    plot.extent.y.sf = 1.4,\n    # Color of the lines and circles\n    group.colours = colors,\n    # Size of the circles\n    group.point.size = 3,\n    # Width of the line connecting the points\n    group.line.width = 1\n    )\n  \n  if (legend | length(colors) &gt; 1) {\n    p &lt;- p + theme(legend.position = \"top\")\n  }\n  \n  return(p)\n  \n}\n\n\nThe improved plot is shown below. From the radar plot, we see that the real economy seems to be in a favorable spot: the activity indicator is high and so are wages and employment. Credit conditions are not favorable as can be seen from the low values of interest rate, financing conditions, atractivity, and real concessions.\n\nplot_ggradar(current_period, \"#264653\")\n\n\n\n\n\n\n\n\nWe can use radar plots to make simple comparisons. This radar plot compares the current period with the comparable period in the previous year. We can see that wages and employment improved considerably while atractivity and real concessions worsened.\n\nplot_ggradar(compare, c(\"#e76f51\", \"#264653\"))\n\n\n\n\n\n\n\n\nFinally, we can compare the current period with the “best” period. For this exercise, I defined “best” as the period we the highest cumulative score using equal weights for all indicators. Compared to the best period, the current period is worse in all indicators with exception of wages and new launches.\n\nplot_ggradar(compare_best, c(\"#e76f51\", \"#264653\"))\n\n\n\n\n\n\n\n\n\nSome Problems\nI’ll discuss some general problems of radar plots below but is should be noted that ggradar produces “circular” plots instead of “web” plots. Indeed, a distinctive feature of the radar charts produced by ggradar is that the radar is circular which means the dots are positioned on a radial frame (polar coordinate frame) instead of a conventional cartesian (rectangular) frame. This causes a slight visual distortion: connection dots with a straight line in a circle doesn’t make much sense geometrically.\nThis distortion is more visible when there are fewer categories; conversly, the distortion is less noticeable when there are many categories. Looking at the documentation of ggradar this might be a conscious decision since the inspiration for the ggradar function is a code developed by Paul Williamson that used radar plots to evaluate clusters defined by 44 different variables."
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#fmsb",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#fmsb",
    "title": "Radar Plots",
    "section": "fmsb",
    "text": "fmsb\nThe fmsb package is a companion to the book “Practices of Medical and Health Data Analysis using R”. It offers a multitude of functions and datasets. The main function we will use is the radarchart function. This function expects a wide dataset with atleast three rows. The first row should indicate the maximum value of each column (variable), the second row should indicate the minimum value of each column, and the third row should be the actual/current value of each column.\n\nwide_dat &lt;- pivot_wider(\n  filter(radar, date == max(date)),\n  id_cols = \"date\",\n  names_from = \"variable_label\",\n  values_from = \"value\"\n)\n\nwide_dat &lt;- rbind(rep(10, 12), rep(0, 12), wide_dat[, 2:13])\n\nwide_dat\n\n# A tibble: 3 × 12\n  Confidence Activity `Interest Rate` `Financing Conditions` `Real Concessions`\n       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n1      10       10              10                    10                  10   \n2       0        0               0                     0                   0   \n3       5.97     9.05            2.67                  0.630               2.53\n  Atractivity Employment Wages `Real Estate Investing` `Input Costs`\n        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;         &lt;dbl&gt;\n1       10         10    10                      10            10   \n2        0          0     0                       0             0   \n3        4.74       8.51  9.00                    3.08          6.38\n  `New Launches` `Real Estate Prices`\n           &lt;dbl&gt;                &lt;dbl&gt;\n1          10                   10   \n2           0                    0   \n3           7.68                 4.82\n\n\nIt’s fairly simple to produce a radar chart using this function.\n\nradarchart(wide_dat)\n\n\n\n\n\n\n\n\nWe can customize the result to get a better visualization.\n\nradarchart(\n  wide_dat,\n  axistype = 1,\n  seg = 5,\n  \n  # Polygon\n  pcol = \"#023047\",\n  pfcol = alpha(\"#023047\", 0.3),\n  plwd = 4, \n\n  # Grid\n  cglcol = \"grey90\",\n  cglty = 1,\n  axislabcol = \"grey90\",\n  caxislabels = seq(0, 10, 2),\n  cglwd = 0.8,\n  # Labels\n  vlcex = 0.8)\n\n\n\n\n\n\n\n\nNote that radarchart uses straight grid lines (instead of circular) so it doesn’t cause the same visual distortion seen in the ggradar examples above. In this sense, radarchart produces radar plots that are more visually accurate.\nAgain, we can create a custom function to help make our radar plots.\n\n\nCode\nplot_radarchart &lt;- function(.dat, colors = \"#023047\", legend = FALSE, labels = radar_labels) {\n  \n  radarchart(\n    .dat,\n    axistype = 1,\n    seg = 5,\n    # Polygon\n    pcol = colors,\n    pfcol = alpha(colors, 0.3),\n    plwd = 4,\n    # Grid\n    cglcol = \"grey90\",\n    cglty = 1,\n    axislabcol = \"grey90\",\n    caxislabels = seq(0, 10, 2),\n    cglwd = 0.8,\n    # Labels\n    vlabels = labels,\n    vlcex = 0.8\n    )\n  \n}\n\n\nNow we can easily create new radar plots.\n\nwide_compare &lt;- rbind(rep(10, 12), rep(0, 12), compare[, 2:13])\n\nplot_radarchart(wide_compare, colors = c(\"#e76f51\", \"#264653\"))\n\n\n\n\n\n\n\n\n\nwide_best &lt;- rbind(rep(10, 12), rep(0, 12), compare_best[, 2:13])\n\nplot_radarchart(wide_best, colors = c(\"#e76f51\", \"#264653\"))"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#problems",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#problems",
    "title": "Radar Plots",
    "section": "Problems",
    "text": "Problems\nDespite their growing prevalence in data visualization, radar charts are subject to several criticisms. To summarize the main issues, specialists argue that:\n\nThe circular layout makes it harder to interpret the data when compared to simple horizontal/vertical axis. This is essentially the same critique made to pie charts.\nRanking and ordering. The overall shape and interpretation of radar plots can vary substantially depending on the layout of the variables along the edges.\nArea distortion. Since a radar plot shows a polygon, it’s area scales quadratically instead of linearly. This can lead to biased interpretation of the data, leading one to over-estimate changes.\nOverplotting. Radar plots are not very useful when comparing 4 groups or more.\n\n\nRanking distortion\nAll four plots below show the exact same data but the order of the variables is changed. This results in different shaped polygons and pottentialy different or even conflicting interpretations from the same underlying data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverplotting\nThe plot below compares 5 different periods. Radar charts suffer from overplotting and are generally not useful for comparing more than 3 groups at a time.\n\n\n\n\n\nFor a more in depth analysis on the shortcomings of radar charts see The Radar Chart and Its Caveats."
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#the-alternatives",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#the-alternatives",
    "title": "Radar Plots",
    "section": "The alternatives",
    "text": "The alternatives\nThere are three alternatives to radar plots.\n\nLollipop charts\nParallel trends charts\nBarplots or Column charts\n\nIn essence, all of these alternatives simplify radar plots by making the underlying space horizontal/vertical instead of “circular”.\n\nLollipop charts\nLollipop charts are good at showing the temporal change between the same group of variables. I featured this chart in my ggplot2 tutorials (in Portuguese). The code below shows how to make a simple lollipop chart using this data.\n\n\nCode\nfont_add_google(\"IBM Plex Sans\", \"IBM Plex Sans\")\n\nlolli &lt;- radar |&gt; \n  mutate(month = lubridate::month(date)) |&gt; \n  filter(year %in% c(2019, 2023), month == 6) |&gt; \n  mutate(year = factor(year))\n\nggplot(lolli, aes(variable_label, ma6, color = year)) +\n  geom_line(aes(group = variable_label), color = \"gray30\") +\n  geom_point(size = 3) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10)) +\n  scale_color_manual(name = \"\", values = c(\"#023047\", \"#fb8500\")) +\n  labs(\n    title = \"Pre and Post-pandemic Market Conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\",\n    x = NULL,\n    y = \"Index (10 = most favorable)\"\n  ) +\n  theme_light(base_family = \"IBM Plex Sans\") +\n  theme(\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\nThe chart shows the average values of all 12 indicators in the first semester of 2019 and 2023. When the dots are close, very little has changed; conversely, when the dots are far apart there has been considerable evolution. The colors help guide the visualization.\nWe can easily see that real estate prices and new launches are on the rise in 2023. Similarly, the job market and overall economy seems stronger as wages, employment, and the level of activity have all increased. Credit conditions, on the other hand, have deteriorated, and the overall actractivity of real estate investment has worsened. The fall in financing conditions and interest rate reflects both the increase in the SELIC rate as well as the increase in all real estate credit lines.\n\n\nParallel Trends\nA parallel trends chart is quite literally a line plot. In a way, it’s a simplification of radar plots since it connects the numerical values of different categories along a simple rectangular frame. These charts can be adapted to encompass many comparison groups simultaneously so they can be used to overcome one of the shortcomings of radar plots. I’m not the greatest fan of parallel trends chart as they can become very messy.\nThe code below shows how to build a simple parallel trends chart.\n\n\nCode\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\n\ncompare &lt;- radar |&gt; \n  mutate(\n    month = lubridate::month(date)\n  ) |&gt; \n  filter(month == 6, year %in% c(2012, 2017, 2019, 2023)) |&gt; \n  mutate(year = factor(year))\n\nggplot(compare, aes(variable_label, ma6, color = year, group = year)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = c(0, 10)) +\n  scale_x_discrete(labels = \\(x) str_wrap(x, 10)) +\n  scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10)) +\n  scale_color_manual(name = \"\", values = c(\"#023047\", \"#219ebc\", \"#8ecae6\", \"#fb8500\")) +\n  labs(\n    x = NULL, y = \"Index (10 = most favorable)\",\n    title = \"Real Estate Market Conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\"\n  ) +\n  theme_ipsum(base_family = \"Roboto Condensed\") +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 8)\n    )\n\n\n\n\n\n\n\n\n\nIn the visualization above we compare four different years: 2012, 2017, 2019, and 2023. The first of these years is a “boom” year, the economy was still riding the end of the commodity cycle and interest rates were at historically low values; the labor market was also doing well, with record low unemployment rates. The indices reflect these facts: the darkblue line is usually at the top of the chart. The “midblue” line, represents 2017, the last year of the recession started in 2015: almost all indicators are low but notably input costs are very favorable, most likely due to the decreased construction demand. Finally, 2019 is a pre-pandemic benchmark and 2023 is the most recent available data.\nThis visualization would look very cluttered on a radar chart but works well as a parallel trend chart. We can easily see how the current period (2023) is different form the 2012 boom: while the real economy and labor markets are similar, credit conditions are much worse; input costs are comparable, while new launches are currently higher and prices lower.\n\n\nBarplots\nA final alternative is to use a simple bar chart combined will small multiples. The code below shows how to make a simple version of this visulization.\n\n\nCode\ndat = radar |&gt; \n  mutate(\n    month = lubridate::month(date),\n    variable_label = forcats::fct_rev(variable_label)\n    ) |&gt; \n  filter(year &gt;= 2021, month == 6)\n\nggplot(dat, aes(variable_label, ma6)) +\n  geom_col(fill = \"#023047\") +\n  geom_hline(yintercept = 0) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 10, 2)) +\n  facet_wrap(vars(year)) +\n  labs(\n    x = NULL, y = \"Index (10 = most favorable)\",\n    title = \"Real Estate Market Conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\"\n  )  +\n  theme_bw(base_family = \"IBM Plex Sans\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\nThis graphic allows for both a vertical analysis, comparing the different index values within the same year, as well as a horizontal analysis, comparing the same index across different time periods. It can also be enhanced using specific colors to highlight key indicators or text labels to make the values more precise. We could, for instance, highlight indicators that show how overall credit conditions have worsened in the past years.\n\n\nCode\ncols_credit &lt;- c(\"interest\", \"finance_condition\", \"real_concession\")\n\ndat &lt;- dat |&gt; \n  mutate(\n    is_credit = factor(if_else(variable %in% cols_credit, 1L, 0L))\n  )\n\nggplot(dat, aes(variable_label, ma6, fill = is_credit)) +\n  geom_col() +\n  geom_text(\n    data = filter(dat, is_credit == 1),\n    aes(y = ma6 + 0.8, label = round(ma6, 1)),\n    family = \"IBM Plex Sans\"\n  ) +\n  geom_hline(yintercept = 0) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 10, 2)) +\n  facet_wrap(vars(year)) +\n  scale_fill_manual(name = \"\", values = c(\"#8ecae6\", \"#023047\")) +\n  guides(fill = \"none\") +\n  labs(\n    x = NULL, y = \"Index (10 = most favorable)\",\n    title = \"Deterioration of real estate credit conditions\",\n    subtitle = \"Real estate average market conditions in the first semester of selected years.\",\n    caption = \"Source: Abrainc/Fipe (2024)\"\n  )  +\n  theme_bw(base_family = \"IBM Plex Sans\") +\n  theme(panel.grid.minor = element_blank())"
  },
  {
    "objectID": "aboutme.html#cv",
    "href": "aboutme.html#cv",
    "title": "Vinicius Oike Reginatto",
    "section": "CV",
    "text": "CV\nVinicius Oike, São Paulo, Brazil. Contact: viniciusoike@gmail.com\n\nEducation\n\nUniversity of São Paulo | São Paulo, SP. MSc Economics1. Macroeconomics, growth theory, time series.\nFederal University of Rio Grande do Sul2 | Porto Alegre, RS Bachelor (B.A.) Economics. Mathematical Economics.\n\n\nWork Experience\n\n02/2022-12/2023: Economist and Data Scientist — QuintoAndar (proptech startup)\n01/2020-02/2022: Data Scientist — Urbit (market intelligence startup)\n08/2019-: Economic Consultant\n\n\n\nPresentation\nMy name is Vinícius Reginatto, and I hold a degree in Economics from UFRGS, recognized as the top university in Brazil by INEP, as well as a Master’s degree in Economics from the University of São Paulo (USP), consistently ranked as the best university in Latin America.\nI have a strong quantitative background, that includes mathematical optimization, time series forecasting, spatial analysis, econometrics and statistics. I am proficient in Python, R, Quarto, Shiny, SQL, and the most popular machine learning algorithms. I’ve also developed packages in R and applications in Shiny. My professional background encompasses both data science and economic consulting.\nThroughout the years I’ve worked in several economic consulting projects, mostly focused on urban intervention projects (e.g., transit-oriented development) and real estate analysis. At these jobs, I worked remotely in large multidisciplinary and multilingual teams.\nMost recently I’ve worked at QuintoAndar, the largest real estate platform in Latin America, as an economist, data scientist and spokesperson. As part of my daily responsibilities, I developed real estate indicators and market reports. I worked directly with the C-level of the company, providing quarterly macroeconomic forecasts of the Brazilian real estate market and the broader economy.\nI am fluent in both English and Portuguese and have a strong grasp of Spanish3.\nProgramming: R, Python, Quarto, RMarkdown, Shiny, SQL (PostGreSQL).\nReports: Office, Google Docs, and LaTeX.\nLanguages: English, Portuguese, and Spanish.\n\n\nSome of my work\n\nHousing Affordability in São Paulo: overview and measurement presented at the 2021 Latin American Real Estate Society.\nMicroapartamentos: mercado e tendências (2022)\nO Mercado Residencial na América Latina (2022)"
  },
  {
    "objectID": "aboutme.html#sobre-o-site",
    "href": "aboutme.html#sobre-o-site",
    "title": "Vinicius Oike Reginatto",
    "section": "Sobre o Site",
    "text": "Sobre o Site\nNeste site você vai encontrar vários posts sobre economia, mercado, visualização e análise de dados. Também escrevo vários tutoriais sobre R, a linguagem de programação que tenho maior proficiência. A maior parte dos posts e textos estão na aba Blog. Tenho alguns aplicativos, desenvolvidos com Shiny, na aba Apps.\nEste site foi originalmente lançado no final de 2019, usando RMarkdown e o pacote blogdown. A versão atual foi lançada oficialmente em 2023, usando Quarto uma linguagem integrada para apresentação de análises de dados, que junta R, Python, Julia e diversas outras linguagens de programação."
  },
  {
    "objectID": "aboutme.html#sobre-mim",
    "href": "aboutme.html#sobre-mim",
    "title": "Vinicius Oike Reginatto",
    "section": "Sobre Mim",
    "text": "Sobre Mim\nMeu nome é Vinicius Oike Reginatto, sou mestre em Economia pela Universidade de São Paulo. No meu mestrado, estudei sobre teorias de crescimento econômico e escrevi sobre a história da matematização da teoria econômica.\nTrabalho como consultor econômico desde 2019. Tenho experiência com projetos de infraestrutura urbana (TOD) e análise de mercado e de conjuntura econômica. Tenho um forte background quantitativo e expertise em análise de dados (R/Python).\n\nVinicius Oike, São Paulo, Brazil. Contact: viniciusoike@gmail.com\n\nSome of my work (outside of the this blog)\n\nHousing Affordability in São Paulo: overview and measurement presented at the 2021 Latin American Real Estate Society.\nMicroapartamentos: mercado e tendências (2022)\nO Mercado Residencial na América Latina (2022)"
  },
  {
    "objectID": "aboutme.html#about-the-site",
    "href": "aboutme.html#about-the-site",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "On this website, you’ll find various posts about economics, data visualization, and data analysis. I also write several tutorials about R, the programming language in which I’m most proficient. Most of the posts and texts are under the Blog tab. I have some applications, developed with Shiny, under the Apps tab.\nThis site was originally launched in late 2019, using RMarkdown and the blogdown package. The current version was officially released in 2023, using Quarto, an integrated language for presenting data analyses, which combines R, Python, Julia, and several other programming languages."
  },
  {
    "objectID": "aboutme.html#about-me-1",
    "href": "aboutme.html#about-me-1",
    "title": "Vinicius Oike Reginatto",
    "section": "About me",
    "text": "About me\nMy name is Vinícius Reginatto, and I hold a degree in Economics from UFRGS, recognized as the top university in Brazil by INEP, as well as a Master’s degree in Economics from the University of São Paulo (USP), consistently ranked as the best university in Latin America.\nI have a strong quantitative background, that includes mathematical optimization, time series forecasting, spatial analysis, econometrics and statistics. I am proficient in Python, R, Quarto, Shiny, SQL, and the most popular machine learning algorithms. I’ve also developed packages in R and applications in Shiny. My professional background encompasses both data science and economic consulting."
  },
  {
    "objectID": "aboutme.html#about-me",
    "href": "aboutme.html#about-me",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "My name is Vinícius Reginatto, I’m an economist and data scientist currently based in São Paulo, Brazil. I have been working as an economic consultant since 2019, focusing on urban economics and the real estate market. I’ve also worked at tech startups as a data scientist and spokesperson. Link to my CV."
  },
  {
    "objectID": "posts/ggplot2-tutorial/0-introduction.html#footnotes",
    "href": "posts/ggplot2-tutorial/0-introduction.html#footnotes",
    "title": "Introdução",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPortais de notícias como BBC, Financial Times e The Economist montam muitas de suas visualizações usando ggplot2. Em alguns casos existem até pacotes abertos como o {bbplot} que ajudam a fazer gráficos no mesmo estilo da publicação. Jornais nacionais como o Nexo também utilizam bastante gráficos feitos usando ggplot2.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html",
    "title": "Analfabetismo no Brasil",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(sidrar)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(showtext)\nlibrary(patchwork)\n\nfont_add_google(\"Fira Mono\", \"Fira Mono\")\nfont_add_google(\"Open Sans\", \"Open Sans\")\nshowtext_auto()\n\nmuni = geobr::read_municipality(year = 2020, showProgress = FALSE)\ndim_muni = as_tibble(st_drop_geometry(muni))\n\nanalf = get_sidra(9543, geo = \"City\", classific = \"c2\")\n\ntbl_gender = analf |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble() |&gt; \n  filter(sexo != \"Total\") |&gt; \n  select(code_muni = municipio_codigo, sex = sexo, rate = valor) |&gt; \n  mutate(code_muni = as.numeric(code_muni))\n\nanalf_gender = tbl_gender |&gt; \n  mutate(sex = factor(sex)) |&gt; \n  pivot_wider(id_cols = \"code_muni\", names_from = \"sex\", values_from = \"rate\") |&gt; \n  rename_with(tolower) |&gt; \n  mutate(gender_gap = homens - mulheres) |&gt; \n  arrange(gender_gap)\n\nanalf_gender = analf_gender |&gt; \n  left_join(dim_muni) |&gt; \n  mutate(\n    is_nordeste = factor(if_else(code_region == 2, 1L, 0L))\n  )\n\ntbl_analf = analf |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble() |&gt; \n  filter(sexo == \"Total\") |&gt; \n  select(code_muni = municipio_codigo, rate = valor) |&gt; \n  mutate(code_muni = as.numeric(code_muni))\n\nanalf_city = left_join(muni, tbl_analf, by = \"code_muni\")\n\nanalf_city = analf_city |&gt; \n  mutate(analf_rate = 100 - rate)\n\nbreaks_jenks = BAMMtools::getJenksBreaks(analf_city$analf_rate, k = 9)[-1]\nbreaks_jenks = ceiling(breaks_jenks)\n\nlabels = c(\"&lt;5%\", \"5-8%\", \"8-11%\", \"11-15%\", \"15-19%\", \"19-23%\", \"23-27%\", \"27-37%\")\n\nanalf_city = analf_city |&gt; \n  mutate(analf_group = factor(findInterval(analf_rate, breaks_jenks, left.open = TRUE)))\n\np1 = ggplot(analf_city) +\n  geom_sf(aes(fill = analf_group, color = analf_group), lwd = 0.15) +\n  scale_fill_brewer(\n    name = \"\",\n    type = \"div\",\n    direction = -1,\n    labels = labels\n    ) +\n  scale_color_brewer(\n    name = \"\",\n    type = \"div\",\n    direction = -1,\n    labels = labels\n    ) +\n  labs(\n    title = \"Taxa de Analfabetismo\",\n    subtitle = \"Taxa de analfabetismo total por município\"\n  ) +\n  coord_sf(xlim = c(NA, -35)) +\n  ggthemes::theme_map(base_family = \"Open Sans\") +\n  theme(\n    legend.position.inside = c(0.1, 0.1),\n    plot.title = element_text(\n      hjust = 0.5,\n      size = 22,\n      margin = margin(5, 0, 5, 0)\n      ),\n    plot.subtitle = element_text(\n      hjust = 0.5,\n      margin = margin(2.5, 0, 0, 0)\n      ),\n    plot.margin = margin(0, 0, 0, 0)\n  )\n\nanalf_city = analf_city |&gt; \n  mutate(is_nordeste = factor(if_else(code_region == 2, 1L, 0L)))\n\ntbl_summary = analf_city |&gt; \n  st_drop_geometry() |&gt; \n  summarise(avg = mean(analf_rate), .by = \"is_nordeste\")\n\np2 = ggplot() +\n  geom_density(\n    data = analf_city,\n    aes(x = analf_rate, fill = is_nordeste),\n    alpha = 0.6\n  ) +\n  geom_vline(\n    data = tbl_summary,\n    aes(xintercept = avg, color = is_nordeste),\n    lty = 2\n  ) +\n  geom_hline(yintercept = 0) +\n  guides(fill = \"none\", color = \"none\") +\n  scale_fill_manual(values = c(\"#01665e\", \"#8c510a\")) +\n  scale_color_manual(values = c(\"#01665e\", \"#8c510a\")) +\n  labs(\n    title = \"Taxa de analfabetismo médio é quase 3x maior entre as cidades do &lt;b&gt;&lt;span style='color:#8c510a'&gt;Nordeste&lt;/span&gt;&lt;/b&gt;, em relação ao resto do &lt;b&gt;&lt;span style='color:#01665e'&gt;Brasil&lt;/span&gt;&lt;/b&gt;.\",\n    subtitle = \"Taxa de analfabetismo total nas cidades brasileiras. Linhas tracejadas indicam taxa média entre as cidades do Nordeste e no restante do país.\",\n    x = \"Taxa de analfabetismo (%)\",\n    y = NULL\n  ) +\n  theme_minimal(base_family = \"Fira Mono\") +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_textbox_simple(\n      size = 16,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      family = \"Open Sans\"\n      ),\n    plot.subtitle = element_textbox_simple(\n      size = 8,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0))\n    )\n\nlabel_1 = \"Em 97,4% das cidades do &lt;b&gt;&lt;span style='color:#8c510a'&gt;Nordeste&lt;/span&gt;&lt;/b&gt;, as pessoas do &lt;i&gt;sexo feminino têm taxas de alfabetização menores&lt;/i&gt;. Em alguns casos, a diferença supera 10 pontos percentuais.\"\n\nlabel_2 = \"Na maior parte das cidades do &lt;b&gt;&lt;span style='color:#01665e'&gt;Brasil&lt;/span&gt;&lt;/b&gt;, não há diferença grande na taxa de alfabetização entre pessoas do sexo masculino e do sexo feminino.\"\n\ntext_labels = tibble(\n  x = c(-10, 7.5), y = c(0.2, 0.25), label = c(label_1, label_2)\n)\n\np3 = ggplot() +\n  geom_density(\n    data = analf_gender,\n    aes(x = gender_gap, fill = is_nordeste),\n    alpha = 0.7) +\n  geom_textbox(\n    data = text_labels,\n    aes(x, y, label = label),\n    family = \"Open Sans\",\n    size = 3\n  ) +\n  scale_fill_manual(values = c(\"#01665e\", \"#8c510a\")) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Gender gap na alfabetização\",\n    subtitle = \"Diferença, em pontos percentuais, da taxa de alfabetização entre pessoas do sexo masculino e pessoas do sexo feminino. Valores próximos de zero indicam diferenças pequenas. Valores negativos indicam que mulheres têm taxas de alfabetização menores do que homens.\",\n    caption = \"Fonte: IBGE (Censo, 2022). @viniciusoike\",\n    y = NULL,\n    x = \"Gender gap\"\n  ) +\n  theme_minimal(base_family = \"Fira Mono\") +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_textbox_simple(\n      size = 16,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      family = \"Open Sans\"\n    ),\n    plot.subtitle = element_textbox_simple(\n      size = 8,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0))\n  )\n\npanel = p1 | (p2 / p3)\npanel = panel + plot_layout(widths = c(0.5, 0.5))"
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html#mapa-dos-municípios",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html#mapa-dos-municípios",
    "title": "Analfabetismo no Brasil",
    "section": "Mapa dos municípios",
    "text": "Mapa dos municípios\nA taxa média de analfabetismo no Brasil é de 7%, segundo o Censo mais recente do IBGE. Isto é, os municípios em tons mais claros de verde e em tons de marrom estão todos acima da média nacional."
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html#nordeste",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html#nordeste",
    "title": "Analfabetismo no Brasil",
    "section": "Nordeste",
    "text": "Nordeste\n\nCidades no Nordeste tem taxas de analfabetismo mais elevadas\nQuando se olha especificamente para as cidades nordestinas, verifica-se taxas de analfabetismo consideravelmente elevadas. A taxa média chega a ser quase três vezes maior do que a observada no restante do país.\n\n\n\n\n\n\n\n\n\n\n\nDiscrepâncias entre homens e mulheres é elevada\nA disparidade nas taxas de alfabetização também se apresenta quando olha-se os dados relativos a pessoas do sexo masculino e pessoas do sexo feminino. O “gender gap”, diferença entre a taxa de alfabetização entre homens e entre mulheres, também é maior entre cidades do Nordeste do que no restante do país. Vale reforçar que o gráfico abaixo mostra a diferença nas taxas de alfabetização, assim valores negativos, indicam que as taxas de analfabetismo são maiores entre mulheres."
  },
  {
    "objectID": "posts/general-posts/2024-05-illiterate-census/index.html#painel-geral",
    "href": "posts/general-posts/2024-05-illiterate-census/index.html#painel-geral",
    "title": "Analfabetismo no Brasil",
    "section": "Painel Geral",
    "text": "Painel Geral\n\n\n\n\n\n\n\nDados: Censo 2022 (IBGE)\nTipografia: Fira Mono e Open Sans\nPaleta: BrBG (ColorBrewer)"
  },
  {
    "objectID": "posts/general-posts/2024-05-generations-brazil/index.html",
    "href": "posts/general-posts/2024-05-generations-brazil/index.html",
    "title": "Generations in Brazil",
    "section": "",
    "text": "Like many other countries, Brazil is aging rapidly. The combination of increasing life expectancy and declining fertility rates means that, in the long run, the proportion of elderly people in the population will rise, while the share of young people will decline. Despite this long-term trend, the current demographic outlook reveals the largest contingent of teens and young adults in Brazilian history. Nearly 100 million people, close to half of the population, belong to Gen Z and Gen Y (Millennials), ranging in age from 12 to 43.\n\n\n\n\n\n\n\n\n\nThe cut-offs for each generation cohort, in the plot above, followed directly Beresford Research’s definition (which adapted the previous definition developed by the Pew Research Center. The data comes from the most recent Brazilian Census (2022).\n\n\n\nGenerations by birth year defined by the Pew Research Center\n\n\nThis should be the last great young population for Brazil which means the country should try to make the best of it. As this population begins to age, dependency ratios will increase rapidly pressuring the already fragile public pension system. This is particularly true of cities in the South and Southeast of Brazil.\n\n\n\n\n\n\n  \n    \n      Generation\n      Age Range\n      Population\n      Share\n      Cumulative Share\n    \n  \n  \n    Alpha\n1-11\n29.515.539\n14,70%\n14,70%\n    Gen Z\n12-27\n47.564.885\n23,70%\n38,40%\n    Millenial\n28-43\n50.847.471\n25,33%\n63,73%\n    Gen X\n44-59\n40.681.106\n20,27%\n84,00%\n    Boomers\n60-79\n27.526.536\n13,71%\n97,71%\n    Elder\n80-100\n4.586.954\n2,29%\n100,00%\n  \n  \n  \n\n\n\n\nA worrying trend is the rising number of unemployed and uneducated among the Brazilian youth. Recent unemployment numbers from PNAD still show a big gap in unemployment rates, specially for the Gen Z’s who are entering the labor market.\n\n\n\n\n\n\n\n\n\nA recent study from IBGE, published in the Summary of Social Indicators, shows that 22,3% of Brazilians aged 15 to 29 neither studied nor worked in 2022. This is equivalent to almost 11 million youngsters.\nA recent report from the OECD also notes that Brazil has a very large share of NEETs (neither employed nor in formal education or training) in the 18-24 age. The NEETs in Brazil are overwhelmingly black, female, and low-income, highlighting the countries unequal access to opportunities.\n\n\n\n\n\n\n\n\nAging Index In Brazil\nBrazilian Census"
  },
  {
    "objectID": "posts/general-posts/2024-05-generations-brazil/index.html#related-posts",
    "href": "posts/general-posts/2024-05-generations-brazil/index.html#related-posts",
    "title": "Generations in Brazil",
    "section": "",
    "text": "Aging Index In Brazil\nBrazilian Census"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "My name is Vinícius Reginatto. I hold a degree in Economics from UFRGS, recognized as Brazil’s leading university by INEP, and a Master’s degree in Economics from the University of São Paulo (USP), consistently ranked as the top university in Latin America.\nWith a strong quantitative foundation, I focus on areas such as spatial analysis, econometrics, and time-series forcasting. I am proficient in Python, R, Quarto, Shiny, SQL, and a range of widely used machine learning algorithms. Additionally, I have developed R packages and Shiny applications to address specific analytical challenges. My professional experience spans data science and economic consulting.\nOver the years, I have worked on economic consulting projects, focused on urban intervention initiatives—such as transit-oriented development (TOD)—and real estate market analysis. In these roles, I collaborated remotely with large, multidisciplinary, and multilingual teams to deliver impactful results."
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Vinicius Oike Reginatto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Economics Graduate course has been evaluated with a grade 7 (the highest grade), since the inception of the Capes’ plurianual review, indicating its high standard of international performance.↩︎\nRanked by INEP (Ministry of Education) as the best public university in Brazil since 2012 and during 2012-2014 also as the best university in Brazil. Commonly ranked in the Top 10 of all Brazilian universities in both national and international rankings.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-04-radar-plots/index.html#problems-with-radar-plots",
    "href": "posts/general-posts/2024-04-radar-plots/index.html#problems-with-radar-plots",
    "title": "Radar Plots",
    "section": "Problems with radar plots",
    "text": "Problems with radar plots\nDespite their growing prevalence in data visualization, radar charts are subject to several criticisms. To summarize the main issues, specialists argue that:\n\nThe circular layout makes it harder to interpret the data when compared to simple horizontal/vertical axis. This is essentially the same critique made to pie charts.\nRanking and ordering. The overall shape and interpretation of radar plots can vary substantially depending on the layout of the variables along the edges.\nArea distortion. Since a radar plot shows a polygon, it’s area scales quadratically instead of linearly. This can lead to biased interpretation of the data, leading one to over-estimate changes.\nOverplotting. Radar plots are not very useful when comparing 4 groups or more.\n\n\nRanking distortion\nAll four plots below show the exact same data but the order of the variables is changed. This results in different shaped polygons and pottentialy different or even conflicting interpretations from the same underlying data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverplotting\nThe plot below compares 5 different periods. Radar charts suffer from overplotting and are generally not useful for comparing more than 3 groups at a time.\n\n\n\n\n\n\n\n\n\nFor a more in depth analysis on the shortcomings of radar charts see The Radar Chart and Its Caveats."
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "Web scraping consists of extracting information from a web page. The difficulty or ease of extracting this information depends on how well the page is constructed. In more complex cases, the information may be behind a captcha or within an interactive panel that depends on user input.\nIn this simple example, I will show how to find the locations of all Starbucks stores in Brazil. The full list of active Starbucks stores can be found on the Starbucks Brasil website. As usual, we will use the tidyverse along with the rvest and xml2 packages.\n\n\n\nlibrary(rvest)   # Used for web scraping and extracting HTML content\nlibrary(xml2)    # Helps in working with XML and HTML data\nlibrary(tidyverse)\n\n\n\n\nThe full list of active Starbucks stores can be found on the Starbucks Brasil website. To read the page, we use read_html.\n\nurl &lt;- \"https://starbucks.com.br/lojas\"\npage &lt;- xml2::read_html(url)   # Fetches the HTML content of the page\n\nThe “xpath” shows the path to a specific element on the page. For example, to find the Starbucks logo in the top-left corner of the page, we can use the following code:\n\n# Extracts the HTML element for the logo image\npage %&gt;%\n  html_element(xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\")\n\n{html_node}\n&lt;img alt=\"Starbucks Logo\" src=\"/public/img/icons/starbucks-nav-logo.svg\"&gt;\n\n\nTo learn more about xpaths, you can consult this cheatsheet.\nIn general, on well-constructed pages, the name of elements will be quite self-explanatory. In the case above, the alt attribute already indicates that the object is the Starbucks logo, and the src links to an image file in svg format called starbucks-nav-logo. Unfortunately, this won’t always be the case. On some pages, elements can be quite confusing.\nTo extract a specific attribute, we use the html_attr function.\n\npage %&gt;%\n  html_element(\n    xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\"\n    ) %&gt;%\n  # Extracts the \"src\" attribute (URL to the image)\n  html_attr(\"src\")  \n\n[1] \"/public/img/icons/starbucks-nav-logo.svg\"\n\n\nIf you combine this last link with “www.starbucks.com.br”, you should arrive at an image of the company’s logo1.\n\n\n\nStarbucks logo\n\n\nTo find the big list of stores in the left panel, we will take advantage of the fact that the div holding this list has a unique class called \"place-list\". It’s easy to verify this directly in your browser. If you use Chrome, for instance, just right-click on the panel and click on Inspect.\n\n\n\n\n\n\n\n\n\n\nAs I mentioned above, things aren’t always well organized. Note that since we want to extract multiple elements and multiple (all) attributes, we use the variants: html_elements and html_attrs.\n\nlist_attr &lt;- page %&gt;%\n  # Selects all divs under \"place-list\" that hold store info\n  html_elements(xpath = '//div[@class=\"place-list\"]/div')  %&gt;%\n  # Extracts all attributes of the selected elements\n  html_attrs()  \n\nThe extracted object is a list where each element is a text vector containing the following information. We have the store name, latitude/longitude, and the address.\n\n# Extracts the first store's information from the list\npluck(list_attr, 1)\n\n                  class           data-latitude          data-longitude \n\"place-item r-place-15\"           \"-23.5658059\"           \"-46.6508012\" \n              data-name             data-street              data-index \n  \"Shopping Top Center\" \"Avenida Paulista, 854\"                     \"0\" \n\n\nAt this point, the web scraping process is complete. Once again, the process was easy because the data is well structured on the Starbucks page. Now, we just need to clean the data.\n\n\n\n\nI won’t go in depth about the data cleaning process. Basically, we need to convert each element of the list into a data.frame, stack the results, and then adjust the data types of each column.\n\n# Convert the elements into data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Stack the results\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Rename the columns\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Select the columns of interest\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng to numeric\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nThe final dataset is presented below\n\n\n\n\n\n\n\n\n\nThe table above is already in a pretty satisfactory format. We can check the data by building a simple map.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nIt’s worth noting that data extracted via web scraping almost always contains some noise. In this case, the data seems relatively clean after a bit of processing. The addresses are not always very informative, like in the case of “Rodovia Hélio Smidt, S/N,” but this happens because many stores are located inside hospitals, shopping malls, or airports.\nWith this data, we can already perform interesting analyses. For example, we can find out that there are five Starbucks stores just on Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n3     2 Paulista Trianon          Avenida Paulista, 1499 -23.6 -46.7\n4     3 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n5     6 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2  (-46.65438 -23.5631)\n3  (-46.6558 -23.56226)\n4 (-46.64809 -23.56784)\n5 (-46.65895 -23.55785)\n\n\nWe can also count the number of stores in each airport. Apparently, there are eight stores at Guarulhos Airport, which seems like quite a high number to me.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 GRU               8\n2 Brasília          3\n3 Florianópolis     3\n4 Confins           2\n5 Galeão            2\n6 Viracopos         2\n7 Congonhas         1\n8 Curitiba          1\n9 Santos            1\n\n\nFinally, we can note that many Starbucks stores are located inside shopping malls. A simple calculation shows that around 75 stores are located inside malls, close to 50% of the total units2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 75"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#web-scraping",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#web-scraping",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "Web scraping consists of extracting information from a web page. The difficulty or ease of extracting this information depends on how well the page is constructed. In more complex cases, the information may be behind a captcha or within an interactive panel that depends on user input.\nIn this simple example, I will show how to find the locations of all Starbucks stores in Brazil. The full list of active Starbucks stores can be found on the Starbucks Brasil website. As usual, we will use the tidyverse along with the rvest and xml2 packages.\n\n\n\nlibrary(rvest)   # Used for web scraping and extracting HTML content\nlibrary(xml2)    # Helps in working with XML and HTML data\nlibrary(tidyverse)\n\n\n\n\nThe full list of active Starbucks stores can be found on the Starbucks Brasil website. To read the page, we use read_html.\n\nurl &lt;- \"https://starbucks.com.br/lojas\"\npage &lt;- xml2::read_html(url)   # Fetches the HTML content of the page\n\nThe “xpath” shows the path to a specific element on the page. For example, to find the Starbucks logo in the top-left corner of the page, we can use the following code:\n\n# Extracts the HTML element for the logo image\npage %&gt;%\n  html_element(xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\")\n\n{html_node}\n&lt;img alt=\"Starbucks Logo\" src=\"/public/img/icons/starbucks-nav-logo.svg\"&gt;\n\n\nTo learn more about xpaths, you can consult this cheatsheet.\nIn general, on well-constructed pages, the name of elements will be quite self-explanatory. In the case above, the alt attribute already indicates that the object is the Starbucks logo, and the src links to an image file in svg format called starbucks-nav-logo. Unfortunately, this won’t always be the case. On some pages, elements can be quite confusing.\nTo extract a specific attribute, we use the html_attr function.\n\npage %&gt;%\n  html_element(\n    xpath = \"/html/body/div[1]/div[1]/header/nav/div/div[1]/a/img\"\n    ) %&gt;%\n  # Extracts the \"src\" attribute (URL to the image)\n  html_attr(\"src\")  \n\n[1] \"/public/img/icons/starbucks-nav-logo.svg\"\n\n\nIf you combine this last link with “www.starbucks.com.br”, you should arrive at an image of the company’s logo1.\n\n\n\nStarbucks logo\n\n\nTo find the big list of stores in the left panel, we will take advantage of the fact that the div holding this list has a unique class called \"place-list\". It’s easy to verify this directly in your browser. If you use Chrome, for instance, just right-click on the panel and click on Inspect.\n\n\n\n\n\n\n\n\n\n\nAs I mentioned above, things aren’t always well organized. Note that since we want to extract multiple elements and multiple (all) attributes, we use the variants: html_elements and html_attrs.\n\nlist_attr &lt;- page %&gt;%\n  # Selects all divs under \"place-list\" that hold store info\n  html_elements(xpath = '//div[@class=\"place-list\"]/div')  %&gt;%\n  # Extracts all attributes of the selected elements\n  html_attrs()  \n\nThe extracted object is a list where each element is a text vector containing the following information. We have the store name, latitude/longitude, and the address.\n\n# Extracts the first store's information from the list\npluck(list_attr, 1)\n\n                  class           data-latitude          data-longitude \n\"place-item r-place-15\"           \"-23.5658059\"           \"-46.6508012\" \n              data-name             data-street              data-index \n  \"Shopping Top Center\" \"Avenida Paulista, 854\"                     \"0\" \n\n\nAt this point, the web scraping process is complete. Once again, the process was easy because the data is well structured on the Starbucks page. Now, we just need to clean the data."
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#limpeza-de-dados",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#limpeza-de-dados",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "Não vou me alongar muito nos detalhes. Basicamente precisamos converter cada elemento da lista em um data.frame, empilhar os resultados e aí converter os tipos de cada coluna.\n\n# Convert os elementos em data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Empilha os resultados\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Renomeia as colunas\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Seleciona as colunas de interesse\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng para numérico\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nclean_dat\n\n# A tibble: 142 × 5\n   index name                            \n   &lt;dbl&gt; &lt;chr&gt;                           \n 1     0 Shopping Top Center             \n 2     1 Shopping Cidade São Paulo       \n 3     2 Paulista Trianon                \n 4     3 Paulista 500                    \n 5     4 Jardim Pamplona Shopping        \n 6     5 Shopping Patio Paulista         \n 7     6 Shopping Center 3               \n 8     7 Hospital Beneficência Portuguesa\n 9     8 Eliseu Guilherme                \n10     9 Haddock Lobo                    \n   street                                    lat   lng\n   &lt;chr&gt;                                   &lt;dbl&gt; &lt;dbl&gt;\n 1 Avenida Paulista, 854                   -23.6 -46.7\n 2 Avenida Paulista, 1154                  -23.6 -46.7\n 3 Avenida Paulista, 1499                  -23.6 -46.7\n 4 Avenida Paulista, 500                   -23.6 -46.6\n 5 Rua Pamplona, 1704                      -23.6 -46.7\n 6 Rua Treze de Maio, 1933                 -23.6 -46.6\n 7 Avenida Paulista, 2064                  -23.6 -46.7\n 8 Rua Maestro Cardim, 769                 -23.6 -46.6\n 9 Rua Desembargador Eliseu Guilherme, 200 -23.6 -46.6\n10 Rua Haddock Lobo, 608                   -23.6 -46.7\n# ℹ 132 more rows"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#mapa",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#mapa",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "A tabela acima já está em um formato bastante satisfatório. Podemos verificar os dados construindo um mapa simples.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nVale notar que dados extraídos a partir de webscraping quase sempre apresentam algum ruído. Neste caso, os dados parecem relativamente limpos após um pouco de limpeza. Os endereços nem sempre são muito instrutivos, como no caso “Rodovia Hélio Smidt, S/N”, mas isto acontece porque muitas unidades estão dentro de hospitais, shoppings ou aeroportos.\nCom estes dados já podemos fazer análises interessantes. Podemos descobrir, por exemplo, que há 5 unidades da Starbucks apenas na Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n3     2 Paulista Trianon          Avenida Paulista, 1499 -23.6 -46.7\n4     3 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n5     6 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2  (-46.65438 -23.5631)\n3  (-46.6558 -23.56226)\n4 (-46.64809 -23.56784)\n5 (-46.65895 -23.55785)\n\n\nPodemos também contar o número de unidades em cada um dos aeroportos. Aparentemente, há 8 unidades no aeroporto de Guarulhos, o que me parece um número muito alto.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 GRU               8\n2 Brasília          3\n3 Florianópolis     3\n4 Confins           2\n5 Galeão            2\n6 Viracopos         2\n7 Congonhas         1\n8 Curitiba          1\n9 Santos            1\n\n\nPor fim, podemos notar que muitas das unidades do Starbucks se localizam dentro de shoppings. Uma conta simples mostra que cerca de 75 unidades estão localizadas dentro de shoppings, perto de 50% das unidades2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 75"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#construindo",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#construindo",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "A partir destes dados podemos acrescentar mais informação. A partir do geobr podemos identificar em quais cidades as unidades se encontram.\n\ndim_city = geobr::read_municipality(showProgress = FALSE)\ndim_city = st_transform(dim_city, crs = 4326)\nsf::sf_use_s2(FALSE)\n\nstarbucks = starbucks %&gt;%\n  st_join(dim_city) %&gt;%\n  relocate(c(name_muni, abbrev_state), .before = lat)\n\nAgora podemos ver quais cidades tem mais Starbucks.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name_muni, abbrev_state, sort = TRUE) \n\n# A tibble: 43 × 3\n   name_muni      abbrev_state     n\n   &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;\n 1 São Paulo      SP              45\n 2 Rio De Janeiro RJ              11\n 3 Guarulhos      SP               9\n 4 Curitiba       PR               8\n 5 Brasília       DF               6\n 6 Campinas       SP               6\n 7 Florianópolis  SC               5\n 8 Jundiaí        SP               4\n 9 Porto Alegre   RS               4\n10 Ribeirão Preto SP               3\n# ℹ 33 more rows\n\n\nOu seja, há mais Starbucks somente na Paulista do que em quase todas as demais cidades do Brasil."
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#footnotes",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#footnotes",
    "title": "Finding All Starbucks in Brazil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://starbucks.com.br/public/img/icons/starbucks-nav-logo.svg↩︎\nHere, we are assuming that the “name” tag always includes the word “shopping” if the store is located inside a mall. This number might eventually be underestimated if there are stores inside malls that don’t have the word “shopping” in their name. Strictly speaking, we also haven’t verified whether the “shopping” tag is always associated with an active shopping mall.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#data-cleaning",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#data-cleaning",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "I won’t go in depth about the data cleaning process. Basically, we need to convert each element of the list into a data.frame, stack the results, and then adjust the data types of each column.\n\n# Convert the elements into data.frame\ndat &lt;- map(list_attr, \\(x) as.data.frame(t(x)))\n# Stack the results\ndat &lt;- bind_rows(dat)\n\nclean_dat &lt;- dat %&gt;%\n  as_tibble() %&gt;%\n  # Rename the columns\n  rename_with(~str_remove(.x, \"data-\")) %&gt;%\n  rename(lat = latitude, lng = longitude) %&gt;%\n  # Select the columns of interest\n  select(index, name, street, lat, lng) %&gt;%\n  # Convert lat/lng to numeric\n  mutate(\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    index = as.numeric(index),\n    name = str_trim(name)\n    )\n\nThe final dataset is presented below"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#building",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#building",
    "title": "Finding All Starbucks in Brazil",
    "section": "Building",
    "text": "Building\nFrom this data, we can add more information. Using the geobr package, we can identify in which cities the stores are located.\n\ndim_city = geobr::read_municipality(showProgress = FALSE)\ndim_city = st_transform(dim_city, crs = 4326)\nsf::sf_use_s2(FALSE)\n\nstarbucks = starbucks %&gt;%\n  st_join(dim_city) %&gt;%\n  relocate(c(name_muni, abbrev_state), .before = lat)\n\nNow we can see which cities have the most Starbucks locations.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name_muni, abbrev_state, sort = TRUE) \n\n# A tibble: 43 × 3\n   name_muni      abbrev_state     n\n   &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;\n 1 São Paulo      SP              45\n 2 Rio De Janeiro RJ              11\n 3 Guarulhos      SP               9\n 4 Curitiba       PR               8\n 5 Brasília       DF               6\n 6 Campinas       SP               6\n 7 Florianópolis  SC               5\n 8 Jundiaí        SP               4\n 9 Porto Alegre   RS               4\n10 Ribeirão Preto SP               3\n# ℹ 33 more rows\n\n\nOu seja, há mais Starbucks somente na Paulista do que em quase todas as demais cidades do Brasil."
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#google-places",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#google-places",
    "title": "Finding All Starbucks in Brazil",
    "section": "Google Places",
    "text": "Google Places\n\nAdding information using Google Places API\nThe Google Places API allows access to data from Google Maps. The googleway package integrates this data into R already in tidy format.\n\nlibrary(googleway)\n\nI’ll create a simple search to return all Starbucks locations in Brazil. A full search across the entire country would take too long, so I will use the coordinates I found via web scraping as a starting point.\nThe function below searches for the term “starbucks” at all the points I provide. To simplify, the function returns only a few of the columns.\n\n# Function to grab starbucks info\nget_starbucks_info &lt;- function(lat, lng) {\n  \n  # Search for 'Starbucks' using the provided latitude and longitude.\n  places = google_places(\n    search_string = \"starbucks\",   # Search term \"starbucks\"\n    location = c(lat, lng)         # Coordinates (lat and lng) for the search\n  )\n  \n  # Define the columns of interest to keep from the results\n  sel_cols = c(\n    \"name\",                        # Store name\n    \"formatted_address\",           # Store address\n    \"lat\",                         # Latitude\n    \"lng\",                         # Longitude\n    \"rating\",                      # Store rating\n    \"user_ratings_total\",          # Number of user ratings\n    \"business_status\"              # Business status (e.g., operational or closed)\n  )\n  \n  # Process the results and select the relevant columns\n  places$results %&gt;%\n    tidyr::unnest(\"geometry\") %&gt;%   # Extract the nested 'geometry' field\n    tidyr::unnest(\"location\") %&gt;%   # Extract the nested 'location' field (lat and lng)\n    dplyr::select(dplyr::all_of(sel_cols))   # Select only the columns of interest\n}\n\nThe code below uses purrr to iterate the get_starbucks_info function over the lat/lng pairs.\n\n# Remove geometry and keep only coordinates\ncoords_starbucks &lt;- starbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  select(index, name, lat, lng)\n\nstarbucks_info &lt;- purrr::map2(\n  coords_starbucks$lat,\n  coords_starbucks$lng,\n  get_starbucks_info\n  )\n\ndat &lt;- starbucks_info %&gt;%\n  bind_rows(.id = \"search_id\") %&gt;%\n  distinct()\n\nTo clean the data, I will keep only the active stores that contain “Starbucks” in their name. Additionally, I will pair the data with my web scraping dataset using st_nearest_feature(x, y). This function finds the nearest point in y for each point in x.\n\ndat &lt;- dat |&gt; \n  # Keep only stores with \"Starbucks\" in the name and that are operational\n  filter(str_detect(name, \"Starbucks\"), business_status == \"OPERATIONAL\") |&gt; \n  # Arrange the results by address\n  arrange(formatted_address)\n\n# Convert to a spatial data frame using longitude and latitude\ngoogle_data &lt;- dat %&gt;%\n  # Set coordinate reference system to WGS 84 (EPSG:4326)\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326)  \n\n# Find the nearest Starbucks locations from the web scraping data (starbucks)\n# for each point in google_data\ninds &lt;- st_nearest_feature(google_data, starbucks)\n\n# Extract the metadata of the nearest points from the web scraping data and\n# convert to a tibble\nmetadata &lt;- starbucks %&gt;%\n  slice(inds) %&gt;%\n  st_drop_geometry() %&gt;%  # Remove spatial geometry\n  as_tibble()\n\n# Rename the columns in google_data and bind the metadata from the web scraping data\ngoogle_data &lt;- google_data |&gt; \n  rename(google_name = name, google_address = formatted_address) |&gt; \n  bind_cols(metadata)  # Combine google_data with the corresponding metadata\n\n\n\nFinal Map\nThe interactive map below shows all Starbucks locations in São Paulo. The color of each circle represents its rating, and the size of the circle represents the number of reviews.\nThe units along the Avenida Paulista corridor, for example, have high average ratings and a large number of reviews. One of the worst-rated units seems to be the one near Mackenzie University, which has a rating of 2.1 and 15 reviews. In the Eastern Zone, the store at Shopping Aricanduva also has a slightly lower rating, 3.9 with 158 reviews.\n\nsp &lt;- filter(google_data, name_muni == \"São Paulo\")\n\nsp &lt;- sp |&gt; \n  mutate(\n    rad = findInterval(user_ratings_total, c(25, 100, 1000, 2500, 5000)) * 2 + 5\n  )\n\npal &lt;- colorNumeric(\"RdBu\", domain = sp$rating)\n\nlabels &lt;- stringr::str_glue(\n  \"&lt;b&gt; {sp$name} &lt;/b&gt; &lt;br&gt;\n   &lt;b&gt; Rating &lt;/b&gt;: {sp$rating} &lt;br&gt;\n   &lt;b&gt; No Ratings &lt;/b&gt; {sp$user_ratings_total}\"\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\nleaflet(sp) |&gt; \n  addTiles() |&gt; \n  addCircleMarkers(\n    radius = ~rad,\n    color = ~pal(rating),\n    label = labels,\n    stroke = FALSE,\n    fillOpacity = 0.5\n  ) |&gt; \n  addLegend(pal = pal, values = ~rating) |&gt; \n  addProviderTiles(\"CartoDB\")"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#map",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#map",
    "title": "Finding All Starbucks in Brazil",
    "section": "",
    "text": "The table above is already in a pretty satisfactory format. We can check the data by building a simple map.\n\nlibrary(sf)\nlibrary(leaflet)\n\nstarbucks &lt;- st_as_sf(clean_dat, coords = c(\"lng\", \"lat\"), crs = 4326, remove = FALSE)\n\nleaflet(starbucks) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)\n\n\n\n\n\nIt’s worth noting that data extracted via web scraping almost always contains some noise. In this case, the data seems relatively clean after a bit of processing. The addresses are not always very informative, like in the case of “Rodovia Hélio Smidt, S/N,” but this happens because many stores are located inside hospitals, shopping malls, or airports.\nWith this data, we can already perform interesting analyses. For example, we can find out that there are five Starbucks stores just on Avenida Paulista.\n\nstarbucks %&gt;%\n  filter(str_detect(street, \"Avenida Paulista\"))\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -46.65895 ymin: -23.56784 xmax: -46.64809 ymax: -23.55785\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 6\n  index name                      street                   lat   lng\n* &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;\n1     0 Shopping Top Center       Avenida Paulista, 854  -23.6 -46.7\n2     1 Shopping Cidade São Paulo Avenida Paulista, 1154 -23.6 -46.7\n3     2 Paulista Trianon          Avenida Paulista, 1499 -23.6 -46.7\n4     3 Paulista 500              Avenida Paulista, 500  -23.6 -46.6\n5     6 Shopping Center 3         Avenida Paulista, 2064 -23.6 -46.7\n               geometry\n*           &lt;POINT [°]&gt;\n1  (-46.6508 -23.56581)\n2  (-46.65438 -23.5631)\n3  (-46.6558 -23.56226)\n4 (-46.64809 -23.56784)\n5 (-46.65895 -23.55785)\n\n\nWe can also count the number of stores in each airport. Apparently, there are eight stores at Guarulhos Airport, which seems like quite a high number to me.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Aeroporto\")) %&gt;%\n  mutate(\n    name_airport = str_remove(name, \"de \"),\n    name_airport = str_extract(name_airport, \"(?&lt;=Aeroporto )\\\\w+\"),\n    name_airport = if_else(is.na(name_airport), \"Confins\", name_airport),\n    .before = \"name\"\n  ) %&gt;%\n  count(name_airport, sort = TRUE)\n\n# A tibble: 9 × 2\n  name_airport      n\n  &lt;chr&gt;         &lt;int&gt;\n1 GRU               8\n2 Brasília          3\n3 Florianópolis     3\n4 Confins           2\n5 Galeão            2\n6 Viracopos         2\n7 Congonhas         1\n8 Curitiba          1\n9 Santos            1\n\n\nFinally, we can note that many Starbucks stores are located inside shopping malls. A simple calculation shows that around 75 stores are located inside malls, close to 50% of the total units2.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(str_detect(name, \"Shopping|shopping\")) %&gt;%\n  nrow()\n\n[1] 75"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#finding-each-city",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#finding-each-city",
    "title": "Finding All Starbucks in Brazil",
    "section": "Finding each city",
    "text": "Finding each city\nFrom this data, we can add more information. Using the geobr package, we can identify in which cities the stores are located.\n\ndim_city &lt;- geobr::read_municipality(showProgress = FALSE)\ndim_city &lt;- st_transform(dim_city, crs = 4326)\nsf::sf_use_s2(FALSE)\n\nstarbucks &lt;- starbucks %&gt;%\n  st_join(dim_city) %&gt;%\n  relocate(c(name_muni, abbrev_state), .before = lat)\n\nNow we can see which cities have the most Starbucks locations. São Paulo alone has more the 40 units.\n\nstarbucks %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name_muni, abbrev_state, sort = TRUE) \n\n# A tibble: 43 × 3\n   name_muni      abbrev_state     n\n   &lt;chr&gt;          &lt;chr&gt;        &lt;int&gt;\n 1 São Paulo      SP              45\n 2 Rio De Janeiro RJ              11\n 3 Guarulhos      SP               9\n 4 Curitiba       PR               8\n 5 Brasília       DF               6\n 6 Campinas       SP               6\n 7 Florianópolis  SC               5\n 8 Jundiaí        SP               4\n 9 Porto Alegre   RS               4\n10 Ribeirão Preto SP               3\n# ℹ 33 more rows"
  },
  {
    "objectID": "posts/general-posts/2024-07-viz-metro-4/index.html",
    "href": "posts/general-posts/2024-07-viz-metro-4/index.html",
    "title": "Linha-4 Amarela Metrô de São Paulo",
    "section": "",
    "text": "Linha 4-Amarela do Metrô\n\nO número de passageiros transportados pela linha 4-Amarela continua abaixo dos níveis pré-pandemia, mesmo como a abertura da estação Vila Sônia.\nAinda é incerto o efeito do aumento da tarifa, instaurado no dia 01/06/2024, sobre a demanda dos passageiros.\nEstações mais movimentadas são as com baldeações, reforçando o papel conector da linha-4.\nFluxo médio nos dias úteis está mais de 10% abaixo dos níveis pré-pandemia. Nos finais de semana o fluxo está apenas 3,6% abaixo.\n\n\nOverview\n\n\n\n\n\n\n\nEstações mais movimentadas (últimos 12 meses)\n\n\n\n\n\n\n\nDias de semana\n\n\n\n\n\n\n\nDados: ViaQuatro\nTipografia: Fira Code e Fira Sans\nPaleta: \"#e6bb3e\""
  },
  {
    "objectID": "posts/general-posts/2024-07-webscrape-metro-4/index.html",
    "href": "posts/general-posts/2024-07-webscrape-metro-4/index.html",
    "title": "Line-4 Metro",
    "section": "",
    "text": "In this post I show how to webscrape all publicly available information on passenger flow from the Line-4 Metro in São Paulo. This post is part of a larger series where I gather all data on the subway lines in São Paulo.\n\n\nFinding data on the subway lines in São Paulo is not easy. There are currently 5 subway lines in São Paulo, identified by number and color:\n\nLine-1 (Blue)\nLine-2 (Green)\nLine-3 (Red)\nLine-4 (Yellow)\nLine-5 (Lilac)\n\nThe first three lines are state-owned by the public company METRO. Line-4 is a PPP an privately operated by Via Quatro Mobilidade. Line-5 is also privately operated by ViaMobilidade."
  },
  {
    "objectID": "posts/general-posts/2024-07-webscrape-metro-4/index.html#subway-data-in-são-paulo",
    "href": "posts/general-posts/2024-07-webscrape-metro-4/index.html#subway-data-in-são-paulo",
    "title": "Line-4 Metro",
    "section": "",
    "text": "Finding data on the subway lines in São Paulo is not easy. There are currently 5 subway lines in São Paulo, identified by number and color:\n\nLine-1 (Blue)\nLine-2 (Green)\nLine-3 (Red)\nLine-4 (Yellow)\nLine-5 (Lilac)\n\nThe first three lines are state-owned by the public company METRO. Line-4 is a PPP an privately operated by Via Quatro Mobilidade. Line-5 is also privately operated by ViaMobilidade."
  },
  {
    "objectID": "posts/general-posts/2024-07-webscrape-metro-4/index.html#downloading",
    "href": "posts/general-posts/2024-07-webscrape-metro-4/index.html#downloading",
    "title": "Line-4 Metro",
    "section": "Downloading",
    "text": "Downloading\nWe can use the rvest package to easily find and download all the pdf files.\n\n# Libraries\nlibrary(rvest)\nlibrary(stringr)\nlibrary(dplyr)\n\nimport::from(xml2, read_html)\nimport::from(here, here)\nimport::from(glue, glue)\nimport::from(pdftools, pdf_text)\nimport::from(purrr, map2, map_lgl)\n\nThe chunk of code below finds the links to all pdf files.\n\n# Site url\nurl &lt;- \"https://www.viaquatro.com.br/linha-4-amarela/passageiros-transportados\"\n\n# Parse the html\npage &lt;- read_html(url)\n\n# Get download links\npdf_links &lt;- page %&gt;%\n  html_elements(xpath = \"//article/ul/li/a\") %&gt;%\n  html_attr(\"href\")\n# Get the name of each pdf file\npdf_names &lt;- page %&gt;%\n  html_elements(xpath = \"//article/ul/li/a\") %&gt;%\n  html_attr(\"title\")\n\npdf_links &lt;- pdf_links[str_detect(pdf_links, \"\\\\.pdf$\")]\n\n# Store links and names in a tibble\nparams &lt;- tibble(\n  link = pdf_links,\n  name = pdf_names\n)\n\nI save the title of each file and the link in a tibble. I also extract some useful information using regex.\n\n# Use regex to extract information from file name\nparams &lt;- params |&gt; \n  mutate(\n    # Find variable name\n    variable = str_extract(name, \".+(?= - [A-Z0-9])\"),\n    # Remove excess whitespace\n    variable = str_replace_all(variable, \"  \", \" \"),\n    # Get the date (either in 20xx or %B%Y format)\n    x1 = str_trim(str_extract(name, \"(?&lt;= - )[A-Z0-9].+\")),\n    # Extract year number\n    year = as.numeric(str_extract(x1, \"[0-9]{4}\")),\n    # Extract month label (in portuguese)\n    month_label = str_extract(x1, \"[[:alpha:]]+\"),\n    # Convert to date\n    ts_date = if_else(\n      is.na(month_label),\n      as.Date(str_c(year, \"/01/01\")),\n      parse_date(paste(year, month_label, \"01\", sep = \"/\"),\n                 format = \"%Y/%B/%d\",\n                 locale = locale(\"pt\"))\n    )\n  )\n\nparams &lt;- params |&gt; \n  arrange(ts_date) |&gt; \n  arrange(variable)\n\nThe simple for-loop below downloads all pdf files locally. I include a progress bar, and a simple check to avoid downloading duplicate files. This check also comes in handy if something goes wrong internet-wise.\n\n#&gt; Download all pdf files\n\nfld &lt;- here(\"static/data/raw/metro_sp/linha_4/\")\nbaseurl &lt;- \"https://www.viaquatro.com.br\"\npb &lt;- txtProgressBar(max = nrow(params), style = 3)\n\n# Loop across params\nfor (i in 1:nrow(params)) {\n  \n  # Define file name\n  name_file &lt;- janitor::make_clean_names(params[[\"name\"]][i])\n  # Add pdf extension\n  name_file &lt;- paste0(name_file, \".pdf\")\n  # Crate file path\n  destfile &lt;- here(fld, name_file)\n  \n  # Simple check:\n  # If downloaded file already exists, skip it\n  # If not, download the file\n  if (file.exists(destfile)) {\n    message(glue(\"File {name_file} already exists.\"))\n    i &lt;- i + 1\n  } else {\n    message(glue(\"Downloading file {name_file}.\"))\n    # Link to the pdf file\n    url &lt;- paste0(baseurl, params[[\"link\"]][i])\n    # Download the file\n    download.file(url = url, destfile = destfile, mode = \"wb\", quiet = TRUE)\n    # For precaution defines a random time-interval between each download\n    Sys.sleep(1 + runif(1))\n  }\n\n  setTxtProgressBar(pb, i)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2024-07-webscrape-metro-4/index.html#functions",
    "href": "posts/general-posts/2024-07-webscrape-metro-4/index.html#functions",
    "title": "Line-4 Metro",
    "section": "Functions",
    "text": "Functions\nTo facilitate the import and cleaning process I create several helper functions.\n\nImporting pdfs\n\n# Read pdf using pdf tools and convert to tibble\nread_pdf &lt;- function(path) {\n  \n  tbl &lt;- pdftools::pdf_text(path)\n  tbl &lt;- stringr::str_split(tbl, \"\\n\")\n  # Get only the first result\n  # Assumes each pdf contains only a single table\n  tbl &lt;- tibble::tibble(text = tbl[[1]])\n  \n  if (all(tbl == \"\")) {\n    warning(\"No text elements found!\")\n  }\n  \n  return(tbl)\n  \n}\n\n\n\nCleaning the tables\n\n# Helper function to extract numbers from text\n# Numbers use , as decimal mark and . as thousand mark\nget_numbers &lt;- Vectorize(function(text) {\n  \n  num &lt;- stringr::str_extract(text, \"([0-9].+)|([0-9])\")\n  num &lt;- stringr::str_remove(num, \"\\\\.\")\n  num &lt;- as.numeric(stringr::str_replace(num, \",\", \".\"))\n  return(num)\n  \n})\n\nclean_pdf_passenger &lt;- function(dat) {\n  \n  mes &lt;- lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\")\n  \n  cat &lt;- c(\n    \"Total\", \"Média dos Dias Úteis\", \"Média dos Sábados\", \"Média dos Domingos\",\n    \"Máxima Diária\"\n  )\n  \n  pat &lt;- paste(str_glue(\"({cat})\"), collapse = \"|\")\n  \n  tbl &lt;- dat |&gt; \n    mutate(\n      variable = str_remove_all(text, \"\\\\d\"),\n      variable = str_trim(variable),\n      variable = str_replace_all(variable, \"  \", \"\"),\n      month = str_extract(text, paste(mes, collapse = \"|\")),\n      metric = str_extract(text, pat),\n      value = get_numbers(text)\n    )\n  \n  tbl_date &lt;- tbl |&gt; \n    filter(!is.na(month)) |&gt; \n    mutate(\n      date = str_glue(\"{value}-{month}-01\"),\n      date = parse_date(date, format = \"%Y-%B-%d\", locale = locale(\"pt\"))\n    ) |&gt; \n    select(date, year = value)\n  \n  tbl_value &lt;- tbl |&gt; \n    filter(!is.na(value), !is.na(metric)) |&gt; \n    select(metric, value) |&gt; \n    mutate(value = as.numeric(value))\n  \n  tbl &lt;- cbind(tbl_date, tbl_value)\n  \n  return(tbl)\n  \n}\n\nclean_pdf_station &lt;- function(dat) {\n\n  name_stations &lt;- c(\n    'Vila Sônia', 'São Paulo - Morumbi', \"Butantã\", \"Pinheiros\", \"Faria Lima\",\n    \"Fradique Coutinho\", \"Oscar Freire\", \"Paulista\", \"Higienópolis - Mackenzie\",\n    \"República\", \"Luz\")\n  \n  pat &lt;- paste(str_glue(\"({name_stations})\"), collapse = \"|\")\n  \n  tbl &lt;- dat |&gt; \n    mutate(\n      month = str_extract(text, paste(mes, collapse = \"|\")),\n      name_station = str_extract(text, pat),\n      value = get_numbers(text)\n    )\n  \n  tbl_date &lt;- tbl |&gt; \n    filter(!is.na(month)) |&gt; \n    mutate(\n      date = str_glue(\"{value}-{month}-01\"),\n      date = parse_date(date, format = \"%Y-%B-%d\", locale = locale(\"pt\"))\n    ) |&gt; \n    select(date, year = value)\n  \n  tbl_value &lt;- tbl |&gt; \n    filter(!is.na(value), !is.na(name_station)) |&gt; \n    select(name_station, value) |&gt; \n    mutate(value = as.numeric(value))\n  \n  tbl &lt;- cbind(tbl_date, tbl_value)\n  \n  return(tbl)\n  \n}\n\n\n\nFinal function\n\nimport_pdf &lt;- function(path, type) {\n  \n  stopifnot(any(type %in% c(\"station\", \"passenger_entrance\", \"passenger_transported\")))\n  \n  file &lt;- read_pdf(path)\n  \n  if (nrow(file) == 1) {\n    return(NA)\n  }\n  \n  if (nrow(file) &gt; 11) {\n    clean_file &lt;- clean_pdf_station(file)\n  } else {\n    clean_file &lt;- clean_pdf_passenger(file)\n  }\n  \n  return(clean_file)\n  \n}"
  },
  {
    "objectID": "posts/general-posts/2024-07-webscrape-metro-4/index.html#data-processing-1",
    "href": "posts/general-posts/2024-07-webscrape-metro-4/index.html#data-processing-1",
    "title": "Line-4 Metro",
    "section": "Data Processing",
    "text": "Data Processing\nThe code below imports all pdf files and cleans the data. I consolidate the information into two tables:\n\nPassengers - show total passenger flow metrics by month.\nStations - shows total montlhy passenger flow by station.\n\n\n# Find path name to all downloaded pdfs\nfld &lt;- here::here(\"static/data/raw/metro_sp/linha_4\")\npath_pdfs &lt;- list.files(fld, \"\\\\.pdf$\", full.names = TRUE)\n\nparams &lt;- tibble(\n  path = path_pdfs\n)\n\nparams &lt;- params |&gt; \n  mutate(\n    name_file = basename(path),\n    type = case_when(\n      str_detect(name_file, \"^entrada_de_passageiros_pelas\") ~ \"passenger_entrance\",\n      str_detect(name_file, \"estac\") ~ \"station\",\n      str_detect(name_file, \"transportados\") ~ \"passenger_transported\",\n      TRUE ~ \"station\"\n    )\n  )\n\npdfs &lt;- params |&gt; \n  mutate(file = map2(path, type, import_pdf))\n\nvalid_files &lt;- pdfs |&gt; \n  filter(map_lgl(file, is.data.frame))\n\ntbl_passengers &lt;- valid_files |&gt; \n  filter(type == \"passenger_transported\") |&gt; \n  reframe(bind_rows(file)) |&gt; \n  arrange(date) |&gt; \n  mutate(name_station = \"Total\")\n\ntbl_onboarding &lt;- valid_files |&gt; \n  filter(type == \"passenger_entrance\") |&gt; \n  reframe(bind_rows(file)) |&gt; \n  arrange(date)\n\n# This pdf is wrong!\ntbl_onboarding_station &lt;- tbl_onboarding |&gt; \n  filter(!is.na(name_station)) |&gt; \n  mutate(metric = \"Média dos Dias Úteis\")\n\ntbl_onboarding &lt;- tbl_onboarding |&gt; \n  filter(is.na(name_station)) |&gt; \n  mutate(name_station = \"Total\")\n\ntbl_passengers &lt;- bind_rows(\n  list(\n    passenger_transported = tbl_passengers,\n    passenger_entrance = tbl_onboarding\n  ),\n  .id = \"variable\")\n\ntbl_station &lt;- valid_files |&gt; \n  filter(type == \"station\") |&gt;\n  reframe(bind_rows(file)) |&gt; \n  arrange(date)\n\nThe passenger table shows various passenger flow metrics by month.\n\nslice_tail(passenger, n = 10) |&gt; \n  gt() |&gt; \n  opt_stylize(style = 6)\n\n\n\n\n\n  \n    \n      variable\n      date\n      year\n      metric\n      value\n      name_station\n    \n  \n  \n    passenger_entrance\n2024-06-01\n2024\nTotal\n6135.54\nTotal\n    passenger_entrance\n2024-06-01\n2024\nMédia dos Dias Úteis\n204.52\nTotal\n    passenger_entrance\n2024-06-01\n2024\nMédia dos Sábados\n106.84\nTotal\n    passenger_entrance\n2024-06-01\n2024\nMédia dos Domingos\n66.72\nTotal\n    passenger_entrance\n2024-06-01\n2024\nMáxima Diária\n218.21\nTotal\n    passenger_entrance\n2024-07-01\n2024\nTotal\n5908.16\nTotal\n    passenger_entrance\n2024-07-01\n2024\nMédia dos Dias Úteis\n183.84\nTotal\n    passenger_entrance\n2024-07-01\n2024\nMédia dos Sábados\n102.30\nTotal\n    passenger_entrance\n2024-07-01\n2024\nMédia dos Domingos\n58.12\nTotal\n    passenger_entrance\n2024-07-01\n2024\nMáxima Diária\n207.26\nTotal\n  \n  \n  \n\n\n\n\nThe station table show total passenger flow by month (in thousands).\n\nslice_tail(station, n = 11) |&gt; \n  gt() |&gt; \n  opt_stylize(style = 6)"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html",
    "title": "Census Tracts in Brazil",
    "section": "",
    "text": "Census tracts are the smallest administrative areas for which socioeconomic and demographic data are available. Broadly speaking, census tracts are small areas that exhibit similar socioeconomic and demographic patterns. In another post, I presented all of the Brazilian administrative and statistical subdivisions.\nCensus tracts are the strata used by National Bureau of Statistics and Geography (IBGE) in their decennial Census. The shape of each census tract usually respects administrative borders, land barriers, public spaces (e.g. parks, beaches, etc.), and follows the shape of roads, highways, or city blocks.\nThough they are typically small, census tracts size vary. Large plots of uninhabited land are commonly grouped into a single tract. In dense urban areas, census tracts are very small.\nCensus tracts exhibit relatively homogeneous socioeconomic and demographic characteristics. This makes census tracts a very useful statistical tool in regression analysis and classification.\nThe map below shows the 2022 census tracts in Curitiba, a major city in the Southern part of Brazil.\n\n\nCode\ncuritiba_setores &lt;- cur_setores %&gt;%\n  mutate(across(pop:dom_prt_ocup, as.numeric)) %&gt;%\n  mutate(\n    area_km2 = as.numeric(area_km2),\n    pop_dens_area = pop / area_km2,\n    pop_dens_hh = pop / dom_prt_ocup\n  )\n\nlabels &lt;- sprintf(\n  \"&lt;b&gt;Code Tract&lt;b/&gt;: %s &lt;br&gt;\n  &lt;b&gt;Code Subdistrict&lt;b/&gt;: %s &lt;br&gt;\n  &lt;b&gt;Name Subdistrict&lt;b/&gt;: %s\",\n  curitiba_setores$code_tract,\n  curitiba_setores$code_subdistrict,\n  curitiba_setores$name_subdistrict\n)\n\nlabels &lt;- lapply(labels, htmltools::HTML)\n\n# center &lt;- curitiba_setores %&gt;%\n#   summarise(geom = st_union(.)) %&gt;%\n#   st_centroid() %&gt;%\n#   st_coordinates()\n\ncuritiba_center &lt;- c(\"lng\" = -49.28824, \"lat\" = -25.47788)\n\nleaflet(curitiba_setores) |&gt; \n  addTiles() |&gt; \n  addPolygons(weight = 2, fillColor = \"gray80\", color = \"#feb24c\", label = labels) |&gt; \n  addProviderTiles(provider = \"CartoDB.Positron\") |&gt; \n  setView(curitiba_center[[1]], curitiba_center[[2]], zoom = 15)\n\n\n\n\n\n\n\n\n\n\nThe data can be downloaded from IBGE’s website.\n\n\n\nThe package geobr (available for both Python and R) provides a convenient way to import census tracts directly into a session. The example code below shows how to import the most recent shape file for São Paulo’s census tracts1.\n\n# Import census tracts for São Paulo\nspo_tract &lt;- geobr::read_census_tract(3550308, year = 2022)\n\n\n\n\n\nCensus tracts come with a large variety of socioeconomic and demographic data. The 2022 Census currently only has a limited set of variables, that include total population and the total number of households. More data should be released in the future.\nThe 2010 census tracts offers a much richer set of variables including demographic information on age, sex, and race as well as income and education. There is a clear trade-off however as this data is almost 15 years old.\n\n\nThe maps below show basic demographic information from the 2022 Census highlighting the central district in Curitiba. For simplicity, I omit the color legend but darker shades of blue represent higher values, while lighter/whiter shades of blue represent lower values. Technically, these are quintile maps, where the underlying numerical data was ordered and binned into five equal-sized groups.\n\n\nCode\nsetores_centro &lt;- cur_setores |&gt; \n  # Administração Regional da Matriz\n  filter(code_subdistrict == 41069020501) |&gt; \n  mutate(\n    pop = as.numeric(pop),\n    pop_quintile = ntile(pop, 5),\n    area_km2 = as.numeric(area_km2),\n    pop_dens = pop / area_km2 * 10,\n    pop_dens = ntile(pop_dens, 5),\n    pop_dom = ntile(pop_dom, 5)\n    )\n\ntheme_map &lt;- theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 14, hjust = 0.5)\n  )\n\nm1 &lt;- ggplot(setores_centro) +\n  geom_sf() +\n  ggtitle(\"Census tracts\") +\n  theme_map\n\nm2 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_quintile), lwd = 0.15, color = \"white\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  ggtitle(\"Population\") +\n  theme_map\n\nm3 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dens), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Pop. Density\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\nm4 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dom), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Persons per Household\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore sophisticated analysis can be made using census tracts, but - currently - this is only possible using 2010 data. The map below shows household income data at a census tract level for the entire city of Curitiba. The map shows higher levels of income in the city center and lower levels of income in the city’s periphery.\nWhile the absolute values of income are certainly out of date, the overall spatial distribution of the data might still be similar. Since this maps uses deciles of income, instead of the actual income value, it can still communicate valuable information. This is, of course, a strong hypothesis that might be more or less valid in different contexts.\n\n\nCode\ncuritiba_income &lt;- curitiba_income |&gt; \n  mutate(decile = ntile(income_pc, 10))\n\nggplot(curitiba_income) +\n  geom_sf(aes(fill = decile, color = decile)) +\n  scale_fill_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  scale_color_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  labs(title = \"Curitiba: Household Income per capita (deciles)\") +\n  ggthemes::theme_map() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.position = c(0.9, 0.05)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#getting-the-data",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#getting-the-data",
    "title": "Census Tracts in Brazil",
    "section": "",
    "text": "The data can be downloaded from IBGE’s website.\n\n\n\nThe package geobr (available for both Python and R) provides a convenient way to import census tracts directly into a session. The example code below shows how to import the most recent shape file for São Paulo’s census tracts1.\n\n# Import census tracts for São Paulo\nspo_tract &lt;- geobr::read_census_tract(3550308, year = 2022)"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#using-census-tracts",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#using-census-tracts",
    "title": "Census Tracts in Brazil",
    "section": "",
    "text": "Census tracts come with a large variety of socioeconomic and demographic data. The 2022 Census currently only has a limited set of variables, that include total population and the total number of households. More data should be released in the future.\nThe 2010 census tracts offers a much richer set of variables including demographic information on age, sex, and race as well as income and education. There is a clear trade-off however as this data is almost 15 years old.\n\n\nThe maps below show basic demographic information from the 2022 Census highlighting the central district in Curitiba. For simplicity, I omit the color legend but darker shades of blue represent higher values, while lighter/whiter shades of blue represent lower values. Technically, these are quintile maps, where the underlying numerical data was ordered and binned into five equal-sized groups.\n\n\nCode\nsetores_centro &lt;- cur_setores |&gt; \n  # Administração Regional da Matriz\n  filter(code_subdistrict == 41069020501) |&gt; \n  mutate(\n    pop = as.numeric(pop),\n    pop_quintile = ntile(pop, 5),\n    area_km2 = as.numeric(area_km2),\n    pop_dens = pop / area_km2 * 10,\n    pop_dens = ntile(pop_dens, 5),\n    pop_dom = ntile(pop_dom, 5)\n    )\n\ntheme_map &lt;- theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 14, hjust = 0.5)\n  )\n\nm1 &lt;- ggplot(setores_centro) +\n  geom_sf() +\n  ggtitle(\"Census tracts\") +\n  theme_map\n\nm2 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_quintile), lwd = 0.15, color = \"white\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  ggtitle(\"Population\") +\n  theme_map\n\nm3 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dens), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Pop. Density\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\nm4 &lt;- ggplot(setores_centro) +\n  geom_sf(aes(fill = pop_dom), lwd = 0.15, color = \"white\") +\n  ggtitle(\"Persons per Household\") +\n  scale_fill_fermenter(direction = 1, breaks = 1:5) +\n  theme_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore sophisticated analysis can be made using census tracts, but - currently - this is only possible using 2010 data. The map below shows household income data at a census tract level for the entire city of Curitiba. The map shows higher levels of income in the city center and lower levels of income in the city’s periphery.\nWhile the absolute values of income are certainly out of date, the overall spatial distribution of the data might still be similar. Since this maps uses deciles of income, instead of the actual income value, it can still communicate valuable information. This is, of course, a strong hypothesis that might be more or less valid in different contexts.\n\n\nCode\ncuritiba_income &lt;- curitiba_income |&gt; \n  mutate(decile = ntile(income_pc, 10))\n\nggplot(curitiba_income) +\n  geom_sf(aes(fill = decile, color = decile)) +\n  scale_fill_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  scale_color_fermenter(\n    name = \"HH Income per capita (deciles)\",\n    palette = \"Spectral\",\n    breaks = 1:10,\n    labels = c(\"Bottom 10%\", rep(\"\", 8), \"Top 10%\"),\n    na.value = \"gray50\",\n    direction = 1) +\n  labs(title = \"Curitiba: Household Income per capita (deciles)\") +\n  ggthemes::theme_map() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    legend.position = c(0.9, 0.05)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#higher-resolution",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#higher-resolution",
    "title": "Census Tracts in Brazil",
    "section": "Higher Resolution",
    "text": "Higher Resolution\nAn obvious improvement from the 2010 census tracts is the gain in spatial resolution. The maps below show the central area of Curitiba where there was a 33% increase in the number of total census tracts."
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#dealing-with-spatial-inconsistency",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#dealing-with-spatial-inconsistency",
    "title": "Census Tracts in Brazil",
    "section": "Dealing with spatial inconsistency",
    "text": "Dealing with spatial inconsistency\n\nSpatial interpolation\nA simple strategy to deal with census tracts’ spatial inconsistency is to define a common spatial grid. This means choosing either a (1) grid of triangles; (2) grid of squares; or (3) grid of hexagons. In this example, I choose a simple squared grid.\nThe process of converting one set of data (stored in a particular shape) into another shape is called spatial interpolation. Spatial interpolation is also referred to as areal interpolation or dasymetric interpolation.\nEssentially, the problem we are tryting to solve is: we have some data stored in a (big) shape and we wish to estimate the same data in another (smaller) shape. In this case, we will dissolve population count data, stored in the shape of the 2010 census tracts and 2020 census tracts, into a finer squared grid.\nTo convert the data from one shape to the other we implicitly assume that the variable (population) is uniformly distributed over the shape’s space. That is, we assume that every single person is evenly distributed across each census tract. This assumption works well in small densely populated tracts, but doesn’t hold as well in larger tracts.\n\n\nSpatial grid for Census Data\nThe maps below show the same Census household data in a squared 500x500m grid. While I omit the color legend of the plots, I scaled them equally as to make them directly comparable; also, darker shades of green indicate higher values, while lighter shades of green indicate lower values.\nThe 2010 Census data was directly imported using the censobr R package. To estimate a simple areal interpolation I use the areal package. Executing the areal interpolation involves a few intermediary steps such as choosing a valid UTM CRS.\nThe final result shows the number of households in 2010 and 2022 in a common spatial grid. This process can be replicated across the entire city to allow an easier comparison of the data.\n\n\nCode\nlibrary(censobr)\nlibrary(areal)\n# Get demographic data for 2010 census\nbasico &lt;- censobr::read_tracts(dataset = \"Basico\")\n# Get number of households (private)\nbasico &lt;- basico %&gt;%\n  filter(code_muni == 4106902) %&gt;%\n  select(code_tract, dom = V001) %&gt;%\n  collect()\n# Join with census tract shape\nsetores_centro_antigo &lt;- left_join(setores_centro_antigo, basico, by = \"code_tract\")\n\n# Create a grid\ngrid_centro &lt;- setores_centro %&gt;%\n  st_union() %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  st_make_grid(cellsize = 500) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  mutate(gid = row_number(.))\n\n# Join census data with the new grid\ngrid_2010 &lt;- setores_centro_antigo %&gt;%\n  select(dom) %&gt;%\n  st_interpolate_aw(grid_centro, extensive = TRUE) \n# Join 2020 census data with the new grid\ngrid_2020 &lt;- setores_centro %&gt;%\n  select(dom_prt) %&gt;%\n  st_interpolate_aw(grid_centro, extensive = TRUE)\n\nd1 &lt;- grid_2010 %&gt;%\n  st_centroid() %&gt;%\n  st_join(grid_centro) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  rename(dom_2010 = dom)\n\nd2 &lt;- grid_2020 %&gt;%\n  st_centroid() %&gt;%\n  st_join(grid_centro) %&gt;%\n  st_drop_geometry() %&gt;%\n  as_tibble() %&gt;%\n  rename(dom_2020 = dom_prt)\n\nfull_grid &lt;- grid_centro %&gt;%\n  left_join(d1) %&gt;%\n  left_join(d2)\n\nbbox &lt;- setores_centro |&gt; \n  st_union() %&gt;%\n  st_transform(crs = 32722) %&gt;%\n  st_buffer(dist = 100) %&gt;%\n  st_transform(crs = 4674) %&gt;%\n  st_bbox()\n\nm3 &lt;- full_grid %&gt;%\n  mutate(\n    chg = (dom_2020 / dom_2010 - 1) * 100,\n    chg_bin = cut(chg, breaks = c(-Inf, 0, 50, 100, Inf))\n  )  %&gt;%\n  st_make_valid() %&gt;%\n  filter(!is.na(chg_bin)) %&gt;%\n  ggplot() +\n  geom_sf(aes(fill = chg_bin), color = \"white\") +\n  scale_fill_brewer(palette = \"Greens\") +\n  guides(fill = \"none\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#footnotes",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#footnotes",
    "title": "Census Tracts in Brazil",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs of June 2024, the shape↩︎\nCensus tracts where there either 0 persons per household or more than 4 persons per household are considered outliers.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-06-finding-the-coffee/index.html",
    "href": "posts/general-posts/2024-06-finding-the-coffee/index.html",
    "title": "Locating all The Coffee shops in Brazil",
    "section": "",
    "text": "In this post, I’ll show you how to map every The Coffee shop in Brazil in less time than it takes to brew a pot of coffee. All this from your laptop and without spending a dime.\nWe’ll use only R and a few packages to webscrape all addresses.\n\n\nThe Coffee is a Japanese-inspired chain of coffee shops with a distinct minimalist visual identity. Their street shops are small, clean, and extremely space-efficient, sometimes taking less than 20 m2. Most shops are for takeout only, with limited seating. They offer a wide variety of high quality coffee at a premium price point.\nThe company was founded in Curitiba, at the southern part of Brazil, in 2018, and has expanded rapidly to 12 countries with over 200 shops. Their franchise model in part explains this strong expansion.\n\nSimilar to Starbucks, product customization is a major selling point. Customers can choose and replace pretty much everything in their drinks, from adding and additional espresso shot, requiring an additional pump of chocolate syrup. Unlike Starbucks, however, most The Coffee shops are strictly to-go, or offer only minimal seating capacity. The Coffee (usually) doesn’t aim at becoming a 3rd place, where friends meet to share a cup of coffee, or work colleagues schedule a meeting. That said, there are exceptions and some shops do include tables and even\nThe Coffee also strays away from the traditional friendly-neighborhood barista and instead focuses on a more technological approach. Customers mainly interact with a tablet that displays the menu and all customization choices. Friendly chatter is optional, as a customer can get in, get his coffee without exchanging any words with the barista."
  },
  {
    "objectID": "posts/general-posts/2024-06-finding-the-coffee/index.html#setup",
    "href": "posts/general-posts/2024-06-finding-the-coffee/index.html#setup",
    "title": "Locating all The Coffee shops in Brazil",
    "section": "Setup",
    "text": "Setup\n\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(sf)\n\nimport::from(tidygeocoder, geocode)\nimport::from(purrr, map, map2)\nimport::from(tidyr, unnest)"
  },
  {
    "objectID": "posts/general-posts/2024-06-finding-the-coffee/index.html#finding-the-data",
    "href": "posts/general-posts/2024-06-finding-the-coffee/index.html#finding-the-data",
    "title": "Locating all The Coffee shops in Brazil",
    "section": "Finding the data",
    "text": "Finding the data\nThe data is extracted from The Coffee’s website. There is no single recipe or approach for webscraping: each website is organized differently, though there are patterns. In the case of The Coffee, units are separated by country and city; additionally each unit is identified by a name and has an address\n\nThe website\nThe site presents every unit by country and city. The print below shows an example unit in Belo Horizonte, Brazil.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R\nThe code below extracts the url related to every individual city in Brazil.\n\n# Base url \nbase_url &lt;- \"https://thecoffee.jp/shortcut/brasil\"\n# Parse HTML\npage_list_brazil &lt;- xml2::read_html(base_url)\n\n# Gets the urls for all cities in Brazil\n\npage_list_cities &lt;- page_list_brazil |&gt; \n  html_elements(xpath = \"//div/ul/li/a\") |&gt; \n  html_attr(\"href\")\n\npage_list_cities &lt;- page_list_cities[str_detect(page_list_cities, \"brasil/\")]\n\nurl_cities &lt;- str_c(base_url, str_remove(page_list_cities, \"shortcut/brasil/\"))\n\nThe code below is a function the scrapes the information of all shops for a given url of a city. The output is a simple data.frame.\n\nscrape_the_coffee &lt;- function(url) {\n  \n  # Parse the html\n  page &lt;- xml2::read_html(url)\n  # Find the the name of the shop\n  coffee_shop_name &lt;- page |&gt; \n    rvest::html_elements(xpath = \"//div/ul/li/div/div/a/h4\") |&gt; \n    rvest::html_text()\n  # Find the address of the shop\n  address_list &lt;- page |&gt; \n    rvest::html_elements(xpath = \"//div/ul/li/div/div/a/p\") |&gt; \n    rvest::html_text()\n  # Remove shops that are not open yet\n  address_list &lt;- address_list[!str_detect(address_list, \"coming soon\")]\n  street_name &lt;- address_list[seq(1, length(address_list), 2)]\n  city_name &lt;- address_list[2]\n  \n  full_address &lt;- paste(street_name, city_name)\n\n  # Store results in a tibble\n  out &lt;- tibble::tibble(\n    name = coffee_shop_name,\n    address = full_address,\n    street_name = street_name,\n    city_name = city_name\n  )\n  \n  return(out)\n\n}\n\n\n\nFunctional approach\nThe simplest approach to implement this function is applying it over a vector with all urls for all cities. This means that the scrape_the_coffee function will be executed on each individual value of the url_cities text vector.\nThis approach is usually quicker and can be scaled with parallel processing (e.g. parallel::mclapply). Speed, however, usually isn’t a top priority with web scraping and can even be a detriment as it might lead to an excess number of requests.\nIt’s important to note that web scraping is liable to errors due to internet connection issues. As such it’s good practice to implement it in a error-prone fashion. The easiest way to do so in the functional approach is to wrap the function with purrr::safely. This way, errors don’t stop the function and are stored in a specific error class, making it easier to debug it afterwards.\n\n# Scrape all cities\n\n# Apply the scrape_the_coffee function over url_cities\nsafe_scrape_the_coffee &lt;- safely(scrape_the_coffee)\ncoffee_locations &lt;- map(url_cities, safe_scrape_the_coffee)\n# Name the list for convinience\nnames(coffee_locations) &lt;- url_cities\n# Stack the results of the list into a single table\ndat &lt;- bind_rows(coffee_locations, .id = \"url\")\n\n\n\nLoop approach\nThe code below shows how to implement the same procedure using a typical for-loop syntax. This approach is usually more intuitive and easier to follow. It’s generally better to use this approach in more complex web scrapping endeavors.\nFor illustration purposes, I show how to add a progress bar to the loop and a timeout.\n\n# Define a progress bar\npb &lt;- txtProgressBar(min = 1, max = length(url_cities), style = 3)\nls &lt;- vector(\"list\", length(url_cities))\n\nfor (i in seq_along(url_cities)) {\n  \n  # Get the current city\n  url &lt;- url_cities[i]\n  current_city &lt;- basename(url)\n  message(\"Scraping data for: \", current_city)\n  # Safely apply the scrape_the_coffee function\n  ls[[i]] &lt;- try(scrape_the_coffee(url))\n  # Update progress bar\n  setTxtProgressBar(pb, i)\n  # Small timeout to avoid problems\n  Sys.sleep(runif(1, min = 1, max = 5))\n  \n}\n\n\n\nCleaning the data\n\nunnabreviate &lt;- function() {\n  c(\"Av\\\\.\" = \"Avenida\",\n    \"Al\\\\.\" = \"Alameda\",\n    \"R\\\\.\"  = \"Rua\",\n    \"Dr\\\\.\" = \"Doutor\",\n    \"Visc\\\\.\" = \"Visconde\",\n    \"Pres\\\\.\" = \"Presidente\",\n    \"Mal\\\\.\" = \"Marechal\")\n}\n\ndat &lt;- dat |&gt; \n  mutate(\n    city_name = str_remove(city_name, \" - Brasil\"),\n    address = str_replace_all(address, unnabreviate()),\n    country = \"Brasil\"\n    )\n\n\n\nGeocoding\nGeocoding is the process of finding a street address based on a latitude/longitude pair or vice-versa. Typically, we need to call an external provider for this. In this case, I use the Google Maps API via the tidygeocoder package to find the corresponding lat/lng pair for each address.\n\n# Geocode using Maps API\ncoffee &lt;- tidygeocoder::geocode(\n  dat,\n  address = address,\n  method = \"google\"\n  )\n\n# Convert to spatial data.frame\nshops &lt;- st_as_sf(\n  coffee,\n  coords = c(\"long\", \"lat\"),\n  crs = 4326,\n  remove = FALSE\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-06-finding-the-coffee/index.html#results",
    "href": "posts/general-posts/2024-06-finding-the-coffee/index.html#results",
    "title": "Locating all The Coffee shops in Brazil",
    "section": "Results",
    "text": "Results\nThe table below shows the final results of the web scraping and geocoding process.\n\n\n\n\n\n\nThe Coffee shops, major cities\n\n\nCity\n# Shops\nShare BR (%)\n\n\n\n\nSão Paulo\n53\n26.50%\n\n\nCuritiba\n29\n14.50%\n\n\nBrasília\n21\n10.50%\n\n\nRio de Janeiro\n13\n6.50%\n\n\nFortaleza\n10\n5.00%\n\n\nPorto Alegre\n9\n4.50%\n\n\nFlorianópolis\n6\n3.00%\n\n\nVitória\n5\n2.50%\n\n\nCampinas\n4\n2.00%\n\n\nBelo Horizonte\n3\n1.50%\n\n\n\n\n\n\n\n\nleaflet(shops) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(label = ~name) %&gt;%\n  addProviderTiles(\"CartoDB\") %&gt;%\n  setView(lng = -46.65590, lat = -23.561197, zoom = 12)"
  },
  {
    "objectID": "posts/general-posts/2024-06-finding-the-coffee/index.html#get-google-maps-ratings",
    "href": "posts/general-posts/2024-06-finding-the-coffee/index.html#get-google-maps-ratings",
    "title": "Finding all The Coffee shops in Brazil",
    "section": "Get Google Maps ratings",
    "text": "Get Google Maps ratings\nStarbucks post\n\ncur_shops &lt;- filter(shops, city_name == \"Curitiba\")\n\nget_ratings &lt;- function(lat, lng) {\n  \n  location &lt;- c(lat, lng)\n  places &lt;- google_places(\"The Coffee\", location = location, radius = 10)\n  res &lt;- places$results\n  \n  subres &lt;- res %&gt;%\n    unnest(cols = \"geometry\") %&gt;%\n    unnest(cols = \"location\") %&gt;%\n    select(\n      business_status, name, formatted_address, rating, user_ratings_total,\n      lat, lng\n    )\n  \n}\n\nratings &lt;- map2(cur_shops$lat, cur_shops$long, get_ratings)\n\ndat_ratings &lt;- ratings |&gt; \n  bind_rows() |&gt; \n  distinct() |&gt; \n  filter(str_detect(name, \"^The Coffee\"))\n\ndat_ratings &lt;- dat_ratings |&gt; \n  mutate(\n    street_name = str_extract(formatted_address, \"^[^,]+\"),\n    street_name = str_replace_all(street_name, unnabreviate()),\n    street_name = stringi::stri_trans_general(street_name, \"latin-ascii\"),\n    street_number = as.numeric(str_extract(formatted_address, \"(?&lt;=, )\\\\d+(?=\\\\b)\"))\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-06-finding-the-coffee/index.html#merging-with-census-information",
    "href": "posts/general-posts/2024-06-finding-the-coffee/index.html#merging-with-census-information",
    "title": "Finding all The Coffee shops in Brazil",
    "section": "Merging with Census information",
    "text": "Merging with Census information\nThe code below shows how to gather census tract data for each The Coffee shop. Census tracts are the smallest administrative division that present socioeconomic and demographic data.\n\n\nCode\n# Interpolation -------------------------------------------------------------\n\n## Functions ---------------------------------------------------------------\n\n# Creates a n minute walk isochrone around a point\nget_buffer &lt;- function(point, radius = 5, simplified = FALSE) {\n  \n  stopifnot(length(radius) == 1 && is.numeric(radius))\n  \n  if (simplified) {\n    point |&gt; \n      sf::st_transform(crs = 31982) |&gt; \n      # Simplified assumption\n      sf::st_buffer(dist = ((1.5 - 1.2) / 2 + 1.2) * 60 * radius)\n  } else {\n    point |&gt; \n      sf::st_transform(crs = 31982) |&gt; \n      osrm::osrmIsochrone(breaks = radius, osrm.profile = \"foot\") |&gt; \n      nngeo::st_remove_holes()\n  }\n  \n}\n\n# Interpolates an area with census tracts and aggregates population and households\ninterpolate_census &lt;- function(census, target, variables = c(\"v0001\", \"v0003\")) {\n\n  if (st_crs(census) != st_crs(target)) {\n    warning(\"CRS mismatch\")\n    \n    census &lt;- st_transform(census, crs = 31982)\n    target &lt;- st_transform(target, crs = 31982)\n    \n  }\n\n  # Select variables\n  census &lt;- dplyr::select(census, dplyr::all_of(variables))\n  # Interpolate areas\n  interp &lt;- st_interpolate_aw(census, target, extensive = TRUE)\n  \n  return(interp)\n  \n}\n\n# Wrapper around get_buffer and interpolate_census\nfind_population &lt;- function(shop, census, radius = 5, simplified = FALSE) {\n  \n  # Compute a 5-minute isochrone around\n  buffer &lt;- get_buffer(shop, radius, simplified)\n  interpolated &lt;- suppressWarnings(interpolate_census(census, buffer))\n  \n  return(interpolated)\n  \n}\n\n## Interpolate -------------------------------------------------------------\n\n# Uniquely identifies each shop\ncity_shops &lt;- city_shops |&gt; \n  mutate(shop_id = row_number())\n\n# To improve speed convert the full census data to 31982\ncity_census_utm &lt;- st_transform(city_census, crs = 31982)\n\ncity_shops_census &lt;- parallel::mclapply(\n  split(city_shops, city_shops$shop_id),\n  \\(x) find_population(x, census = city_census_utm)\n  )\n\ncity_shops_census &lt;- bind_rows(city_shops_census, .id = \"shop_id\")\n\ncity_shops &lt;- city_shops |&gt; \n  mutate(shop_id = as.character(shop_id)) |&gt; \n  left_join(st_drop_geometry(city_shops_census))\n\n\n\n\nReading layer `curitiba_tcf_interpolate' from data source \n  `/Users/viniciusoike/Documents/GitHub/restateinsight/static/data/curitiba_tcf_interpolate.gpkg' \n  using driver `GPKG'\nSimple feature collection with 29 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -49.32655 ymin: -25.45011 xmax: -49.21461 ymax: -25.40498\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-coffee-shops/index.html",
    "href": "posts/general-posts/2024-07-finding-coffee-shops/index.html",
    "title": "Finding coffee shops in Brazil",
    "section": "",
    "text": "Finding Coffee Shops in Brazil\nIn this series of posts, I will explore the coffee shop scene across Brazil. We’ll focus on how we can use data to understand where these shops are located, their characteristics, and why they matter in the larger picture. All of this will be done using R and a few handy packages to web scrape addresses of coffee shops throughout the country.\n\nPatterns in Coffee Shop Locations\nThe way coffee shops are spread out across Brazil is influenced by a variety of factors, like population density, tourism, and local economic activity. Cities like São Paulo and Rio de Janeiro are expected to have a high concentration of coffee shops, catering to both locals and tourists alike.\nInterestingly, coffee shops are popping up not just in major urban areas but also in suburban and rural spots, where they often serve as community hubs. This shift reflects changing consumer behaviors, as more people seek quality coffee experiences outside traditional settings.\nCoffee is a huge part of Brazilian culture and the economy. Not only is Brazil the world’s largest coffee producer, but it also boasts a lively coffee culture that ranges from classic cafes to trendy specialty coffee shops. You can find coffee everywhere, with prices ranging from R$1 for a simple brew at a street vendor to R$40 for a premium crafted experience in a high-end café. This diversity showcases the variety of coffee experiences available, catering to different tastes and budgets.\n\n\nThe Business of Coffee Shops\nThe coffee shop industry in Brazil has undergone significant changes over the years, influenced by various “waves” of coffee culture. Each wave has brought new trends and consumer expectations, creating a dynamic market.\n\nFirst Wave (Traditional Coffee): This wave was all about mass production and convenience. Coffee was consumed in large quantities with little focus on quality or origin. Traditional cafes and street vendors provided quick access to coffee for the masses.\nSecond Wave (Specialty Coffee): Emerging in the late 20th century, this wave brought a greater appreciation for coffee quality and origin. Specialty coffee shops began to emphasize artisanal brewing methods and direct trade relationships with farmers, making coffee experiences more sophisticated.\nThird Wave (Craft Coffee): This wave focuses on coffee as a specialty product, highlighting transparency in sourcing, sustainable practices, and premium quality. Coffee shops in this category feature single-origin beans and unique brewing methods, with baristas serving as guides to the coffee experience.\nFourth Wave (Experiential Coffee): The current wave emphasizes the overall experience of coffee consumption. Coffee shops are becoming community spaces that host events, workshops, and art exhibits. The focus is on creating a unique atmosphere where customers can relax, work, and engage with their surroundings.\n\nUnderstanding these waves is essential for anyone looking to enter the coffee shop market. By staying aware of current trends and customer preferences, shop owners can position themselves to meet the needs of their clientele.\n\n\nStrategic Insights for Businesses\nFor coffee shop owners and entrepreneurs, knowing where competitors are located is key to making informed decisions about new shop locations. A map showing coffee shop density can reveal saturated markets where competition is high, as well as underserved areas that might represent fresh opportunities.\nHere are a few important factors to consider:\n\nPopulation Density: Areas with higher populations often indicate greater demand for coffee shops.\nAverage Income Levels: Understanding the economic status of an area can help shape your pricing strategy.\nAge Demographics: Targeting different age groups may require tailored marketing and product offerings.\nProximity to Points of Interest (POI): Locations near universities, shopping centers, and public transport can significantly boost foot traffic.\n\n\n\nWhy Use R?\nR is a powerful tool for businesses looking to make data-driven decisions. Here are some reasons why R is a great fit for analyzing the coffee shop market in Brazil:\n\nData Visualization: R excels at creating engaging visualizations. With libraries like ggplot2, you can easily craft informative graphs and maps that highlight key trends and insights. This makes it easier to present data in a way that’s clear and actionable.\nIntegration with Leaflet: R integrates smoothly with the leaflet package, allowing you to create interactive maps that visualize coffee shop locations and other geographic data. These maps help identify areas of high demand, competition, and potential growth—making it easier to strategize.\nShiny Applications: Using Shiny, you can build interactive web applications that let users explore data in real-time. This is especially useful for coffee shop owners who want to analyze customer trends and geographic data on the fly.\nEase of Statistical Analysis: R simplifies complex statistical analyses, making it easier to assess demand patterns and demographic trends. With built-in functions for regression and forecasting, R helps you gain deeper insights into market dynamics and consumer behavior.\nRich Ecosystem of Packages: The vast collection of R packages allows you to tackle a wide range of analytical tasks. Whether you need to manipulate data or apply advanced statistical methods, there’s likely a package available to suit your needs.\n\nBy harnessing R, businesses in the coffee shop industry can turn data into actionable insights, optimize their strategies, and ultimately drive growth. Its capabilities in data visualization, interactive applications, and statistical analysis make R an invaluable asset in navigating the coffee shop landscape in Brazil.\nIn this tutorial, we’ll dive into spatial analysis using R, focusing on geocoding all coffee shops in Brazil. We will cover how to collect data, analyze it, and visualize it effectively, providing actionable insights for business owners and stakeholders.\n\n\nGeocoding\nSpatial analysis allows us to examine geographical patterns, relationships, and trends, providing valuable insights for businesses, urban planning, and research. Geocoding is the process of converting addresses into geographic coordinates, making it a pivotal step in spatial analysis.\nGeocoding translates textual location descriptions, such as addresses or place names, into geographic coordinates (latitude and longitude). At its core, geocoding converts a human-readable address (e.g., “Rua dos Bobos, 0, São Paulo, Brazil”) into a precise point on a map. This process involves matching the input address with geographic data, usually from a reference database like Google Maps or OpenStreetMap, and outputting the corresponding coordinates.\nThese coordinates are essential for placing locations on a map and performing further spatial analysis. Understanding the importance of geocoding helps frame the rest of the analysis and clarifies why it’s a necessary first step. Once we have geocoded our coffee shop addresses, we can visualize their distribution, analyze spatial relationships, and identify patterns that may inform business strategies.\n\n\n\nFAQ\n\nWhy Not Use Public Databases?\nThe national statistical bureau in Brazil, IBGE, has a universal classification of companies called CNAE (similar to NAICS). Unfortunately, this classification is too broad and gathers bars, bakeries, ice cream stores, and fast-food restaurants together with coffee shops. This lack of specificity can make it challenging to derive meaningful insights focused solely on coffee shops.\n\n\nWhy Not Use OpenStreetMaps?\nWhile there is a specific tag for coffee shops, the OSM database is not always up to date. Especially in smaller towns, the data can be unreliable. This can lead to missed opportunities in identifying potential locations for new coffee shops.\n\n\nWhy Not Use Google Maps?\nThe Google Maps API is paid and can be expensive. While I do rely on the Google Maps API to geocode the addresses, web scraping helps me fine-tune my data so I can ration my API requests. This approach allows for more efficient use of resources while still obtaining high-quality, accurate data for analysis."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Vinicius Oike Reginatto",
    "section": "Education",
    "text": "Education\n\n\nMsC in Economics\nUniversity of São Paulo | São Paulo, SP. MSc Economics1. Macroeconomics, growth theory, time series.\n\nAssistant teacher: (1) Mathematical Economics (dynamic optimization); (2) econometrics; (3) time series econometrics.\nDissertation work in dynamic optimization (calculus of variations and optimal control) and the history of economics.\n\n\n\nBA in Economics\nFederal University of Rio Grande do Sul2 | Porto Alegre, RS Bachelor (B.A.) Economics. Mathematical Economics."
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "Vinicius Oike Reginatto",
    "section": "Work Experience",
    "text": "Work Experience\n\n\nHeartman House Consultants | Associate Consultant\n2022-2023: Associate Consultant - Heartman House\nDeveloped a comprehensive framework on the Attention Economy and its implications for the real estate market, analyzing key trends and major players, including iBuyers and digital brokerages.\n\nContributed to the restructuring of a client in the education sector by:\n\nAuditing critical databases to uncover and address revenue collection inefficiencies.\nImproving data workflows, enhancing operational efficiency and accuracy.\n\nCollaborated with stakeholders to diagnose challenges and implement initial process improvements, despite limited access to information.\n\n\n\nQuintoAndar | Data Scientist & Spokesperson\n2022-2023: Economist and Data Scientist — QuintoAndar (proptech startup)\nQuintoAndar is Brazil’s leading proptech unicorn, redefining how people rent, buy, and manage properties. As one of the founding members of DataHouse, QuintoAndar’s data hub, I played a key role in establishing the company as a leader in the real estate sector.\n\nResponsible for real estate reports and indices that helped establish QuintoAndar as the leading proptech brand in Brazil. Leader in SoV among real estate players.\nRedesigned and maintained the QuintoAndar ImovelWeb Rent Price Index.\nBuilt a centralized database to streamline and support market analysis and reporting.\nProvided insights as a recognized real estate expert, guiding strategic market initiatives.\n\n\n\nUrbit | Data Scientist\n2020-2022: Data Scientist — Urbit (market intelligence startup)\nAt Urbit I helped develop proprietary software to deliver data-driven insights do developers and urban planners.\n\nDeveloped real estate indices, successfully used by clients to enhance mortgage pipelines, property developments, and urban planning projects.\nProduced automated, on-demand real estate reports tailored to specific locations.\nManaged large spatial databases using PostgreSQL, R, and Python.\n\n\n\nEconomic Consultant\nAdvised multilateral organizations, government agencies, and private clients on urban planning, infrastructure, and real estate development through economic modeling and data-driven strategies.\n\nConducted in-depth analysis of urban and socioeconomic data (e.g., income, mobility, housing conditions) to inform policy and development decisions.\nSupported major transit-oriented development projects, including the TIC train (São Paulo-Campinas) and metro/BRT initiatives in Rio de Janeiro and Belo Horizonte.\nDeveloped econometric models and delivered economic and demographic forecasts for long-term infrastructure planning and private equity investment strategies.\nCreated internal indices (e.g., House Price Index) to provide actionable insights for market analysis and decision-making.\nContributed to zoning reform in São Paulo by presenting findings on housing affordability and advocating for public housing policies.\n\n\n\nAssistant Teacher\nDuring my MsC I worked as private tutor in econometrics and also as an assistant teacher in undergraduate and graduate courses:\n\nCourses: (1) Mathematical Economics (dynamic optimization); (2) econometrics (core); (3) time series econometrics.\n\n\n\nInternships\n2014-12/2014: Internship at Secretary of Treasury of Rio Grande do Sul"
  },
  {
    "objectID": "cv.html#presentation",
    "href": "cv.html#presentation",
    "title": "Vinicius Oike Reginatto",
    "section": "",
    "text": "My name is Vinícius Reginatto. I hold a degree in Economics from UFRGS, recognized as Brazil’s leading university by INEP, and a Master’s degree in Economics from the University of São Paulo (USP), consistently ranked as the top university in Latin America.\nWith a strong quantitative foundation, I focus on areas such as spatial analysis, econometrics, and time-series forcasting. I am proficient in Python, R, Quarto, Shiny, SQL, and a range of widely used machine learning algorithms. Additionally, I have developed R packages and Shiny applications to address specific analytical challenges. My professional experience spans data science and economic consulting.\nOver the years, I have worked on economic consulting projects, focused on urban intervention initiatives—such as transit-oriented development (TOD)—and real estate market analysis. In these roles, I collaborated remotely with large, multidisciplinary, and multilingual teams to deliver impactful results."
  },
  {
    "objectID": "cv.html#mini-cv",
    "href": "cv.html#mini-cv",
    "title": "Vinicius Oike Reginatto",
    "section": "Mini CV",
    "text": "Mini CV\nEducation\nUniversity of São Paulo (USP) | São Paulo, Brazil\nMsC Economics | Jan 2017 - Aug 2019\nFederal University of RS | Porto Alegre, Brazil\nBA Economics | Jan 2012 - Dec 2016\nWork Experience\nHeartman House | Associate Consultant | May 2024-\nQuintoAndar | Data Scientist | 2022-2024\nUrbit | Data Scientist | 2019-2022\nFreelance Economic Consultant | 2019-"
  },
  {
    "objectID": "cv.html#short-cv",
    "href": "cv.html#short-cv",
    "title": "Vinicius Oike Reginatto",
    "section": "Short CV",
    "text": "Short CV\nEducation\nUniversity of São Paulo (USP) | São Paulo, Brazil\nMsC Economics | Jan 2017 - Aug 2019\nFederal University of RS | Porto Alegre, Brazil\nBA Economics | Jan 2012 - Dec 2016\nWork Experience\nHeartman House | Associate Consultant | May 2024-\nQuintoAndar | Data Scientist | 2022-2024\nUrbit | Data Scientist | 2019-2022\nFreelance Economic Consultant | 2019-\nLanguages\nPortuguese (fluent/native), English (fluent, C2), Spanish (Full professional proficiency)\nProgramming Languages\nR, Python, SQL, Quarto, RMarkdown."
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "",
    "text": "In this post I’ll show how to replicate the infamous GitHub contributions graphic. This visualization shows all the days of the year and highlights the most “productive” days in darker colors. It’s similar to a heat map and also known as a cluster map. You can check other examples of this plot in my tutorial post (in Portuguese). In a way, it’s also fairly similar to a “punchcard” plot."
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#libraries",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#libraries",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Libraries",
    "text": "Libraries\nI use basic tidyverse libraries to replicate the plot. The import::from is a neat way to get only some functions from a large package. This is not strictly necessary but is a good coding practice. Since I use many functions from the ggplot2 and lubridate packages, I import them fully.\nFor this tutorial I’m using ggplot2_3.5.1.\n\nlibrary(ggplot2)\nlibrary(lubridate)\n\nimport::from(dplyr, mutate, if_else, summarise)\nimport::from(tidyr, expand_grid)\nimport::from(forcats, fct_rev)\nimport::from(RQuantLib, isBusinessDay)\n\nAn important feature of the original plot is that each square has round borders. Unfortunately, drawing these type of squares isn’t very straightforward with ggplot2. To solve this, I draw from a solution posted on StackOverflow, which copies part of the code from the statebins package to make a geom_rtile function.\nWhile the code may seem daunting, using it is very simple.\n\n\nCode\n`%||%` &lt;- function(a, b) {\n  if(is.null(a)) b else a\n}\n\nGeomRtile &lt;- ggplot2::ggproto(\n  \"GeomRtile\", \n  statebins:::GeomRrect, # 1) only change compared to ggplot2:::GeomTile\n                     \n  extra_params = c(\"na.rm\"),\n  setup_data = function(data, params) {\n    data$width &lt;- data$width %||% params$width %||% resolution(data$x, FALSE)\n    data$height &lt;- data$height %||% params$height %||% resolution(data$y, FALSE)\n\n    transform(data,\n      xmin = x - width / 2,  xmax = x + width / 2,  width = NULL,\n      ymin = y - height / 2, ymax = y + height / 2, height = NULL\n    )\n  },\n  default_aes = ggplot2::aes(\n    fill = \"grey20\", colour = NA, size = 0.1, linetype = 1,\n    alpha = NA, width = NA, height = NA\n  ),\n  required_aes = c(\"x\", \"y\"),\n\n  # These aes columns are created by setup_data(). They need to be listed here so\n  # that GeomRect$handle_na() properly removes any bars that fall outside the defined\n  # limits, not just those for which x and y are outside the limits\n  non_missing_aes = c(\"xmin\", \"xmax\", \"ymin\", \"ymax\"),\n  draw_key = draw_key_polygon\n)\n\ngeom_rtile &lt;- function(mapping = NULL, data = NULL,\n                       stat = \"identity\", position = \"identity\",\n                       radius = grid::unit(6, \"pt\"), # 2) add radius argument\n                       ...,\n                       na.rm = FALSE,\n                       show.legend = NA,\n                       inherit.aes = TRUE) {\n  ggplot2::layer(\n    data = data,\n    mapping = mapping,\n    stat = stat,\n    geom = GeomRtile,\n    position = position,\n    show.legend = show.legend,\n    inherit.aes = inherit.aes,\n    params = rlang::list2(\n      radius = radius,\n      na.rm = na.rm,\n      ...\n    )\n  )\n}"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#data",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#data",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Data",
    "text": "Data\nFor the visualization, we need 365 observations indicating how many contributions were submitted each day of the year. To make the simulated data more realistic I’ll account for holidays and weekends using the RQuantLib package.\nOn an average workday I assume the number of contributions follows a Poisson distribution with \\(\\lambda = 6.5\\)1. This returns an average of around 4-8 contributions per day and guarantees that the number of contributions will be integer and non-negative. For weekdays or holidays I assumed a small chance (10%) that our worker will have to do some light work.\n\n# Tibble that contains all days of the year\ncontributions &lt;- expand_grid(\n  date = seq(as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), by = \"1 day\")\n)\n\n# Function to simulate number of contributions conditional on workday\nsample_contribution &lt;- Vectorize(function(x) {\n  if (x == 1) {\n    # Normal day of work\n    rpois(1, 6.5)\n  } else {\n    # Small chance of working overtime on weekends or holidays\n    sample(c(0, 1, 2), 1, prob = c(0.9, 0.05, 0.05))\n  }}\n)\n\ncontributions &lt;- contributions |&gt; \n  mutate(\n    # Gets the number of the week in the year\n    n_week = week(date),\n    # Gets weekday number - Starts the week at sunday\n    n_day = wday(date, week_start = 7),\n    # Weekday labels for the plot\n    weekday_label = wday(date, week_start = 7, label = TRUE, abbr = TRUE),\n    weekday_label = fct_rev(weekday_label),\n    # Month labels for the plot\n    month = month(date, label = TRUE, abbr = TRUE),\n    is_workday = as.numeric(RQuantLib::isBusinessDay(\"Brazil\", date)),\n    # is_weekday = if_else(n_day == 7 | n_day == 1, 0L, 1L),\n    n = sample_contribution(is_workday)\n  )\n\ncontributions &lt;- contributions |&gt; \n  mutate(\n    n = if_else(n == 0, NA, n),\n    n_week = if_else(n_day == 1, n_week + 1, n_week)\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#plotting",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#plotting",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Plotting",
    "text": "Plotting\n\nBase plot\nThe basic version of this plot is essentially a cluster map. On the horizontal axis, the weeks of the year are plotted; on the vertical axis, the days of each year are plotted. The intensity of the color highlights the amount of GitHub contributions.\n\nggplot(contributions, aes(n_week, n_day)) +\n  geom_rtile(\n    aes(fill = n),\n    color = \"white\",\n    radius = unit(2, \"pt\"),\n    width = 0.9,\n    height = 0.9\n    )\n\n\n\n\n\n\nFinal plot\nTo arrive at a more polished version of this visualization I adjusted the scales and tweaked some minor thematic elements. To my surprise this was easier than I had anticipated.\n\n# Find the positions of the month labels\ntab &lt;- contributions |&gt; \n  summarise(nmin = min(n_week), .by = \"month\")\n\nggplot(contributions, aes(n_week, weekday_label)) +\n  geom_rtile(\n    aes(fill = n),\n    color = \"white\",\n    radius = unit(2, \"pt\"),\n    width = 0.9,\n    height = 0.9\n    ) +\n  # Highlight the months on the horizontal axis\n  scale_x_continuous(\n    breaks = tab$nmin,\n    labels = as.character(tab$month),\n    position = \"top\",\n    expand = c(0, 0)\n  ) +\n  # Highlight days of the week on the vertical axis\n  scale_y_discrete(breaks = c(\"Mon\", \"Wed\", \"Fri\")) +\n  # Adjust color palette\n  scale_fill_distiller(\n    palette = \"Greens\",\n    direction = 1,\n    na.value = \"gray90\") +\n  # Removes x and y labels\n  labs(x = NULL, y = NULL) +\n  # Removes the color legend\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    text = element_text(color = \"gray10\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-github-contributions-plot/index.html#footnotes",
    "href": "posts/general-posts/2024-12-github-contributions-plot/index.html#footnotes",
    "title": "Replicating the Github Contributions plot in R using ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis number was a guess roughly based on my own history of contributions. The actual distribution might be more right-skewed than a Poisson, but for the purposes of this post it works fine.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "A “punchcard” plot shows the occurrence/frequency of a pair of discrete variables. Each discrete variable is plotted onto the X-Y axis and the intensity of the frequency is represented by the size or color of point. The use of colors can also help visualize a third discrete variable.\nThere are several possible applications of a “punchcard” plot. They might include:\n\nthe number of visitors received by a chain of stores in each state (x) and each day of the week (y);\nthe aggregated rating of products (y) based on their category (x)\nthe number of students attending each class (x) based on their major (y); and many other examples.\n\nI haven’t found much formal documentation on “punchcard plots” and have myself only found out about this term while looking for a name for the plot below. This visualization shows the IMDB ratings for the top 250 films by decade. The size of each bubble shows how many films appear in each rating bin in each decade.\n\n\n\n\n\nThere are some other posts discussing this type of plot (here, and here) but overall this is definitely not a well-established term.\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nThe simplest way to make a punchcard plot is to use the geom_count function. This will count the ocurrences of the x-y pair variables selected. The example below uses the diamonds data.\n\nggplot(diamonds, aes(cut, color)) +\n  geom_count()\n\n\n\n\n\n\n\n\n\n\n\n\nThis function is a simple wrapper around geom_point(aes(size = n)). The code below makes the same plot, but first we count the number of occurrences of each of the cut and color variables.\nIt’s important to note that there’s nothing special about n. It’s simply the default column name that results from using the count function.\n\ntab &lt;- diamonds |&gt; \n  count(cut, color)\n\nggplot(tab, aes(cut, color)) +\n  geom_point(aes(size = n))\n\n\n\n\n\n\n\n\nIf our data is already aggregated we might desire to show the sum of some pair of variables. In the txhousing data we have the total number of house sales in each month and year by city. To aggregate the total number of sales, across all cities, we can use weight = sales.\nNote that both year and month are continuous variables but we can treat them as if they were discrete. To actually force R to treat them as categorical variables they must be converted to factor. Also note the use na.omit since there are missing values in the sales column.\n\ntxhousing &lt;- na.omit(txhousing)\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales))\n\n\n\n\n\n\n\n\nAgain, the same result can be achieved using count and geom_point but more code is necessary.\n\ntxhousing |&gt; \n  na.omit() |&gt; \n  count(year, month, wt = sales) |&gt; \n  ggplot(aes(month, year)) +\n  geom_point(aes(size = n))\n\n\n\nThere isn’t much customization available for punchcard plots. To change the size of each bubble we use scale_size_*.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nSince geom_count is essentially the same as geom_point we can alter its shape.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales), shape = 22) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nFinally, we can use geom_color_* and geom_fill_ to map variables as a color on the plot. In the plot below I highlight the top-selling month in each year. It becomes clear the June and July are the most active months for the housing market.\n\nagghousing &lt;- txhousing |&gt; \n  count(month, year, wt = sales) |&gt; \n  mutate(\n    year = as.factor(year),\n    month = as.factor(month),\n    highlight = factor(if_else(n == max(n), 1L, 0L)),\n    .by = \"year\"\n    )\n\nggplot(agghousing, aes(month, year)) +\n  geom_point(aes(size = n, color = highlight)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\n\n\n\n\nFor a more interesting example we can calculate the share of countries facing an economic recession by region. We use the maddison database from the homonyms package.\n\n\nCode\nimport::from(maddison, maddison)\n\ndat &lt;- maddison |&gt; \n  filter(year &gt;= 1999) |&gt; \n  mutate(\n    growth = rgdpnapc / lag(rgdpnapc) - 1,\n    is_growth = if_else(growth &gt;= 0, 1L, 0L),\n    .by = \"iso3c\"\n  )\n\nrecession_region &lt;- dat |&gt; \n  filter(year &gt;= 2000) |&gt; \n  summarise(\n    growth = sum(is_growth, na.rm = TRUE),\n    total = n(),\n    .by = c(\"region\", \"year\")\n  ) |&gt; \n  mutate(\n    share = (1 - growth / total) * 100,\n    highlight = factor(if_else(share &gt; 20, 1, 0)),\n    region = as.factor(region),\n    region = forcats::fct_rev(region)\n    )\n\n\nThe plot shows the share of countries in each region that are facing an economic recession in a given year in the 2000-2016 period. For simplicity, I define an economic recession simply as country facing negative GDP per capita growth (year on year). I highlight the years when over 20% of the countries were facing a recession.\nNote that this visualization is mostly illustrative and a better categorization of each region would likely be required. Even so, we can see how some regions such as Western Africa faced several recessions. We can also see the impact of the Great Financial Recession in 2008-09.\n\nggplot(recession_region, aes(year, region)) + \n  geom_count(aes(size = share, color = highlight)) +\n  scale_size_continuous(\n    name = \"Share of countries\\nin recession (%)\",\n    breaks = c(0, 20, 40, 60, 80, 100)\n  ) +\n  scale_y_discrete(labels = \\(x) stringr::str_wrap(x, width = 17)) +\n  scale_color_manual(values = RColorBrewer::brewer.pal(3, \"RdBu\")[c(3, 1)]) +\n  guides(color = \"none\", size = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  labs(\n    title = \"Economic Recessions across regions\",\n    subtitle = \"Red circles show that over 20% of countries in the region are in recession.\",\n    x = NULL,\n    y = NULL,\n    caption = \"Source: Maddison Project (2018)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n\n\n\nFully replicating the Nexo plot shown in the beginning of this post requires a substantial amount of effort. For the purposes of this tutorial, I’ll show how to replicate the most prevalent aspects of the visualization, ignoring the annotations, arrows, and fonts.\nFor a complete replication of the Nexo plot see my other post.\n\n\nCode\nnexo_labels &lt;- c(\"até\\n1950\", \"1960\", \"70\", \"80\", \"90\", \"00\", \"10\", \"20\", \"até\\nhoje\")\n\ncolors &lt;- c(\"#328bff\", \"#88bce4\")\n\nggplot(imdb, aes(trunc_decade, trunc_rating)) +\n  geom_count(aes(color = is_top20)) +\n  geom_hline(yintercept = 7.9) +\n  scale_x_continuous(breaks = seq(1940, 2020, 10), labels = nexo_labels) +\n  scale_y_continuous(\n    limits = c(7.9, 9.45),\n    breaks = seq(7.9, 9.3, 0.1),\n    labels = scales::label_number(decimal.mark = \",\", accuracy = 0.1),\n    expand = c(0, 0)\n  ) +\n  scale_color_manual(values = rev(colors)) +\n  scale_size_area(name = \"\", breaks = c(4, 8, 12, 16)) +\n  guides(\n    color = \"none\",\n    size = guide_legend(\n      label.position = \"bottom\",\n      override.aes = list(color = \"gray80\"))\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \"\\n\\n\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.major.y = element_line(linetype = 3, color = \"#d9d9d9\", linewidth = 0.35),\n    panel.grid.major.x = element_line(color = \"#838484\", linewidth = 0.35),\n    panel.grid.minor = element_blank(),\n    legend.position = c(0.05, 1.1),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 10),\n    \n    axis.title.y = element_text(color = \"#767676\"),\n    axis.ticks.x = element_line(color = \"#000000\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nPunchcard plots are an excellent way to visualize data distributed across two categorical dimensions, such as days of the week and hours of the day. Whether you’re analyzing time-based activity patterns, attendance trends, or any other data with two categorical dimensions, punchcard plots are a powerful and visually engaging tool.\nNow that you’ve mastered the basics, consider applying this technique to your own datasets—whether it’s analyzing call center activity, website traffic, or any other time-based data. Experiment with colors, sizes, and annotations to make your punchcard plots even more impactful. Happy plotting!"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#ggplot2",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#ggplot2",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\n\nThe simplest way to make a punchcard plot is to use the geom_count function. This will count the ocurrences of the x-y pair variables selected. The example below uses the diamonds data.\n\nggplot(diamonds, aes(cut, color)) +\n  geom_count()"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#section",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#section",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "This function is a simple wrapper around geom_point(aes(size = n)). The code below makes the same plot, but first we count the number of occurrences of each of the cut and color variables.\nIt’s important to note that there’s nothing special about n. It’s simply the default column name that results from using the count function.\n\ntab &lt;- diamonds |&gt; \n  count(cut, color)\n\nggplot(tab, aes(cut, color)) +\n  geom_point(aes(size = n))\n\n\n\n\n\n\n\n\nIf our data is already aggregated we might desire to show the sum of some pair of variables. In the txhousing data we have the total number of house sales in each month and year by city. To aggregate the total number of sales, across all cities, we can use weight = sales.\nNote that both year and month are continuous variables but we can treat them as if they were discrete. To actually force R to treat them as categorical variables they must be converted to factor. Also note the use na.omit since there are missing values in the sales column.\n\ntxhousing &lt;- na.omit(txhousing)\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales))\n\n\n\n\n\n\n\n\nAgain, the same result can be achieved using count and geom_point but more code is necessary.\n\ntxhousing |&gt; \n  na.omit() |&gt; \n  count(year, month, wt = sales) |&gt; \n  ggplot(aes(month, year)) +\n  geom_point(aes(size = n))\n\n\n\nThere isn’t much customization available for punchcard plots. To change the size of each bubble we use scale_size_*.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nSince geom_count is essentially the same as geom_point we can alter its shape.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales), shape = 22) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nFinally, we can use geom_color_* and geom_fill_ to map variables as a color on the plot. In the plot below I highlight the top-selling month in each year. It becomes clear the June and July are the most active months for the housing market.\n\nagghousing &lt;- txhousing |&gt; \n  count(month, year, wt = sales) |&gt; \n  mutate(\n    year = as.factor(year),\n    month = as.factor(month),\n    highlight = factor(if_else(n == max(n), 1L, 0L)),\n    .by = \"year\"\n    )\n\nggplot(agghousing, aes(month, year)) +\n  geom_point(aes(size = n, color = highlight)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#modifying-the-plot",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#modifying-the-plot",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "There isn’t much customization available for punchcard plots. To change the size of each bubble we use scale_size_*.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nSince geom_count is essentially the same as geom_point we can alter its shape.\n\nggplot(txhousing, aes(month, year)) +\n  geom_count(aes(weight = sales), shape = 22) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))\n\n\n\n\n\n\n\n\nFinally, we can use geom_color_* and geom_fill_ to map variables as a color on the plot. In the plot below I highlight the top-selling month in each year. It becomes clear the June and July are the most active months for the housing market.\n\nagghousing &lt;- txhousing |&gt; \n  count(month, year, wt = sales) |&gt; \n  mutate(\n    year = as.factor(year),\n    month = as.factor(month),\n    highlight = factor(if_else(n == max(n), 1L, 0L)),\n    .by = \"year\"\n    )\n\nggplot(agghousing, aes(month, year)) +\n  geom_point(aes(size = n, color = highlight)) +\n  scale_size_continuous(breaks = c(15000, 20000, 25000, 30000))"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#countries-in-recession",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#countries-in-recession",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "For a more interesting example we can calculate the share of countries facing an economic recession by region. We use the maddison database from the homonyms package.\n\n\nCode\nimport::from(maddison, maddison)\n\ndat &lt;- maddison |&gt; \n  filter(year &gt;= 1999) |&gt; \n  mutate(\n    growth = rgdpnapc / lag(rgdpnapc) - 1,\n    is_growth = if_else(growth &gt;= 0, 1L, 0L),\n    .by = \"iso3c\"\n  )\n\nrecession_region &lt;- dat |&gt; \n  filter(year &gt;= 2000) |&gt; \n  summarise(\n    growth = sum(is_growth, na.rm = TRUE),\n    total = n(),\n    .by = c(\"region\", \"year\")\n  ) |&gt; \n  mutate(\n    share = (1 - growth / total) * 100,\n    highlight = factor(if_else(share &gt; 20, 1, 0)),\n    region = as.factor(region),\n    region = forcats::fct_rev(region)\n    )\n\n\nThe plot shows the share of countries in each region that are facing an economic recession in a given year in the 2000-2016 period. For simplicity, I define an economic recession simply as country facing negative GDP per capita growth (year on year). I highlight the years when over 20% of the countries were facing a recession.\nNote that this visualization is mostly illustrative and a better categorization of each region would likely be required. Even so, we can see how some regions such as Western Africa faced several recessions. We can also see the impact of the Great Financial Recession in 2008-09.\n\nggplot(recession_region, aes(year, region)) + \n  geom_count(aes(size = share, color = highlight)) +\n  scale_size_continuous(\n    name = \"Share of countries\\nin recession (%)\",\n    breaks = c(0, 20, 40, 60, 80, 100)\n  ) +\n  scale_y_discrete(labels = \\(x) stringr::str_wrap(x, width = 17)) +\n  scale_color_manual(values = RColorBrewer::brewer.pal(3, \"RdBu\")[c(3, 1)]) +\n  guides(color = \"none\", size = guide_legend(label.position = \"bottom\", nrow = 1)) +\n  labs(\n    title = \"Economic Recessions across regions\",\n    subtitle = \"Red circles show that over 20% of countries in the region are in recession.\",\n    x = NULL,\n    y = NULL,\n    caption = \"Source: Maddison Project (2018)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", legend.direction = \"horizontal\")"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#replicating-the-nexo-plot",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#replicating-the-nexo-plot",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "Fully replicating the Nexo plot shown in the beginning of this post requires a substantial amount of effort. For the purposes of this tutorial, I’ll show how to replicate the most prevalent aspects of the visualization, ignoring the annotations, arrows, and fonts.\nFor a complete replication of the Nexo plot see my other post.\n\n\nCode\nnexo_labels &lt;- c(\"até\\n1950\", \"1960\", \"70\", \"80\", \"90\", \"00\", \"10\", \"20\", \"até\\nhoje\")\n\ncolors &lt;- c(\"#328bff\", \"#88bce4\")\n\nggplot(imdb, aes(trunc_decade, trunc_rating)) +\n  geom_count(aes(color = is_top20)) +\n  geom_hline(yintercept = 7.9) +\n  scale_x_continuous(breaks = seq(1940, 2020, 10), labels = nexo_labels) +\n  scale_y_continuous(\n    limits = c(7.9, 9.45),\n    breaks = seq(7.9, 9.3, 0.1),\n    labels = scales::label_number(decimal.mark = \",\", accuracy = 0.1),\n    expand = c(0, 0)\n  ) +\n  scale_color_manual(values = rev(colors)) +\n  scale_size_area(name = \"\", breaks = c(4, 8, 12, 16)) +\n  guides(\n    color = \"none\",\n    size = guide_legend(\n      label.position = \"bottom\",\n      override.aes = list(color = \"gray80\"))\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \"\\n\\n\") +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.major.y = element_line(linetype = 3, color = \"#d9d9d9\", linewidth = 0.35),\n    panel.grid.major.x = element_line(color = \"#838484\", linewidth = 0.35),\n    panel.grid.minor = element_blank(),\n    legend.position = c(0.05, 1.1),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 10),\n    \n    axis.title.y = element_text(color = \"#767676\"),\n    axis.ticks.x = element_line(color = \"#000000\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-punchcard-plot/index.html#conclusion",
    "href": "posts/general-posts/2024-12-punchcard-plot/index.html#conclusion",
    "title": "Punchcard plots in R",
    "section": "",
    "text": "Punchcard plots are an excellent way to visualize data distributed across two categorical dimensions, such as days of the week and hours of the day. Whether you’re analyzing time-based activity patterns, attendance trends, or any other data with two categorical dimensions, punchcard plots are a powerful and visually engaging tool.\nNow that you’ve mastered the basics, consider applying this technique to your own datasets—whether it’s analyzing call center activity, website traffic, or any other time-based data. Experiment with colors, sizes, and annotations to make your punchcard plots even more impactful. Happy plotting!"
  },
  {
    "objectID": "posts/general-posts/2024-12-replicating-nexo-plot/index.html",
    "href": "posts/general-posts/2024-12-replicating-nexo-plot/index.html",
    "title": "Film ratings over the decades: replicating a Nexo plot",
    "section": "",
    "text": "In this tutorial, we delve into the world of cinema ratings over the decades, examining how films from various eras are rated on IMDb. Using ggplot2 in R, this guide will demonstrate how to visualize these ratings to understand trends and highlight significant films.\nWe’ll replicate a plot originally published in Nexo, a Brazilian media outlet that produces fantastic data visualizations. This replication will serve as a practical application of ggplot2 techniques, making it a valuable exercise for data analysts looking to sharpen their skills or movie enthusiasts curious about film ratings across time.\nI replicate plots as a practice and have a compilation of ggplot2 code to replicate visualizations from OurWorldInData, The Financial Times, and The Economist. Nexo is great source for inspiration not only because of the quality of their work, but also because most of their plots are made using ggplot2."
  },
  {
    "objectID": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#data",
    "href": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#data",
    "title": "Film ratings over the decades: replicating a Nexo plot",
    "section": "Data",
    "text": "Data\nTo replicate this plot we need the full top 250 IMDB list. There are several ways to acquire this info but I believe the easiest is 250.took.nl. Scrapping this page is straightforward since all the data is neatly arranged as a table element in HTML.\n\nurl &lt;- \"https://250.took.nl/compare/full\"\npage &lt;- xml2::read_html(url)\npage_tables &lt;- rvest::html_table(page)\ntab &lt;- page_tables[[9]]\n\nA curious feature of the top 250 IMDB list is that its not ranked by rating. IMDB clarifies this in their FAQ:\n\n\nThe list is ranked by a formula which includes the number of ratings each movie received from users, and value of ratings received from regular users\n\n\nIMDB, unfortunately, doesn’t disclose how they define “regular users”. As a result,\n\nimdb &lt;- imdb |&gt;  \n  mutate(\n    decade = floor(year / 10) * 10,\n    trunc_rating = round(wr, 1),\n    trunc_decade = case_when(\n      decade &lt; 1950 ~ 1940,\n      decade &gt; 2020 ~ 2020,\n      TRUE ~ decade\n    ),\n    #trunc_decade = factor(trunc_decade),\n    #trunc_decade = forcats::fct_reorder(trunc_decade, decade),\n    is_top20 = factor(if_else(rank &lt;= 20, 1L, 0L))\n  )"
  },
  {
    "objectID": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#basic-plot",
    "href": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#basic-plot",
    "title": "Film ratings over the decades: replicating a Nexo plot",
    "section": "Basic plot",
    "text": "Basic plot\nThe Nexo visualization is a sort of “punchcard” plot, that shows the frequency of a pair of categorical variables. I discussed these kind of plots in another post. In this case, our variables are decades and binned film ratings.\n\nggplot(imdb, aes(trunc_decade, trunc_rating)) +\n  geom_count(aes(color = is_top20)) +\n  geom_hline(yintercept = 7.9)\n\n\n\n\n\n\n\n\nTo improve our base plot we’ll:\n\nFormat the text on both axis\nAdd colors to highlight the top 20 films\nAdjust the size legend\n\n\nnexo_labels &lt;- c(\"até\\n1950\", \"1960\", \"70\", \"80\", \"90\", \"00\", \"10\", \"20\", \"até\\nhoje\")\n\ncolors &lt;- c(\"#328bff\", \"#88bce4\")\n\nplot_base &lt;- ggplot(imdb, aes(trunc_decade, trunc_rating)) +\n  geom_count(aes(color = is_top20)) +\n  geom_hline(yintercept = 7.9) +\n  scale_x_continuous(breaks = seq(1940, 2020, 10), labels = nexo_labels) +\n  scale_y_continuous(\n    limits = c(7.9, 9.55),\n    breaks = seq(7.9, 9.3, 0.1),\n    labels = scales::label_number(decimal.mark = \",\", accuracy = 0.1),\n    expand = c(0, 0)\n  ) +\n  scale_color_manual(values = rev(colors)) +\n  scale_size_area(name = \"\", breaks = c(4, 8, 12, 16)) +\n  guides(\n    color = \"none\",\n    size = guide_legend(\n      label.position = \"bottom\",\n      override.aes = list(color = \"gray80\"))\n  )\n\nplot_base"
  },
  {
    "objectID": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#adding-labels-and-arrows",
    "href": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#adding-labels-and-arrows",
    "title": "Film ratings over the decades: replicating a Nexo plot",
    "section": "Adding labels and arrows",
    "text": "Adding labels and arrows\nAdding annotations and arrows can be very frustrating in ggplot2. If multiple annotations/arrows are needed I’d recommend using another software to complement R. I’ll split this section into smaller parts, first showing how to handle the text labels and then showing how to draw curved arrows.\nText annotations require at minimum three arguments: x, y, and label. Getting the right values for the text position usually requires multiple tests. Additionally, we usually desire to change both the font and its size. In this case, to get a proper replication we should use the Gotham Rounded font used by Nexo.\nArrows function as line segments in ggplot2 and are defined by a starting and end position on each axis: this means we must specify 4 arguments to draw an arrow. This is made using the geom_arrow function. To make curved arrows, such as those in the original plot, however, we use instead the geom_curve function.\n\nUsing the Nexo font\nTo achieve a more accurate plot we need to use the Gotham Rounded font used by Nexo. If you do don’t have access to this font, I recommend using the Montserrat font available at Google Fonts.\nThe code below defines a simple function called load_fonts_nexo to load the fonts into the R session.\nThis step is entirely optional but does make a big difference for the end result.\n\n\nCode\n# Verifica a fonte do texto\n\ncheck_fonts_nexo &lt;- function() {\n  \n  dbfonts &lt;- sysfonts::font_files()\n  \n  nexo_fonts &lt;- paste(\"Gotham Rounded\", c(\"Bold\", \"Medium\", \"Light\"))\n  nexo_fonts_path &lt;- paste0(nexo_fonts, \".otf\")\n  \n  cond &lt;- stringr::str_glue(\n    \"({nexo_fonts[1]})|({nexo_fonts[2]})|({nexo_fonts[3]})\"\n  )\n  \n  check_fonts &lt;- sum(stringr::str_detect(dbfonts$family, cond))\n  check_fonts &lt;- ifelse(check_fonts == 3, TRUE, FALSE)\n  \n  if (!check_fonts) {\n    \n    fonts_found &lt;- dbfonts$family[stringr::str_detect(dbfonts$family, cond)]\n    \n    message(glue::glue(\n      \"Missing fonts. Only found: {paste(fonts_found, collapse = ', ')}\"\n    ))\n  }\n  \n  return(check_fonts)\n  \n}\n\nload_fonts_nexo &lt;- function(dpi = 96, ...) {\n\n  check_fonts &lt;- check_fonts_nexo()\n  \n  if (check_fonts) {\n    # Adiciona as fonts Gotham Rounded Bold e Light\n    sysfonts::font_add(\"Gotham Rounded Bold\", \"Gotham Rounded Bold.otf\")\n    sysfonts::font_add(\"Gotham Rounded Medium\", \"Gotham Rounded Medium.otf\")\n    sysfonts::font_add(\"Gotham Rounded Light\", \"Gotham Rounded Light.otf\")\n  } else {\n    # Adiciona Montserrat caso as fontes Gotham nao estejam disponiveis\n    sysfonts::font_add_google(\"Montserrat\", \"Montserrat\")\n  }\n  \n  sysfonts::font_add_google(\"Crimson Pro\", \"Crimson Text\")\n  \n  if (dpi &gt; 0) {\n    showtext::showtext_opts(dpi = dpi, ...)\n  }\n  \n  showtext::showtext_auto()\n  \n  if (check_fonts) {\n    message(\"Gotham Rounded fonts successfully loaded.\")\n  } else {\n    message(\"Gotham Rounded font not found. Montserrat was loaded instead.\")\n  }\n  \n}\n\nload_fonts_nexo()\n\nfont &lt;- ifelse(check_fonts_nexo(), \"Gotham Rounded Bold\", \"Montserrat\")\nfont_axis &lt;- ifelse(check_fonts_nexo(), \"Gotham Rounded Light\", \"Montserrat\")\nfont_title &lt;- \"Crimson Text\"\n\n\n\n\nAdding the annotations\nText annotations require a position and a text label. Additionally, we insert a font and other customizations.\nNote that I insert some blank lines in the subtitle. This extra space will later be needed to make room for the color legend.\n\nplot_annotations &lt;- plot_base +\n  labs(\n    title = \"Notas dos 250 filmes melhor\\navaliados por década de estreia\",\n    subtitle = \"SEGUNDO AVALIAÇÕES DO IMDB ATÉ DEZ. DE 2024\\n\\n\\n\\n\",\n    x = NULL,\n    y = NULL\n  ) +\n  annotate(\n    \"label\",\n    x = 1970,\n    y = 9.31,\n    label = \"entre os melhores\\n20 filmes\",\n    # Font of the text\n    family = font,\n    # Size of the text\n    size = 3,\n    # Color of the text\n    color = colors[1],\n    # Centralized text\n    hjust = 0.5,\n    # Remove borders\n    label.size = 0\n  ) +\n  annotate(\n    \"label\",\n    x = 2015,\n    y = 9.15,\n    label = \"entre os\\nmelhores\\n250 filmes\",\n    family = font,\n    size = 3,\n    color = colors[2],\n    # Centralized text\n    hjust = 0.5,\n    # Remove borders\n    label.size = 0\n  )\n\nplot_annotations\n\n\n\n\n\n\n\n\n\n\nAdding the arrows\nThe code below adds the arrows. Again, getting the right values will usually require multiple attempts.\nStoring the arguments inside a data.frame is not necessary but I feel it makes for more organized code.\n\nplot_arrows &lt;- plot_annotations +\n  geom_curve(\n    data = data.frame(x = 1990, xend = 1978, y = 9.31, yend = 9.4),\n    aes(x = x, xend = xend, y = y, yend = yend),\n    arrow = arrow(length = unit(0.03, \"npc\"))\n  ) +\n  geom_curve(\n    data = data.frame(x = 2020, xend = 2015, y = 8.6, yend = 9),\n    aes(x = x, xend = xend, y = y, yend = yend),\n    curvature = -0.45,\n    arrow = arrow(length = unit(0.03, \"npc\"))\n  )\n\nplot_arrows"
  },
  {
    "objectID": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#final-plot",
    "href": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#final-plot",
    "title": "Film ratings over the decades: replicating a Nexo plot",
    "section": "Final plot",
    "text": "Final plot\nLike spices in food, thematic elements are what set a plot apart. The final ingredient for a neat replication is modifying the theme elements.\nThis may seem the hardest step but my personal experience points to the opposite. Thematic elements follow a well defined and consistent structure, making it easy to change them.\n\nplot_full &lt;- plot_arrows +\n  theme_minimal(base_family = font, base_size = 14) +\n  theme(\n    panel.grid.major.y = element_line(linetype = 3, color = \"#d9d9d9\", linewidth = 0.35),\n    panel.grid.major.x = element_line(color = \"#838484\", linewidth = 0.35),\n    panel.grid.minor = element_blank(),\n    legend.position = c(0.09, 1.1),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 10),\n    plot.subtitle = element_text(size = 10),\n    plot.title = element_text(family = font_title, size = 22, hjust = 0),\n    \n    axis.title.y = element_text(color = \"#767676\"),\n    axis.ticks.x = element_line(color = \"#000000\")\n  )\n\nplot_full"
  },
  {
    "objectID": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#conclusion",
    "href": "posts/general-posts/2024-12-replicating-nexo-plot/index.html#conclusion",
    "title": "Film ratings over the decades: replicating a Nexo plot",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve reached the end of our exploration into IMDb ratings of the top 250 films by decade. This tutorial provided a step-by-step guide on using ggplot2 in R to visualize data effectively, showing the power of good visualizations in making data easier to understand and insights more apparent. With these techniques, you can now apply similar methods to analyze and present your own data sets. As you continue to work with ggplot2, you’ll find it an invaluable tool for revealing patterns and stories within the data."
  },
  {
    "objectID": "posts/general-posts/2024-04-setores-censitarios/index.html#related-posts",
    "href": "posts/general-posts/2024-04-setores-censitarios/index.html#related-posts",
    "title": "Census Tracts in Brazil",
    "section": "Related posts",
    "text": "Related posts\n\nStatistical and administrative divisions of Brazil"
  },
  {
    "objectID": "posts/general-posts/2024-07-finding-starbucks/index.html#related-posts",
    "href": "posts/general-posts/2024-07-finding-starbucks/index.html#related-posts",
    "title": "Finding All Starbucks in Brazil",
    "section": "Related posts",
    "text": "Related posts\n\nFinding coffee shop in Brazil\nFinding all The Coffee shops in Brazil"
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "While some find it odd to rank movie tastes, I’ve always enjoyed “greatest of all times” lists. When not taken too literally, these lists provide a resourceful almanac of good movies, including classics, and culturally relevant movies. By construction, these lists will always be lacking and always be biased: it’s a subjective evaluation of the best of the best films after all.\nTraditionally, these lists were penned by movie critics or experts; more recently, however, there are several “greatest of all times” lists voted by the public. Perhaps the most well known of these is the Top 250 IMDB. As of December 2024, the top 250 films alone aggregate almost 180 million votes.\nWhen comparing these lists, the ones made by critics and those voted by the broader public, one notices a kind “temporal bias” in the rankings. New movies, released in the past 10-15 years, rarely appear in prestigious lists such as the BFI’s Greatest Films. In fact, in it’s most recent edition, published in 2022, only 24 of the 264 films were released in the 21st Century (less than 10%). On the flip side, online rankings such as the Top 250 IMDB are flooded with new releases. Almost 36% of the films featured in the Top 250 IMDB were released from 2001 to 2021.\nIn this post I delve into the data to try to visualize these patterns. Are movie critics biased towards a certain period of cinema? Are new movies overrated? Are they underrated?\n\n\nThere are several “greatest movies of all time” lists. Comparing them directly is often impossible due to conflicting methodologies. For simplicity, I’ll delve mostly into the Top 250 IMDB and the BFI Greatest Films of All Time. IMDB’s ranking compiles the largest number of online votes and has a relatively sturdy methodology for both its ratings and its rankings. The BFI list compiles votes from academics, critics, curators, archivists, and programmers. Its most recent edition was published in 2022.\nThe lists are relatively comparable in size: IMDB’s has (obviously) 250 films while BFI’s has 264. While BFI’s list has ties, I ignore these to make comparisons simpler.\nIn a latter part of the post I extend the analysis to other lists published by media outlets such as Variety and TimeOut and also to other online sites such as Letterboxd.\n\n\nGetting all the data involves a lot of webscraping. More information on the data collection process can be found on my Github.\n\n\n\n\n\n\nLooking at the data by decade seems to reveal a clear pattern. BFI’s list peaks around the 1960’s, while IMDB’s list peaks around the 2000’s. There’s a small bump in the 1950’s but nothing noteworthy.\n\n\n\n\n\n\n\n\n\nWhile one could argue that IMDB’s ranking suggests some level of recency bias among the public, the numbers might also be a reflection of (1) the age demographic of online voters; (2) the (lack of) availability of older films, specially in mainstream streaming services.\n\n\n\nGoing into greater details reveals some curious features about how these lists compare. Some films appear to be appreciated by both the public and the critics. Films like Kurosawa’s Seven Samurai, Hitchcock’s Psycho, and Coppola’s The Godfather all rank highly in both lists.\nThere are, of course, divergences: some films, like Metropolis (1927) and The Apartment (1960) are slightly favored by the BFI’s critics; meanwhile, Ran (1985) and Modern Times (1936) are favored by the public. These divergences don’t seem to correlate directly with the year of release of each movie or even with its director.\nIts important to note that rankings work in a counter-intuitive fashion to most graphics, since the best (highest-ranking) films are associated with lower numbers and appear on the left. To make reading the graphics easier, colors and symbols were used to identify which films ranked higher on each list.\n\n\n\n\n\n\n\n\n\n\n\n\nLooking even deeper into the discrepancies between the lists reveals an interesting trend. The movies on the top of the plot are overwhelmingly favored by the critics: these include Tokyo Story, Citizen Kane, Barry Lyndon, and The Third Man. Movies more to the bottom of the list are favorites of the public, including: Raiders of Lost Ark, The Matrix, Star Wars - New Hope, and Pulp Fiction.\nWhile year of release seems to play a significant role, one can also theorize that the content of these films plays a much larger one.\n\n\n\n\n\n\n\n\n\nFinally, it’s also important to note movies that appear exclusively in one list. These last two tables more clearly reflect the difference between critically appraised movies and favorites of the public.\nThe best ranking films in BFI’s list that don’t appear at all in IMDB’s list include The Rules of the Game (1939), Persona (1966), and Mulholland Drive (2001). This is definitely a very snobish list, but again, year of release doesn’t seem to matter much.\n\n\n\n\n\n\nHighest ranked BFI films not appearing on IMDB\n\n\nName\nYear\nRank (BFI)\n\n\n\n\nJeanne Dielman, 23, quai du commerce, 1080 Bruxelles\n1975\n1\n\n\nIn the Mood for Love\n2000\n5\n\n\nBeau Travail\n1999\n7\n\n\nMulholland Drive\n2001\n8\n\n\nMan with a Movie Camera\n1929\n9\n\n\nSunrise\n1927\n11\n\n\nThe Rules of the Game\n1939\n13\n\n\nCléo from 5 to 7\n1962\n14\n\n\nThe Searchers\n1956\n15\n\n\nMeshes of the Afternoon\n1943\n16\n\n\nClose-Up\n1990\n17\n\n\nPersona\n1966\n18\n\n\nLate Spring\n1949\n22\n\n\nPlaytime\n1967\n23\n\n\nDo the Right Thing\n1989\n24\n\n\n\n\n\n\n\nThe best ranking films in IMDB’s list that don’t appear at all in BFI’s list include The Lord of the Rings trilogy (2001-2003), The Shawshank Redemption (1994), and several Christopher Nolan movies. Outside of Lumet’s 12 Angry Men (1957) most of this list heavily skewed to the 1990’s onwards.\n\n\n\n\n\n\nHighest ranked IMDB films not appearing on BFI\n\n\nName\nYear\nRank (IMDB)\n\n\n\n\nThe Shawshank Redemption\n1994\n1\n\n\nThe Dark Knight\n2008\n3\n\n\n12 Angry Men\n1957\n5\n\n\nSchindler's List\n1993\n6\n\n\nThe Lord of the Rings: The Return of the King\n2003\n7\n\n\nThe Lord of the Rings: The Fellowship of the Ring\n2001\n9\n\n\nForrest Gump\n1994\n11\n\n\nThe Lord of the Rings: The Two Towers\n2002\n12\n\n\nFight Club\n1999\n13\n\n\nInception\n2010\n14\n\n\nStar Wars: Episode V - The Empire Strikes Back\n1980\n15\n\n\nOne Flew Over the Cuckoo's Nest\n1975\n18\n\n\nSe7en\n1995\n19\n\n\nInterstellar\n2014\n20\n\n\nDune: Part Two\n2024\n21\n\n\n\n\n\n\n\n\n\n\n\nGathering data from other sources reveals more curious patterns. The plot below aggregates data from 8 different rankings: four of them voted by critics and four of them voted by the public1. The latter are online polls voted by users and for simplicity are all shown in yellow; the former are polls voted by experts and critics and are shown in blue.\n\n\n\n\n\n\n\n\n\nAll four lists organized by critics present similar characteristics. They are shaped almost like a triangle with its vertex somewhere around the 1960’s with a small outlier peak in the late 1990’s. AFI’s ranking differs slightly from this pattern exhibiting a relatively larger peak in the 1940’s.\nThe rankings voted by the public are much more scattered. Criticker’s ranking is the most similar to the critics’. It has a large share of films in the late 1940’s and 1950’s, but fewer in the 1960’s and 70’s relative to the critics.\nIMDB’s ranking, as seen previously, is heavily skewed to right, with several films from the 1990’s and 2000’s. Empire’s ranking is similar in shape to IMDB’s, almost resembling a ladder that peaks in 1999.\nFinally, Letterboxd’s list seems to be a mixture of Criticker’s 1950’s bias with IMDB’s 1990’s bias.\n\n\nSimilarly to the BFI x IMDB comparison, these lists aren’t directly comparable. Most importantly, they were (1) published at different times2; and (2) have different sample sizes3.\nA simple way to compare the similarity between these lists is to count the overlaps in films. That is, how many films in list A are present in list B. To overcome the differences in sample size, I truncate the larger list by the smaller list size. When comparing, for instance, the Variety Top 100 with the IMDB Top 250, I simply truncate the IMDB list at 100.\nThis should work well for most comparisons except for AFI and Empire. AFI’s ranking lists only American movies up to 2000; and Empire ranks movies only up until 2008.\nThe plot below shows this simple measure of overlap where the number represents the percentage of overlap between a pair of lists. This means that 64.9% of the films listed on Criticker also appear on Letterboxd’s Top 250.\n\n\n\n\n\n\n\n\n\nOverall, the public rankings are most similar to other public rankings and dissimilar to critic’s rankings. Letterboxd has a high similarity with Criticker and IMDB and a low similarity with Variety and Timeout. Likewise, Variety’s list is most similar to AFI, BFI, and Timeout."
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#rankings",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#rankings",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "There are several “greatest movies of all time” lists. Comparing them directly is often impossible due to conflicting methodologies. For simplicity, I’ll delve mostly into the Top 250 IMDB and the BFI Greatest Films of All Time. IMDB’s ranking compiles the largest number of online votes and has a relatively sturdy methodology for both its ratings and its rankings. The BFI list compiles votes from academics, critics, curators, archivists, and programmers. Its most recent edition was published in 2022.\nThe lists are relatively comparable in size: IMDB’s has (obviously) 250 films while BFI’s has 264. While BFI’s list has ties, I ignore these to make comparisons simpler.\nIn a latter part of the post I extend the analysis to other lists published by media outlets such as Variety and TimeOut and also to other online sites such as Letterboxd.\n\n\nGetting all the data involves a lot of webscraping. More information on the data collection process can be found on my Github."
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-vs-public",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-vs-public",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "Looking at the data by decade seems to reveal a clear pattern. BFI’s list peaks around the 1960’s, while IMDB’s list peaks around the 2000’s. There’s a small bump in the 1950’s but nothing noteworthy.\n\n\n\n\n\n\n\n\n\nWhile one could argue that IMDB’s ranking suggests some level of recency bias among the public, the numbers might also be a reflection of (1) the age demographic of online voters; (2) the (lack of) availability of older films, specially in mainstream streaming services.\n\n\n\nGoing into greater details reveals some curious features about how these lists compare. Some films appear to be appreciated by both the public and the critics. Films like Kurosawa’s Seven Samurai, Hitchcock’s Psycho, and Coppola’s The Godfather all rank highly in both lists.\nThere are, of course, divergences: some films, like Metropolis (1927) and The Apartment (1960) are slightly favored by the BFI’s critics; meanwhile, Ran (1985) and Modern Times (1936) are favored by the public. These divergences don’t seem to correlate directly with the year of release of each movie or even with its director.\nIts important to note that rankings work in a counter-intuitive fashion to most graphics, since the best (highest-ranking) films are associated with lower numbers and appear on the left. To make reading the graphics easier, colors and symbols were used to identify which films ranked higher on each list.\n\n\n\n\n\n\n\n\n\n\n\n\nLooking even deeper into the discrepancies between the lists reveals an interesting trend. The movies on the top of the plot are overwhelmingly favored by the critics: these include Tokyo Story, Citizen Kane, Barry Lyndon, and The Third Man. Movies more to the bottom of the list are favorites of the public, including: Raiders of Lost Ark, The Matrix, Star Wars - New Hope, and Pulp Fiction.\nWhile year of release seems to play a significant role, one can also theorize that the content of these films plays a much larger one.\n\n\n\n\n\n\n\n\n\nFinally, it’s also important to note movies that appear exclusively in one list. These last two tables more clearly reflect the difference between critically appraised movies and favorites of the public.\nThe best ranking films in BFI’s list that don’t appear at all in IMDB’s list include The Rules of the Game (1939), Persona (1966), and Mulholland Drive (2001). This is definitely a very snobish list, but again, year of release doesn’t seem to matter much.\n\n\n\n\n\n\nHighest ranked BFI films not appearing on IMDB\n\n\nName\nYear\nRank (BFI)\n\n\n\n\nJeanne Dielman, 23, quai du commerce, 1080 Bruxelles\n1975\n1\n\n\nIn the Mood for Love\n2000\n5\n\n\nBeau Travail\n1999\n7\n\n\nMulholland Drive\n2001\n8\n\n\nMan with a Movie Camera\n1929\n9\n\n\nSunrise\n1927\n11\n\n\nThe Rules of the Game\n1939\n13\n\n\nCléo from 5 to 7\n1962\n14\n\n\nThe Searchers\n1956\n15\n\n\nMeshes of the Afternoon\n1943\n16\n\n\nClose-Up\n1990\n17\n\n\nPersona\n1966\n18\n\n\nLate Spring\n1949\n22\n\n\nPlaytime\n1967\n23\n\n\nDo the Right Thing\n1989\n24\n\n\n\n\n\n\n\nThe best ranking films in IMDB’s list that don’t appear at all in BFI’s list include The Lord of the Rings trilogy (2001-2003), The Shawshank Redemption (1994), and several Christopher Nolan movies. Outside of Lumet’s 12 Angry Men (1957) most of this list heavily skewed to the 1990’s onwards.\n\n\n\n\n\n\nHighest ranked IMDB films not appearing on BFI\n\n\nName\nYear\nRank (IMDB)\n\n\n\n\nThe Shawshank Redemption\n1994\n1\n\n\nThe Dark Knight\n2008\n3\n\n\n12 Angry Men\n1957\n5\n\n\nSchindler's List\n1993\n6\n\n\nThe Lord of the Rings: The Return of the King\n2003\n7\n\n\nThe Lord of the Rings: The Fellowship of the Ring\n2001\n9\n\n\nForrest Gump\n1994\n11\n\n\nThe Lord of the Rings: The Two Towers\n2002\n12\n\n\nFight Club\n1999\n13\n\n\nInception\n2010\n14\n\n\nStar Wars: Episode V - The Empire Strikes Back\n1980\n15\n\n\nOne Flew Over the Cuckoo's Nest\n1975\n18\n\n\nSe7en\n1995\n19\n\n\nInterstellar\n2014\n20\n\n\nDune: Part Two\n2024\n21"
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-x-public---2",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#critics-x-public---2",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "",
    "text": "Gathering data from other sources reveals more curious patterns. The plot below aggregates data from 8 different rankings: four of them voted by critics and four of them voted by the public1. The latter are online polls voted by users and for simplicity are all shown in yellow; the former are polls voted by experts and critics and are shown in blue.\n\n\n\n\n\n\n\n\n\nAll four lists organized by critics present similar characteristics. They are shaped almost like a triangle with its vertex somewhere around the 1960’s with a small outlier peak in the late 1990’s. AFI’s ranking differs slightly from this pattern exhibiting a relatively larger peak in the 1940’s.\nThe rankings voted by the public are much more scattered. Criticker’s ranking is the most similar to the critics’. It has a large share of films in the late 1940’s and 1950’s, but fewer in the 1960’s and 70’s relative to the critics.\nIMDB’s ranking, as seen previously, is heavily skewed to right, with several films from the 1990’s and 2000’s. Empire’s ranking is similar in shape to IMDB’s, almost resembling a ladder that peaks in 1999.\nFinally, Letterboxd’s list seems to be a mixture of Criticker’s 1950’s bias with IMDB’s 1990’s bias.\n\n\nSimilarly to the BFI x IMDB comparison, these lists aren’t directly comparable. Most importantly, they were (1) published at different times2; and (2) have different sample sizes3.\nA simple way to compare the similarity between these lists is to count the overlaps in films. That is, how many films in list A are present in list B. To overcome the differences in sample size, I truncate the larger list by the smaller list size. When comparing, for instance, the Variety Top 100 with the IMDB Top 250, I simply truncate the IMDB list at 100.\nThis should work well for most comparisons except for AFI and Empire. AFI’s ranking lists only American movies up to 2000; and Empire ranks movies only up until 2008.\nThe plot below shows this simple measure of overlap where the number represents the percentage of overlap between a pair of lists. This means that 64.9% of the films listed on Criticker also appear on Letterboxd’s Top 250.\n\n\n\n\n\n\n\n\n\nOverall, the public rankings are most similar to other public rankings and dissimilar to critic’s rankings. Letterboxd has a high similarity with Criticker and IMDB and a low similarity with Variety and Timeout. Likewise, Variety’s list is most similar to AFI, BFI, and Timeout."
  },
  {
    "objectID": "posts/general-posts/2024-04-best-movies-bias/index.html#footnotes",
    "href": "posts/general-posts/2024-04-best-movies-bias/index.html#footnotes",
    "title": "The Greatest Films of All-time: a data approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be fair, Empire’s list combines both critics and general public. The numbers however, skew it toward the public.↩︎\nThe Criticker (2022), Letterboxd (2022), IMDB (2024), TimeOut (2024), BFI (2022), and Variety (2022) lists are relatively comparable among themselves. Empire’s ranking was published in 2008. AFI’s ranking is more recent but doesn’t include films released after the 2000’s.↩︎\nFor some reason 250, and its multiples, is a favorite among the public and 100 is preferred by the critics.↩︎"
  },
  {
    "objectID": "posts/general-posts/2024-12-demographic-pyramid/index.html",
    "href": "posts/general-posts/2024-12-demographic-pyramid/index.html",
    "title": "Demographic Pyramids in R",
    "section": "",
    "text": "library(ggplot2)\nlibrary(data.table)\nlibrary(wpp2024)\n\n# remotes::install_github(\"PPgp/wpp2024\")\ndata(popAge1dt)\n# Brazil, Ireland, Pakistan, Japan\n\ncountries &lt;- data.table(\n  country_code = c(392, 586, 372, 76),\n  name = c(\"Japan\", \"Pakistan\", \"Ireland\", \"Brazil\")\n)\n\nsub &lt;- popAge1dt[country_code %in% countries$country_code]\n# Create age-group data.frames for merges\ndf_age_group &lt;- function(age_min = 0, age_max = 90, group_interval = 5) {\n  # Age-group auxiliary data.frames\n  group &lt;- seq(age_min, age_max, group_interval)\n  interval &lt;- findInterval(0:150, group, rightmost.closed = FALSE)\n  # Text labels for each group\n  label &lt;- paste(group, (group - 1)[-1], sep = \"-\")\n  label[length(label)] &lt;- paste0(age_max, \"+\")\n  age_label &lt;- rep(label, each = group_interval)\n  age_label &lt;- c(age_label, rep(paste0(age_max, \"+\"), 151 - length(age_label)))\n  # 5 year age groups 0-4 to 90+\n  age_group &lt;- data.frame(\n    age = 0:150,\n    age_group = interval,\n    age_label = factor(age_label)\n    #age_label = factor(age_label, levels = age_label[order(interval)])\n  )\n  \n  age_group &lt;- age_group |&gt; \n    dplyr::mutate(age_label = forcats::fct_reorder(age_label, age_group))\n  \n  return(age_group)\n}\nsub50 &lt;- sub[year == 1950 & country_code == 392]\n\nsub50\n\n     country_code   name  year   age     popM     popF      pop\n            &lt;int&gt; &lt;char&gt; &lt;int&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n  1:          392  Japan  1950     0 1208.092 1148.878 2356.970\n  2:          392  Japan  1950     1 1174.236 1170.458 2344.694\n  3:          392  Japan  1950     2 1294.455 1290.571 2585.026\n  4:          392  Japan  1950     3 1278.061 1273.618 2551.679\n  5:          392  Japan  1950     4 1213.995 1204.389 2418.384\n ---                                                           \n 97:          392  Japan  1950    96    0.108    0.469    0.577\n 98:          392  Japan  1950    97    0.069    0.302    0.371\n 99:          392  Japan  1950    98    0.039    0.173    0.212\n100:          392  Japan  1950    99    0.018    0.082    0.100\n101:          392  Japan  1950   100    0.006    0.025    0.031\ndim_age &lt;- df_age_group(age_min = 0, age_max = 90, group_interval = 5)\n\ndim_age\n\n    age age_group age_label\n1     0         1       0-4\n2     1         1       0-4\n3     2         1       0-4\n4     3         1       0-4\n5     4         1       0-4\n6     5         2       5-9\n7     6         2       5-9\n8     7         2       5-9\n9     8         2       5-9\n10    9         2       5-9\n11   10         3     10-14\n12   11         3     10-14\n13   12         3     10-14\n14   13         3     10-14\n15   14         3     10-14\n16   15         4     15-19\n17   16         4     15-19\n18   17         4     15-19\n19   18         4     15-19\n20   19         4     15-19\n21   20         5     20-24\n22   21         5     20-24\n23   22         5     20-24\n24   23         5     20-24\n25   24         5     20-24\n26   25         6     25-29\n27   26         6     25-29\n28   27         6     25-29\n29   28         6     25-29\n30   29         6     25-29\n31   30         7     30-34\n32   31         7     30-34\n33   32         7     30-34\n34   33         7     30-34\n35   34         7     30-34\n36   35         8     35-39\n37   36         8     35-39\n38   37         8     35-39\n39   38         8     35-39\n40   39         8     35-39\n41   40         9     40-44\n42   41         9     40-44\n43   42         9     40-44\n44   43         9     40-44\n45   44         9     40-44\n46   45        10     45-49\n47   46        10     45-49\n48   47        10     45-49\n49   48        10     45-49\n50   49        10     45-49\n51   50        11     50-54\n52   51        11     50-54\n53   52        11     50-54\n54   53        11     50-54\n55   54        11     50-54\n56   55        12     55-59\n57   56        12     55-59\n58   57        12     55-59\n59   58        12     55-59\n60   59        12     55-59\n61   60        13     60-64\n62   61        13     60-64\n63   62        13     60-64\n64   63        13     60-64\n65   64        13     60-64\n66   65        14     65-69\n67   66        14     65-69\n68   67        14     65-69\n69   68        14     65-69\n70   69        14     65-69\n71   70        15     70-74\n72   71        15     70-74\n73   72        15     70-74\n74   73        15     70-74\n75   74        15     70-74\n76   75        16     75-79\n77   76        16     75-79\n78   77        16     75-79\n79   78        16     75-79\n80   79        16     75-79\n81   80        17     80-84\n82   81        17     80-84\n83   82        17     80-84\n84   83        17     80-84\n85   84        17     80-84\n86   85        18     85-89\n87   86        18     85-89\n88   87        18     85-89\n89   88        18     85-89\n90   89        18     85-89\n91   90        19       90+\n92   91        19       90+\n93   92        19       90+\n94   93        19       90+\n95   94        19       90+\n96   95        19       90+\n97   96        19       90+\n98   97        19       90+\n99   98        19       90+\n100  99        19       90+\n101 100        19       90+\n102 101        19       90+\n103 102        19       90+\n104 103        19       90+\n105 104        19       90+\n106 105        19       90+\n107 106        19       90+\n108 107        19       90+\n109 108        19       90+\n110 109        19       90+\n111 110        19       90+\n112 111        19       90+\n113 112        19       90+\n114 113        19       90+\n115 114        19       90+\n116 115        19       90+\n117 116        19       90+\n118 117        19       90+\n119 118        19       90+\n120 119        19       90+\n121 120        19       90+\n122 121        19       90+\n123 122        19       90+\n124 123        19       90+\n125 124        19       90+\n126 125        19       90+\n127 126        19       90+\n128 127        19       90+\n129 128        19       90+\n130 129        19       90+\n131 130        19       90+\n132 131        19       90+\n133 132        19       90+\n134 133        19       90+\n135 134        19       90+\n136 135        19       90+\n137 136        19       90+\n138 137        19       90+\n139 138        19       90+\n140 139        19       90+\n141 140        19       90+\n142 141        19       90+\n143 142        19       90+\n144 143        19       90+\n145 144        19       90+\n146 145        19       90+\n147 146        19       90+\n148 147        19       90+\n149 148        19       90+\n150 149        19       90+\n151 150        19       90+\njpn50 &lt;- merge(sub50, dim_age, by = \"age\")\n\n# jpn50[, age_label := factor(age_label, levels = unique(jpn50$age_label))]"
  },
  {
    "objectID": "posts/general-posts/2024-12-demographic-pyramid/index.html#grouped-data",
    "href": "posts/general-posts/2024-12-demographic-pyramid/index.html#grouped-data",
    "title": "Demographic Pyramids in R",
    "section": "Grouped data",
    "text": "Grouped data\n\ngrouped_jpn50 &lt;- jpn50[, .(pop_male = sum(popM), pop_female = sum(popF)), by = \"age_label\"]\n\ngrouped_jpn50 &lt;- melt(\n  grouped_jpn50,\n  id.vars = c(\"age_label\"),\n  measure.vars = c(\"pop_male\", \"pop_female\"),\n  variable.name = \"sex\",\n  value.name = \"pop\"\n  )\n\ngrouped_jpn50[, share := pop / sum(pop), by = \"sex\"]\ngrouped_jpn50[, share_total := pop / sum(pop)]\n\ngrouped_jpn50[, y := pop][sex == \"pop_male\", y := -pop]\ngrouped_jpn50[, y_share := share][sex == \"pop_male\", y_share := -share]\ngrouped_jpn50[, y_share_total := share_total][sex == \"pop_male\", y_share := -share_total]\n\n\nggplot(grouped_jpn50, aes(x = age_label, y = y)) +\n  geom_col(aes(fill = sex)) +\n  coord_flip() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\njpn &lt;- sub[year %in% c(1950, 1960, 1970, 1980) & country_code == 392]\n\n\njpn &lt;- merge(jpn, dim_age, by = \"age\")\njpn &lt;- jpn[,\n           .(pop_male = sum(popM), pop_female = sum(popF)),\n           by = c(\"year\", \"age_label\")]\n\njpn &lt;- melt(jpn, id.vars = c(\"year\", \"age_label\"))\n\njpn[, share := value / sum(value), by = c(\"year\")]\n\nggplot(jpn, aes())"
  }
]