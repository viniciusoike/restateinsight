[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "RealEstate Insight",
    "section": "",
    "text": "Tutoriais\n\nggplot2: Do básico ao intermediário\nVisualização de dados é a base fundamental de uma boa análise e pode ser o fator que separara uma trabalho amador de um trabalho profissional. Esta série de posts ensina desde o ggplot2 desde o início.\n\n\nR: Um breve intensivo de R\n[Em breve] A jornada da programação não é simples. Estes posts provêm uma introdução geral ao R. Apesar dos exemplos serem focados em economia, o material serve para o público geral.\n\n\n\nPosts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nA filosofia do Tidyverse\n\n\n\n\n\n\n\ntidyverse\n\n\ndata-science\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nLife Satisfaction and GDP per capita\n\n\n\n\n\n\n\ndata-visualization\n\n\ntutorial-R\n\n\nggplot2\n\n\nenglish\n\n\n\n\nIn this post I make a step-by-step replication of a plot from OurWorldInData. The plot shows the correlation between average self-reported life satisfaction and GDP per capita.\n\n\n\n\n\n\nSep 22, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nPipes\n\n\n\n\n\n\n\ndata-science\n\n\ntutorial-r\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nWeekly Viz: Ruas de Porto Alegre\n\n\n\n\n\n\n\nmapas\n\n\nggplot2\n\n\nweekly-viz\n\n\ndata-visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nMapa de altitude de ruas de Brasília\n\n\n\n\n\n\n\ndata-visualization\n\n\nmapas\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nShiny Dashboard: IDH municípios\n\n\n\n\n\n\n\nshiny\n\n\ndata-visualization\n\n\nbrasil\n\n\n\n\nNeste post apresento o Dashboard do IDH de municípios que fiz com base nos dados da Firjan. O Dashboard foi feito com Shiny.\n\n\n\n\n\n\nAug 25, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nWeekly Viz: Recife em mapas\n\n\n\n\n\n\n\nmapas\n\n\nggplot2\n\n\nweekly-viz\n\n\ndata-visualization\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nO impacto dos juros na demanda imobiliário\n\n\n\n\n\n\n\nreal-estate\n\n\neconomia\n\n\n\n\nA taxa de juros é talvez a variável macroeconômica mais importante para se observar quando se pensa em financiamento imobiliário. Neste post apresento o básico do financiamento habitacional e mostro como o aumento dos juros impactou a acessibilidade à moradia. Por fim, comento sobre a recente queda na taxa SELIC.\n\n\n\n\n\n\nAug 17, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nImportando dados do SIDRA\n\n\n\n\n\n\n\ndata-science\n\n\neconomia\n\n\ntutorial-R\n\n\n\n\nImportando dados abertos do IBGE via API usando o sidrar no R.\n\n\n\n\n\n\nAug 10, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nDefinindo objetos no R. = ou &lt;- ?\n\n\n\n\n\n\n\ntutorial-R\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nPacotes Essenciais R\n\n\n\n\n\n\n\ndata-science\n\n\neconometria\n\n\ntutorial-R\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nVisualizando uma única variável\n\n\n\n\n\n\n\ndata-visualization\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\nAlgumas ideias soltas sobre como visualizar uma única variável usando vários tipos de gráficos distintos em ggplot2.\n\n\n\n\n\n\nJun 28, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nPreços de Imóveis e Demografia\n\n\n\n\n\n\n\nreal-estate\n\n\neconomia\n\n\n\n\nEm algum sentido, o preço dos imóveis reflete tendências demográficas de longo prazo: a mais simples delas é o crescimento populacional. Neste post analiso padrões de crescimento demográfico e de preços de imóveis.\n\n\n\n\n\n\nJun 20, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nSéries de Tempo no R\n\n\n\n\n\n\n\ntime-series\n\n\ntutorial-R\n\n\neconometria\n\n\n\n\nNeste post faço um panorama geral de como estimar um modelo SARIMA no R usando majoritariamente funções base. O R vem equipado com diversas funções úteis e poderosas para lidar com séries de tempo.\n\n\n\n\n\n\nJun 1, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nUsando fontes com showtext no R\n\n\n\n\n\n\n\ntutorial-R\n\n\n\n\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar. Neste post ensino como importar fontes no R para criar visualizações mais refinadas e profissionais.\n\n\n\n\n\n\nMay 10, 2023\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nVisualizando o IPCA\n\n\n\n\n\n\n\ndata-visualization\n\n\nbrasil\n\n\neconomia\n\n\ntutorial-R\n\n\n\n\nNeste post discuto e apresento diversas maneiras de visualizar o IPCA incluindo alguns gráficos peculiares para enxergar a distribuição dos valores da inflação.\n\n\n\n\n\n\nDec 1, 2022\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nARMA: um exemplo simples\n\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\nrepost\n\n\ntutorial-R\n\n\n\n\nNeste post mostro como modelar um ARMA simples no R usando o pacote {astsa}.\n\n\n\n\n\n\nMar 1, 2021\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nEMV no R\n\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\nOs estimadoes de máxima verossimilhança possuem várias boas propriedades. Neste post discuto tanto aspectos teóricos como aplicados, com exemplos, e faço algumas simulações para comprovar resultados assintóticos.\n\n\n\n\n\n\nFeb 1, 2020\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nAquecimento Global\n\n\n\n\n\n\n\ndata-visualization\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2020\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nOLS com matrizes\n\n\n\n\n\n\n\neconometria\n\n\nrepost\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nRepost: Otimização numérica - métodos de Newton\n\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nCrescimento do PIB per capita no mundo\n\n\n\n\n\n\n\nrepost\n\n\ndata-visualization\n\n\nggplot2\n\n\ntutorial-R\n\n\n\n\nReproduzindo uma visualização do portal Nexo\n\n\n\n\n\n\nJun 1, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nRepost: Expectativa de vida e Crescimento Econômico\n\n\n\n\n\n\n\ndata-visualization\n\n\neconomia\n\n\nrepost\n\n\ntutorial-R\n\n\n\n\nNeste tutorial mostro como criar visualizações ricas mostrando correlações, usando os dados de crescimento econômico e expectativa de vida do Gapminder.\n\n\n\n\n\n\nMay 6, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nMQO - teoria assintótica\n\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\nNeste post discuto e apresento simulações de alguns resultados assintóticos de MQO. Estes resultados permitem entender a distribuição dos estimadores MQO e o comportamento dos coeficientes à medida que o tamanho da amostra aumenta. Compreender esses resultados é fundamental para realizar inferências estatísticas precisas na análise de regressão.\n\n\n\n\n\n\nMar 23, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nTeoria Assintótica - LGN e TCL\n\n\n\n\n\n\n\neconometria\n\n\ntutorial-R\n\n\nrepost\n\n\n\n\nSimulações de dois resultados centrais para a econometria: a Lei dos Grandes Números e o Teorema Central do Limite\n\n\n\n\n\n\nMar 23, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nRegressão Linear com Séries de Tempo\n\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\nrepost\n\n\ntutorial-R\n\n\n\n\nCursos de econometria em séries de tempo às vezes omitem o uso de MQO num contexto de séries de tempo. Neste post discuto um pouco da teoria e de aplicações no R.\n\n\n\n\n\n\nJan 1, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\n  \n\n\n\n\nSARIMA no R\n\n\n\n\n\n\n\neconometria\n\n\ntime-series\n\n\nrepost\n\n\ntutorial-R\n\n\n\n\nUm tutorial conciso sobre como utilizar o modelo SARIMA para análise e previsão de séries temporais no R. Aprenda a identificar componentes sazonais, ajustar parâmetros e fazer previsões precisas com este guia passo a passo.\n\n\n\n\n\n\nJan 1, 2019\n\n\nVinicius Oike\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "IDH dos municípios do Brasil\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nA filosofia do Tidyverse\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\n\n\n\n\n\n\nLife Satisfaction and GDP per capita\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\nPipes\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\nWeekly Viz: Ruas de Porto Alegre\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\nEstético: Escalas e Cores\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nMapa de altitude de ruas de Brasília\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\n\n\n\n\n\n\nEstético: Destacando informação\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\n\n\nShiny Dashboard: IDH municípios\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\n\n\n\n\n\n\nWeekly Viz: Recife em mapas\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\n\n\n\n\n\n\nFundamentos: gráfico de linha\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\nO impacto dos juros na demanda imobiliário\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\n\n\n\n\n\n\nFundamentos: histograma\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n\n\nImportando dados do SIDRA\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\n\n\n\n\n\n\nFundamentos: gráfico de coluna\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\n\n\n\n\n\n\nFundamentos: gráfico de dispersão\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\nDefinindo objetos no R. = ou &lt;- ?\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\n\n\n\n\n\n\nApêndice: manipular para enxergar\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\n\n\n\n\n\n\nIntrodução\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\nPacotes Essenciais R\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\nVisualizando uma única variável\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\nPreços de Imóveis e Demografia\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\nSéries de Tempo no R\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\nUsando fontes com showtext no R\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\nVisualizando o IPCA\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\nARMA: um exemplo simples\n\n\n\n\n\n\n\n\n\nMar 1, 2021\n\n\n\n\n\n\n\n\nEMV no R\n\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\nAquecimento Global\n\n\n\n\n\n\n\n\n\nJan 10, 2020\n\n\n\n\n\n\n\n\nOLS com matrizes\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\n\n\nRepost: Otimização numérica - métodos de Newton\n\n\n\n\n\n\n\n\n\nSep 28, 2019\n\n\n\n\n\n\n\n\nCrescimento do PIB per capita no mundo\n\n\n\n\n\n\n\n\n\nJun 1, 2019\n\n\n\n\n\n\n\n\nRepost: Expectativa de vida e Crescimento Econômico\n\n\n\n\n\n\n\n\n\nMay 6, 2019\n\n\n\n\n\n\n\n\nMQO - teoria assintótica\n\n\n\n\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\n\nTeoria Assintótica - LGN e TCL\n\n\n\n\n\n\n\n\n\nMar 23, 2019\n\n\n\n\n\n\n\n\nRegressão Linear com Séries de Tempo\n\n\n\n\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\n\n\nSARIMA no R\n\n\n\n\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "RealEstate Insight",
    "section": "",
    "text": "Neste site você vai encontrar vários posts sobre economia, mercado imoibiliário e economia urbana. Também escrevo vários tutoriais sobre R, a linguagem de programação que tenho maior proficiência. A maior parte dos posts e textos estão na aba Blog. Tenho alguns aplicativos na aba Apps.\nEste site foi originalmente lançado no final de 2019, usando RMarkdown e o pacote blogdown. A versão atual foi lançada oficialmente em 2023, usando Quarto uma linguagem integrada para apresentação de análises de dados, que junta R, Python, Julia e diversas outras linguagens de programação."
  },
  {
    "objectID": "aboutme.html#mini-cv",
    "href": "aboutme.html#mini-cv",
    "title": "RealEstate Insight",
    "section": "Mini-CV",
    "text": "Mini-CV\nUniversity of São Paulo | São Paulo, SP. MSc Economics1. CAPES\nFederal University of Rio Grande do Sul2 | Porto Alegre, RS Bachelor (B.A.) Economics. Mathematical Economics.\nHolds a Master’s degree in Economics from the University of São Paulo, specialized in mathematical economics and time series. Working in data analysis and real estate market since 2019, with a focus on the residential market. Has worked on various consultancy projects related to the residential market and urban public policy (TOD).\nLanguages: English, Portuguese, and Spanish.\n\nSome of my work\n\nHousing Affordability in São Paulo: overview and measurement presented at the 2021 Latin American Real Estate Society.\nMicroapartamentos: mercado e tendências (2022)\nO Mercado Residencial na América Latina (2022)"
  },
  {
    "objectID": "aboutme.html#footnotes",
    "href": "aboutme.html#footnotes",
    "title": "RealEstate Insight",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Economics Graduate course has been evaluated with a grade 7 (the highest grade), since the inception of the Capes’ plurianual review, indicating its high standard of international performance.↩︎\nRanked by INEP (Ministry of Education) as the best public university in Brazil since 2012 and during 2012-2014 also as the best university in Brazil. Commonly ranked in the Top 10 of all Brazilian universities in both national and international rankings.↩︎"
  },
  {
    "objectID": "posts/general-posts/arima-no-r/index.html",
    "href": "posts/general-posts/arima-no-r/index.html",
    "title": "Séries de Tempo no R",
    "section": "",
    "text": "Neste post vou explorar um pouco das funções base do R para montar um modelo SARIMA. O R vem “pré-equipado” com um robusto conjunto de funções para lidar com séries de tempo. Inclusive, como se verá, existe uma class específica de objeto para trabalhar com séries de tempo."
  },
  {
    "objectID": "posts/general-posts/arima-no-r/index.html#modelagem-sarima",
    "href": "posts/general-posts/arima-no-r/index.html#modelagem-sarima",
    "title": "Séries de Tempo no R",
    "section": "Modelagem SARIMA",
    "text": "Modelagem SARIMA\nAqui a ideia é experimentar com alguns modelos simples. Em especial, o modelo que Box & Jenkins sugerem para a série é de um SARIMA (0, 1, 1)(0, 1, 1)[12] da forma\n\\[\n(1 - \\Delta)(1 - \\Delta^{12})y_{t} = \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t_1} + \\Theta_{1}\\varepsilon_{t-12}\n\\]\nA metodologia correta para a análise seria primeiro fazer testes de raiz unitária para avaliar a estacionaridade da série. Mas só de olhar para as funções de autocorrelação e autocorrelação parcial, fica claro que há algum componente sazonal e que a série não é estacionária.\n\n\n\n\n\n\n\n\n\n\nTeste de raiz unitária\nApenas a título de exemplo, faço um teste Dickey-Fuller (ADF), bastante geral, com constante e tendência temporal linear. Para uma boa revisão metodológica de como aplicar testes de raiz unitária, em partiular o teste ADF, consulte o capítulo de séries não-estacionárias do livro Applied Econometric Time Series do Enders\nAqui, a escolha ótima do lag é feita usando o critério BIC (também conhecido como Critério de Schwarz). Não existe uma função que aplica o teste ADF no pacote base o R. A implementação é feita na função ur.df do pacote urca.\nA estatísitica de teste mais relevante é a tau3 e vê-se, surpreendentemente, que se rejeita a hipótese nula de raiz unitária. As estatísticas phi2 e phi3 são testes-F da significânica conjunta dos termos de constante e de tendência temporal. As estatísticas de teste são convencionais e seguem a notação do livro do Enders citado acima e também do clássico livro do Hamilton.\n\nlibrary(urca)\nadf_test &lt;- ur.df(y, type = \"trend\", selectlags = \"BIC\", lags = 13)\nsummary(adf_test)\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090139 -0.022382 -0.002417  0.021008  0.110003 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.8968049  0.3999031   2.243 0.026859 *  \nz.lag.1      -0.1809364  0.0842729  -2.147 0.033909 *  \ntt            0.0016886  0.0008609   1.961 0.052263 .  \nz.diff.lag1  -0.2588927  0.1134142  -2.283 0.024301 *  \nz.diff.lag2  -0.0986455  0.1070332  -0.922 0.358665    \nz.diff.lag3  -0.0379799  0.1045583  -0.363 0.717097    \nz.diff.lag4  -0.1392651  0.0981271  -1.419 0.158560    \nz.diff.lag5  -0.0283998  0.0963368  -0.295 0.768686    \nz.diff.lag6  -0.1326313  0.0889223  -1.492 0.138581    \nz.diff.lag7  -0.1096365  0.0865862  -1.266 0.208019    \nz.diff.lag8  -0.2348880  0.0829892  -2.830 0.005497 ** \nz.diff.lag9  -0.0926604  0.0843594  -1.098 0.274344    \nz.diff.lag10 -0.2053937  0.0789245  -2.602 0.010487 *  \nz.diff.lag11 -0.1081091  0.0786801  -1.374 0.172127    \nz.diff.lag12  0.6633101  0.0752086   8.820 1.54e-14 ***\nz.diff.lag13  0.3197783  0.0883636   3.619 0.000443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04011 on 114 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8621 \nF-statistic: 54.75 on 15 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -2.147 4.9781 3.4342 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n\n\nPela análise do correlograma do resíduo da regressão, fica claro que ainda há autocorrelação. Novamente, o mais correto seria aplicar o teste Ljung-Box sobre os resíduos para verificar a presença de autocorrelação conjunta nas primeiras k defasagens, mas este não é o foco deste post. Esta pequena digressão exemplifica como a aplicação destes testes em séries de tempo pode não ser tão direto/simples.\n\n\n\n\n\n\n\n\n\nOs gráficos abaixo mostram o correlograma da série após tirarmos a primeira diferença e a primeira diferença sazonal. A partir da análise destes correlogramas poderíamos inferir algumas especificações alternativas para modelos SARIMA e aí, poderíamos escolher o melhor modelo usando algum critério de informação.\nA metodologia Box & Jenkins de análise de séries de tempo tem, certamente, um pouco de arte e feeling. Não é tão imediato entender como devemos proceder e, na prática, faz sentido experimentar com vários modelos alternativos de ordem baixa como SARIMA(1, 1, 1)(0, 1, 1), SARIMA(2, 1, 0)(0, 1, 1), etc.\n\n\n\n\n\n\n\n\n\n\n\nOs modelos\nPara não perder muito tempo experimentando com vários modelos vou me ater a três modelos diferentes. Uma função bastante útil é a auto.arima do pacote forecast que faz a seleção automática do melhor modelo da classe SARIMA/ARIMA/ARMA.\nEu sei que o Schumway/Stoffer, autores do ótimo Time Series Analysis and Its Applications, tem um post crítico ao uso do auto.arima. Ainda assim, acho que a função tem seu mérito e costuma ser um bom ponto de partida para a sua análise. Quando temos poucas séries de tempo para analisar, podemos nos dar ao luxo de fazer a modelagem manualmente, mas quando há centenas de séries, é muito conveniente poder contar com o auto.arima.\nComo o auto.arima escolhe o mesmo modelo do Box & Jenkins eu experimento com uma especificação diferente. Novamente, a título de exemplo eu comparo ambos os modelos SARIMA com uma regressão linear simples que considera uma tendência temporal linear e uma série de dummies sazonais. O modelo é algo da forma\n\\[\ny_{t} = \\alpha_{0} + \\alpha_{1}t + \\sum_{i = 1}^{11}\\beta_{i}s_{i} + \\varepsilon_{t}\n\\]\nOnde \\(s_{i}\\) é uma variável indicadora igual a 1 se \\(t\\) corresponder ao mês \\(i\\) e igual a 0 caso contrário. Vale notar que não podemos ter uma dummy para todos os meses se não teríamos uma matriz de regressores com colinearidade perfeita!\nAqui vou contradizer um pouco o espírito do post novamente para usar o forecast. O ganho de conveniência vem na hora de fazer as previsões. Ainda assim, indico como estimar os mesmos modelos usando apenas funções base do R.\n\n# Usando o forecast\nlibrary(forecast)\nmodel1 &lt;- auto.arima(train)\nmodel2 &lt;- Arima(train, order = c(1, 1, 1), seasonal = c(1, 1, 0))\nmodel3 &lt;- tslm(train ~ trend + season)\n\n\n# Usando apenas funções base\nmodel2 &lt;- arima(\n  trains,\n  order = c(1, 1, 1),\n  seasonal = list(order = c(1, 1, 1), period = 12)\n)\n\n# Extrai uma tendência temporal linear \ntrend &lt;- time(train)\n# Cria variáveis dummies mensais\nseason &lt;- cycle(train)\nmodel3 &lt;- lm(train ~ trend + season)\n\nA saída dos modelos segue abaixo. As saídas dos modelos SARIMA não são muito interessantes. Em geral, não é muito comum avaliar nem a significância e nem o sinal dos coeficientes, já que eles não têm muito valor interpretativo. Uma coisa que fica evidente dos dois modelos abaixo é que o primeiro parece melhor ajustado aos dados pois tem valores menores em todos os critérios de informação considerados.\n\nmodel1\n\nSeries: train \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.3424  -0.5405\ns.e.   0.1009   0.0877\n\nsigma^2 = 0.001432:  log likelihood = 197.51\nAIC=-389.02   AICc=-388.78   BIC=-381\n\n\n\nmodel2\n\nSeries: train \nARIMA(1,1,1)(1,1,0)[12] \n\nCoefficients:\n         ar1      ma1     sar1\n      0.0668  -0.4518  -0.4426\ns.e.  0.3046   0.2825   0.0875\n\nsigma^2 = 0.001532:  log likelihood = 195.14\nAIC=-382.28   AICc=-381.89   BIC=-371.59\n\n\nJá o modelo de regressão linear tem uma saída mais interessante. Note que, por padrão, o primeiro mês foi omitido e seu efeito aparece no termo constante. Na tabela abaixo, vemos que há um efeito positivo e significativo, por exemplo, nos meses 6-8 (junho a agosto), que coincidem com o período de férias de verão no hemisfério norte. Já, em novembro (mês 11) parece haver uma queda na demanda por passagens aéreas.\nNote que o R quadrado da regressão é extremamente elevado e isso é um indício de que algo está errado. Isto, muito provavelmente é resultado da não-estacionaridade da série.\n\nbroom::tidy(model3)\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic   p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  4.71     0.0191    247.     2.81e-149\n 2 trend        0.0106   0.000145   73.2    3.09e- 93\n 3 season2     -0.0134   0.0245     -0.549  5.84e-  1\n 4 season3      0.120    0.0245      4.91   3.32e-  6\n 5 season4      0.0771   0.0245      3.15   2.14e-  3\n 6 season5      0.0675   0.0245      2.75   6.93e-  3\n 7 season6      0.191    0.0245      7.81   4.23e- 12\n 8 season7      0.288    0.0245     11.7    6.32e- 21\n 9 season8      0.278    0.0245     11.4    4.36e- 20\n10 season9      0.143    0.0245      5.82   6.13e-  8\n11 season10     0.00108  0.0245      0.0441 9.65e-  1\n12 season11    -0.141    0.0245     -5.77   7.97e-  8\n13 season12    -0.0248   0.0245     -1.01   3.14e-  1\n\n\nDe fato, olhando para a função de autocorrelação do resíduo do modelo de regressão linear, fica evidente que há autocorrelação. Uma forma de contornar isso seria de incluir um termo ARMA no termo de erro. Novamente, este não é o foco do post e vamos seguir normalmente.\n\nacf(resid(model3))"
  },
  {
    "objectID": "posts/general-posts/arima-no-r/index.html#comparando-as-previsões",
    "href": "posts/general-posts/arima-no-r/index.html#comparando-as-previsões",
    "title": "Séries de Tempo no R",
    "section": "Comparando as previsões",
    "text": "Comparando as previsões\nA maneira mais prática de trabalhar com vários modelos ao mesmo tempo é agregando eles em listas e aplicando funções nessas listas.\nAbaixo eu aplico a função forecast para gerar as previsões 24 períodos a frente nos três modelos. Depois, eu extraio somente a estimativa pontual de cada previsão.\n\nmodels &lt;- list(model1, model2, model3)\nyhat &lt;- lapply(models, forecast, h = 24)\nyhat_mean &lt;- lapply(yhat, function(x) x$mean)\n\nComparamos a performance de modelos de duas formas: (1) olhando para medidas de erro (o quão bem o modelo prevê os dados do test) e (2) olhando para critérios de informação (o quão bem o modelo se ajusta aos dados do train).\nOs critérios de informação têm todos uma interpretação bastante simples: quanto menor, melhor. Tipicamente, o AIC tende a escolher modelos sobreparametrizados enquanto o BIC tende a escolher modelos mais parcimoniosos.\nJá a comparação de medidas de erro não é tão simples. Pois ainda que um modelo tenha, por exemplo, um erro médio quadrático menor do que outro, não é claro se esta diferença é significante. Uma maneira de testar isso é via o teste Diebold-Mariano, que compara os erros de previsão de dois modelos. Implicitamente, contudo, ele assume que a diferença entre os erros de previsão é covariância-estacionária (também conhecido como estacionário de segunda ordem ou fracamente estacionário). Dependendo do contexto, esta pode ser uma hipótese mais ou menos razoável.\n\ncompute_error &lt;- function(model, test) {\n  y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    y - yhat\n}\n\ncompute_error_metrics &lt;- function(model, test) {\n\n    y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    error &lt;- y - yhat\n    \n    # Raiz do erro quadrado medio\n    rmse &lt;- sqrt(mean(error^2))\n    # Erro medio absoluto\n    mae &lt;- mean(abs(error))\n    # Erro medio percentual\n    mape &lt;- mean(abs(100 * error / y))\n    # Root mean squared scaled error\n    rmsse &lt;- sqrt(mean(error^2 / snaive(train)$mean))\n\n    # Devolve os resultados num list\n    list(rmse = rmse, mae = mae, mape = mape, rmsse = rmsse)\n    \n\n}\n\ncompute_ics &lt;- function(model) {\n\n    # Extrai criterios de informacao\n    aic  &lt;- AIC(model)\n    bic  &lt;- BIC(model)\n\n    # Devolve os resultados num list\n    list(aic = aic, bic = bic)\n\n} \n\nfcomparison &lt;- lapply(yhat, function(yhat) compute_error_metrics(yhat, test))\nicc &lt;- lapply(models, compute_ics)\n\ncomp_error &lt;- do.call(rbind.data.frame, fcomparison)\nrownames(comp_error) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\ncomp_ics &lt;- do.call(rbind.data.frame, icc)\nrownames(comp_ics) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\n\nA tabela abaixo mostra os critérios AIC e BIC para os três modelos. Em ambos os casos, o SARIMA(0, 1, 1)(0, 1, 1)[12] parece ser o escolhido. Este tipo de feliz coincidência não costuma acontecer frequentemente na prática, mas neste caso ambos os critérios apontam para o mesmo modelo.\n\ncomp_ics\n\n                    aic       bic\nAutoArima     -389.0155 -380.9970\nManual SARIMA -382.2807 -371.5894\nOLS           -342.2930 -303.2681\n\n\nNa comparação de medidas de erro, o SARIMA(0, 1, 1)(0, 1, 1)[12] realmente tem uma melhor performance, seguido pelo OLS e pelo SARIMA(1, 1, 1)(1, 1, 0)[12].\n\ncomp_error\n\n                    rmse        mae     mape      rmsse\nAutoArima     0.09593236 0.08959921 1.463477 0.03939512\nManual SARIMA 0.11688549 0.10780460 1.762201 0.04806577\nOLS           0.10333715 0.09384411 1.549261 0.04266214\n\n\nSerá que esta diferença é significante? Vamos comparar os modelos SARIMA. Pelo teste DM ela é sim. Lembre-se que o teste DM é, essencialmente, um teste Z de que \\(e_{1} - e_{2} = 0\\) ou \\(e_{1} = e_{2}\\), onde os valores são a média dos erros de previsão dos modelos.\n\nerrors &lt;- lapply(yhat, function(yhat) compute_error(yhat, test))\ne1 &lt;- errors[[1]]\ne2 &lt;- errors[[2]]\n\ndm.test(e1, e2, power = 2)\n\n\n    Diebold-Mariano Test\n\ndata:  e1e2\nDM = -4.4826, Forecast horizon = 1, Loss function power = 2, p-value =\n0.0001691\nalternative hypothesis: two.sided\n\n\nVale notar que o teste DM serve para comparar os erros de previsão de quaisquer modelos. Como o teste não faz qualquer hipótese sobre “de onde vem” os erros de previsão, ele pode ser utilizado livremente. Vale lembrar também que este teste não deve ser utilizado para escolher o melhor modelo, já que ele compara apenas a capacidade preditiva de dois modelos alternativos.\nOutro ponto, também complicado, é de qual a medida de erro que se deve escolher. O teste DM implicitamente usa o erro médio quadrático, mas há várias outras alternativas. Uma breve discussão pode ser vista aqui.\nPor fim, o gráfico abaixo mostra as previsões dos modelos alternativos contra a série real.\n\nplot(test, ylim = c(5.8, 6.5), col = \"#8ecae6\", lwd = 2, type = \"o\")\nlines(yhat_mean[[1]], col = \"#ffb703\")\nlines(yhat_mean[[2]], col = \"#fb8500\")\nlines(yhat_mean[[3]], col = \"#B86200\")\ngrid()\nlegend(\"topleft\", lty = 1,\n       legend = c(\"Test\", \"AutoArima\", \"Arima\", \"OLS\"),\n       col = c(\"#8ecae6\", \"#ffb703\", \"#fb8500\", \"#B86200\"))"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html",
    "href": "posts/general-posts/repost-ols-timeseries/index.html",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "",
    "text": "[Este post foi originalmente escrito no início de 2019. Muitos dos pacotes apresentados aqui evoluíram bastante, mas acredito que o post original ainda tenha bastante valor didático para quem está iniciando seus estudos em econometria e R.]\nOs cursos de econometria de séries de tempo, usualmente, começam pelo ensino de modelos lineares univariados para séries estacionárias. Estes modelos são da família ARMA e tentam representar uma série de tempo \\(y_{t}\\) em função de suas defasagens \\(y_{t-1}, y_{t-2}, \\dots, y_{t-n}\\) e de choques aleatórios (inovações) \\(\\epsilon_{t}, \\epsilon_{t-1}, \\epsilon_{t-2}, \\dots, y_{t-n}\\). Contudo, pode ser mais interessante relacionar duas séries de tempo \\(y_{t}\\) e \\(x_{t}\\) diferentes via um modelo linear. Em alguns casos isto pode ser equivalente a um VAR ou VARMA, mas o modelo linear tem a vantagem de ser mais simples de implementar e de interpretar. O tipo de modelo linear que estamos interessados é da forma\n\\[\n  y_{t} = \\beta_{0} + \\beta_{1}x_{t} + w_{t}\n\\]\nonde \\(y_{t}\\) é a série que queremos “explicar” em função da série \\(x_{t}\\). É evidente que podemos estender este modelo para incluir defasagens das variáveis \\(x_{t}\\) e \\(y_{t}\\), além de incluir outras séries, dummies, efeitos sazonais e tendências temporais.\nQuando se usa dados em forma de séries de tempo numa regressão linear, é bastante comum que se enfrente algum nível de autocorrelação nos resíduos. Uma das hipóteses do modelo “clássico” de regressão linear é de que as observações são i.i.d., isto é, que os dados são independentes e identicamente distribuídos. Isto obviamente não se aplica no contexto de séries de tempo (os dados não são independentes), então é preciso algum cuidado no uso de modelos de regressão linear. Neste sentido, o diagnósito dos resíduos é o mais importante passo para verificar a qualidade do modelo. Idealmente, os resíduos do modelo devem se comportar como ruído branco (i.e., não devem apresentar autocorrelação).\nOutro problema típico deste tipo de análise, chamado de “regressão espúria”, acontece quando se faz a regressão de séries não-estacionárias. Quaisquer duas séries com tendência serão fortemente linearmente relacionadas. Isto leva a uma regressão com \\(R^2\\) altíssimo e estatísticas-t muito significativas e a vários modelos sem sentido. Exemplos disto podem ser vistos neste site (em inglês).\nAinda assim, há casos em que podemos utilizar estas regressões para encontrar relações úteis. Em particular, se as séries forem cointegradas podemos usar uma metodologia Engle-Granger. Esta abordagem não será discutida neste post."
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-simples",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-simples",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo simples",
    "text": "Exemplo simples\nUm modelo para explicar \\(y\\) em função de seu valor defasado em um período, do valor contemporâneo de \\(x\\) e do valor defasado de \\(x\\) em um período.\n\\[\n  y_{t} = \\beta_{0} + \\beta_{1}y_{t - 1} + \\beta_{2}x_{t} + \\beta_{3}x_{t - 1} + w_{t}\n\\]\nNote que o modelo acima não seria muito útil para gerar previsões de \\(y_{t + 1}\\) pois ele exigiria conhecimento de \\(x_{t + 1}\\). Então, seria necessário primeiro prever o valor de \\(x_{t + 1}\\) para computar uma estimativa para \\(y_{t + 1}\\).\n\\[\n\\mathbb{E}(y_{t + 1} | \\mathbb{I}_{t}) = \\beta_{0} + \\beta_{1}y_{t} + \\beta_{2}x_{t + 1} + \\beta_{3}x_{t}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-índice-de-produção-industrial",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-índice-de-produção-industrial",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: Índice de Produção Industrial",
    "text": "Exemplo: Índice de Produção Industrial\nPara o exemplo abaixo uso o pacote GetBCBData para carregar a série do Índice de Produção Industrial (IPI).\n\n\nCode\n# Baixa os dados\nipi &lt;- gbcbd_get_series(21859, first.date = as.Date(\"2002-01-01\"))\n# Converte a série para ts\nprod &lt;- ts(ipi$value, start = c(2002, 01), frequency = 12)\n# Gráfico da série\nautoplot(prod) +\n  labs(title = \"Índice de Produção Industiral - geral (2012 = 100)\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nPode-se visualizar a relação linear entre valores correntes e defasados do IPI usando a função lag.plot. Na imagem abaixo, cada quadrado mostra um gráfico de dispersão dos valores do IPI em \\(t\\) contra os valores do IPI em \\(t-k\\). Alguns lags parecem não exibir muita relação como o lag 6. Já o primeiro e último lag parecem apresentar uma relação linear mais acentuada.\n\n\nCode\ngglagplot(prod, 12, do.lines = FALSE, colour = FALSE) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nComo exemplo, podemos propor o modelo abaixo para o IPI. A escolha dos lags aqui foi um tanto arbitrária e há métodos mais apropriados para escolhê-los.\n\\[\n  IPI_{t} = \\beta_{0} + \\beta_{1}IPI_{t - 1} + \\beta_{2}IPI_{t - 2} + \\beta_{3}IPI_{t - 4} + \\beta_{2}IPI_{t - 12}\n\\]\nPara estimar este modelo no R há um pequeno inconveniente da função lag que, na verdade, funciona como um operador foreward. Além disso, agora a função base lm (e por conseguinte, também a função tslm) se prova um tanto inconveniente, pois ela não funciona bem com variáveis defasadas. Para usar a função lm seria necessário primeiro “emparelhar” as diferentes defasagens da série, isto é, seria necessário criar um data.frame (ou ts) em que cada coluna mostra os valores das defasagens escolhidas. Por motivo de completude, deixo um código de exemplo que faz isto. Na prática, vale mais a pena escolher alguma outra função como dynlm::dynlm ou dyn::dyn$lm. O código abaixo usa o forecast::tslm, mas nos exemplos seguintes uso o dyn::dyn$lm.\n\n\nCode\n# Exemplo usando forecast::tslm (tb funcionaria com stats::lm)\ndf &lt;- ts.intersect(\n  prod, lag(prod, -1), lag(prod, -2), lag(prod, -4), lag(prod, -12),\n  dframe = TRUE\n  )\n\nfit &lt;- tslm(prod ~ ., data = df)\n\n\nPode-se contornar o problema da função lag definindo uma nova função, L, que funciona da maneira desejada. O código abaixo estima a regressão usando dyn::dyn$lm. A sintaxe dentro da função é praticamente idêntica à que vimos acima com as funções lm e tslm.\n\n\nCode\n# Define uma função L\nL &lt;- function(x, k) {lag(x, -k)}\nfit &lt;- dyn$lm(prod ~ L(prod, 1) + L(prod, 2) + L(prod, 4) + L(prod, 12))\n\n\nOs resultados da regressão acima estão resumidos na tabela abaixo.\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nipi\n\n\n\n\n\n\n\n\nipi [1]\n\n\n0.442***\n\n\n\n\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\nipi [4]\n\n\n0.139**\n\n\n\n\n\n\n(0.062)\n\n\n\n\n\n\n\n\n\n\nipi [8]\n\n\n-0.114***\n\n\n\n\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\nipi [12]\n\n\n0.446***\n\n\n\n\n\n\n(0.040)\n\n\n\n\n\n\n\n\n\n\nconstante\n\n\n9.478**\n\n\n\n\n\n\n(3.960)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n245\n\n\n\n\nR2\n\n\n0.783\n\n\n\n\nAdjusted R2\n\n\n0.779\n\n\n\n\nResidual Std. Error\n\n\n5.304 (df = 240)\n\n\n\n\nF Statistic\n\n\n216.286*** (df = 4; 240)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nPodemos combinar a informação de outros indicadores industriais para adicionar informação potencialmente relevante para nossa regressão. Neste exemplo, uso outros indicadores industriais para encontrar aqueles que “ajudam a explicar” o indicador geral.\nO código abaixo importa uma série de indicadores industriais e junta todos eles num único objeto ts.\nOs códigos utilizados são diretamente copiados do sistema de séries temporais do BCB.\n\n\nCode\n# Codigos das series do BACEN\ncodigos_series = c(21859, 21861:21868)\n# Vetor com nomes para facilitar o uso dos dados\nnomes = c(\n  \"geral\", \"extrativa_mineral\", \"transformacao\", \"capital\", \"intermediarios\",\n  \"consumo\", \"consumo_duraveis\", \"semiduraveis_e_nao_duraveis\",\n  \"insumos_da_construcao_civil\"\n  )\n# Junta estes dados num data.frame que serve de dicionário (metadata)\ndicionario &lt;- data.frame(id.num = codigos_series, nome_serie = nomes)\n\n# Baixa todas as series\nseries &lt;- gbcbd_get_series(codigos_series, first.date = as.Date(\"2002-01-01\"))\n# Junta as séries com o dicionário\nseries &lt;- merge(series, dicionario, by = \"id.num\")\n# Converte para wide usando os nomes do dicionario como nome das colunas\nseries_wide &lt;- reshape2::dcast(series, ref.date ~ nome_serie, value.var = \"value\")\n# Convert para ts\nseries &lt;- ts(as.matrix(series_wide[, -1]), start = c(2002, 1), frequency = 12)\n\n# Visualizar todas as series\nautoplot(series) +\n  facet_wrap(vars(series)) +\n  scale_color_viridis_d() +\n  theme_light() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFeito o trabalho de importação dos dados podemos propor um modelo simples que toma o valor defasado das variáveis. Novamente a escolha das defasagens e dos regressores foi completamente arbitrária. O modelo estimado usa defasagens de outras séries para modelar o comportamento da série do índice de produção da indústria de tranformação\n\\[\n  Transf_{t} = \\beta_{0} + \\beta_{1}Durav_{t - 1} + \\beta_{2}Durav_{t - 12} + \\beta_{3}Capital_{t - 1} + \\beta_{4}Capital_{t - 6} + \\beta_{5}Transf_{t - 1} + \\alpha_{1}t + \\sum_{k = 2}^{12}\\alpha_{k}d_{k}\n\\]\n\n\nCode\nfit &lt;- dyn$lm(\n  transformacao ~ L(consumo_duraveis, 1) + L(consumo_duraveis, 12) +\n                  L(capital, 1) + L(capital, 6) +\n                  L(intermediarios, 12) +\n                  L(transformacao, 1) +\n                  time(transformacao) + as.factor(cycle(transformacao)),\n  data = series\n  )\n\nautoplot(series[, 1]) +\n  autolayer(fitted(fit)) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nOlhando apenas para as observações mais recentes vemos que, com exceção do período da pandemia, o ajuste aos dados é relativamente satisfatório.\n\n\nCode\n# Filtra apenas as observações mais recentes, após jan/2015\nprod_recente &lt;- window(prod, start = c(2015, 1))\n# Reestima o modelo\nsummary(fit &lt;- tslm(prod_recente ~ trend + season))\n\n\n\nCall:\ntslm(formula = prod_recente ~ trend + season)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.8428  -1.8730   0.5559   2.3254   8.8825 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 95.60468    1.73833  54.998  &lt; 2e-16 ***\ntrend       -0.04273    0.01593  -2.682 0.008745 ** \nseason2     -2.56839    2.19658  -1.169 0.245455    \nseason3      5.54101    2.19676   2.522 0.013456 *  \nseason4      0.17262    2.19705   0.079 0.937553    \nseason5      7.93757    2.19745   3.612 0.000505 ***\nseason6      8.27116    2.26418   3.653 0.000440 ***\nseason7     14.48889    2.26413   6.399 7.35e-09 ***\nseason8     17.80661    2.26418   7.864 8.76e-12 ***\nseason9     14.04934    2.26435   6.205 1.75e-08 ***\nseason10    16.35457    2.26463   7.222 1.75e-10 ***\nseason11     9.44729    2.26502   4.171 7.09e-05 ***\nseason12    -1.25998    2.26553  -0.556 0.579517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.66 on 88 degrees of freedom\nMultiple R-squared:  0.722, Adjusted R-squared:  0.6841 \nF-statistic: 19.05 on 12 and 88 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nautoplot(prod_recente) +\n  autolayer(fitted(fit)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-tendência-e-sazonalidade",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-tendência-e-sazonalidade",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: tendência e sazonalidade",
    "text": "Exemplo: tendência e sazonalidade\nÉ relativamente simples prever os valores futuros de modelos de tendência e sazonalidade determinísticas. Como o adjetivo “determinístico” sugere sabe-se de antemão todos os valores que esta série vai exibir. O exemplo abaixo estima um modelo simples para a demanda por passagens aéreas (voos internacionais).\nVale notar que não se costuma fazer previsões de longo prazo com este tipo de modelo, pois a hipótese de que a sazonalidade/tendência continua exatamente igual ao longo do tempo vai se tornando cada vez mais frágil. A curto prazo, contudo, pode ser razoável supor que este modelo linear simples ofereça uma boa aproximação da realidade.\n\n\nCode\nfit &lt;- tslm(AirPassengers ~ trend + season)\n\nautoplot(forecast(fit, h = 24), include = 24) +\n  theme_light()"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-previsão-de-cenário",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-previsão-de-cenário",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: previsão de cenário",
    "text": "Exemplo: previsão de cenário\nNo caso da regressão acima do IPCA, pode-se estimar o impacto de uma nova greve dos caminhoneiros. [Mal sabiamos que em 2020 teriamos um evento extraordinário…].\n\n\nCode\ndummies = cbind(greve_caminhao, greve_2013, precos_adm)\nfit &lt;- Arima(ipca, order = c(1, 0, 0), xreg = coredata(dummies))\ngreve_caminhao_2020 = c(rep(0, 9), 1, 0, 0)\nnovo_xreg = cbind(greve_caminhao = greve_caminhao_2020, greve_2013 = rep(0, 12), precos_adm = rep(0, 12))\n\nautoplot(forecast(fit, xreg = novo_xreg), include = 20) +\n  theme_light()"
  },
  {
    "objectID": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-variáveis-defasadas",
    "href": "posts/general-posts/repost-ols-timeseries/index.html#exemplo-variáveis-defasadas",
    "title": "Regressão Linear com Séries de Tempo",
    "section": "Exemplo: variáveis defasadas",
    "text": "Exemplo: variáveis defasadas\nPode ser um tanto difícil fazer previsões com modelos que usam informação de outras séries. Num modelo simples como \\(y_{t} = \\beta_{0} + \\beta_{1}x_{t - 1}\\) para prever valores futuros de \\(y_{t}\\) é preciso fazer previsõs para a série \\(x_{t}\\), pois, \\(y_{t + 2}\\) é função linear de \\(x_{t + 1}\\). Há muitas maneiras de abordar este problema e eu provavelmente vou discutir mais sobre as alternativas num post futuro. O exemplo abaixo mostra como usar informação disponível de outras séries\n\n\nCode\ndf_ajustada &lt;-\n  ts.intersect(transf = series[, \"transformacao\"],\n               lag(series[, \"consumo_duraveis\"], -1),\n               lag(series[, \"consumo_duraveis\"], -12),\n               lag(series[, \"capital\"], -1),\n               lag(series[, \"capital\"], -6),\n               lag(series[, \"intermediarios\"], -12),\n               lag(series[, \"transformacao\"], -1),\n               dframe = TRUE\n               )\nfit &lt;- tslm(transf ~ ., data = df_ajustada)\nsub &lt;- df_ajustada[(length(df_ajustada[, \"transf\"]) - 12):length(df_ajustada[, \"transf\"]), ]\n\n\n\n\nCode\nautoplot(forecast(fit, sub), include = 36) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nPode-se, como de costume, separar os dados em train e test para avaliar a qualidade das previsões dentro da amostra."
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html",
    "href": "posts/general-posts/definindo-objetos/index.html",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Há dois operadores para definir um objeto no R: = e &lt;-. A maior parte dos usuários parece preferir o último apesar dele parecer um tanto inconveniente. Em teclados antigos, havia uma tecla específica com o símbolo &lt;-, mas em teclados ABNT modernos ele exige três teclas para ser escrito.\nPara contornar este incômodo é comum criar um atalho no teclado para esse símbolo; o RStudio, por exemplo, tem um atalho usando a teclas Alt e - em conjunto. Mas ainda assim fica a questão: por que não utilizar o =? A resposta curta é que o símbolo &lt;- é a melhor e mais consistente forma de definir objetos R. Na prática, contudo, há poucas diferenças entre as expressões e elas dificilmente vão fazer alguma diferença. Podemos começar com um exemplo bastante simples para entender estas diferenças.\n\n\nO código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1\n\n\n\n\n\nEu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%.\n\n\n\nNo geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html#qual-a-diferença",
    "href": "posts/general-posts/definindo-objetos/index.html#qual-a-diferença",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "O código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1"
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html#mais-um-adendo",
    "href": "posts/general-posts/definindo-objetos/index.html#mais-um-adendo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Eu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%."
  },
  {
    "objectID": "posts/general-posts/definindo-objetos/index.html#resumo",
    "href": "posts/general-posts/definindo-objetos/index.html#resumo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "No geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html",
    "title": "Visualizando uma única variável",
    "section": "",
    "text": "Há algum tempo atrás tive o seguinte problema: como visualizar várias observações de uma única variável numérica num gráfico? Tentei algumas soluções óbvias, mas nenhuma pareceu funcionar muito bem. Neste post junto algumas das minhas tentativas.\nOs dados provêm do Mapa da Desigualdade 2019. Neste site pode-se baixar todos os dados além de baixar o relatório completo que apresenta informações socioeconômicas atualizadas para todos os distritos de São Paulo, compilando e sistematizando dados de diversas fontes públicas."
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#histograma",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#histograma",
    "title": "Visualizando uma única variável",
    "section": "Histograma",
    "text": "Histograma\nUma solução bastante clássica seria de fazer um histograma. Neste tipo de gráfico a dispersão fica clara, mas pode ser difícil de dar destaque para distritos específicos. O histograma conta a frequência de observações dentro de janelas de tamanho fixo. A princípio, a única dificuldade em usar o histograma está em definir a amplitude de cada um destes intervalos, mas, na prática, este problema não costuma ser muito complexo.\nAinda que o gráfico seja comumente usado, ele não é tão popular, sendo raramente visto em publicações de jornal, por exemplo.\n\n\nCode\nggplot(df, aes(x = expec_vida)) +\n  geom_histogram(bins = 11, colour = \"white\", fill = \"#08519c\") +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    data = df_aux_label,\n    aes(x = expec_vida,\n        y = c(15, 17, 19, 16, 12),\n        label = stringr::str_wrap(label_distrito, 5)),\n    colour = \"gray25\",\n    family = \"Roboto Light\",\n    size = 4\n  ) +\n  geom_segment(\n    data = df_aux_label,\n    aes(x = expec_vida,\n        xend = expec_vida,\n        y = 0,\n        yend = c(15, 17, 19, 15, 12) - 0.5),\n    colour = \"gray70\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 20, 4)) +\n  scale_x_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"Anos de idade\",\n    y = \"\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022.\"\n  ) +\n  theme_vini +\n  theme(panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-colunas",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-colunas",
    "title": "Visualizando uma única variável",
    "section": "Gráfico de colunas",
    "text": "Gráfico de colunas\nUm simples gráfico de colunas também poderia ser uma alternativa. Neste caso, como há muitas observações (distritos) diferentes, o gráfico acaba sobrecarregado e confuso.\n\n\nCode\nordered_df &lt;- df %&gt;%\n  mutate(\n    label_distrito = factor(label_distrito),\n    label_distrito = forcats::fct_reorder(label_distrito, expec_vida)\n  )\n\nggplot(ordered_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_col(colour = \"white\", fill = \"#08519c\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"Anos de idade\",\n    y = \"\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022.\"\n  ) +\n  theme_vini +\n  theme(\n    text = element_text(size = 6),\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    panel.grid.major.y = element_blank()\n    )\n\n\n\n\n\n\n\n\n\nOutra saída seria dividir os dados em grupos menores (e.g. alta, média-alta, média-baixa, baixa) e usar a função facet_wrap. O lado negativo disto, além de tornar o código mais complexo, é de acrescentar divisões nos dados que eventualmente são muito artificiais.\nAqui eu faço um divisão por quintil e crio uma denominação um tanto arbitrária para permitir a leitura dos dados.\n\n\nCode\nxl &lt;- c(\"Baixo\", \"Médio-Baixo\", \"Médio\", \"Médio-Alto\", \"Alto\")\n\nquintile_df &lt;- ordered_df %&gt;%\n  mutate(\n    life_group = ntile(expec_vida, 5),\n    life_group = factor(life_group, labels = xl)\n    )\n\nggplot(quintile_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_col(colour = \"white\", fill = \"#08519c\") +\n  coord_flip() +\n  facet_wrap(vars(life_group), scales = \"free_y\") +\n  labs(x = NULL, y = NULL, title = \"Expectativa de Vida\") +\n  theme_vini +\n  theme(\n    text = element_text(size = 8),\n    strip.text = element_text(size = 10),\n    panel.grid.major.y = element_blank()\n    )"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#lolipop",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#lolipop",
    "title": "Visualizando uma única variável",
    "section": "Lolipop",
    "text": "Lolipop\nEste tipo de gráfico vem ganhando espaço mesmo em veículos de mídia populares por ser bastante simples. Ele é mais comumente usado para mostrar a evolução de uma variável em dois momentos do tempo, mas também pode-se usá-lo analogamente a um gráfico de colunas.\nInfelizmente, neste exemplo, ele vai sofrer do mesmo problema que o gráfico de colunas. A título de exemplo faço um gráfico deste estilo apenas para os distritos com as maiores e menores expectativas de vida.\n\n\nCode\n# Ordena a base de dados pela expectativa de vida\nordered_df &lt;- arrange(ordered_df, expec_vida)\n# Cria uma tabela com as primeiras 5 e últimas 5 linhas\ntop_df &lt;- rbind(head(ordered_df, 5), tail(ordered_df, 5))\n\nggplot(top_df, aes(x = label_distrito, y = expec_vida)) +\n  geom_segment(aes(xend = label_distrito, yend = 55)) +\n  geom_point(colour = \"black\", fill = \"#08519c\", size = 4, shape = 21) +\n  coord_flip() +\n    labs(\n      y = \"Anos de vida\",\n      x = NULL,\n      title = \"Expectativa de vida\",\n      subtitle = \"Idade média ao morrer em 2022. Dados apenas dos distritos\\ncom maiores e menores expectativas de vida.\",\n      caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\"\n    ) +\n  theme_vini +\n  theme(panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-dispersão",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#gráfico-de-dispersão",
    "title": "Visualizando uma única variável",
    "section": "Gráfico de dispersão",
    "text": "Gráfico de dispersão\nGráficos de dispersão apresentam a relação entre duas variáveis. Neste caso, podemos fazer alguns truques para inventar uma variável falsa que serve somente para que o R faça o gráfico.\nUma alternativa seria impor um expec_vida constante e arbitrário para uma das variáveis. Neste caso escolho x = 1 e escondo o eixo. No gráfico abaixo, cada distrito é um ponto sobre uma mesma linha. Para amenizar a sobreposição de observações uso alpha = 0.5 que acrescenta um pouco de transparência nas observações.\n\n\nCode\ndf_aux &lt;- tibble(\n  x = 1.025,\n  y = df_media$expec_vida,\n  label = paste0(\"Média = \", round(y, 1))\n)\n\nggplot(df, aes(x = 1, y = expec_vida)) +\n  geom_vline(xintercept = 1, colour = \"gray60\", alpha = 0.5) +\n  geom_point(\n    aes(colour = highlight, alpha = highlight, size = populacao_total),\n    shape = 21,\n    fill = \"#08519c\",\n  ) +\n  geom_text(\n    data = df_aux_label,\n    aes(x = c(0.95, 0.95, 1.05, 1.05, 1.05), y = expec_vida, label = label_distrito),\n    family = \"Roboto Light\",\n    size = 4\n  ) +\n  geom_segment(\n    data = df_aux_label,\n    colour = \"gray25\",\n    aes(\n      x = 1, xend = c(0.955, 0.955, 1.045, 1.045, 1.045),\n      y = expec_vida, yend = expec_vida\n    )\n  ) +\n  geom_segment(\n    data = tibble(x = 1, xend = 1.02, y = df_media$expec_vida, yend = y),\n    aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"gray25\",\n  ) +\n  geom_text(\n    data = df_aux,\n    aes(x = x, y = y, label = label),\n    family = \"Roboto Light\",\n    size = 3\n  ) +\n  coord_flip() +\n  scale_x_continuous(limits = c(0.94, 1.06)) +\n  scale_y_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  scale_alpha_manual(values = c(0.45, 0.8)) +\n  scale_size_continuous(range = c(1, 7.5)) + \n  scale_colour_manual(values = c(\"#08519c\", \"black\")) +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = \"\",\n    y = \"Anos de idade\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\",\n    subtitle = \"Idade média ao morrer em 2022. Cada ponto representa um distrito de São Paulo.\"\n  ) +\n  theme_vini +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nUma solucão alternativa seria criar uma variável aleatória qualquer para servir como a variável “falsa”. Estranhamente, cria-se uma sensação melhor de dispersão e não há mais o problema de sobreposição. Contudo, o gráfico pode ser bastante confuso, pois dá a entender que estamos vendo a relação entre duas variáveis distintas, quando uma delas, na verdade, não representa coisa alguma.\nAinda assim, ideias similares foram implementadas pelo portal Nexo nesta postagem.\n\n\nCode\n# Cria uma variável aleatória qualquer seguindo uma Gaussiana\ndf &lt;- df %&gt;% mutate(x = rnorm(nrow(.)))\n\n# Para destacar o expec_vida médio\ndf_aux &lt;- tibble(\n  x = 2.25,\n  y = df_media$expec_vida + 0.75,\n  label = paste(\"Média =\", round(y - 0.75, 1))\n)\n\ndf_aux_label &lt;- df %&gt;%\n  mutate(label_distrito = if_else(highlight == 1L, label_distrito, \"\"))\n\nggplot(df, aes(x = x, y = expec_vida)) +\n  geom_jitter(aes(alpha = highlight, size = populacao_total),\n    shape = 21,\n    fill = \"#08519c\"\n  ) +\n  ggrepel::geom_text_repel(\n    data = df_aux_label,\n    aes(label = label_distrito),\n    force = 5,\n    family = \"Roboto Light\",\n    size = 3\n  ) +\n  geom_text(\n    data = df_aux,\n    aes(x = x, y = y, label = label),\n    hjust = -0.15\n  ) +\n  geom_hline(aes(yintercept = mean(expec_vida)), colour = \"gray70\", size = 1) +\n  coord_flip() +\n  scale_alpha_manual(values = c(0.65, 1)) +\n  scale_y_continuous(limits = c(min(df$expec_vida) - 2, max(df$expec_vida) + 2)) +\n  scale_x_continuous(limits = c(-3, 3)) +\n  labs(\n    title = \"Expectativa de Vida (idade média ao morrer)\",\n    x = \"\",\n    y = \"Anos de idade\",\n    caption = \"Fonte: SMS (Secretaria Municipal de Saúde)\"\n  ) +\n  guides(colour = FALSE, alpha = FALSE) +\n  theme_vini +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-mapa-desigualdade/index.html#mapa",
    "href": "posts/general-posts/repost-mapa-desigualdade/index.html#mapa",
    "title": "Visualizando uma única variável",
    "section": "Mapa",
    "text": "Mapa\nDeixei a visualização mais óbvia para o final. Como a variável está distribuída espacialmente, pode-se fazer um simples mapa de São Paulo separado por distritos.\nAqui, eu uso o shapefile de distritos da Pesquisa Origem e Destino. Apesar de não haver um identificador comum entre as bases, foi relativamente simples fazer o join compatibilizando o nome dos distritos.\n\n\nCode\n# Importa o shapefile\ndistritos &lt;- st_read(\n  here::here(\"posts\", \"general-posts\", \"repost-mapa-desigualdade\", \"districts.gpkg\"),\n  quiet = TRUE\n  )\n\ndistritos &lt;- distritos %&gt;%\n  # Filtra apenas distritos de SP (capital)\n  filter(code_district &lt;= 96) %&gt;%\n  # Renomeia a coluna para facilitar o join\n  rename(distrito = name_district)\n\n# Verifica se ha distritos com nome diferente (Mooca)\nanti &lt;- anti_join(select(df, distrito), distritos, by = \"distrito\")\n# Altera a grafia para garantir o join\ndf &lt;- df %&gt;%\n  mutate(\n    distrito = if_else(distrito == \"Moóca\", \"Mooca\", distrito)\n  )\n# Junta os dados no sf\ndistritos &lt;- left_join(distritos, df, by = \"distrito\")\n\n\nO gráfico abaixo mostra a expectativa de vida em cada distrito na cidade.\n\n\nCode\nggplot(distritos) +\n  geom_sf(aes(fill = expec_vida), linewidth = 0.1) +\n  scale_fill_viridis_c(name = \"Expectativa\\nde Vida\") +\n  ggtitle(\"Expectativa de Vida por Distrito\") +\n  theme_void() +\n  theme(\n    legend.title = element_text(hjust = 0.5, size = 10),\n    legend.position = c(0.8, 0.3)\n  )\n\n\n\n\n\n\n\n\n\nOutra maneira de apresentar este dado é agrupando-o de alguma forma. Eu sou particularmente parcial ao algoritmo de Jenks.\n\n\nCode\n# Encontra os intervalos de cada grupo\nbreaks &lt;- classInt::classIntervals(distritos$expec_vida, n = 7, style = \"jenks\")\n# Classifica os valores em grupos\ndistritos &lt;- distritos %&gt;%\n  mutate(\n    jenks_group = cut(expec_vida, breaks = breaks$brks, include.lowest = TRUE)\n  )\n\nggplot(distritos, aes(fill = jenks_group)) +\n  geom_sf(linewidth = 0.1, color = \"gray80\") +\n  scale_fill_brewer(palette = \"BrBG\", name = \"Expectativa\\nde Vida\") +\n  ggtitle(\"Expectativa de Vida por Distrito\") +\n  theme_void() +\n  theme(\n    legend.title = element_text(hjust = 0.5, size = 10),\n    legend.position = c(0.8, 0.3)\n  )"
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html",
    "href": "posts/general-posts/tutorial-showtext/index.html",
    "title": "Usando fontes com showtext no R",
    "section": "",
    "text": "Criar boas visualizações é parte importante de qualquer análise de dados.\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar e o mesmo vale para visualizações com dados. A fonte do texto ajuda a transmitir informação e pode comunicar, por exemplo, maior sobriedade, profissionalismo, etc.\nO pacote showtext, desenvolvido por yixuan, facilita a importação e o uso de fontes em gráficos no R. O pacote funciona com uma variedade de extensões de fontes, não sendo limitado como o extrafont, por exemplo, a arquivos .ttf."
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html#base-r",
    "href": "posts/general-posts/tutorial-showtext/index.html#base-r",
    "title": "Usando fontes com showtext no R",
    "section": "Base R",
    "text": "Base R\nPara modificar a fonte dos elementos textuais dos gráficos feitos com o plot() é preciso ajustar o argumento family. Usando as funções base do R, este argumento aparece dentro da função par (que configura vários parâmetros dos gráficos).\nO código abaixo mostra como trocar a fonte do gráfico.\n\n# Define a fonte padrão do gráfico\npar(family = \"alice\")\n# Monta um scatter plot de exemplo\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30)\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nNote que todos os objetos textuais (título, legenda, etc.) são convertidos para a mesma fonte. Caso se queira fontes diferentes para estes elementos é preciso especificá-los adequadamente. Por exemplo, para trocar somente a fonte do título\ntitle(\"nome_do_titulo\", family = \"nome_fonte\")\n\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\", family = \"RobCond\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30, family = \"Montserrat\")\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nVale notar que, uma vez definida a fonte usando a função par, todos os gráficos subsequentes vão usar esta fonte. Para trocar a fonte é preciso usar a função par novamente. Além da fonte também é possível trocar a ênfase (e.g. face = c(\"bold\", \"italic\")) e também o tamanho da letra (e.g. cex.axis = 1.5, cex.main = 2)."
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html#ggplot2",
    "href": "posts/general-posts/tutorial-showtext/index.html#ggplot2",
    "title": "Usando fontes com showtext no R",
    "section": "ggplot2",
    "text": "ggplot2\nTambém é possível trocar a fonte de gráficos feitos com outros pacotes, como o ggplot2. O exemplo abaixo monta um gráfico similar ao que foi feito acima. Modifica-se a fonte dentro da função theme. Esta função é um tanto particular, então vale a pena discorrer um pouco sobre ela. Ela é basicamente usada para modificar elementos do gráfico. Há quatro elementos principais, dos quais só nos interessa um: o element_text. São seis os principais elementos textuais que pode-se modificar:\n\naxis.text - texto dos eixos (em geral, os números do eixo);\naxis.title - nome do eixo (e.g. “Milhas por galão” no exemplo acima);\nlegend.text - texto da legenda;\nlegend.title - título da legenda;\nplot.title - título do gráfico;\ntext - todos os acima.\n\nPode-se ser mais específico com o texto dos eixos usando axis.text.x e axis.text.y, por exemplo. O último dos elementos listados acima funciona como um “coringa”, ele serve para modificar de uma vez só todos os elementos textuais de um gráfico. No exemplo abaixo modifico somente o text.\n\nlibrary(ggplot2)\n\np &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE) +\n  labs(\n    x = \"Peso (ton.)\",\n    y = \"Milhas por galão\",\n    title = \"Eficiência e peso de carros\",\n    subtitle = \"Regressão entre o peso de diferentes carros e sua eficiência energética\",\n    caption = \"Fonte: Motor Trend US Magazine 1974\"\n    )\n\n\np + theme(text = element_text(family = \"Montserrat\", size = 10))\n\n\n\n\n\n\n\n\nO próximo exemplo mostra como modificar alguns dos diferentes elementos do gráfico. Aproveito a variável cyl (cilindradas) para diferenciar os carros em três grupos para que o gráfico agora tenha uma legenda.\n\np +\n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme(\n    # Modifica o texto (números) dos eixos x e y\n    axis.text = element_text(family = \"alice\"),\n    # Modifica o título do eixo (i.e. Milhas por galão)\n    axis.title = element_text(family = \"Montserrat\"),\n    # Modifica o título da legenda (Cilindros)\n    legend.title = element_text(family = \"HelveticaNeue\"),\n    # Modifica o texto da legenda (i.e. 4, 6, 8)\n    legend.text = element_text(family = \"HelveticaNeue\"),\n    # Modifica o título do gráfico\n    plot.title = element_text(family = \"RobCond\", size = 20)\n    )\n\n\n\n\n\n\n\n\nTalvez o jeito mais sensato de usar fontes com o ggplot2 seja primeiro especificar uma fonte padrão para o gráfico usando text e depois calibrar as exceções. Os elementos textuais como axis.title e legend.text copiam as propriedades definidas em text.\nNo exemplo abaixo defino que todos os elementos textuais ser escritos em Arial simples em tamanho 12 na cor \"gray20\". Depois disso defino que o título deve ter mais destaque com Arial em negrito (bold) num tamanho maior e numa cor mais escura. Por fim, defino que o rodapé do gráfico seja escrito em fonte menor e em itálico.\n\ntheme_custom &lt;- theme_light() +\n  theme(\n    # Modifica todos os elementos textuais do gráfico\n    text = element_text(family = \"Arial\", size = 12, color = \"gray20\"),\n    # Modifica apenas o título\n    plot.title = element_text(face = \"bold\", size = 14, color = \"gray10\"),\n    # Modifica apenas a nota no rodapé\n    plot.caption = element_text(face = \"italic\", size = 8)\n  )\n\np + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n\n\n\n\n\n\n\nPor último, também pode ser interessante usar fontes diferentes para representar dados diferentes. Isto é possível usando o argumento family dentro do aes. Da mesma forma, seria possível também representar grupos de dados diferentes com tamanhos de fontes diferentes ou mesmo destacar algum grupo específico com itálico.\n\nnomes &lt;- row.names(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_text(aes(label = nomes, family = c(\"Arial\", \"alice\", \"Montserrat\")[cyl]))"
  },
  {
    "objectID": "posts/general-posts/tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "href": "posts/general-posts/tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "title": "Usando fontes com showtext no R",
    "section": "Anexo: problemas com DPI e RMarkdown",
    "text": "Anexo: problemas com DPI e RMarkdown\nApesar de muito conveniente, o showtext não é inteiramente desprovido de problemas. Dois problemas que enfrento com alguma recorrência são diferenças de DPI na hora de exportar gráficos e problemas com RMarkdown.\nO problema com o RMarkdown é mais simples. Em versões antigas do RMarkdown e do showtext era necessário adicionar um argumento fig.showtext = TRUE em todos os chunks em que um gráfico usando showtext fosse renderizado. Alternativamente, podia-se modificar esta opção globalmente inserido o seguinte código no início do documento RMarkdown.\n\nknitr::opts_chunk$set(\n  fig.showtext = TRUE,\n  fig.retina = 1\n  )\n\nAcredito, mas não tenho certeza, de que este problema sumiu em versões mais recentes dos pacotes, pois com frequência eu esqueço de adicionar estes argumentos mas não encontro problemas na prática.\nO problema com o DPI na hora de exportar gráficos é mais complicado. Por problemas de DPI quero dizer quando o showtext “desenha” o texto num DPI diferente do ggplot2. O resultado é que o texto fica ou grande ou pequeno demais. Por padrão o showtext utiliza o DPI em 96.\nVamos montar um gráfico para ilustrar o problema.\n\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exportar o gráfico em alta resolução\nggsave(\"meu_grafico.jpeg\", grafico, dpi = 300)\n\nQuando vamos abrir o arquivo que foi exportado temos o resultado abaixo.\n\nComo a imagem foi salva com DPI mais elevado o texto fica menor do que deveria; em casos mais extremos o texto fica minúsculo a ponto de ser ilegível. Há duas formas de tentar contornar este problema: (1) reduzir o DPI dentro de ggsave; ou (2) modificar o DPI do showtext.\nVamos tentar a primeira solução: modificar o ggsave para o DPI padrão do showtext. Agora o texto está maior mas a proporção dos elementos está péssima! O resultado é pior do que o problema inicial.\n\n# Exportar a imagem num dpi menor\nggsave(\"meu_grafico_96.jpeg\", grafico, dpi = 96)\n\n\n\n\n\n\nA segunda solução é modificar as opções internas do showtext. Isto é bastante simples e pode ser feito com o showtext_opts(dpi = 300) e chamando novamente a função showtext_auto().\n\n# Ajusta o DPI do showtext\nshowtext_opts(dpi = 300)\n# \"Ativa\" o showtext novamente\nshowtext_auto()\n\n# Refaz o gráfico (isto é importante!)\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exporta um novo gráfico\nggsave(\"meu_grafico_300.jpeg\", grafico, dpi = 300)\n\nAgora o tamanho do texto está correto e a imagem como um todo está em alta resolução. Um problema é que a imagem ficou bastante grande, mas isto pode ser ajustado variando os argumentos width e height da função ggsave.\n\nVale notar que, a depender do seu sistema operacional, modificar o DPI padrão do showtext pode distorcer os gráficos dentro do R ou RStudio. Na prática o melhor workflow pode ser de modificar o DPI do showtext apenas no momento de exportar os gráficos."
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "",
    "text": "Este é um repost antigo que fiz ainda na época do mestrado em economia. Apesar de intuitivo o código dos loops abaixo pode ser muito ineficiente. De maneira geral, for-loops são melhores do que loops feitos com repeat; melhor ainda é montar funções e usar parallel::mclapply ou furrr::future_map. Vale notar que é sempre bom pré-alocar (ou pré-definir) vetores antes de um for-loop\n\n# Bom\nx &lt;- vector(\"numeric\", 1000)\n# Ruim\nx &lt;- c()"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html#lei-dos-grande-números",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html#lei-dos-grande-números",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "Lei dos Grande Números",
    "text": "Lei dos Grande Números\nA Lei dos Grandes Números (LGN) é um resultado assintótico bastante utilizado em econometria. Numa definição informal, a LGN nos diz que uma média amostral converge para para a média verdadeira dos dados. Isto é, se temos uma sequência de variáveis aleatórias \\(x_{1}, x_{2}, \\dots , x_{n}\\) independentes e identicamente distribuídas:\n\\[\\begin{equation}\n  \\frac{1}{n}\\sum_{i = 1}^{n} x_{1} \\to \\mathbb{E}(x)\n\\end{equation}\\]\nVamos criar uma amostra de cinco observações a partir de uma distribuição uniforme e tirar a média destas observações. Lembre-se que este distribuição depende de dois parâmetros, digamos \\(a\\) e \\(b\\). A esperança de uma uniforme é simplesmente \\(\\frac{b-a}{2}\\). Podemos repetir este processo 1000 vezes e fazer um histograma dos resultados.\nNo código abaixo cria-se um vetor \\(x = (x_{1}, x_{2}, \\dots)\\) genérico para armazenar valoes. O loop vai inserindo neste vetor a média de uma amostra de cinco observações a partir de uma distribuição uniforme com \\(a = 0\\) e \\(b = 5\\). A cada iteração do loop uma nova amostra é gerada e sua média é salva no vetor \\(x\\) na posição \\(x_{i}\\). Depois de gerar estes valores faz-se um histograma deles.\n\nx &lt;- vector(\"numeric\", length = 1000) # cria um vetor para armazenar os valores\n\nfor(i in 1:1000){ #loop para gerar os valores\n  # computa a media de uma amostra com 5 observacoes\n  x[i] &lt;- mean(runif(n = 5, min = 0, max = 5)) \n\n}\n# Histograma\nhist(x, main = \"Histograma da media das amostras para n = 5\", xlab = \"\")\n# Linha vertical\nabline(v = 2.5, col = \"red\")\n\n\n\n\n\n\n\n\nPodemos fazer o mesmo para diferentes tamanhos de amostra. O código abaixo simplesmente faz um loop do código acima; o loop de fora varia n.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\n\nfor (n in c(10, 50, 100, 200)) { # loop para os diferentes tamanhos de amostra\n  x &lt;- vector(\"numeric\", 1000)\n  for (i in 1:1000) { # mesmo loop que o anterior\n    \n    x[i] &lt;- mean(runif(n, 0, 5))\n    \n  }\n    # Plotando o histograma\n    hist(x, main = paste(\"Histograma para n = \", n, sep = \"\"),\n       xlab = \"\")\n    abline(v = 2.5, col = \"red\")\n}\n\n\n\n\n\n\n\n\nNote que as escalas dos gráficos são diferentes. Como era de se esperar, à medida que cresce o tamanho da amostra os valores vão se acumulando em torno da média verdadeira.\nOutra maneira de visualizar a LGN é fazendo o seguinte experimento: sorteie um número a partir de uma distribuição particular e grave seu valor. Agora sorteie dois números a partir da mesma distribuição, tire a média dos valores e grave o resultado. Agora faça o mesmo com três números, quatro números e assim por diante. O código abaixo faz isto para uma distribuição normal padrão.\n\nx &lt;- vector(\"numeric\", 200)\nfor (n in 1:200){\n\n    x[n] &lt;- mean(rnorm(n))\n\n}\n\nplot(x, type = \"l\", xlab = \"\", ylab = \"\")\nabline(h = 0, col = \"red\", lty = 2)"
  },
  {
    "objectID": "posts/general-posts/repost-teoria-assintotica/index.html#teorema-central-do-limite",
    "href": "posts/general-posts/repost-teoria-assintotica/index.html#teorema-central-do-limite",
    "title": "Teoria Assintótica - LGN e TCL",
    "section": "Teorema Central do Limite",
    "text": "Teorema Central do Limite\nO segundo resultado importante que se usa em econometria é o Teorema Central do Limite (TCL). Existem algumas variantes do TCL que usam diferentes hipóteses, mas, novamente sendo informal, o TCL diz que se tivermos uma amostra qualquer \\(x_{1}, x_{2}, \\dots , x_{n}\\), então \\(\\sqrt{n}\\frac{\\overline{x} - \\mu}{\\sigma}\\) segue uma distribuição normal padrão, onde \\(\\overline{x}\\) é a média amostral, \\(\\mathbb{E}(x) = \\mu\\) e \\(\\text{Var}(x) = \\sigma^{2}\\). Para visualizar este resultado podemos novamente fazer o experimento usando a distribuição uniforme.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 200)){ # loop para os diferentes tamanhos de amostra\n  for (i in 1:1000){ # mesmo loop que o anterior\n\n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n\n  x_normalizado &lt;- sqrt(n)*(x - 5) / sqrt(100/12) # transforma a variavel\n  # plota o histograma usando a densidade da frequencia de cada observacao\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\", breaks = 20)\n  # superimpoe uma curva normal padrao \n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)  \n}\n\n\n\n\n\n\n\n\nNote que este resultado vale para qualquer sequência de variáveis i.i.d (independentes e identicamente distribuídas). Considere, por exemplo, uma sequência de variáveis aleatórias independentes que segue uma distribuição beta.\n\\[\\begin{equation}\nf(x) = \\frac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\end{equation}\\]\nonde \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\). A esperança da distribuição beta é dada por\n\\[\\begin{equation}\n    \\mathbb{E}(x) = \\frac{\\alpha}{\\alpha + \\beta}\n\\end{equation}\\]\nUma distribuição beta depende de dois parâmetros. Usando a função dbeta podemos simular algumas pdfs.\n\n\n\n\n\n\n\n\n\nNote que uma implicação do TCL é que, se \\(x_{i}\\) for i.i.d. com esperança igual a \\(\\mu\\), então\n\\[\\begin{equation}\n    \\sqrt{N} \\left (  \\frac{1}{n}\\sum_{i = 1}^{N}x_{i} - \\mu \\right ) \\to \\text{N}(0, \\sigma^{2})\n\\end{equation}\\]\nIsto é, não precisamos saber qual a forma da variância da distribuição para aplicar o TCL. Os loops abaixo são essencialmente idênticos aos anteriores: a diferença é que desta vez os histogramas vão representar variáveis normais de média zero com variância \\(\\sigma^{2}\\), que é aproximadamente igual à variância da distribuição beta.\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # loop para os diferentes tamanhos de amostra\n \n  for (i in 1:1000){ # mesmo loop que o anterior\n\n    x[i] &lt;- mean(rbeta(n, 2, 5))\n    \n  }\n\n  x_normalizado &lt;- sqrt(n)*(x - 2/7) # transforma a variavel\n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\") # plota o histograma usando a densidade da frequencia de cada observacao\n}\n\n\n\n\n\n\n\n\n\nDois casos anômalos\nO TCL nos diz: \\[\\begin{equation}\n    \\sqrt{n} \\left ( \\frac{\\overline{x} - \\mu}{\\sigma} \\right ) \\to N(0,1)\n\\end{equation}\\] O termo \\(\\sqrt{n}\\) é essencial para garantir este resultado. Qualquer transformação maior do que \\(\\sqrt{n}\\) faz a variância crescer indefinidamente; qualquer transformação menor do que \\(\\sqrt{n}\\) faz a variância diminuir indefinidamente, isto é, faz a distribuição colapsar num único ponto. Os dois códigos abaixo apresentam exemplos destes casos. O primeiro usa \\(n^{\\frac{3}{4}}\\), o segundo \\(n^{\\frac{1}{4}}\\).\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # mesmo loop que o anterior\n  for (i in 1:1000){ \n    \n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n  \n  x_normalizado &lt;- n^(3/4)*(x - 5)/sqrt(100/12) # muda apenas o expoente de n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\")\n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)\n}\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2)) # para exibir 4 graficos\nfor (n in c(10, 50, 100, 1000)){ # mesmo loop que o anterior\n  for (i in 1:1000){ \n    \n    x[i] &lt;- mean(runif(n, 0, 10))\n    \n  }\n  \n  x_normalizado &lt;- n^(1/4)*(x - 5)/sqrt(100/12) # muda apenas o expoente de n\n  hist(x_normalizado, main = paste(\"Histograma para n = \", n, sep =\"\"),\n       freq = F, xlab = \"\")\n  lines(seq(-4, 4, by = .1), dnorm(seq(-4, 4, by = .1), 0, 1),\n        col = \"dodgerblue4\", lwd = 3)\n}"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html",
    "title": "Pacotes Essenciais R",
    "section": "",
    "text": "R é uma das linguagens de programação mais populares para ciência de dados, estatística, economia e várias outras áreas quantitativas. O R já vem com alguns pacotes “imbutidos” que, em geral, são referidos como base R, são os pacotes que são instalados automaticamente junto com o R como o stats, utils, graphics, datasets entre outros.\nOu seja, já é possível importar dados e limpá-los, fazer análises estatísticas, gráficos e tabelas sem carregar nenhum pacote adicional.\nMas para o usufruir de todo o potencial que o R pode oferecer é essencial conhecer os melhores pacotes e as suas funções."
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#dplyr-x-data.table",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#dplyr-x-data.table",
    "title": "Pacotes Essenciais R",
    "section": "dplyr x data.table",
    "text": "dplyr x data.table\nO dplyr é bastante intuitivo, poderoso e há centenas de tutoriais e livros sobre ele. Ele serve para transformar os dados: criar colunas, filtrar linhas, reordernar dados, etc. Ele integra um “ecossistema” de pacotes chamado tidyverse, uma coleção de pacotes para processamento e visualização de dados, construídos em torno de uma filosofia comum.\nO data.table surgiu com o intuito de fazer o R funcionar melhor num mundo de big data. Seu desenvolvimento foi focado em ser o mais rápido, eficiente e sucinto possível, permitindo trabalhar com bases de dados muito grandes dentro do R (~100GB). Em termos de velocidade, o data.table ganha facilmente de praticamente todas as outras linguagens populares em data science. Ele é centena de vezes mais rápido que base R e também ganha com folga do seu competidor dplyr.\nSe o data.table é tão mais eficiente então como o dplyr se tornou mais popular do que ele?\nAcontece que o dplyr é melhor integrado com vários outros pacotes podersos do tidyverse. Também há muito material de apoio ao dplyr na forma de tutoriais, vídeos, livros, etc. disponíveis na internet. Quase todo curso de R ensina a usar o dplyr. Assim, a curva de aprendizado fica mais fácil.\nO data.table funciona muito bem com funções do base R como lapply, colMeans(), etc. e tem uma sintaxe concisa e flexível, onde o usuário acaba tendo bastante liberdade para criar as suas soluções. Já o dplyr vai pelo caminho de tentar facilitar ao máximo a vida do usuário criando várias funções com usos bastante específicos para otimizar pequenas tarefas do dia-a-dia do processamento de dados.\nOutra pequena desvantagem do data.table é que ele não funciona de primeira no Mac. Como ele usa OpenMP é preciso baixar outros programas e modificar algumas configurações no R o que pode ser bem trabalhoso e chato.\nNa comparação fica o seguinte:\n\n\n\n\n\n\n\ndplyr\ndata.table\n\n\n\n\nMais funções para aprender. Em geral, o código fica mais comprido\nSintaxe sucinta\n\n\nFunções melhor integradas com o tidyverse\nFunções melhor integradas com base R\n\n\nMuito mais veloz que o base R\nOpção mais veloz possível\n\n\nSintaxe mais intuitiva, mais material de apoio na internet, mais popular.\nMenos material de apoio disponível"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#tidyverse",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#tidyverse",
    "title": "Pacotes Essenciais R",
    "section": "tidyverse",
    "text": "tidyverse\nIndepedentemente da sua escolha entre dplyr e/ou data.table (recomendo usar os dois!) vale a pena explorar os demais pacotes do tidyverse.\n\ntidyr\nO tidyr acrescenta algumas importantes funcionalidades que faltam no dplyr. Talvez as duas funções mais importantes do pacote sejam pivot_longer e pivot_wider que servem para converter seus dados de transversais (wide) para longitudinais (long) e vice-versa. O tidyr também traz um novo tipo de objeto o tibble, uma versão moderna do data.frame.\nVale também explorar algumas funções muito úteis como separate, fill, complete, replace_na entre outras.\n\nlibrary(tidyr)\nlibrary(dplyr)\n# Exemplo de tibble\ndata &lt;- as_tibble(USPersonalExpenditure)\ndata &lt;- mutate(data, variable = rownames(USPersonalExpenditure))\ndata\n\n# A tibble: 5 × 6\n  `1940` `1945` `1950` `1955` `1960` variable           \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              \n1 22.2   44.5    59.6    73.2  86.8  Food and Tobacco   \n2 10.5   15.5    29      36.5  46.2  Household Operation\n3  3.53   5.76    9.71   14    21.1  Medical and Health \n4  1.04   1.98    2.45    3.4   5.4  Personal Care      \n5  0.341  0.974   1.8     2.6   3.64 Private Education  \n\n\n\n# Exemplo de dado longitudinal\nlong &lt;- pivot_longer(data, cols = !variable, names_to = \"year\")\nlong\n\n# A tibble: 25 × 3\n   variable            year  value\n   &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt;\n 1 Food and Tobacco    1940   22.2\n 2 Food and Tobacco    1945   44.5\n 3 Food and Tobacco    1950   59.6\n 4 Food and Tobacco    1955   73.2\n 5 Food and Tobacco    1960   86.8\n 6 Household Operation 1940   10.5\n 7 Household Operation 1945   15.5\n 8 Household Operation 1950   29  \n 9 Household Operation 1955   36.5\n10 Household Operation 1960   46.2\n# ℹ 15 more rows\n\n\n\n# Calcula a variação e apresenta de maneira transversal\nlong %&gt;%\n  group_by(variable) %&gt;%\n  mutate(variacao = (value / lag(value) - 1) * 100) %&gt;%\n  na.omit() %&gt;%\n  pivot_wider(\n    id_cols = \"variable\",\n    names_from = \"year\",\n    values_from = \"variacao\")\n\n# A tibble: 5 × 5\n# Groups:   variable [5]\n  variable            `1945` `1950` `1955` `1960`\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Food and Tobacco     100.    33.9   22.8   18.6\n2 Household Operation   47.6   87.1   25.9   26.6\n3 Medical and Health    63.2   68.6   44.2   50.7\n4 Personal Care         90.4   23.7   38.8   58.8\n5 Private Education    186.    84.8   44.4   40  \n\n\n\n\nstringr\nO stringr é um pacote voltado para facilitar a manipulação de vetores de texto (character). Todas as funções do pacote são convenientemente precedidas pelo prefixo str_ e a lista dos argumentos segue um padrão uniforme. O nome das funções também é bastante intuitivo.\nNovamente, há funções base como gsub, grep, strsplit, sub, entre outras, que servem para manipular vetores de string, mas essas funções carecem de “coesão interna” e em alguns casos elas podem retornar valores inesperados ou “erros silenciosos”.\nUma função muito divertida do pacote é a str_glue que facilita na hora de concatenar strings. Note como é simples incluir variáveis como parte do texto e inclusive fazer transformações nas variáveis.\n\nlibrary(stringr)\n\nnome &lt;- \"Vinicius Oike\"\nidade &lt;- 29\nnfav &lt;- runif(1, min = 0, max = 10)\n\nstr_glue(\"Olá, meu nome é {nome}, tenho {idade} anos. Ano que vem, terei {idade + 1} anos. Meu número favorito é {round(nfav)}.\")\n\nOlá, meu nome é Vinicius Oike, tenho 29 anos. Ano que vem, terei 30 anos. Meu número favorito é 3.\n\n\n\n\nlubridate\nO lubridate é um pacote exclusivamente focado em manipulação de vetores de datas e tudo relacionado ao tempo como variável. Variáveis de datas podem ser uma dor de cabeça tremenda no processo de limpeza de dados, pois há inúmeros formatos diferentes, que variam para cada país, fora os problemas potenciais de ano bissextos, fusos-horários diferentes, etc.\nEspecialmente para quem precisa trabalhar com séries de tempo ou dados em painel, o lubridate é um pacote é essencial.\nO pacote também facilita “operações aritméticas” com datas como no exemplo abaixo:\n\nlibrary(lubridate)\n\nstart &lt;- ymd(\"2020-01-15\")\nstart + years(1) + months(3)\n\n[1] \"2021-04-15\"\n\n\nEle também permite a extração de qualquer informação específica de uma data\n\nagora &lt;- ymd_hms(\"2022-06-10 20:36:15\")\n\nComo o dia da semana:\n\nwday(agora)\n\n[1] 6\n\n\nO trimestre:\n\nquarter(agora)\n\n[1] 2\n\n\nOu o mês (com a opção de ter a abreviação do mês já no padrão desejado!):\n\nmonth(agora, label = TRUE, locale = \"pt_BR\")\n\n[1] Jun\n12 Levels: Jan &lt; Fev &lt; Mar &lt; Abr &lt; Mai &lt; Jun &lt; Jul &lt; Ago &lt; Set &lt; ... &lt; Dez\n\n\n\n\ndbplyr\nO dbplyr é um pacote muito interessante que transforma o R numa “interface” para o SQL. O pacote traduz código escrito em dplyr para código em SQL. Assim, é possível acessar, transformar e baixar dados de uma Base de Dados em SQL, PostGreSQL, etc. diretamente no R.\nO código abaixo é um exemplo (bem artficial) que demonstra o funcionamento da pacote. A função show_query() não costuma ser usada na prática, mas ela serve para mostrar o que o pacote está fazendo.\n\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Abre um servidor SQL para servir de exemplo\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\n# Faz o update da tabela mtcars para esse servidor\ncopy_to(con, mtcars)\n# Acessa a tabela do servidor usando tbl\ndb &lt;- tbl(con, \"mtcars\")\n\n# Trabalha normalmente usando comandos do dplyr\ndb %&gt;%\n  filter(cyl %in% c(4, 6)) %&gt;%\n  mutate(disp = log(round(disp))) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(a1 = mean(disp, na.rm = TRUE), a2 = median(wt, na.rm = TRUE)) %&gt;%\n  arrange(desc(a1)) %&gt;%\n  # No lugar de show_query() use collect() para baixar os dados\n  show_query()\n\n&lt;SQL&gt;\nSELECT `cyl`, AVG(`disp`) AS `a1`, MEDIAN(`wt`) AS `a2`\nFROM (\n  SELECT\n    `mpg`,\n    `cyl`,\n    LOG(ROUND(`disp`, 0)) AS `disp`,\n    `hp`,\n    `drat`,\n    `wt`,\n    `qsec`,\n    `vs`,\n    `am`,\n    `gear`,\n    `carb`\n  FROM `mtcars`\n  WHERE (`cyl` IN (4.0, 6.0))\n)\nGROUP BY `cyl`\nORDER BY `a1` DESC"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#rmarkdown",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#rmarkdown",
    "title": "Pacotes Essenciais R",
    "section": "RMarkdown",
    "text": "RMarkdown\nUma análise de dados, em geral, vira um de dois produtos: um relatório/artigo ou um aplicativo interativo. Novamente o R tem funcionalidades incríveis para ambos os objetivos. Existe um tipo de arquivo chamado RMarkdown que junto com o pacote knitr permite compilar um arquivo misto de Markdown e R em formato pdf, html, doc ou ppt.\nEssa extensão é tão poderosa que não só é possível escrever um artigo científico com ela, mas também, um dashboard interativo, um livro inteiro, ou até um blog/site (este blog é escrito em RMarkdown).\nAtualmente o formato do código é ainda mais flexível, permitindo misturar linguagens como R, Python, SQL e outras num mesmo arquivo. Um resumo das funcionalidades pode ser visto aqui."
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#shiny",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#shiny",
    "title": "Pacotes Essenciais R",
    "section": "Shiny",
    "text": "Shiny\nO pacote shiny tem se tornado cada vez mais potente nos últimos anos. Inicialmente, ele servia para montar dashboards relativamente simples, que permitiam ao usuário mais liberdade para explorar e montar suas próprias análises. Atualmente, tanto o pacote como as suas extensões melhoraram muito o seu potencial.\nA galeria de apps do RStudio já está um pouco desatualizada, mas dá um sabor do que é possível fazer com shiny.\nAlguns exemplos:\n\nRadiant - Esta é, sem dúvida, uma das aplicações mais completas e impressionantes de Shiny, vale um post inteiro por si só.\nMonitor de Covid\nAnaálise do Perfil de Eleitor no Brasil"
  },
  {
    "objectID": "posts/general-posts/pacotes-essenciais-r/index.html#quarto",
    "href": "posts/general-posts/pacotes-essenciais-r/index.html#quarto",
    "title": "Pacotes Essenciais R",
    "section": "Quarto",
    "text": "Quarto\nRecentemente lançado, o quarto não é um pacote de R propriamente dito. O Quarto é como um RMarkdown turbinado, é um meio de publicar análises de dados com textos. De maneira geral o Quarto te permite:\n\nEscrever e rodar análises de dados em R, python e julia\nPublicar relatórios, dashboards, livros, sites, etc.\nPublicar artigos científicos utilizando equações, citações, etc.\n\nPara conhecer mais sobre o Quarto vale a pena checar o site. Como o Quarto foi desenvolvido pela Posit, que desenvolvou o RStudio a IDE mais popular de R, o Quarto e o R funcionam muito bem no RStudio."
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html",
    "href": "posts/general-posts/repost-sarima-no-r/index.html",
    "title": "SARIMA no R",
    "section": "",
    "text": "Neste post apresento como estimar um modelo SARIMA simples no R usando os pacotes astsa e forecast. O pacote astsa foi elaborado pelos autores do livro Time Series Analysis. Já o forecast foi desenvolvido por Rob. Hyndman, autor do livro Forecasting: Principles and Practice."
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html#identificação-e-tranformações",
    "href": "posts/general-posts/repost-sarima-no-r/index.html#identificação-e-tranformações",
    "title": "SARIMA no R",
    "section": "Identificação e tranformações",
    "text": "Identificação e tranformações\nComo a variância da série cresce ao longo do tempo aplico uma transformação log nos valores da série. Seja \\(y_{t}\\) nossa série. Então fazemos \\(ly_{t} \\equiv \\text{log}(y_{t}).\\)\n\n# Aplica transformação log (logaritmo natural)\nly &lt;- log(AirPassengers)\n# Gráfico\nautoplot(ly) +\n  geom_point(shape = 21) +\n  labs(x = \"\",\n       y = \"Passagens aéreas (log)\",\n       title = \"Demanda mensal de passagens aéreas internacionais\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nPara testar a acurácia do modelo vamos remover algumas das últimas observações. Estas serão testadas contra as previsões do modelo. Aqui sigo a nomenclatura de train (treino) e test (teste). O gráfico abaixo permite visualizar esta divisão, onde os valores em vermelho foram excluídos da nossa amostra.\n\ntrain &lt;- window(ly, end = c(1957, 12))\ntest &lt;- window(ly, start = c(1958, 1))\n\nautoplot(train) +\n  autolayer(test) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nTirando a primeira diferença da série removemos a sua tendência de crescimento. Fazemos \\(dly_{t} \\equiv (1-L)ly_{t} = ly_{t} - ly_{t-1}\\) usando a função diff(). Note pelo gráfico que ainda parece haver forte sazonalidade na série.\n\n# Tira a primeira diferença da série\ndly &lt;- diff(train)\n\nautoplot(dly) +\n  ggtitle(\"Primeira diferença do log da série\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nPodemos ver isto mais claramente na análise do correlograma da série diferenciada. No gráfico abaixo, os “lags” seguem a periodicidade da série, isto é, cada “lag” representa o equivalente a 12 meses. Parece haver uma forte correlação entre \\(dly_{t}\\) com \\(dly_{t-12}, dly_{t-24}, \\dots, dly_{t-12k}\\) com \\(k = 1, 2, \\dots\\).\n\n# Gráfico da FAC e FACP\nacf2(dly)\n\n\n\n\n\n\n\n\nAgora tiramos uma diferença sazonal de 12 meses. A série resultante, \\(sdly_{t}\\) fica: \\[\\begin{align}\n  sdly_{t} \\equiv (1-L^{12})dly_{t} & = (1-L^{12})(1-L)ly_{t} \\\\\n                                    & = (1 - L^{12} - L + L^{13})ly_{t} \\\\\n                                    & = ly_{t} - ly_{t-1} - ly_{t-12} + ly_{t-13}\n\\end{align}\\]\nAbaixo temos o correlograma de \\(sdly_{t}\\). A primeira defasagem é significativa tanto no ACF como no PACF. Além disso a 12ª defasagem também é significativa em ambos. Assim, vamos primeiro tentar um modelo de “ordem máxima” SARIMA\\((1,1,1)(1,1,1)[12]\\). A partir deste modelo, vamos tentar estimar outros de ordens mais baixas para evitar o problema de sobreparametrização.\n\n# Tira a primeira diferença sazonal da série\nsdly &lt;- diff(dly, 12)\n# Gráfico da FAC e FACP\nacf2(sdly)"
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html#estimação",
    "href": "posts/general-posts/repost-sarima-no-r/index.html#estimação",
    "title": "SARIMA no R",
    "section": "Estimação",
    "text": "Estimação\nA equação do modelo que estamos estimando é: \\[\n  (1 - \\phi L)(1 - \\Phi L^{12})(1 - L)(1 - L^{12})ly_{t} = (1 + \\theta L)(1 + \\Theta L^{12})\\epsilon_{t}\n\\] Vamos o usar o comando sarima do pacote astsa. Note que os resíduos do modelo parecem se comportar como ruído branco, indicando que nosso modelo está bem ajustado.\n\n(m1 &lt;- sarima(p = 1, d = 1, q = 1, P = 1, D = 1, Q = 1, S = 12, xdata = train))\n\ninitial  value -3.031632 \niter   2 value -3.170590\niter   3 value -3.246467\niter   4 value -3.246688\niter   5 value -3.247295\niter   6 value -3.247539\niter   7 value -3.247841\niter   8 value -3.248283\niter   9 value -3.248794\niter  10 value -3.249345\niter  11 value -3.249360\niter  12 value -3.249378\niter  13 value -3.249379\niter  14 value -3.249380\niter  15 value -3.249381\niter  16 value -3.249381\niter  17 value -3.249381\niter  17 value -3.249381\niter  17 value -3.249381\nfinal  value -3.249381 \nconverged\ninitial  value -3.248923 \niter   2 value -3.252561\niter   3 value -3.258267\niter   4 value -3.265627\niter   5 value -3.267071\niter   6 value -3.267585\niter   7 value -3.267745\niter   8 value -3.268029\niter   9 value -3.268401\niter  10 value -3.268494\niter  11 value -3.268499\niter  12 value -3.268506\niter  13 value -3.268511\niter  14 value -3.268511\niter  15 value -3.268511\niter  15 value -3.268511\niter  15 value -3.268511\nfinal  value -3.268511 \nconverged\n\n\n\n\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.2565  -0.6243  -0.0599  -0.5574\ns.e.  0.2759   0.2289   0.1765   0.1670\n\nsigma^2 estimated as 0.001367:  log likelihood = 175.71,  aic = -341.42\n\n$degrees_of_freedom\n[1] 91\n\n$ttable\n     Estimate     SE t.value p.value\nar1    0.2565 0.2759  0.9299  0.3549\nma1   -0.6243 0.2289 -2.7271  0.0077\nsar1  -0.0599 0.1765 -0.3392  0.7353\nsma1  -0.5574 0.1670 -3.3380  0.0012\n\n$AIC\n[1] -3.593882\n\n$AICc\n[1] -3.589203\n\n$BIC\n[1] -3.459467\n\n\nPara evitar o problema de sobreparametrização (overfitting) temos que tentar ajustar modelos de ordens similares, porém mais baixas. Este processo costuma ser iterativo, isto é, na base da tentativa e erro seguindo algum critério de informação (i.e.: AIC, AICc, BIC, etc.). Depois de tentar vários modelos diferentes chegamos, por exemplo, no SARIMA\\((0,1,1)(0,1,1)[12]\\). A equação do modelo pode ser expressa como: \\[\n  (1 - L)(1 - L^{12})ly_{t} = (1 + \\theta L)(1 + \\Theta L^{12})\\epsilon_{t}\n\\]\nNovamente, os resíduos do modelo indicam que ele está bem ajustado aos dados.\n\nm2 &lt;- sarima(p = 0, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12, xdata = train)\n\ninitial  value -3.047456 \niter   2 value -3.251231\niter   3 value -3.251514\niter   4 value -3.268396\niter   5 value -3.270049\niter   6 value -3.270196\niter   7 value -3.270197\niter   8 value -3.270198\niter   8 value -3.270198\niter   8 value -3.270198\nfinal  value -3.270198 \nconverged\ninitial  value -3.263453 \niter   2 value -3.264133\niter   3 value -3.264159\niter   4 value -3.264160\niter   4 value -3.264160\niter   4 value -3.264160\nfinal  value -3.264160 \nconverged\n\n\n\n\n\n\n\n\n\n\nm2$fit\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     sma1\n      -0.3864  -0.5885\ns.e.   0.1097   0.0927\n\nsigma^2 estimated as 0.001383:  log likelihood = 175.3,  aic = -344.59\n\n\nA estimativa tem a forma:\n\\[\n  (1 - L)(1 - L^{12})ly_{t} = (1 - 0.3864 L)(1 - 0.5885 L^{12})\\epsilon_{t}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-sarima-no-r/index.html#previsão",
    "href": "posts/general-posts/repost-sarima-no-r/index.html#previsão",
    "title": "SARIMA no R",
    "section": "Previsão",
    "text": "Previsão\nPara computar as previsões do modelo usamos a função sarima.for. Aqui podemos comparar as previsões do modelo (construindo usando apenas as observações dentro de train) com as observações. Esta função automaticametne retorna um gráfico com as previsões fora da amostra.\n\npred &lt;- sarima.for(train, n.ahead = length(test),\n                   p = 0, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)\n\n\n\n\n\n\n\n\nPodemos construir um gráfico que inclui as observações reservadas no test usando as seguintes funções base do R.\n\nplot.ts(pred$pred, col = \"red\", ylim = c(5.7, 6.5),\n        ylab = \"\",\n        main = \"Previsão do modelo SARIMA(0, 1, 1)(0, 1, 1)[12]\")\nlines(test, type = \"o\")\nlegend(\"topleft\", legend = c(\"Previsto\", \"Observado\"), lty = 1,\n       pch = c(NA, 1), col = c(\"red\", \"black\"))\n\n\n\n\n\n\n\n\nComo comentado em outro post, pode-se produzir visualizações mais elegantes usando o ggplot2, mas o pacote não “conversa” bem com os objetos típicos de séries de tempo. Um jeito de contornar isto é usando a função autoplot() do pacote forecast. Abaixo, reestimo o modelo usando a função arima. Este passo é necessário para usar a função autoplot().\n\nm &lt;- arima(train, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 12))\n\nAgora podemos verificar a qualidade das nossas previsões. O gráfico abaixo foi construído funções base do R. A linha azul representa as previsões do modelo SARIMA especificado acima, enquanto que a linha vermelha representa as observações. As áreas sombreadas são intervalos de confiança: o mais escuro é de 80% e o mais claro de 95%.\n\nplot(forecast(m, h = length(test)))\nlines(test, col = \"black\")\nlegend(\"topleft\",\n       legend = c(\"Previsto\", \"Observado\"),\n       lty = 1, col = c(\"red\", \"black\"))\n\n\n\n\n\n\n\n\nUsando o autoplot temos o seguinte resultado:\n\nautoplot(forecast(m, h = length(test)), include = 50) +\n  autolayer(test) +\n  labs(x = \"\", y = \"\") + \n  theme_bw() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/general-posts/aquecimento-global/index.html",
    "href": "posts/general-posts/aquecimento-global/index.html",
    "title": "Aquecimento Global",
    "section": "",
    "text": "Uma recente edição da revista inglesa The Economist exibe uma série de listras coloridas em sua capa. Elas formam um degradê que vai de um azul escuro até um vermelho intenso. Cada listra representa a temperatura de um ano e a linha do tempo vai desde o 1850 até o presente. A mensagem é bastante clara: o planeta esta cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Esta imagem é creditada a Ed Hawkings, editor do Climate Lab Book.\nPara ser preciso, a imagem não plota a temperatura de cada ano, mas sim o quanto cada ano se desvia da temperatura média do período 1971-2000. Isto é, anos acima dessa média têm um valor positivo, valores abaixo dessa média, valores negativos. Esta é uma forma bastante comum de representar este tipo de dado climático. De imediato, quando vi a imagem me ocorreu que seria bastante simples reproduzir uma versão aproximada dela usando o R."
  },
  {
    "objectID": "posts/general-posts/aquecimento-global/index.html#o-código",
    "href": "posts/general-posts/aquecimento-global/index.html#o-código",
    "title": "Aquecimento Global",
    "section": "O código",
    "text": "O código\nO código necessário para gerar a imagem é bastante enxuto. Vou descrever em linhas gerais o que ele faz:\nPrimeiro carrego dois pacotes (linhas 1, 2), depois a série de temperatura (linha 3), faço algumas transformações nos dados (linhas 4, 5) e, por fim, ploto os dados (linhas 6, 7, 8). O resultado inicial já é bastante satisfatório e a partir destas poucas linhas de código pode-se chegar num resultado muito próximo ao da imagem original. Vale notar que a imagem fica um pouco diferente da original porque eu uso uma base de dados diferente.\n\n# Carrega pacotes\nlibrary(ggplot2)\nlibrary(astsa)\n# Carrega a base de dados 'globtemp'\ndata(\"globtemp\")\n# Converte o objeto para data.frame\ndf &lt;- data.frame(ano = as.numeric(time(globtemp)),\n temp = as.numeric(globtemp))\n\n# Monta o gráfico\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")"
  },
  {
    "objectID": "posts/general-posts/aquecimento-global/index.html#os-detalhes-do-código",
    "href": "posts/general-posts/aquecimento-global/index.html#os-detalhes-do-código",
    "title": "Aquecimento Global",
    "section": "Os detalhes do código",
    "text": "Os detalhes do código\nVou explicar cada linha de código para ser didático. O R funciona, grosso modo, como um repositório de pacotes: cada pacote contem funções e, às vezes, bases de dados. O primeiro pacote que carrego é o ggplot2. Ele serve para fazer visualizações de dados. O pacote astsa traz várias funções para fazer análise de séries de tempo, mas eu carrego ele somente para usar a base de dados globtemp, que traz informação sobre a temperatura anual da terra coletada pela NASA.\nO objeto globtemp é uma série de tempo (um objeto da classe ts), que tem alguns atributos especiais. Um deles pode ser acesado pela função time que extrai um vetor numérico com as datas desta série de tempo. No código abaixo mostro os primeiros dez valores do time(globtemp).\n\nclass(globtemp)\n\n[1] \"ts\"\n\ntime(globtemp)[1:10]\n\n [1] 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n\n\nPara extrair somente os valores da série, uso a função as.numeric, que converte o vetor de ts para numeric (numérico). Este tipo de função é bastante comum já que frequentemente é preciso trocar a classe de um objeto. O objetivo destes primeiros passos é de inserir as informações do globtemp num data.frame em que a data aparece na primeira coluna e os valores da série são armazenados na segunda coluna. O procedimento pode parecer um tanto trabalhoso (e acho que é mesmo), mas é o jeito. Um data.frame é como uma tabela com dados. Este é um objeto bastante típico em análise de dados e é necessário para usar a função ggplot que vai fazer o gráfico. Abaixo pode-se ver as primeiras linhas desta tabela.\n\nhead(df)\n\n   ano  temp\n1 1880 -0.20\n2 1881 -0.11\n3 1882 -0.10\n4 1883 -0.20\n5 1884 -0.28\n6 1885 -0.31\n\n\nAgora que tenho os dados no formato apropriado posso usar o ggplot. O argumento que pode ser um pouco confuso é o aes. Nele especifica-se quais dados serão mapeados no gráfico. Depois disso adicionamos um geom. Há vários tipos de geom (geom_line, geom_bar, geom_histogram, etc.) e cada um deles produz uma imagem diferente. O geom_tile faz um pequeno quadrado. Para que a função consiga desenhar o quadrado é preciso informar uma variável x e uma variável y. Além disso, também especifico fill = temp. O fill se refere à cor que vai preencher (fill) o quadrado. Como especifico fill = temp a cor do quadrado vai representar a variável temp (temperatura).\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile()\n\n\n\n\n\n\n\n\nO resultado é exatamente como o esperado, mas ainda é preciso mudar a escala de cores. Faço isto com o scale_fill_gradient2. Aqui cada termo tem um signficado: scale_fill pois estamos mudando a escala do fill (outra opção seria scale_color que muda a escala do color). scale_fill_gradient pois queremos um gradiente (degradê) de cores. Por fim, o 2 é adicionado no final pois queremos um escala que diferencie dois grupos distintos: temperaturas acima da média em vermelho, temperaturas abaixo da média em azul. A escala de cores é determinada pelos argumentos low, mid e high.\nOs valores negativos serão coloridos pelo low, os próximos de zero pelo mid e os valores grandes pelo high. Abaixo escrevo as cores em hexa-decimal, mas elas podem ser lidas, essencialmente, como: azul-escuro, cinza-azulado-claro e vermelho-escuro.\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")\n\n\n\n\n\n\n\n\nComo comentei acima, pode-se melhorar o gráfico acima adicionando outros elementos e detalhes. A versão final que fiz do gráfico fica no código abaixo.\n\n# Pacote para carregar fontes externas no R\n# Necessário para utilizar 'Georgia' no gráfico\nlibrary(extrafont)\n# Data.frames auxiliares para plotar as anotações de texto\ndf_aux_title &lt;- data.frame(x = 1930, y = 0, label = \"The Climate Issue\")\ndf_aux_anos &lt;- data.frame(\n  label = c(1880, 1920, 1960, 2000),\n  x = c(1890, 1925, 1960, 1995)\n  )\n\nggplot() +\n  geom_tile(data = df, aes(x = ano, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 6,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = 1950, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 11,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_gradientn(\n    colors = c(\"#213A82\", \"#3B60CE\", \"#8DA2E2\", \"#DE2E02\", \"#9d0208\")\n   ) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n    )"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html",
    "href": "posts/general-posts/ipca-visualizacao/index.html",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "A inflação voltou a ser uma pauta, não só no Brasil, mas no mundo todo, nos últimos meses. No Brasil, a combinação de câmbio desvalorizado, desajustes logísticos, crise hídrica e choques de preços externos, culminaram no maior nível de inflação desde 2002.\nMesmo em países avançados, os níveis de inflação estão em altas históricas. Nos Estados Unidos, por exemplo, o nível do CPI está no valor mais alto desde o final dos anos 1970.\nVisualizar a magnitude da inflação no Brasil pode ser um pouco desafiador. A série do IPCA é calculada desde 1979. O número de cidades avaliadas pelo índice cresceu no tempo: nos primeiros anos o índice contemplava Rio de Janeiro, Porto Alegre, Belo Horizonte, Recife, São Paulo, Brasília, Belém, Fortaleza, Salvador e Curitiba. Em 1991, Goiânia entra no índice e, mais recentemente, em 2014, Vitória e Campo Grande também entraram no cômputo do índice.\nMais importante do que a variação no número das cidades, é o período hiperinflacionário da década de 1980. Os números da inflação são incomparavelmente mais altos do que os atuais. Como regra, os cortes temporais mais relevantes para enxergar a inflação são Jul/94 (Plano Real), Jul/99 (Regime de Metas de Inflação), Mai/00 (Lei de Responsabilidade Fiscal) e Mai/03 (pós choque de 2002).\nNeste post vou mostrar o comportamento da inflação desde 1999.\n\n\n\n# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)\n\n\n\n\nImporto os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  right_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)\n\n\n\n\nO cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nUma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso.\n\n\n\n\nOutra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com um\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#pacotes",
    "href": "posts/general-posts/ipca-visualizacao/index.html#pacotes",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#importando-os-dados",
    "href": "posts/general-posts/ipca-visualizacao/index.html#importando-os-dados",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Importo os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  right_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#inflação",
    "href": "posts/general-posts/ipca-visualizacao/index.html#inflação",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "O cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#no-longo-prazo",
    "href": "posts/general-posts/ipca-visualizacao/index.html#no-longo-prazo",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Uma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso."
  },
  {
    "objectID": "posts/general-posts/ipca-visualizacao/index.html#enxergando-a-distribuição",
    "href": "posts/general-posts/ipca-visualizacao/index.html#enxergando-a-distribuição",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Outra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com um\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html",
    "title": "Fundamentos: histograma",
    "section": "",
    "text": "Um histograma serve para visualizar a distribuição de um conjunto de dados. Ele consiste em colunas que representam a frequência de ocorrência de determinados valores nos dados: quanto mais alta for a coluna, mais frequente é uma observação. Isto permite ver a forma da distribuição dos dados e identificar padrões e tendências.\n\n\n\n\n\n\n\n\n\nHistogramas aparecem naturalmente na hora de visualizar, por exemplo:\n\nDistribuição de notas de alunos em testes padronizados.\nDistribuição da renda familiar na população de um país.\nDistribuição de preços de imóveis numa cidade.\nDistribuição da altura das pessoas.\nDistribuição de variáveis aleatórias em estatística.\n\nNeste post vamos entender como montar histogramas no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para visualizar a taxa de poupança nos EUA aos longo dos anos. Depois vamos trabalhar um exemplo mais complexo, analisando a distribuição do preço dos imóveis no Texas, EUA."
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#ggplot2",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#ggplot2",
    "title": "Fundamentos: histograma",
    "section": "ggplot2",
    "text": "ggplot2\nPara criar um histograma com o pacote ggplot2 no R, usamos a função geom_histogram().\nA estrutura de um gráfico do ggplot2 parte de três elementos básicos: (1) a base de dados, isto é, um objeto data.frame; (2) um mapeamento de variáveis, feito com auxílio da função aes(); e (3) a escolha da forma do gráfico, feito com as funções geom.\nO ggplot2 funciona adicionando camadas e elementos subsequentemente sobre um gráfico inicial. Cada elemento novo que adicionamos ao gráfico é somado usando o operador +.\nPara resumir o processo: começamos com a função ggplot() e vamos adicionando geoms, funções auxiliares que especificam a forma do gráfico. Este processo construtivo de adicionar elementos a um gráfico é o principal diferencial do ggplot.\nOu seja, temos três elementos essenciais:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEsta estrutura básica é esquematizada no pseudo-código abaixo.\n\nggplot(data = base_de_dados, aes(x = variavel_x)) +\n  geom_histogram()\n\nVamos montar um exemplo usando a base economics, que vem carregada junto com o pacote ggplot2. Esta base compila uma série de informações econômicas e demográficas no período julho/1967 a abril/2014 nos EUA. Para explorar os dados podemos usar a função head() que exibe as primieras linhas da tabela.\n\nhead(economics)\n\n\n\n\n\n\ndate\npce\npop\npsavert\nuempmed\nunemploy\n\n\n\n\n1967-07-01\n507\n198712\n13\n4\n2944\n\n\n1967-08-01\n510\n198911\n13\n5\n2945\n\n\n1967-09-01\n516\n199113\n12\n5\n2958\n\n\n1967-10-01\n512\n199311\n13\n5\n3143\n\n\n1967-11-01\n517\n199498\n13\n5\n3066\n\n\n1967-12-01\n525\n199657\n12\n5\n3018\n\n\n\n\n\n\n\nInicialmente, vamos nos focar na coluna psavert, que é a taxa de poupança individual, isto é, o percentual da renda que as famílias poupam. O código abaixo monta um histograma desta variável.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram()\n\nVamos decompor o código acima em partes. Primeiro temos que informar onde estão os nossos dados. Fazemos isto dentro da função ggplot() usando o argumento data = economics.\nDepois, precisamos indicar qual a variável (coluna) que queremos visualizar, isto é, indicar qual é a variável que deve ser mapeada em um elemento visual. Fazemos isto usando a função aes(x = psavert).\nPor fim, como queremos desenhar um gráfico de histograma escolhemos o geom_histogram(). Esta última função é adicionada (somada) à função inicial com o sinal de soma +.\nSegue abaixo o código comentado junto com o gráfico produzido. Vemos que, historicamente, a taxa de poupança gira entre 5% e 15% da renda pessoal.\n\n# Chamada inical da função ggplot\nggplot(\n  # Define a base de dados\n  data = economics,\n  # Escolhe qual a variável deve ser visualizda\n  aes(x = psavert)\n  ) +\n  # Escolhe o tipo de gráfico (histograma)\n  geom_histogram()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#elementos-estéticos",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#elementos-estéticos",
    "title": "Fundamentos: histograma",
    "section": "Elementos estéticos",
    "text": "Elementos estéticos\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nUm gráfico de histograma tem cinco elementos estéticos principais:\n\ncolor - Define a cor do contorno da coluna.\nfill - Define a cor que preenche a coluna.\nalpha - Define o nível de transparência das cores.\nbinwidth - Define a largura da coluna.\nbins - Define o número de colunas.\n\nOs dois últimos elementos são parâmetros estatísticos que são interpretados como estéticos neste contexto. Vamos explorar cada um destes elementos em exemplos abaixo.\nVale notar que o argumento x também é um elemento estético. Mais especificamente ele é um elemento estético variável, logo é mapeado com a função aes(), e é obrigatório (pois é exigido pela função geom_histogram())\n\nCores\nTemos duas opções principais de cores: color é a cor da linha do contorno da coluna e fill é a cor que preenche o interior da coluna. O código abaixo ilustra como utilizar estes argumentos dentro da função geom_histogram(). Note que ambos os elementos estéticos são constantes.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém podemos fazer referência a cores via código hexadecimal. No exemplo abaixo uso as cores \"#e76f51 (laranja-escuro) e \"#264653\" (azul-escuro).\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(color = \"#E76F51\", fill = \"#264653\")\n\n\n\n\n\n\n\n\n\n\nTransparência\nO parâmetro alpha controla o nível de transparência das cores. O valor dele deve estar sempre entre 0 e 1. Quanto mais próximo de 0, mais transparente será o gráfico final. Os gráficos abaixo mostram o efeito de alguns valores distintos de alpha.\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.9)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.7)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.3)\n\nggplot(data = economics, aes(x = psavert)) +\n  geom_histogram(alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\nColunas\nPodemos controlar o número de colunas do histograma de duas formas: (1) escolhendo o número via bins; (2) escolhendo o tamanho dos intervalos/colunas via binwidth.\nA escolha padrão da função geom_histogram() é definir bins = 30. Isto raramente resulta num gráfico ideal. O número ótimo de intervalos depende do tipo de dado que estamos visualizando.\nEm geral, um número muito pequeno resulta num gráfico agrupado demais, enquanto um número muito grande resulta num gráfico disperso demais. Em ambos os casos fica difícil enxergar o padrão nos dados.\nO código abaixo reduz o número de intervalos para 5. Note como as observações estão mais agrupadas.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 5)\n\n\n\n\n\n\n\n\nJá o código seguinte aumenta o número de intervalos para 70. Agora conseguimos identificar mais facilmente os outliers, mas as observações estão dispersas demais para conseguir enxergar algum tipo de padrão.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nPor fim, o gráfico abaixo tenta chegar num meio termo. Vemos que a taxa de poupança tem uma distribuição parecida com uma normal e possui alguns outliers tanto à esquerda como à direita.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nComo mencionado acima, podemos definir o tamanho dos intervalos usando binwidth. Como nossa variável está expressa em formato de percentual, podemos experimentar intervalos de tamanho unitário. O resultado, neste caso, é bastante satisfatório.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: histograma",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  labs(\n    # Título\n    title = \"Taxa de poupança pessoal nos EUA\",\n    # Subtítulo\n    subtitle = \"Distribuição da taxa de poupança, como proporção da renda disponível, no período 1967-2014.\",\n    # Nome do eixo-x\n    x = \"Taxa de poupança (%)\",\n    # Nome do eixo-y\n    y = \"Frequência\",\n    # Nota de rodapé\n    caption = \"Fonte: FREDR\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: histograma",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes(). Como expliquei acima, esta função “transforma” nossos dados em elementos visuais. Nos casos acima, ela mapeia a variável x nas colunas do histograma.\nMas também podemos mapear uma coluna para um elemento estético como o fill, por exemplo. O resultado é um gráfico em que a cor de cada coluna vai corresponder a uma variável da nossa base de dados.\nAgora, vamos utilizar a base de dados txhousing que compila informações do mercado imobiliário das principais cidades do estado do Texas, nos EUA. Como a base inclui mais de 40 cidades vamos restringi-la para apenas quatro cidades: Austin, Dallas, Houston e San Angelo. Vamos visualizar a distribuição da variável median que registra o valor mediano de venda mensal dos imóveis em cada cidade. A variável city indica o nome da cidade.\n\nhead(txhousing)\n\n\n\n\n\n\ncity\nyear\nmonth\nsales\nvolume\nmedian\nlistings\ninventory\ndate\n\n\n\n\nAbilene\n2000\n1\n72\n5380000\n71400\n701\n6\n2000\n\n\nAbilene\n2000\n2\n98\n6505000\n58700\n746\n7\n2000\n\n\nAbilene\n2000\n3\n130\n9285000\n58100\n784\n7\n2000\n\n\nAbilene\n2000\n4\n98\n9730000\n68600\n785\n7\n2000\n\n\nAbilene\n2000\n5\n141\n10590000\n67300\n794\n7\n2000\n\n\nAbilene\n2000\n6\n156\n13910000\n66900\n780\n7\n2000\n\n\n\n\n\n\n\nQueremos um gráfico em que cada cidade tenha uma cor diferente, então, a variável city deve aparecer dentro da função aes(). O código abaixo primeiro organiza os dados e depois monta o gráfico. Agora, cada cidade tem uma cor diferente e as colunas são “empilhadas” umas sobre as outras.\n\n# Cria um vetor com as cidades selecionadas\ncities &lt;- c(\"Austin\", \"Dallas\", \"Houston\", \"San Angelo\")\n# Seleciona apenas as linhas que contêm informações sobre estas cidades\nsubtxhousing &lt;- subset(txhousing, city %in% cities)\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(aes(fill = city))\n\n\n\n\n\n\n\n\nNo gráfico acima, conseguimos ver, por exemplo, que o valor mais frequente de venda está em torno de 150 mil. Além disso, pode-se ver como os valores de venda em San Angelo costumam ser menores do que os valores de venda em Austin.\nPara ter maior controle sobre as cores e sobre a legenda usamos a função scale_fill_manual() e a função theme().\n\nggplot(data = subtxhousing, aes(x = median)) +\n  geom_histogram(\n    # Mapeaia a variável city nas cores das colunas\n    aes(fill = city),\n    # Define o número de colunas\n    bins = 25,\n    # Define a cor (contorno) das colunas\n    color = \"white\") +\n  # Controla as cores e a legenda\n  scale_fill_manual(\n    # Título da legenda\n    name = \"Cidade\",\n    # Cores das colunas\n    values = c(\"#264653\", \"#2a9d8f\", \"#f4a261\", \"#e76f51\")\n  ) +\n  # Posiciona a legenda acima do gráfico\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/3-grafico-histograma.html#resumo",
    "href": "posts/ggplot2-tutorial/3-grafico-histograma.html#resumo",
    "title": "Fundamentos: histograma",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar alguns histogramas interessantes em poucas linhas de código. Em qualquer gráfico temos três elementos básicos:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos importantes:\n\nElementos estéticos podem ser constantes (números ou texto) ou variáveis (colunas da base de dados). Elementos variáveis precisam estar dentro da função aes().\nA escolha do número de colunas/intervalos depende do dado que queremos visualizar. Em geral, é preciso experimentar com números diferentes.\nSe o elemento fill for variável é preciso usar a função scale_fill_manual() para controlar as cores e a legenda de cores.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos."
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html",
    "title": "Fundamentos: gráfico de coluna",
    "section": "",
    "text": "Um gráfico do colunas é uma ferramenta de visualização poderosa e versátil. Tipicamente, cada coluna corresponde ao valor de uma classe.\n\n\n\n\n\n\n\n\n\nNeste post vamos aprender a montar gráficos de colunas usando o pacote ggplot2. Há duas funções para criar gráficos de colunas: o geom_bar() e geom_col(). A primeira função conta uma quantidade de ocorrências, é útil para resumir visualmente uma base de dados. Já a segunda função plota a altura da coluna segundo os valores nos dados, então é mais útil quando os dados já estão agregados. Estas diferenças ficarão mais claras nos exemplos abaixo."
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#ggplot2",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#ggplot2",
    "title": "Fundamentos: gráfico de coluna",
    "section": "ggplot2",
    "text": "ggplot2\nO pacote ggplot2 segue uma sintaxe bastante consistente, que permite “somar” elementos visuais sobre um mesmo gráfico. Isto permite que se crie uma infinidade de gráficos complexos a partir de elementos simples. Os elementos visuais são todos chamados por funções geom como as funções geom_bar (barra) e geom_col (coluna) citadas acima. Estes elementos são somados de maneira intuitiva usando o sinal de soma +.\nEssencialmente, temos os seguintes elementos principais:\n\ndata - Uma base de dados.\naes - Variáveis que são mapeadas em elementos visuais.\ngeom - Um objeto geométrico.\n\nEstes elementos são combinados numa sintaxe recorrente. A função ggplot tem apenas dois argumentos data e aes. Adicionamos uma função geom nesta chamada inicial como no exemplo abaixo. Note que usamos o geom_col mas poderíamos utilizar qualquer outra função como geom_point.\n\nggplot(data = dados, aes(x = varivel_x, y = variavel_y)) +\n  geom_col()\n\n\ngeom_bar\nA função geom_bar() exige apenas um argumento x que é o nome da variável que será “contada”. Esta função conta a quantidade de vezes que os elementos da variável x se repetem e plota este valor num gráfico de barras.\nVamos utilizar a base hprice1, que agrega o preço de venda de imóveis em Boston em 1990. Para inspecionar os dados utilizamos a função head.\n\nhead(hprice1)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810\n\n\nA coluna bdrms indica o número de dormitórios do imóvel. Podemos contar o número de imóveis pelo número de dormitórios usando a função geom_bar().\n\nggplot(data = hprice1, aes(x = factor(bdrms))) +\n  geom_bar()\n\n\n\n\n\n\n\n\nVamos quebrar o código acima em detalhes. Primeiro, usamos a função ggplot() para declarar que queremos fazer um gráfico.\nColocamos os argumentos data e aes() dentro desta função. O argumento data deve ser o nome da nossa base de dados: neste caso, data = hprice1.\nA função aes() é a que transforma as variáveis (as colunas da base de dados) em elementos visuais. Neste caso ele vai transformar o número de dormitórios em algum elemento visual. Escolhemos aes(x = factor(bdrms)). Aqui, a função factor é opcional, mas fortemente recomendada. Ela força a variável bdrms a se comportar como uma variável categórica (ao invés de uma variável contínua).\nEspecificamos qual deve ser o elemento visual somando a função geom_bar() no código inicial. Esta função indica que queremos um gráfico de barras.\nUnindo todas estes elementos temos um código enxuto que plota o gráfico. Vemos que os imóveis de 3 e 4 dormitórios são os mais comuns.\n\nggplot(data = hprice1, aes(x = factor(bdrms))) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\ngeom_col\nA função geom_col desenha o mesmo tipo de gráfico que a função geom_bar, mas ela exige que sejam inputados dois argumentos: x e y. Geralmente, x é a classe ou categoria, enquanto que y é o valor que será transformado na “altura” da coluna.\nVamos montar um exemplo simples. Nosso objetivo é visualizar as medalhas olímpicas brasileiras na mais recente edição das Olimpíadas, em 2020 (que foi realizada apenas em 2021). Para isto, vamos inserir os dados diretamente usando a função data.frame(). Esta função permite criar uma base de dados manualmente. A sintaxe da função é bastante simples: primeiro declaramos o nome da coluna e depois os seus valores; vamos acrescentando colunas separando-as por vírgulas.\n\n# Exemplo de como usar a função data.frame\ndados &lt;- data.frame(\n  nome_1 = c(...),\n  nome_2 = c(...),\n  ...\n)\n\nNa última Olimpíada, o Brasil obteve 7 medalhas de ouro, 6 de prata e 8 de bronze. O código abaixo estrutura estes dados num data.frame. Aqui a função factor ajuda a organizar os dados, pois impõe uma ordem de grandeza na variável categórica medalha.\n\n# Cria a base de dados\nolimpiadas &lt;- data.frame(\n  medalha = factor(c(\"ouro\", \"prata\", \"bronze\"), levels = c(\"ouro\", \"prata\", \"bronze\")),\n  contagem = c(7, 6, 8)\n)\n# Exibe os dados\nolimpiadas\n\n\n\n\n\n\nmedalha\ncontagem\n\n\n\n\nouro\n7\n\n\nprata\n6\n\n\nbronze\n8\n\n\n\n\n\n\n\nMontamos um gráfico de colunas usando o código abaixo.\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNovamente, vamos decompor o código. Primeiro, usamos a função ggplot() para declarar que queremos fazer um gráfico. Precisamos informar: (1) uma base de dados, data; (2) duas variáveis que serão mapeadas aes(x, y); (3) um “elemento geométrico” geom.\nO argumento data deve ser o nome da nossa base de dados: neste caso, data = hprice1.\nA função aes() é o que transforma as variáveis (as colunas da base de dados) em elementos visuais. Escolhemos aes(x = medalha, y = contagem).\nA primeira variável é categórica e indica o tipo da medalha. Como especificamos a ordem de grandeza no código anterior, a função aes() sabe que ouro &gt; prata &gt; bronze.\nA segunda variável é numérica e indica quantas medalhas de cada tipo foram ganhas. Na prática, o aes() usa esta informação para definir a altura da coluna.\nEspecificamos qual deve ser o elemento visual somando a função geom_col() no código inicial. Esta função indica que queremos um gráfico de colunas."
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#características-estéticas",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#características-estéticas",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Características estéticas",
    "text": "Características estéticas\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nUm gráfico de colunas tem três elementos estéticos principais:\n\ncolor - Define a cor do contorno da coluna.\nfill - Define a cor que preenche a coluna.\nalpha - Define o nível de transparência das cores.\n\nVale notar que os argumento x e y também são elementos estéticos. Mais especificamente eles são elementos estéticos variáveis, logo são mapeado com a função aes(). No caso da função geom_bar() apenas x é obrigatório enquanto que a função geom_col() exige tanto x como y.\n\nCores\nTemos o controle de duas cores: do contorno da coluna (color) e da cor que preenche a coluna (fill). O exemplo abaixo ilustra como podemos modificar estes elementos estéticos. No exemplo usamos a cor steelblue para preencher a coluna e definimos um contorno escuro usando color = \"black\".\nUma lista completa de cores com nomes está disponível aqui. Também podemos especificar as cores usando código hexadecimal.\n\n# Exemplo chamando as cores por nomes\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(color = \"black\", fill = \"steelblue\")\n\n\n\n\n\n\n\n\n\n# Exemplo usando cores em hexadecimal\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(color = \"#E5E5E5\", fill = \"#2A9D8F\")\n\n\n\n\n\n\n\n\n\n\nAlpha\nO parâmetro alpha varia entre 0 e 1 e indica a transparências das cores. Quanto menor o valor de alpha, maior será a transparência no resultado final.\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(color = \"black\", fill = \"steelblue\", alpha = 0.3)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes. Como expliquei acima, esta função mapeia nossos dados em elementos visuais. No caso da função geom_col(), ela mapeia a variável x no eixo-x, representando a categoria e variável y no eixo-y é a altura da barra, que indica seu valor.\nMas podemos mapear as variáveis nos elementos estéticos: color, fill, alpha.\nO uso prático mais comum é de variar o elemento fill, a cor que preenche a coluna, segundo alguma variável nos dados.\nNosso gráfico de medalhas olímpicas ficaria mais intuitivo se as cores das colunas correspondessem às cores das medalhas. Vamos tentar construir este gráfico.\nComo queremos que a cor de cada barra seja difernete para cada tipo de medalha temos de incluir o argumento aes(fill = medalha) dentro de geom_col().\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = medalha))\n\n\n\n\n\n\n\n\nAgora o R mapeou uma cor diferente para cada valor distinto de medalha. Infelizmente, as cores padrão não nos ajudam neste caso. Podemos escolher estas cores manualmente usando a função scale_fill_manual(). O código abaixo faz este ajuste. As cores são inseridas dentro do argumento values em formato hexadecimal. Por fim, adicionamos a linha guides(fill = \"none\") para remover a legenda redundante.\n\nggplot(data = olimpiadas, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = medalha)) +\n  # Escolhe manualmente as cores das colunas\n  scale_fill_manual(values = c(\"#FFD700\", \"#C0C0C0\", \"#CD7F32\")) +\n  # Remove a legenda\n  guides(fill = \"none\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-quando-há-múltiplos-grupos",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#usando-cores-quando-há-múltiplos-grupos",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Usando cores quando há múltiplos grupos",
    "text": "Usando cores quando há múltiplos grupos\nQuando temos muitos grupos há duas opções para representar os dados: (1) empilhamos os diferentes grupos; ou (2) colocamos as colunas lado a lado.\nPara exemplificar, vamos aumentar nossa base de dados com medalhas olímpicas do Japão, Itália, Brasil e Nova Zelândia.\n\nolimp &lt;- data.frame(\n  pais = factor(\n    c(\"Japão\", \"Japão\", \"Japão\", \"Itália\", \"Itália\", \"Itália\", \"Brasil\",\n      \"Brasil\", \"Brasil\", \"Nova Zelândia\", \"Nova Zelândia\", \"Nova Zelândia\"),\n    levels = c(\"Japão\", \"Itália\", \"Brasil\", \"Nova Zelândia\")),\n  medalha = factor(\n    c(\"Ouro\", \"Prata\", \"Bronze\", \"Ouro\", \"Prata\", \"Bronze\", \"Ouro\", \"Prata\",\n      \"Bronze\", \"Ouro\", \"Prata\", \"Bronze\"),\n    levels = c(\"Ouro\", \"Prata\", \"Bronze\")),\n  contagem = c(20, 28, 23, 10, 10, 20, 7, 6, 8, 7, 6, 7)\n)\n\n\n\n\n\n\npais\nmedalha\ncontagem\n\n\n\n\nJapão\nOuro\n20\n\n\nJapão\nPrata\n28\n\n\nJapão\nBronze\n23\n\n\nItália\nOuro\n10\n\n\nItália\nPrata\n10\n\n\nItália\nBronze\n20\n\n\nBrasil\nOuro\n7\n\n\nBrasil\nPrata\n6\n\n\nBrasil\nBronze\n8\n\n\nNova Zelândia\nOuro\n7\n\n\nNova Zelândia\nPrata\n6\n\n\nNova Zelândia\nBronze\n7\n\n\n\n\n\n\n\nAgora, queremos que cada país seja representado por uma cor distinta. Vamos, então, inserir aes(fill = pais) dentro de geom_col(). Note que o padrão da função é de empilhar os resultados.\nOu seja, temos o número total de medalhas de ouro, prata e bronze, onde cada cor representa um país diferente.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais))\n\n\n\n\n\n\n\n\nSe quiseremos ter uma visualização que represente mais diretamente a performance de cada país temos que incluir um argumento adicional position = \"dodge\" dentro da função geom_col(). Agora fica mais evidente, por exemplo, que o Japão teve um número grande de medalhas de prata e que o Brasil e a Nova Zelândia tiveram desempenhos muito semelhantes - o Brasil ficou na frente por causa de uma medalha de bronze.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"dodge\")\n\n\n\n\n\n\n\n\nO padrão da função geom_col() é position = \"stack\", que empilha as observações. Novamente, podemos escolher manualmente as cores dos grupos usando a função scale_fill_manual()\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"dodge\") +\n  # Escolhe manualmente as cores das colunas\n  scale_fill_manual(\n    # Título da legenda (opcional)\n    name = \"País\",\n    # Valores das cores\n    values = c(\"#d62828\", \"#008C45\", \"#FFDF00\", \"#012169\"))\n\n\n\n\n\n\n\n\nVale notar que existem vários pacotes e funções com cores pré-definidas que simplificam o processo manual de escolher as cores. O exemplo mais simples é o scale_fill_brewer() que utiliza as paletas de cores do Color Brewer.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"dodge\") +\n  scale_fill_brewer(\n    # Título da legenda (opcional) - omite o título\n    name = \"\",\n    # Tipo (\"qual\" - qualitativo, \"div\" - divergente ou \"seq\" - sequencial)\n    type = \"qual\",\n    # Escolha da paleta\n    palette = 6\n    )\n\n\n\n\n\n\n\n\nUma última opção é de forçar o eixo-y a operar dentro do intervalo 0-1, isto é, fazer com que as barras somem 1 e representem a proporção de cada grupo. Fazemos isto utilizando o argumento position = \"fill\" dentro de geom_col().\nNo gráfico abaixo temos a participação relativa de cada país no total de medalhas. Note que este tipo de gráfico faz mais sentido quando temos a totalidade dos grupos.\n\nggplot(olimp, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = pais), position = \"fill\")\n\n\n\n\n\n\n\n\nNo exemplo abaixo montamos uma nova base de dados com o número de medalhas por continente. Para reduzir a digitação manual utilizamos a função rep() que repete uma mesma palavra. Agora que temos a totalidade dos grupos faz mais sentido um gráfico de colunas que represente a proporção de cada grupo.\n\nolimp_continentes &lt;- data.frame(\n  continente = factor(\n    rep(c(\"África\", \"América\", \"Ásia\", \"Europa\", \"Oceania\"), each = 3),\n    levels = c(\"Europa\", \"Ásia\", \"América\", \"Oceania\", \"África\")\n  ),\n  medalha = factor(\n    rep(c(\"Ouro\", \"Prata\", \"Bronze\"), times = 5)\n  ),\n  contagem = c(11, 12, 14, 72, 70, 70, 92, 80, 98, 141, 164, 193, 26, 13, 29)\n)\n\n\nggplot(data = olimp_continentes, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = continente), position = \"fill\") +\n  scale_fill_brewer(name = \"Continente\", type = \"qual\", palette = 6)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(data = olimp_continentes, aes(x = medalha, y = contagem)) +\n  geom_col(aes(fill = continente), position = \"fill\") +\n  scale_fill_brewer(type = \"qual\", palette = 6) +\n  labs(\n    title = \"Europa lidera Olimpíadas\",\n    subtitle = \"Medalhas obtidas por continente nas Olimpíadas de Tóquio 2020\",\n    x = \"Tipo da medalha\",\n    y = \"Share de medalhas\",\n    caption = \"Fonte: www.olympiandatabase.com\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#invertendo-os-eixos",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#invertendo-os-eixos",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Invertendo os eixos",
    "text": "Invertendo os eixos\nPara facilitar a visualização das categorias pode ser interessante inverter os eixos do gráfico. Para isto usa-se o coord_flip(). Vale notar que é possível inverter os eixos de qualquer tipo de gráfico do ggplot.\nPara este exemplo, vamos utilizar a base de dados gapminder. Este base compila dados de PIB per capita, população e expectativa de vida de vários países ao longo dos anos. Como o número de países é muito grande, vamos nos concentrar somente nos países do continente americano. Além disso, vamos olhar somente para as observações em 2007.\n\nlibrary(\"gapminder\")\ndata(\"gapminder\")\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n29\n8425333\n779\n\n\nAlbania\nEurope\n1952\n55\n1282697\n1601\n\n\nAlgeria\nAfrica\n1952\n43\n9279525\n2449\n\n\nAngola\nAfrica\n1952\n30\n4232095\n3521\n\n\nArgentina\nAmericas\n1952\n62\n17876956\n5911\n\n\nAustralia\nOceania\n1952\n69\n8691212\n10040\n\n\nAustria\nEurope\n1952\n67\n6927772\n6137\n\n\nBahrain\nAsia\n1952\n51\n120447\n9867\n\n\nBangladesh\nAsia\n1952\n37\n46886859\n684\n\n\nBelgium\nEurope\n1952\n68\n8730405\n8343\n\n\n\n\n\n\n\nO código abaixo seleciona as linhas relevantas.\n\n# Selecionar apenas os países das Américas em 2007\namericas &lt;- subset(gapminder, continent == \"Americas\" & year == 2007)\n\nVamos montar um gráfico de colunas para comparar a expectativa de vida nos países do continente americano.\n\nggplot(data = americas, aes(x = country, y = lifeExp)) +\n  geom_col() +\n  labs(title = \"Expectativa de Vida\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nPodemos melhorar o gráfico acima reordenando as colunas. Como comentei acima, podemos definir a ordem de uma variável categórica usando o argumento levels da função factor().\nImagine que você tem uma série de avaliações que podem ser “Bom”, “Médio” ou “Ruim” armazenadas num vetor chamado feedback. Para estruturar esta variável como factor é preciso definir qual a ordem destes valores. No exemplo abaixo define-se uma relação crescente: de “Ruim” até “Bom”.\n\nfeedback &lt;- c(\"Bom\", \"Bom\", \"Médio\", \"Ruim\", \"Médio\", \"Médio\")\nsatisfacao &lt;- factor(feedback, levels = c(\"Ruim\", \"Médio\", \"Bom\"))\n\nsatisfacao\n\n[1] Bom   Bom   Médio Ruim  Médio Médio\nLevels: Ruim Médio Bom\n\n\nNo nosso caso, queremos que a ordem do nome dos países seja a mesma que a ordem de grandeza da expectativa de vida. Para fazer isto usamos a função order(). O código abaixo pode parecer confuso à primeira vista, mas veremos maneiras de simplificá-lo em posts futuros.\n\nlvls &lt;- americas[[\"country\"]][order(americas[[\"lifeExp\"]])]\namericas[\"country_order\"] &lt;- factor(americas[[\"country\"]], levels = lvls)\n\nO gráfico de colunas agora está ordenado. Note que também modifico o título dos eixos para suprimir um deles definindo x = NULL.\n\nggplot(data = americas, aes(x = country_order, y = lifeExp)) +\n  geom_col() +\n  labs(\n    title = \"Expectativa de Vida\",\n    x = NULL,\n    y = \"Anos\") +\n  coord_flip()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#estatísticas-descritivas",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#estatísticas-descritivas",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Estatísticas descritivas",
    "text": "Estatísticas descritivas\nO ggplot permite visualizar estatísticas descritivas simples diretamente, dispensando a manipulação dos dados. Voltando ao nosso exemplo inicial do preços de imóveis na base hprice1 podemos visualizar o preço mediano dos imóveis por número de dormitórios.\nPara fazer isto inserimos o argumento stat = \"summary_bin\" dentro da função geom_bar(). Além disso, precisamos adicionar a função desejada pelo argumento fun`.\n\n# Preço mediano por número de dormitórios\nggplot(data = hprice1, aes(x = factor(bdrms), y = price)) +\n  geom_bar(stat = \"summary_bin\", fun = median)\n\n\n\n\n\n\n\n\nDa mesma forma, podemos comparar o preço médio dos imóveis por número de dormitórios.\n\n# Preço médio por número de dormitórios\nggplot(data = hprice1, aes(x = factor(bdrms), y = price)) +\n  geom_bar(stat = \"summary_bin\", fun = mean)\n\n\n\n\n\n\n\n\nIsto pode ser utilizado para visulizar rapidamente o valor médio entre grupos distintos. Na base hprice1, a varíavel colonial indica o estilo arquitetônico do imóvel. Em particular se colonial = 1 o imóvel tem um estilo colonial (rústico). Caso contrário colonial = 0.\n\n# Preço médio comparando imóveis coloniais e não-coloniais\nggplot(data = hprice1, aes(x = factor(colonial), y = price)) +\n  geom_bar(stat = \"summary_bin\", fun = mean)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#resumo",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#resumo",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar gráficos de colunas/barras sofisticados usando poucas linhas de código. Em qualquer gráfico temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos importantes:\n\nA ordem das colunas é definida pelos níveis da variável x. Para reordenar as colunas é preciso definir uma nova ordem usando factor(x, levels = c(...)).\nAlém dos elementos estéticos, gráficos de colunas tem o argumento position que define o comportamento do gráfico.\nA função geom_bar() conta a ocorrência dos valores nos dados e pode ser utilizada também para informar outras estatísticas descritivas.\nA função geom_col() exige os argumentos x e y, onde y define a altura da coluna no gráfico.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos.\n\nggplot(data = hprice1, aes(x = factor(bdrms), y = price)) +\n  geom_bar(\n    stat = \"summary_bin\",\n    fun = median,\n    fill = \"#264653\") +\n  labs(\n    title = \"Preço mediano do imóvel por número de dormitórios\",\n    x = \"Número de dormitórios\",\n    y = \"Preço (USD milhares)\",\n    caption = \"Fonte: Wooldridge (Boston Globe)\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/2-grafico-coluna.html#footnotes",
    "href": "posts/ggplot2-tutorial/2-grafico-coluna.html#footnotes",
    "title": "Fundamentos: gráfico de coluna",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara mais informações sobre a ONG Gapminder veja o link.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "",
    "text": "O gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente qual a relação entre as duas variáveis escolhidas.\n\n\n\n\n\n\n\n\n\nEm geral, colocamos a variável explicativa (regressor) no eixo horizontal e a variável explicada (resposta) no eixo vertical.\nNeste post vamos entender como montar gráficos de dispersão no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para explorar uma base de preços de imóveis. Vamos entender como customizar o gráfico, variando as cores, os formatos e o tamanho dos círculos; além disso, vamos montar um gráfico de dispersão junto com uma linha de regressão."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#gráfico-de-dispersão",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#gráfico-de-dispersão",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "",
    "text": "O gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente qual a relação entre as duas variáveis escolhidas.\n\n\n\n\n\n\n\n\n\nEm geral, colocamos a variável explicativa (regressor) no eixo horizontal e a variável explicada (resposta) no eixo vertical.\nNeste post vamos entender como montar gráficos de dispersão no R usando o pacote ggplot2. Primeiro vamos trabalhar um exemplo, passo a passo, para explorar uma base de preços de imóveis. Vamos entender como customizar o gráfico, variando as cores, os formatos e o tamanho dos círculos; além disso, vamos montar um gráfico de dispersão junto com uma linha de regressão."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#r",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#r",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "R",
    "text": "R\nO primeiro passo é instalar o pacote ggplot2. O R funciona como um repositório de pacotes: cada pacote é como uma família de funções. Em geral, cada pacote tem uma finalidade específica. O ggplot2 contém uma série de funções que permitem a construção de gráficos.\nO R tem um funcionalidade embutida que facilita o download e a instalação de pacotes. Usamos a função install.packages(\"nome_do_pacote\"). Então, para instalar o ggplot2 executamos o código abaixo.\nSe você estiver usando o R fora do RStudio é provável que a função abaixo solicite que você escolha um servidor a partir de uma lista. Escolha o que for mais próximo - geograficamente - de onde você está. No meu caso eu sempre utilizo o “Brazil (SP 1) [https] - University of Sao Paulo, Sao Paulo”. Se você usa o R dentro do RStudio pode ignorar este passo.\n\n# Instalar o pacote ggplot2 (se necessário)\ninstall.packages(\"ggplot2\")\n\nA cada vez que abrimos o R precisamos carregar os pacotes adicionais que instalamos previamente. Isto pode parecer trabalhoso à primeira vista, mas faz muito sentido: evita conflitos entre pacotes e é mais eficiente.\nPara carregar o ggplot2 usamos a função library (biblioteca).\n\n# Carrega o pacote ggplot2\nlibrary(ggplot2)\n\nEnquanto a maioria dos pactoes funciona como repositórios de funções alguns servem como repositórios de bases de dados. É o caso do pacote wooldridge que carrega as bases de dados utilizadas no livro Introductory Econometrics: A Modern Approach do economista Jeffrey Wooldridge.\n\n# Instalar o pacote ggplot2 (se necessário)\ninstall.packages(\"wooldridge\")\n# Carregar o pacote wooldridge\nlibrary(wooldridge)\n\nNos primeiros exemplos abaixo vamos trabalhar com a base de dados hrpice1 que coleta informações de preços de venda de imóveis na região metropolitana de Boston, nos EUA, em 1990. Para carregar a base usamos a função data().\n\ndata(\"hprice1\")\n\nVisualização e análise de dados são habilidades complementares. Aqui, vamos nos focar apenas nas habilidades visuais.\nA função head(), quando aplicada a um objeto data.frame mostra as primeiras linhas da tabela. Há muitas colunas mas vamos focar inicialmente na price que é o preço em milhares de dólares e na coluna sqrft que é o tamanho do imóvel em pés quadrados.\n\nhead(hprice1)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#ggplot2",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#ggplot2",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "ggplot2",
    "text": "ggplot2\nA estrutura de um gráfico do ggplot2 parte de três elementos básicos: (1) uma base de dados, isto é, um objeto data.frame; (2) um mapeamento de variáveis, feito com auxílio da função aes(); (3) a escolha da forma do gráfico, feito com as funções geom.\nO ggplot2 funciona adicionando camadas sobre um gráfico inicial.\nComeçamos com a função ggplot() e vamos adicionando geoms, funções auxiliares que especificam a forma do gráfico. Este processo construtivo de adicionar elementos a um gráfico é o principal diferencial do ggplot.\nOu seja, temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEsta estrutura básica é esquematizada no pseudo-código abaixo.\n\nggplot(data = base_de_dados, aes(x = variavel_x, y = variavel_y)) +\n  geom_point()\n\nNo nosso caso a base de dados é a hprice1 e as variáveis são sqrft e price.\nTemos que informar isto usando data = hprice e aes(x = sqrft, y = price).\nPor fim, como queremos um gráfico de dispersão escolhemos o geom_point(). Esta última chamada é adicionada à função inicial com o sinal de soma +.\nO código abaixo junta todos estes elementos e resulta num gráfico de dispersão entre o tamanho do imóvel (sqrft) e seu preço de venda (price).\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point()\n\n\n\n\n\n\n\n\nO gráfico acima ainda está bastante cru e podemos melhorá-lo de diversas formas. Ainda assim, ele já é interessante: revela uma relação crescente entre o tamanho do imóvel e do seu preço de venda."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#elementos-estéticos",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#elementos-estéticos",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Elementos estéticos",
    "text": "Elementos estéticos\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nSão quatro os principais elementos estéticos que podemos manipular no caso do geom_point:\n\ncolor - a cor do objeto\nalpha - a transparência da cor\nsize - o tamanho do objeto\nshape - o formato do objeto\n\nQuando executamos o código acima o valor destes parâmetros foi definido automaticamente. Podemos modificá-los chamando eles explicitamente.\n\nColor - cores\nA maneira mais simples de alterar as cores é chamando ela por nome. No exemplo usamos a cor steelblue. Uma lista completa de cores está disponível aqui.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém é possível escolher a cor via hexadecimal.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"#e76f51\")\n\n\n\n\n\n\n\n\n\n\nAlpha - transparência\nO parâmetro alpha controla o nível de transparência da cor. Este artifício costuma ser útil para evitar que muitos pontos fiquem sobrepostos (overplotting).\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\", alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize - tamanho\nPodemos manipular o tamanho dos pontos usando size e ajustando o valor numérico.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\", size = 5)\n\n\n\n\n\n\n\n\nAgora que os círculos estão maiores há mais casos de sobreposição. Uma solução para evitar isto é aplicar algum valor de alpha.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"steelblue\", size = 5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nShape - formato\nPor padrão o formato do geom_point é um círculo mas há muitas outras opções. Para trocar o formato do objeto usamos shape = 2\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(shape = 2)\n\n\n\n\n\n\n\n\nO gráfico abaixo ilustra os principais tipos de formatos disponíveis.\n\n\n\n\n\n\n\n\n\nNote que em alguns casos como o 21 é possível controlar tanto a cor do contorno do círculo como também da cor de dentro.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(shape = 21, color = \"steelblue\", fill = \"orange\")\n\n\n\n\n\n\n\n\n\n\nCombinando todos os elementos\nO gráfico abaixo serve apenas para ilustrar o uso de todos os parâmetros. Naturalmente, o uso destes elementos estéticos deve favorecer o melhor entendimento do gráfico e não deve ser utilizado de forma gratuita.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(\n    shape = 21,\n    color = \"steelblue\",\n    fill = \"orange\",\n    size = 7,\n    alpha = 0.75)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nOs elementos estéticos também podem ser utilizados para representar variáveis nos dados. Vamos voltar para a função aes. Como expliquei acima, esta função “transforma” nossos dados em elementos visuais. Nos casos acima, ela mapeia as variáveis x e y nas suas respectivas coordenadas.\nMas podemos mapear as variáveis nos elementos estéticos: color, alpha, size, shape.\nVamos primeiro voltar à nossa base de dados. Olhando para as primeiras linhas vemos que há uma coluna chamada colonial que é uma variável binária que indica se o estilo arquitetônico do imóvel é colonial.\n\nhead(hprice1)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810\n\n\nPodemos plotar o mesmo gráfico de dispersão mas fazer com que a cor do círculo represente a variável colonial. No gráfico abaixo, os pontos em azul são imóveis com estilo colonial, enquanto que os pontos em vermelho (salmão) são os imóveis de outros estilos.\nVemos que parece haver uma tendência de que os imóveis coloniais vendem por valores mais elevados, pois os pontos azuis aparecem acima dos pontos vermelhos, mas há exceções. Isso sugere que imóveis de estilo colonial têm preço mais elevado do que imóveis de tamanho similar, mas construídos em estilos diferentes.\nEste exemplo mostra como a visualização ajuda a formar algumas hipóteses iniciais que depois podem ser verificadas usando modelos estatísticos.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(aes(color = factor(colonial)))\n\n\n\n\n\n\n\n\nDa mesma forma que modificamos a cor podemos modificar o tamanho segundo, por exemplo, o número de dormitórios.\nNo exemplo abaixo o tamanho do círculo é proporcional ao número de dormitórios.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(aes(size = bdrms), alpha = 0.5)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point() +\n  labs(\n    title = \"Quanto maior, mais caro\",\n    subtitle = \"Relação entre o preço do imóvel e sua área útil.\",\n    x = \"Área útil (pés quadrados)\",\n    y = \"Preço (USD milhares)\",\n    caption = \"Fonte: Wooldridge (Boston Globe)\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#incluindo-uma-linha-de-regressão",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#incluindo-uma-linha-de-regressão",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Incluindo uma linha de regressão",
    "text": "Incluindo uma linha de regressão\nComo falamos no início do post, parte da mágica do ggplot é ir “somando” objetos. Podemos desejar incluir, por exemplo, uma linha de regressão em cima do gráfico de dispersão. No caso de uma regressão linear simples, esta linha mostra a correlação linear entre a variável no eixo horizontal com a variável no eixo vertical.\nA função geom_smooth facilita a inclusão de linhas de regressão. Se não for fornecido argumento à função ela tentará uma aproximação LOESS. No exemplo abaixo eu escolho method = \"lm\" para que a função aproxime a relação linear (lm de linear model).\nNo nosso caso, a linha mostra a relação linear entre o preço do imóvel e a sua área útil. O argumento se = FALSE serve para omitir a estimativa do erro-padrão do coeficiente e deixar a visualização mais limpa.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nPode-se especificar uma forma particular para a regressão. No caso abaixo faço uma regressão polinomial de segunda ordem (quadrática) usando a função poly.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point() +\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nA curva quadrática parece ter um ajuste visual melhor aos dados."
  },
  {
    "objectID": "posts/ggplot2-tutorial/1-grafico-dispersao.html#resumo",
    "href": "posts/ggplot2-tutorial/1-grafico-dispersao.html#resumo",
    "title": "Fundamentos: gráfico de dispersão",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar gráficos de dispersão sofisticados usando poucas linhas de código. Em qualquer gráfico temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos.\n\nggplot(data = hprice1, aes(x = sqrft, y = price)) +\n  geom_point(color = \"#e63946\", alpha = 0.75, size = 2) +\n  geom_smooth(formula = y ~ poly(x, 2), method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Quanto maior, mais caro\",\n    subtitle = \"Relação quadrática entre o preço do imóvel e sua área útil.\",\n    x = \"Área útil (pés quadrados)\",\n    y = \"Preço (USD milhares)\",\n    caption = \"Fonte: Wooldridge (Boston Globe)\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/0-introduction.html",
    "href": "posts/ggplot2-tutorial/0-introduction.html",
    "title": "Introdução",
    "section": "",
    "text": "Nesta série, exploraremos uma das ferramentas mais poderosas e versáteis para visualização de dados - o pacote ggplot2 em R.\nA visualização de dados desempenha um papel fundamental na compreensão e comunicação de dados. O pacote ggplot2, desenvolvido por Hadley Wickham, é amplamente reconhecido como uma das melhores opções para a criação de gráficos esteticamente elegantes e informativos: atualmente, o ggplot2 é utilizado nas melhores publicações acadêmicas e pelos melhores jornalistas de dados.\n\nGrammar of graphics: O ggplot2 cria gráficos seguindo uma abordagem consistente e “construtiva” baseada numa “gramática”. Essencialmente, montamos os gráficos somando elementos estéticos individualmente; isto significa que você pode criar visualizações complexas a partir de um pequeno conjunto de componentes gráficos. Este é um conceito fundamental e sua poderosa versatilidade ficará mais evidente com a prática.\nGráficos elegantes: O ggplot2 é permite criar gráficos de alta qualidade e visualmente atraentes, que vão imbuir seu trabalho de profissionalismo.\nPersonalização: O ggplot2 oferece uma ampla gama de opções de personalização, permitindo que você ajuste cada aspecto do seu gráfico. Pode-se modificar cores, fontes, tamanhos, e muito mais para atender às suas necessidades específicas.\nRecursos adicionais: a popularidade quase universal do ggplot2 significa que há diversos recursos riquíssimos para explorar. A lista oficial inclui mais de 100 pacotes que oferecem diversas funções que potencializam o ggplot2.\n\nAo longo desta série de tutoriais, abordaremos os conceitos fundamentais do ggplot2 e guiaremos você passo a passo na criação de visualizações incríveis. Desde a construção de gráficos básicos até técnicas mais avançadas, você aprenderá a aproveitar todo o potencial do ggplot2 para apresentar e analisar seus dados de forma impactante.\nSe você está animado para começar, vá para aqui para acessar o primeiro tutorial da série.\n\n\nO conteúdo destes posts segue uma filosofia básica: aprender a programar envolve prática e repetição. A melhor maneira de consumir este material é reescrevendo as linhas de código e executando elas.\nO material começa com quatro gráfico fundamentais. A apresentação do texto nestes posts é introdutória e supõe conhecimento nenhum de R. Os textos começam com a instalação do pacote ggplot2 e discutem brevemente o que é um pacote e como escrever linhas de código no R.\n\nGráfico de dispersão (ou scatterplot)\nGráfico de coluna/barra\nGráfico de histograma\nGráfico de linha\n\nAntes de introduzir outros tipos de gráfico foco então alguns aspectos essenciais de gráficos. É importante que um gráfico seja o mais autoexplicativo possível. A maneira mais simples e efetiva de fazer isto é com títulos, legendas e caixas de texto; mas há também outras formas mais sutis de alcançar este objetivo: destacando certas áreas do gráfico ou usando cores chamativas.\n\nPlotando texto e destacando observações\nEscalas, legendas e temas (scales, labels and themes)\nGuia básico de cores e temas\n\nEstes quatro gráficos costumam ser a base de qualquer análise e vão resolver o seu problema em 90% dos casos. Depois, introduzo alguns gráficos diferentes:\n\nFacet plots\nGráficos lollipop\nGráficos de área\nGráficos de tile (ou mapas de calor)\n\nPor fim, vou discutir algumas extensões populares do ggplot2 e mostrar algumas estratégias para produzir gráficos de qualidade em grande escala.\n\nExtensões: indo muito além do básico\nProdução: gráficos em grande escala\n\n\n\nCopiar e colar é uma opção tentadora, mas que pode atrapalhar seu aprendizado: se você sempre copia e cola você evita de cometer erros; e cometer erros é parte importante de aprender algo novo.\nConsidere o código abaixo. Se eu executo ele numa nova sessão de R eu encontro um erro.\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in ggplot(mtcars, aes(x = wt, y = MPG)): could not find function \"ggplot\"\n\n\nO retorno do comando acima indica que não foi possível encontrar a função ggplot. Isto acontece porque eu esqueci de carregar o pacote ggplot2. Para carregar o pacote eu rodo library(ggplot2). O comando agora retorna um novo erro.\n\nlibrary(ggplot2)\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'MPG' not found\n\n\nAgora temos um novo erro: o objeto MPG não foi encontrado. Talvez seja um erro de digitação. Vamos conferir o nome das colunas da base de dados mtcars usando a função names().\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nDe fato, após a verificação, vemos que a variável mpg é minúscula. Agora o comando funciona.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\nO processo iterativo de tentativa e erro é parte natural e rotineira da tarefa de programação. É importante entender que o seu código provavelmente não vai funcionar “de primeira” (e às vezes nem de segunda, nem de terceira…). É normal errar e devemos aproveitar o fato que as mensagens de erro do R costumam ser bastante instrutivas!"
  },
  {
    "objectID": "posts/ggplot2-tutorial/0-introduction.html#roteiro",
    "href": "posts/ggplot2-tutorial/0-introduction.html#roteiro",
    "title": "Introdução",
    "section": "",
    "text": "O conteúdo destes posts segue uma filosofia básica: aprender a programar envolve prática e repetição. A melhor maneira de consumir este material é reescrevendo as linhas de código e executando elas.\nO material começa com quatro gráfico fundamentais. A apresentação do texto nestes posts é introdutória e supõe conhecimento nenhum de R. Os textos começam com a instalação do pacote ggplot2 e discutem brevemente o que é um pacote e como escrever linhas de código no R.\n\nGráfico de dispersão (ou scatterplot)\nGráfico de coluna/barra\nGráfico de histograma\nGráfico de linha\n\nAntes de introduzir outros tipos de gráfico foco então alguns aspectos essenciais de gráficos. É importante que um gráfico seja o mais autoexplicativo possível. A maneira mais simples e efetiva de fazer isto é com títulos, legendas e caixas de texto; mas há também outras formas mais sutis de alcançar este objetivo: destacando certas áreas do gráfico ou usando cores chamativas.\n\nPlotando texto e destacando observações\nEscalas, legendas e temas (scales, labels and themes)\nGuia básico de cores e temas\n\nEstes quatro gráficos costumam ser a base de qualquer análise e vão resolver o seu problema em 90% dos casos. Depois, introduzo alguns gráficos diferentes:\n\nFacet plots\nGráficos lollipop\nGráficos de área\nGráficos de tile (ou mapas de calor)\n\nPor fim, vou discutir algumas extensões populares do ggplot2 e mostrar algumas estratégias para produzir gráficos de qualidade em grande escala.\n\nExtensões: indo muito além do básico\nProdução: gráficos em grande escala\n\n\n\nCopiar e colar é uma opção tentadora, mas que pode atrapalhar seu aprendizado: se você sempre copia e cola você evita de cometer erros; e cometer erros é parte importante de aprender algo novo.\nConsidere o código abaixo. Se eu executo ele numa nova sessão de R eu encontro um erro.\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in ggplot(mtcars, aes(x = wt, y = MPG)): could not find function \"ggplot\"\n\n\nO retorno do comando acima indica que não foi possível encontrar a função ggplot. Isto acontece porque eu esqueci de carregar o pacote ggplot2. Para carregar o pacote eu rodo library(ggplot2). O comando agora retorna um novo erro.\n\nlibrary(ggplot2)\n\nggplot(mtcars, aes(x = wt, y = MPG)) +\n  geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'MPG' not found\n\n\nAgora temos um novo erro: o objeto MPG não foi encontrado. Talvez seja um erro de digitação. Vamos conferir o nome das colunas da base de dados mtcars usando a função names().\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nDe fato, após a verificação, vemos que a variável mpg é minúscula. Agora o comando funciona.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\nO processo iterativo de tentativa e erro é parte natural e rotineira da tarefa de programação. É importante entender que o seu código provavelmente não vai funcionar “de primeira” (e às vezes nem de segunda, nem de terceira…). É normal errar e devemos aproveitar o fato que as mensagens de erro do R costumam ser bastante instrutivas!"
  },
  {
    "objectID": "posts/general-posts/repost-otimizacao-newton/index.html",
    "href": "posts/general-posts/repost-otimizacao-newton/index.html",
    "title": "Repost: Otimização numérica - métodos de Newton",
    "section": "",
    "text": "Métodos de otimização numérica são frequentemente necessários para estimar modelos econométricos e resolver problemas de máximo/mínimo em geral. Em particular, para encontrar estimadores de extremo — como estimadores do método generalizado de momentos (GMM) ou estimadores de máxima verossimilhança (MLE) — é necessário resolver um problema de otimização. Na maior parte dos casos, estes métodos rodam no background de funções prontas, mas não custa entender um pouco mais sobre como eles funcionam, até porque um dos métodos mais populares, o método de Newton (e suas variantes) é bastante simples e intuitivo.\n\n\nComo primeiro exemplo do método de Newton vou tomar emprestada uma sugestão deste tweet.\n\n\n\n\n\nO algoritmo descrito no tweet para encontrar \\(\\sqrt{y}\\) é basicamente o seguinte:\n\nEncontre um número \\(x\\) tal que \\(x^2 \\approx y\\) (ex: \\(y = 17\\), \\(4^2 = 16\\));\nCompute a diferença \\(d = y - x^2\\) (\\(d = 17 - 16 = 1\\));\nAtualize \\(x\\) pela diferença \\(d/2x\\) (\\(x = 4 + 1/8 = 4.125\\))\nRepita 2 e 3, com o novo valor de x.\n\nO código abaixo implementa os passos acima. Começo escolhendo um inteiro aleatório \\(y\\) entre \\(0\\) e \\(10^7\\). O desafio será encontrar \\(\\sqrt{y}\\). Usando o comando set.seed(1984) você poderá replicar os resultados abaixo. O número sorteado foi \\(6588047\\) e \\(\\sqrt{6588047} = 2566.719\\).\nPara demonstrar a poder do simples método acima vou começar com um chute inicial bem ruim, isto é, vou escolher um \\(x\\) inicial igual a \\(1\\). Como quero mostrar os passos intermediários do algoritmo vou criar um vetor numérico que salva todos os valores de x a cada iteração. Há diversas maneiras de escrever o loop abaixo e ao longo do post vou mostrar três tipos de loop (for-loop, repeat e while).\n\n# Para replicar os resultados\nset.seed(1984)\n# Sorteia um número inteiro\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\n# Cria um vetor x de comprimento 15\nx &lt;- vector(length = 20)\n# Primeiro valor de x = 1\nx[1] &lt;- 1\n# Loop\nfor (i in 1:19) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nPodemos ver graficamente como o algortimo se aproxima do valor verdadeiro de \\(\\sqrt{y}\\) rapidamente, mesmo partindo de um ponto inicial distante. Em vermelho, está o valor correto de \\(\\sqrt{y}\\).\n\nplot(2:20, x[2:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nComo os primeiros valores do gráfico acima são muito altos, faço um segundo gráfico usando somente os últimos valores que foram computados.\n\nplot(13:20, x[13:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nNote que depois de 16 repetições, a diferença entre \\(x\\) e \\(y\\) é tão próxima de zero que o R arredonda ela para 0.\n\nx - sqrt(y)\n\n [1] -2.565719e+03  3.291457e+06  1.644446e+06  8.209418e+05  4.091915e+05\n [6]  2.033204e+05  1.003928e+05  4.894506e+04  2.325311e+04  1.047078e+04\n[11]  4.204686e+03  1.305444e+03  2.200559e+02  8.688284e+00  1.465521e-02\n[16]  4.183812e-08  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n\n\nMas como funciona este algoritmo? Basicamente, estamos procurando a raiz de um polinômio. No caso, estamos buscando resolver \\(f(x) = x^2 - y = 0\\) (em que \\(y\\) é dado). A solução proposta pelo método acima é de aproximar este polinômio linearmente (usando a primeira derivada) e então encontrar a raiz desta aproximação. Para facilitar o exemplo vamos tomar \\(y = 2\\). Então temos o problema de encontrar a raiz do seguinte polinômio:\n\\[\nP(x) = x^2 - 2\n\\]\nVamos tomar o valor \\(x_{0} = 5\\) como chute inicial. A aproximação linear de \\(P(x)\\) é a derivada com respeito a \\(x\\), isto é, \\(2x\\). Avaliada no ponto \\(x_{0}\\), temos \\(P'(5) = 10\\). Assim, temos uma reta de inclinação \\(10\\) que passa no ponto \\((5, 23)\\). Com isso podemos traçar a reta vermelha no gráfico abaixo e calcular o ponto em que esta reta corta o eixo-x. A equação da reta pode ser escrita como:\n\\[\n\\frac{y_{2} - y_{1}}{x_{2} - x_{1}} = f'(x_{1})\n\\]\nComo queremos encontrar o ponto em que a curva corta o eixo-x estamos, na verdade, buscando o ponto \\((x_{2}, 0)\\). Logo, podemos substituir \\(y_{2} = 0\\) na expressão acima e resolver para \\(x_{2}\\):\n\\[\nx_{2} = x_{1} - \\frac{y_{1}}{f'(x_{1})} = x_{1} - \\frac{f(x_{1})}{f'(x_{1})}\n\\]\nA expressão acima é essência do método de Newton. Substituindo nossos valores temos que:\n\\[\n\\frac{23}{5 - x} = 10 \\longrightarrow x = 2.7\n\\]\nNo gráfico abaixo podemos ver como funciona este processo. Parte-se de um ponto inicial \\(x = 5\\). Encontra-se a reta tangente neste ponto (linha em vermelho) e computa-se o valor de \\(x\\) em que esta reta cruza o eixo-x (ponto vermelho). O processo repete-se agora tomando o ponto em vermelho como ponto de partida. De maneira geral, o processo de atualização segue a regra abaixo:\n\\[\n  x_{n + 1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}\n\\]\n\n\n\n\n\n\ny &lt;- 2\nx &lt;- vector(length = 10)\n# Primeiro valor de x = 1\nx[1] &lt;- 5\n# Loop\nfor (i in 1:9) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nO gráfico abaixo mostra os próximos passos do algoritmo. Após poucas repetições já estamos muito próximos da resposta correta. Note que isto acontece porque estamos trabalhando com uma função bastante simples.\n\n\n\n\n\nPode-se generalizar o problema acima facilmente para encontrar a n-ésima raiz de qualquer valor. No código abaixo isto pode ser feito variando o valor de n. O código abaixo também é uma variação em relação aos anteiores. Ele é um repeat-break, ao invés do for-loop que estávamos usando acima. Isto é, ele repete uma operação enquanto alguma condição for válida. Assim, podemos exigir que o algoritmo pare quando algum critério de convergência for atingido. No caso abaixo, especifico que o loop pare quando a distância entre os valores sucessivos de \\(x\\) for muito pequena (menor que \\(10^{-8}\\)) ou quando o número de iterações chegar a 100 repetições.\n\n# Dá pra melhorar um pouco o loop (usando repeat) e agora generalizar para n\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\nx &lt;- vector(length = 100)\nx[1] &lt;- (10^7 - 1) / 2\ni &lt;- 1\nn &lt;- 4\nrepeat {\n  x_chute &lt;- x_chute_novo\n  dist &lt;- y - x_chute^n\n  x_chute_novo &lt;- x_chute + dist / (n * x_chute^(n - 1))\n  x[i + 1] &lt;- x_chute_novo\n  # print(paste(\"iteration =\", i))\n  if (abs(x_chute_novo - x[i]) &lt; 10^(-8) | i &gt; 99) {\n    break\n  }\n  i &lt;- i + 1\n}\nplot(x[1:i + 1] - y^(1 / n), main = \"Distância entre valor computado e valor verdadeiro\", xlab = \"Contagem da iteração\", ylab = \"Distância\")"
  },
  {
    "objectID": "posts/general-posts/repost-otimizacao-newton/index.html#exemplo-simples",
    "href": "posts/general-posts/repost-otimizacao-newton/index.html#exemplo-simples",
    "title": "Repost: Otimização numérica - métodos de Newton",
    "section": "",
    "text": "Como primeiro exemplo do método de Newton vou tomar emprestada uma sugestão deste tweet.\n\n\n\n\n\nO algoritmo descrito no tweet para encontrar \\(\\sqrt{y}\\) é basicamente o seguinte:\n\nEncontre um número \\(x\\) tal que \\(x^2 \\approx y\\) (ex: \\(y = 17\\), \\(4^2 = 16\\));\nCompute a diferença \\(d = y - x^2\\) (\\(d = 17 - 16 = 1\\));\nAtualize \\(x\\) pela diferença \\(d/2x\\) (\\(x = 4 + 1/8 = 4.125\\))\nRepita 2 e 3, com o novo valor de x.\n\nO código abaixo implementa os passos acima. Começo escolhendo um inteiro aleatório \\(y\\) entre \\(0\\) e \\(10^7\\). O desafio será encontrar \\(\\sqrt{y}\\). Usando o comando set.seed(1984) você poderá replicar os resultados abaixo. O número sorteado foi \\(6588047\\) e \\(\\sqrt{6588047} = 2566.719\\).\nPara demonstrar a poder do simples método acima vou começar com um chute inicial bem ruim, isto é, vou escolher um \\(x\\) inicial igual a \\(1\\). Como quero mostrar os passos intermediários do algoritmo vou criar um vetor numérico que salva todos os valores de x a cada iteração. Há diversas maneiras de escrever o loop abaixo e ao longo do post vou mostrar três tipos de loop (for-loop, repeat e while).\n\n# Para replicar os resultados\nset.seed(1984)\n# Sorteia um número inteiro\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\n# Cria um vetor x de comprimento 15\nx &lt;- vector(length = 20)\n# Primeiro valor de x = 1\nx[1] &lt;- 1\n# Loop\nfor (i in 1:19) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nPodemos ver graficamente como o algortimo se aproxima do valor verdadeiro de \\(\\sqrt{y}\\) rapidamente, mesmo partindo de um ponto inicial distante. Em vermelho, está o valor correto de \\(\\sqrt{y}\\).\n\nplot(2:20, x[2:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nComo os primeiros valores do gráfico acima são muito altos, faço um segundo gráfico usando somente os últimos valores que foram computados.\n\nplot(13:20, x[13:20], main = \"\", xlab = \"Iteração\", ylab = \"x\")\nabline(h = sqrt(y), col = 2)\ngrid()\n\n\n\n\nNote que depois de 16 repetições, a diferença entre \\(x\\) e \\(y\\) é tão próxima de zero que o R arredonda ela para 0.\n\nx - sqrt(y)\n\n [1] -2.565719e+03  3.291457e+06  1.644446e+06  8.209418e+05  4.091915e+05\n [6]  2.033204e+05  1.003928e+05  4.894506e+04  2.325311e+04  1.047078e+04\n[11]  4.204686e+03  1.305444e+03  2.200559e+02  8.688284e+00  1.465521e-02\n[16]  4.183812e-08  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n\n\nMas como funciona este algoritmo? Basicamente, estamos procurando a raiz de um polinômio. No caso, estamos buscando resolver \\(f(x) = x^2 - y = 0\\) (em que \\(y\\) é dado). A solução proposta pelo método acima é de aproximar este polinômio linearmente (usando a primeira derivada) e então encontrar a raiz desta aproximação. Para facilitar o exemplo vamos tomar \\(y = 2\\). Então temos o problema de encontrar a raiz do seguinte polinômio:\n\\[\nP(x) = x^2 - 2\n\\]\nVamos tomar o valor \\(x_{0} = 5\\) como chute inicial. A aproximação linear de \\(P(x)\\) é a derivada com respeito a \\(x\\), isto é, \\(2x\\). Avaliada no ponto \\(x_{0}\\), temos \\(P'(5) = 10\\). Assim, temos uma reta de inclinação \\(10\\) que passa no ponto \\((5, 23)\\). Com isso podemos traçar a reta vermelha no gráfico abaixo e calcular o ponto em que esta reta corta o eixo-x. A equação da reta pode ser escrita como:\n\\[\n\\frac{y_{2} - y_{1}}{x_{2} - x_{1}} = f'(x_{1})\n\\]\nComo queremos encontrar o ponto em que a curva corta o eixo-x estamos, na verdade, buscando o ponto \\((x_{2}, 0)\\). Logo, podemos substituir \\(y_{2} = 0\\) na expressão acima e resolver para \\(x_{2}\\):\n\\[\nx_{2} = x_{1} - \\frac{y_{1}}{f'(x_{1})} = x_{1} - \\frac{f(x_{1})}{f'(x_{1})}\n\\]\nA expressão acima é essência do método de Newton. Substituindo nossos valores temos que:\n\\[\n\\frac{23}{5 - x} = 10 \\longrightarrow x = 2.7\n\\]\nNo gráfico abaixo podemos ver como funciona este processo. Parte-se de um ponto inicial \\(x = 5\\). Encontra-se a reta tangente neste ponto (linha em vermelho) e computa-se o valor de \\(x\\) em que esta reta cruza o eixo-x (ponto vermelho). O processo repete-se agora tomando o ponto em vermelho como ponto de partida. De maneira geral, o processo de atualização segue a regra abaixo:\n\\[\n  x_{n + 1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}\n\\]\n\n\n\n\n\n\ny &lt;- 2\nx &lt;- vector(length = 10)\n# Primeiro valor de x = 1\nx[1] &lt;- 5\n# Loop\nfor (i in 1:9) {\n  x_chute &lt;- x[i]\n  # Computa a distância\n  dist &lt;- y - x_chute^2\n  # Atualiza o valor de x\n  x_chute_novo &lt;- x_chute + dist / (2 * x_chute)\n  # Grava o valor de x para vermos a convergência\n  x[i + 1] &lt;- x_chute_novo\n}\n\nO gráfico abaixo mostra os próximos passos do algoritmo. Após poucas repetições já estamos muito próximos da resposta correta. Note que isto acontece porque estamos trabalhando com uma função bastante simples.\n\n\n\n\n\nPode-se generalizar o problema acima facilmente para encontrar a n-ésima raiz de qualquer valor. No código abaixo isto pode ser feito variando o valor de n. O código abaixo também é uma variação em relação aos anteiores. Ele é um repeat-break, ao invés do for-loop que estávamos usando acima. Isto é, ele repete uma operação enquanto alguma condição for válida. Assim, podemos exigir que o algoritmo pare quando algum critério de convergência for atingido. No caso abaixo, especifico que o loop pare quando a distância entre os valores sucessivos de \\(x\\) for muito pequena (menor que \\(10^{-8}\\)) ou quando o número de iterações chegar a 100 repetições.\n\n# Dá pra melhorar um pouco o loop (usando repeat) e agora generalizar para n\ny &lt;- round(runif(n = 1, min = 0, max = 10^7))\nx &lt;- vector(length = 100)\nx[1] &lt;- (10^7 - 1) / 2\ni &lt;- 1\nn &lt;- 4\nrepeat {\n  x_chute &lt;- x_chute_novo\n  dist &lt;- y - x_chute^n\n  x_chute_novo &lt;- x_chute + dist / (n * x_chute^(n - 1))\n  x[i + 1] &lt;- x_chute_novo\n  # print(paste(\"iteration =\", i))\n  if (abs(x_chute_novo - x[i]) &lt; 10^(-8) | i &gt; 99) {\n    break\n  }\n  i &lt;- i + 1\n}\nplot(x[1:i + 1] - y^(1 / n), main = \"Distância entre valor computado e valor verdadeiro\", xlab = \"Contagem da iteração\", ylab = \"Distância\")"
  },
  {
    "objectID": "posts/general-posts/repost-otimizacao-newton/index.html#na-prática-a-teoria-é-outra",
    "href": "posts/general-posts/repost-otimizacao-newton/index.html#na-prática-a-teoria-é-outra",
    "title": "Repost: Otimização numérica - métodos de Newton",
    "section": "Na prática, a teoria é outra",
    "text": "Na prática, a teoria é outra\nHá vários casos em que o otimizador de Newton falha em encontrar o valor desejado. Isto pode acontecer tanto porque a função viola alguma das hipóteses do método como também porque a função é complicada. A função abaixo, por exemplo, falha em convergir pois a sua derivada é descontínua no zero (há uma “quina” no zero). O resultado é que o método diverge para \\(\\infty\\).\n\\[\nf(x) = |x|^{1/3}\n\\]\n\n\n\n\n\nA tabela abaixo mostra os valores computados para as primeiras 10 iterações.\n\n\n\n\nIteração\nx\nf(x)\n\n\n\n\n1\n5\n1.709976\n\n\n2\n-10\n2.154435\n\n\n3\n-40\n3.419952\n\n\n4\n-160\n5.428835\n\n\n5\n-640\n8.617739\n\n\n6\n-2560\n13.679808\n\n\n7\n-10240\n21.715341\n\n\n8\n-40960\n34.470955\n\n\n9\n-163840\n54.719230\n\n\n10\n-655360\n86.861364\n\n\n\n\n\n\nOutro exemplo curioso em que o algoritmo falha em convergir é quando temos:\n\\[\nf(x) = \\left\\{\\begin{matrix}\n0 & \\text{se } x =0\\\\\nx + x^2 sin(\\frac{2}{x})) & \\text{se } x \\neq 0\n\\end{matrix}\\right.\n\\]\nAinda que a função seja contínua, temos uma descontinuidade na sua derivada e, como resultado, o otmizador fica osciliando entre valores sem nunca convergir para o zero.\n\n\n\n\n\nA tabela abaixo mostra os primeiros dez resultados:\n\n\n\n\n\nIteração\nx\nf(x)\n\n\n\n\n1\n5.0000000\n14.7354586\n\n\n2\n0.1719653\n0.1481520\n\n\n3\n0.4920907\n0.2990380\n\n\n4\n0.2819031\n0.3395412\n\n\n5\n-10.3200755\n-30.8312710\n\n\n6\n-0.0854544\n-0.0782425\n\n\n7\n-0.0171313\n-0.0169891\n\n\n8\n-0.0109142\n-0.0110166\n\n\n9\n-3.4206997\n-9.8789235\n\n\n10\n-0.2423414\n-0.2964612\n\n\n\n\n\n\n\nPode-se visualizar o comportamento do otimizador na animação abaixo. Mostro os primeiros 50 resultados. Note como o algoritmo fica pulando para todos os lados.\n\ntbl &lt;- data.frame(\n  x = x,\n  y = f(x),\n  label = 1:length(x)\n)\n\nggplot() +\n  geom_function(fun = ~ ifelse(.x == 0, 0, .x + .x^2 * sin(2 / .x))) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_point(data = tbl, aes(x = x, y = y), size = 2, color = \"firebrick\") +\n  geom_label(data = tbl, aes(x = x, y = y + 0.15, label = label)) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  transition_states(label) +\n  theme_light()\n\n\n\n\nEm geral, o método de Newton sofre problemas com:\n\nPontos inciais ruins. Às vezes uma escolha de ponto inicial ruim pode levar o otimizador a convergir para pontos diferentes. Em alguns casos, um ponto inicial ruim pode também levar o otimizador a ficar preso num ciclo.\n\nexemplo: \\(f(x) = x^3 - 2x + 2\\) com \\(x_{0} = 0\\). O otimizador fica preso entre \\(0\\) e \\(1\\).\n\n\n\n\n\nIteração\nx\nf(x)\n\n\n\n\n1\n0\n2\n\n\n2\n1\n1\n\n\n3\n0\n2\n\n\n4\n1\n1\n\n\n5\n0\n2\n\n\n6\n1\n1\n\n\n7\n0\n2\n\n\n8\n1\n1\n\n\n9\n0\n2\n\n\n10\n1\n1\n\n\n\n\n\n\n\n\nDescontinuidades. Descontinuidades tanto na raiz da função, como em outras partes da função (como também na sua derivada) podem levar a problemas sérios de convergência.\n\nexemplo: \\(f(x) = |x|^{1/3}\\) como apresentado acima.\n\nFunções difíceis. Algumas funções são simplesmente muito complicadas para o método de Newton. Em alguns casos, a convergência pode ser muito lenta e em outros o otimizador pode falhar completamente.\n\nexemplo: \\(f(x) = 7x - \\text{ln}(x)\\). A função tem mínimo global em \\(x = 1/7\\). Ainda assim, o método de Newton tem dificuldade em encontrar este valor (a não ser que o valor incial escolhido esteja muito próximo de \\(1/7\\)).\n\nf &lt;- function(x) {\n  7 * x - log(x)\n}\ngrad_f &lt;- function(x) {\n  7 - 1 / x\n}\nhess_f &lt;- function(x) {\n  1 / x^2\n}\ntheta &lt;- 5\nerror &lt;- 5\ni &lt;- 1\nx &lt;- vector(length = 50)\nx[1] &lt;- theta\nerror &lt;- 1\nwhile (error &gt; 10^(-8) & i &lt; 50) {\n  theta_0 &lt;- theta\n  G &lt;- grad_f(theta)\n  theta &lt;- theta_0 - grad_f(theta) / hess_f(theta)\n  x[i + 1] &lt;- theta\n  error &lt;- abs((theta - theta_0) / theta_0)\n  i &lt;- i + 1\n}"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "",
    "text": "Como desafio pessoal às vezes tento replicar gráficos que acho interessante. O portal Nexo, em particular, costuma ter lindas visualizações de dados. Vou tentar replicar os gráficos desta publicação. Como o foco desta postagem está na visualização e em mostrar exemplos de aplicações do ggplot2 vou omitir as (longas) manipulações de dados, deixando indicadas as fontes (com links) que usei. Numa postagem futura pretendo fazer um tutorial mais detalhado de como reproduzir estes gráficos.\n\n\n\n\nCode\nlibrary(\"readxl\")\nlibrary(\"here\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"dplyr\")\nlibrary(\"reshape2\")\nlibrary(\"kableExtra\")\n\n\n\n\n\nDados são do Maddison Project Database, disponíveis aqui\n\n\n\n\nPaís\nAno\nRegião\nPIB\nCrescimento\n\n\n\n\nAngola\n2005\nAfrica\n66351984\n1\n\n\nAngola\n2011\nAfrica\n123013536\n1\n\n\nAlbania\n2005\nEurope\n21452400\n1\n\n\nAlbania\n2006\nEurope\n22717995\n1\n\n\nAlbania\n2007\nEurope\n24080895\n1\n\n\nAlbania\n2008\nEurope\n25890410\n1\n\n\nAlbania\n2009\nEurope\n26754552\n1\n\n\nAlbania\n2010\nEurope\n27741824\n1\n\n\nAlbania\n2011\nEurope\n28452000\n1\n\n\nAlbania\n2012\nEurope\n28852736\n1"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#pacotes",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#pacotes",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "",
    "text": "Code\nlibrary(\"readxl\")\nlibrary(\"here\")\nlibrary(\"ggplot2\")\nlibrary(\"ggrepel\")\nlibrary(\"dplyr\")\nlibrary(\"reshape2\")\nlibrary(\"kableExtra\")"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#dados",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#dados",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "",
    "text": "Dados são do Maddison Project Database, disponíveis aqui\n\n\n\n\nPaís\nAno\nRegião\nPIB\nCrescimento\n\n\n\n\nAngola\n2005\nAfrica\n66351984\n1\n\n\nAngola\n2011\nAfrica\n123013536\n1\n\n\nAlbania\n2005\nEurope\n21452400\n1\n\n\nAlbania\n2006\nEurope\n22717995\n1\n\n\nAlbania\n2007\nEurope\n24080895\n1\n\n\nAlbania\n2008\nEurope\n25890410\n1\n\n\nAlbania\n2009\nEurope\n26754552\n1\n\n\nAlbania\n2010\nEurope\n27741824\n1\n\n\nAlbania\n2011\nEurope\n28452000\n1\n\n\nAlbania\n2012\nEurope\n28852736\n1"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-2",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-2",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "Gráfico 2",
    "text": "Gráfico 2\nEste gráfico conta o número absoluto de países que estava em recessão durante um determinado ano. Como o número de países dentro da amostra também cresce com o tempo, uma linha cinza foi adicionada para representar isto. Resolvi suprimir as flechas que aparecem na postagem original, pois a implementação disto no ggplot é muito trabalhosa e o resultado final não é tão bonito.\n\n\nCode\n# Primeiro: conta o número de países pela variável var_pib\n# (1 = expansão, 0 = recessão)\nrecession_countries &lt;- d %&gt;%\n  # Seleciona as observações a partir de 1900\n  filter(year &gt;= 1900) %&gt;%\n  # Agrupa os dados por ano e var_pib\n  group_by(year, var_pib) %&gt;%\n  # Soma os valores que não são NA por tipo (1 = expansão, 0 = recessão)\n  summarise(tipo = sum(!is.na(var_pib)))\n\n# Segundo: conta o número de países incluídos na amostra\nrecession_countries &lt;- recession_countries %&gt;%\n  group_by(year) %&gt;%\n  mutate(amostra = sum(tipo)) %&gt;%\n  ungroup()\n\n# Terceiro: remove as contagens de anos de expansão e converte os dados para longitudinal (melhor para plotar)\nrecession_countries &lt;- recession_countries %&gt;%\n  # Remove os valores que são referentes a anos de expansão\n  filter(var_pib != 1) %&gt;%\n  # Tranforma os dados em longitudinais por ano e 'var_pib'\n  # (tipo = recessão, amostra = observações válidas)\n  melt(id.vars = c(\"year\", \"var_pib\"))\n\n# Base de dados para os textos no gráfico\nhighlight &lt;- data.frame(\n  evento = c(\n    \"1ª GUERRA\\nMUNDIAL\", \"CRISE\\nDE 1929\", \"2ª GUERRA\\nMUNDIAL\",\n    \"CRISE DO\\nPETRÓLEO\", \"TRANSIÇÃO CAPITALISTA\\nDO LESTE EUROPEU\",\n    \"CRISE DE 2008\"),\n  year = c(1918, 1929, 1945, 1971, 1988, 2008),\n  y = c(52, 65, 35, 62, 78, 90)\n)\n\n\n\n\nCode\n# Obs: como plotamos dados de duas bases diferentes a sintaxe é diferente\n\nggplot() +\n  geom_line(\n    data = recession_countries, \n    aes(year, value, group = variable, colour = variable),\n    # Espessura da linha\n    linewidth = 1.2\n    ) +\n  # Destaques de texto\n  geom_text(\n    data = highlight,\n    aes(year, y, label = evento), \n    size = 3,\n    family = \"Gotham Rounded Light\",\n    vjust = \"center\",\n    hjust = \"center\") +\n  # Troca as cores das linhas e altera a legenda\n  scale_colour_manual(\n    name = NULL,\n    breaks = c(\"amostra\", \"tipo\"),\n    values = c(\"tomato\", \"gray50\"),\n    labels = c(\"países incluídos na amostra\", \"países com queda no PIB\")\n    ) +\n  scale_x_continuous(\n    breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n    expand = c(0.025,0.025)\n    ) +\n  # Define o título e subtítulo do gráfico e o título dos eixos\n  labs(\n    x = NULL,\n    y = \"número\\nde países\",\n    title = \"NÚMERO DE PAÍSES COM QUEDA NO PIB NO ANO\",\n    subtitle = \"Em valores absolutos\"\n    ) +\n  theme(\n    # Título e subtítulo\n    plot.title = element_text(family = \"Gotham Rounded Bold\"),\n    plot.subtitle = element_text(family = \"Gotham Rounded Light\", size = 14),\n    # Eixos\n    axis.text.x = element_text(family = \"Gotham Rounded Bold\", size = 10),\n    axis.text.y = element_text(family = \"Gotham Rounded Light\", size = 10),\n    axis.ticks.x = element_line(colour = \"grey70\"),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_text(angle = 360, face = \"bold\", size = 8),\n    # Legenda\n    legend.text = element_text(\n      family = \"Gotham Rounded Bold\",\n      size = 12,\n      vjust = .8,\n      hjust = 0\n      ),\n    legend.background = element_rect(fill = NA),\n    legend.key = element_rect(fill = \"white\"),\n    legend.position = c(0.17,0.75),\n    # Fundo do gráfico  \n    panel.background = element_rect(fill = \"white\"),\n    # Linhas de grade\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(colour = \"grey70\")\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-3",
    "href": "posts/general-posts/repost-crescimento-pib-mundo/index.html#gráfico-3",
    "title": "Crescimento do PIB per capita no mundo",
    "section": "Gráfico 3",
    "text": "Gráfico 3\nEste último gráfico mostra o número relativo de países em recessão.\n\n\nCode\n# Calcula o share de países em recessão na amostra a cada ano\nshare_recession &lt;- d %&gt;% \n  filter(year &gt;= 1900) %&gt;%\n  # Agrupa os dados por ano e var_pib (indicadora)\n  group_by(year, var_pib) %&gt;%\n  # Remove as observações ausentes \n  filter(!is.na(var_pib)) %&gt;%\n  # Conta o número de casos\n  summarise(count = n()) %&gt;%\n  # Calcula a proporção dos casos acima (em porcentagem)\n  group_by(year) %&gt;%\n  mutate(freq = count / sum(count) * 100)\n\n# Tabela auxiliar para guardar os valores que vão ser plotados como\n# texto no gráfico\nhighlight &lt;- data.frame(\n  evento = c(\n    \"1ª GUERRA\\nMUNDIAL\", \"CRISE\\nDE 1929\", \"2ª GUERRA\\nMUNDIAL\",\n    \"CRISE DO\\nPETRÓLEO\", \"TRANSIÇÃO\\nCAPITALISTA\\nDO LESTE\\nEUROPEU\",\n    \"CRISE DE 2008\"),\n  year = c(1916, 1929, 1942, 1971, 1988, 2006),\n  y = c(70, 45, 75, 77, 81, 70)\n)\n\n\n\n\nCode\nggplot() +\n  geom_col(data = share_recession, aes(year, freq, fill = var_pib)) +\n  # Superimpõe linhas horizontais no gráfico\n  geom_hline(\n    yintercept = seq(0, 100, 25),\n    colour = \"gray70\",\n    alpha = .5,\n    size = .8\n    ) +\n  # Mapeia o texto no gráfico (geom_label permite escolher fill = \"salmon\")\n  geom_label(\n    data = highlight,\n    aes(year, y, label = evento),\n    colour = \"white\",\n    size = 3,\n    fill = \"salmon\",\n    family = \"Gotham Rounded Medium\")+\n  # Define título, subtítulo e o título dos eixos\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"NÚMERO DE PAÍSES COM QUEDA NO PIB\",\n    subtitle = \"Em percentual dos países incluídos na amostra\")+\n  scale_fill_discrete(\n    name = NULL,\n    breaks = c(1,0),\n    labels = c(\"PIB EM CRESCIMENTO\", \"PIB EM QUEDA\"))+\n  scale_x_continuous(\n    breaks = c(1900, 1925, 1950, 1975, 2000, 2016),\n    expand = c(0.025,0.025)\n    ) +\n  scale_y_continuous(\n    breaks = seq(0, 100, 25),\n    labels = c(0, 25, 50, 75, \"100%\")\n    )+\n  theme(\n    # Fundo do gráfico\n    panel.background = element_rect(fill = \"white\"),\n    # Linhas de grade\n    panel.grid = element_blank(),\n    # Legenda\n    legend.position = \"top\",\n    legend.direction = \"horizontal\",\n    legend.justification='left',\n    legend.text = element_text(family = \"Gotham Rounded Bold\"),\n    # Título e subtítulo\n    plot.title = element_text(family = \"Gotham Rounded Bold\", size = 16),\n    plot.subtitle = element_text(family = \"Gotham Rounded Light\", size = 12),\n    # Eixos\n    axis.text.x = element_text(family = \"Gotham Rounded Bold\", size = 10),\n    axis.text.y = element_text(family = \"Gotham Rounded Light\", size = 10),\n    axis.ticks.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html",
    "title": "Importando dados do SIDRA",
    "section": "",
    "text": "O SIDRA, ou Sistema IBGE de Recuperação Automática, é um sistema automatizado de coleta de dados do IBGE, que disponbiliza tabelas das várias pesquisas feitas pelo IBGE. O sistema é centralizado no site. Encontrar pesquisas e dados pode não ser fácil e, em geral, exige conhecimento prévio sobre as pesquisas do IBGE. O site oferece uma página de ajuda.\nNeste post vou comentar mais especificamente sobre o pacote sidrar que dialoga diretamente com a API do SIDRA para importar tabelas diretamente para dentro do R. O pacote tem duas funções centrais:"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#variando-os-parâmetros-de-busca",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#variando-os-parâmetros-de-busca",
    "title": "Importando dados do SIDRA",
    "section": "Variando os parâmetros de busca",
    "text": "Variando os parâmetros de busca\nA função get_sidra() permite refinar a query que gera a tabela final que é importada dentro do R. Os principais argumentos são:\n\nperiod - escolhe a janela temporal dos dados.\nvariable - escolhe a variável a ser importada.\ngeo - nível geográfico dos dados (e.g. \"Brazil\", \"City\", \"Neighborhood\", etc.).\ngeo.filter - lista com filtros geográficos.\nclassific - classificação dos dados.\ncategory - escolhe quais categorias (do classific) importar.\n\nDe maneira geral, todos os parâmetros acima tem algumas nuances. Vamos explorá-los caso a caso"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-dados-de-vários-períodos",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-dados-de-vários-períodos",
    "title": "Importando dados do SIDRA",
    "section": "Importando dados de vários períodos",
    "text": "Importando dados de vários períodos\nPara selecionar múltiplos períodos podemos montar um vetor character com os períodos desejados. Para selecionar uma sequência de períodos deve-se montar um string único encadeando os períodos com o sinal de “-”. O código abaixo deve deixar isto mais claro.\nNote que se você rodar o código abaixo, provavelmente, vai enfrentar um problema de limite de query. A API do SIDRA restringe o número de linhas que um usuário pode chamar de uma só vez. Assim, para importar bases de dados maiores é preciso montar estratégias para particionar os dados.\n\n# Importa os dados de 2010 e de 2020\npop &lt;- get_sidra(6579, period = c(\"2010\", \"2020\"), geo = \"City\")\n\n# Importa todos os dados de 2010 até 2020 (erro: limite de requisição)\npop &lt;- get_sidra(6579, period = c(\"2010-2020\"), geo = \"City\")\n\nUm detalhe curioso é que a função não retorna erros quando o período extrapola os valores disponíveis nos dados. Assim a função abaixo retorna apenas os valores de 2021.\n\n# Importa todos os dados de 2021 até 2030\npop &lt;- get_sidra(6579, period = c(\"2021-2030\"), geo = \"City\")\n\nA mesma lógica acima vale para quando os dados são mensais ou trimestrais. Considere o exemplo da taxa de desocupação, levantada pela PNADC Trimestral. Note que os períodos estão no formato YYYYQQ.\n\ninfo_sidra(4099)\n\nVamos importar todos as observações da taxa de desocupação em 2012. Note que agora há múltiplas variáveis na mesma tabela e que o resultado já está em formato “long”.\n\nunemp &lt;- get_sidra(4099, period = \"201201-201204\")\n\n\nhead(unemp)\n\n  Nível Territorial (Código) Nível Territorial Unidade de Medida (Código)\n2                          1            Brasil                          2\n3                          1            Brasil                          2\n4                          1            Brasil                          2\n5                          1            Brasil                          2\n6                          1            Brasil                          2\n7                          1            Brasil                          2\n  Unidade de Medida Valor Brasil (Código) Brasil Trimestre (Código)\n2                 %   8.0               1 Brasil             201201\n3                 %   0.9               1 Brasil             201201\n4                 %  15.3               1 Brasil             201201\n5                 %   0.7               1 Brasil             201201\n6                 %  14.0               1 Brasil             201201\n7                 %   0.7               1 Brasil             201201\n          Trimestre Variável (Código)\n2 1º trimestre 2012              4099\n3 1º trimestre 2012              4103\n4 1º trimestre 2012              4114\n5 1º trimestre 2012              4115\n6 1º trimestre 2012              4116\n7 1º trimestre 2012              4117\n                                                                                                                                                                           Variável\n2                                                                                             Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n3                                                                   Coeficiente de variação - Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n4                           Taxa combinada de desocupação e de subocupação por insuficiência de horas trabalhadas, na semana de referência, das pessoas de 14 anos ou mais de idade\n5 Coeficiente de variação - Taxa combinada de desocupação e de subocupação por insuficiência de horas trabalhadas, na semana de referência, das pessoas de 14 anos ou mais de idade\n6                                                     Taxa combinada de desocupação e força de trabalho potencial, na semana de referência, das pessoas de 14 anos ou mais de idade\n7                           Coeficiente de variação - Taxa combinada de desocupação e força de trabalho potencial, na semana de referência, das pessoas de 14 anos ou mais de idade\n\n\nPara refinar a busca acima temos de usar o argumento variable.\n\nunemp &lt;- get_sidra(4099, period = \"201201-202203\", variable = 4099)\n\n\nhead(unemp)\n\n  Nível Territorial (Código) Nível Territorial Unidade de Medida (Código)\n2                          1            Brasil                          2\n3                          1            Brasil                          2\n4                          1            Brasil                          2\n5                          1            Brasil                          2\n6                          1            Brasil                          2\n7                          1            Brasil                          2\n  Unidade de Medida Valor Brasil (Código) Brasil Trimestre (Código)\n2                 %   8.0               1 Brasil             201201\n3                 %   7.6               1 Brasil             201202\n4                 %   7.1               1 Brasil             201203\n5                 %   6.9               1 Brasil             201204\n6                 %   8.1               1 Brasil             201301\n7                 %   7.5               1 Brasil             201302\n          Trimestre Variável (Código)\n2 1º trimestre 2012              4099\n3 2º trimestre 2012              4099\n4 3º trimestre 2012              4099\n5 4º trimestre 2012              4099\n6 1º trimestre 2013              4099\n7 2º trimestre 2013              4099\n                                                                               Variável\n2 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n3 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n4 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n5 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n6 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n7 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n\n\nAgora temos a série completa apenas da taxa de desocupação no Brasil. Podemos buscar também a taxa em cada uma das cidades do Brasil.\n\nunemp &lt;- get_sidra(\n  4099,\n  period = \"201201-202203\",\n  variable = 4099,\n  geo = \"City\"\n  )\n\n\n\n  Nível Territorial (Código) Nível Territorial Unidade de Medida (Código)\n1                          6         Município                          2\n2                          6         Município                          2\n3                          6         Município                          2\n4                          6         Município                          2\n5                          6         Município                          2\n6                          6         Município                          2\n  Unidade de Medida Valor Município (Código)        Município\n1                 %   9.0            1100205 Porto Velho - RO\n2                 %  10.4            1200401  Rio Branco - AC\n3                 %  12.9            1302603      Manaus - AM\n4                 %   9.3            1400100   Boa Vista - RR\n5                 %  10.5            1501402       Belém - PA\n6                 %  12.6            1600303      Macapá - AP\n  Trimestre (Código)         Trimestre Variável (Código)\n1             201201 1º trimestre 2012              4099\n2             201201 1º trimestre 2012              4099\n3             201201 1º trimestre 2012              4099\n4             201201 1º trimestre 2012              4099\n5             201201 1º trimestre 2012              4099\n6             201201 1º trimestre 2012              4099\n                                                                               Variável\n1 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n2 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n3 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n4 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n5 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n6 Taxa de desocupação, na semana de referência, das pessoas de 14 anos ou mais de idade\n\n\nNote, contudo, que temos o resultado somente nas capitais. Apesar do argumento “City” ser o mesmo que usamos na pesquisa de Estimativas da População, aqui o argumento “City” retorna apenas as capitais. Isto acontece porque a PNADC é uma pesquisa que é feita apenas nas capitais brasileiras e não tem uma desagregação a nível municipal para todos os municípios do Brasil. Este importante detalhe não fica evidente e depende do conhecimento prévio do usuário a respeito das pesquisas do IBGE."
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-muitos-dados",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#importando-muitos-dados",
    "title": "Importando dados do SIDRA",
    "section": "Importando muitos dados",
    "text": "Importando muitos dados\nComo comentei acima, a API do SIDRA tem um limite de linhas nas queries. Isto significa que é preciso montar algumas estratégias para pegar as tabelas em partes menores. Na prática, isto vai depender muito da tabela e da informação que se quer.\nO exemplo abaixo é bastante simples mas pode eventualmente ser útil em alguma outra aplicação. O código importa a tabela de estimativas populacionais ano a ano. O mesmo poderia ter sido feito num for-loop, mas prefiro usar o lapply por simplicidade.\nPara acelerar o processo pode-se trocar o lapply por parallel::mclapply que roda a função em paralelo. Contudo, na minha experiência, esta estratégia acaba resultando em erros imprevistos. Não sei se isto é resultado de alguma restrição da API. Na prática, é mais seguro evitar o mclapply.\n\n# Define um vetor com todos os anos da pesquisa\nperiods &lt;- seq(2001:2021)\n# Converte para character\nperiods &lt;- as.character(periods)\n# Aplica a função get_sidra a cada elemento de periods (a cada ano)\nreq &lt;- lapply(periods, function(p) get_sidra(6579, period = p, geo = \"City\"))\n# Junta o resultado numa única tabela\ntab &lt;- dplyr::bind_rows(req)"
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#tabelas-do-censo",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#tabelas-do-censo",
    "title": "Importando dados do SIDRA",
    "section": "Tabelas do Censo",
    "text": "Tabelas do Censo\nTabelas que vem do Censo costumam ter maior complexidade. Considere, por exemplo, a tabela 1378, que retorna a população residente, por situação do domicílio (urbano x rural), sexo e idade, segundo a condição no domicílio e compartilhamento da responsabilidade (responsável, cônjuge, filho, etc.)\nO resultado da função abaixo é omitido para poupar espaço pois a saída é muito grande. Em casos como este é muito importante filtrar o resultado para conseguir uma tabela que faça sentido.\n\ninfo_sidra(1378)\n\nVamos nos focar somente na população residente (var 93) de pessoas responsáveis do domicílio (cod 11941) por bairro de Porto Alegre (4314902).\n\nresp &lt;- get_sidra(\n  1378,\n  variable = 93,\n  classific = c(\"c455\"),\n  geo = \"Neighborhood\",\n  geo.filter = list(\"City\" = 4314902)\n  )\n\nApós um pouco de limpeza no nome das colunas podemos ver o resultado abaixo.\n\n\n\n\n\n\n\nbairro\ncond_domi\nsexo\nidade\nsituacao_domi\nvalor\n\n\n\n\n2\nMedianeira\nTotal\nTotal\nTotal\nTotal\n11568\n\n\n3\nMedianeira\nPessoa responsável\nTotal\nTotal\nTotal\n4206\n\n\n4\nMedianeira\nPessoa responsável - com responsabilidade compartilhada\nTotal\nTotal\nTotal\n1054\n\n\n5\nMedianeira\nPessoa responsável - sem responsabilidade compartilhada\nTotal\nTotal\nTotal\n3152\n\n\n6\nMedianeira\nCônjuge ou companheiro(a)\nTotal\nTotal\nTotal\n2101\n\n\n7\nMedianeira\nCônjuge ou companheiro(a) - de sexo diferente\nTotal\nTotal\nTotal\n2094\n\n\n8\nMedianeira\nCônjuge ou companheiro(a) - de mesmo sexo\nTotal\nTotal\nTotal\n7\n\n\n9\nMedianeira\nFilho(a)\nTotal\nTotal\nTotal\n3429\n\n\n10\nMedianeira\nFilho(a) - da pessoa responsável e do cônjuge\nTotal\nTotal\nTotal\n1916\n\n\n11\nMedianeira\nFilho(a) - somente da pessoa responsável\nTotal\nTotal\nTotal\n1513\n\n\n12\nMedianeira\nEnteado(a)\nTotal\nTotal\nTotal\n145\n\n\n13\nMedianeira\nGenro ou nora\nTotal\nTotal\nTotal\n170\n\n\n14\nMedianeira\nPai, mãe, padastro ou madrasta\nTotal\nTotal\nTotal\n183\n\n\n15\nMedianeira\nSogro(a)\nTotal\nTotal\nTotal\n51\n\n\n16\nMedianeira\nNeto(a)\nTotal\nTotal\nTotal\n549\n\n\n\n\n\n\n\n\nNote que temos a coluna da condição do domicílio desagregada (c455), mas todas as outras (sexo, idade, etc.) estão agregadas."
  },
  {
    "objectID": "posts/general-posts/2023-08-importando-dados-sidra/index.html#api-do-sidra",
    "href": "posts/general-posts/2023-08-importando-dados-sidra/index.html#api-do-sidra",
    "title": "Importando dados do SIDRA",
    "section": "API do SIDRA",
    "text": "API do SIDRA\nEntão imagine, por exemplo, que queremos esta mesma informação, por grupos de idade. Esta requisição quebra o limite da query. Além disso, os grupos de idade disponíveis são caóticos e não é possível filtrar dentro deste grupo da mesma maneira como fazemos com o argumento variable.\nA solução é interagir diretamente com a API do SIDRA! O código acaba sendo bastante complexo/confuso. Muito provavelmente há maneiras de melhorar o código abaixo, mas foi como fiz para conseguir esta informação. Por conveniência chamo explicitamente todos os pacotes que uso. A limpeza dos dados foi a parte mais desafiadora do processo visto que não foi possível transformar o input de JSON para um data.frame de maneira direta.\nEm partes, acho que dei o azar de ter escolhido uma tabela especialmente complicada. Se jsonlite::fromJSON(x, simplifyDataFrame = TRUE) tivesse funcionado, o código seria muito mais simples.\n\n\nCode\n# Bibliotecas usadas\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(tidyjson)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(geobr)\n\n# 1. Montar a query\n\n# Montar um vetor com o código de todos os bairros de Porto Alegre\n\n# Importa o shapefile de todos os bairros IBGE (2010)\nbairros &lt;- geobr::read_neighborhood()\n\nbairros_poa &lt;- bairros |&gt; \n  # Pega apenas os códigos dos bairros de POA\n  filter(code_muni == 4314902) |&gt; \n  # O código do bairro tem alguns números sobrando no meio\n  mutate(code_nb = paste0(\n    str_sub(code_neighborhood, 1, 7),\n    str_sub(code_neighborhood, 10, 12))\n    )\n# Extrai o vetor com os códgios dos bairros\ncodenb_poa &lt;- bairros_poa[[\"code_nb\"]]\n# Colapsa o vetor num único string separado por vírgulas\nlocal = paste(codenb_poa, collapse = \",\")\n# Insere o string no formato da API\nlocal = stringr::str_glue(\"localidades=N102[{local}]\")\n\n# Base do url da API\nbase = \"https://servicodados.ibge.gov.br/api/v3/agregados\"\n# Número da tabela\ntable &lt;- 1378\n# Período da extração (ano)\nperiod &lt;- 2010\n# Código da variável (Pessoas residentes)\nvariable &lt;- 93\n# Códigos das classes\nclass &lt;- c(1, 2, 287, 455)\n# Filtro das classes (veja info_sidra(1378))\ncategory &lt;- list(c(1), c(4, 5), c(93087:93100), c(11941))\n# Colapsa os strings\ncategory &lt;- sapply(category, paste, collapse = \",\")\n# Coloca os strings no format da API\ncat_filter &lt;- stringr::str_glue(\"{class}[{category}]\")\n# Colapsa os strings num único string separado pelo operador booleano |\ncat_filter &lt;- paste(cat_filter, collapse = \"|\")\n\n# Monta a query\nquery &lt;- stringr::str_glue(\n  \"{base}/{table}/periodos/{period}/variaveis/{variable}?{local}&classificacao={cat_filter}\"\n)\n\n# 2. Importar dados\n\n# Importar os dados da API\nreq &lt;- httr::GET(url = query)\njson &lt;- httr::content(req, as = \"text\", encoding = \"UTF-8\")\njson &lt;- jsonlite::fromJSON(json, simplifyVector = FALSE)\n\n# 3. Limpar os dados\n\n# Simplificar os dados de JSON para um tibble\nvalores &lt;- json |&gt; \n  spread_all() |&gt; \n  enter_object(resultados) |&gt; \n  gather_array() |&gt; \n  spread_all() |&gt; \n  enter_object(series) |&gt; \n  gather_array() |&gt; \n  spread_all()\n\n# Extrair somente os valores da série\nvalores &lt;- valores |&gt; \n  as_tibble() |&gt; \n  mutate(serie.2010 = as.numeric(serie.2010)) |&gt; \n  pivot_wider(\n    id_cols = \"array.index\",\n    names_from = \"localidade.id\",\n    values_from = \"serie.2010\"\n  )\n\n# Extrair a classificação das variáveis (sexo, idade)\nclassificacao &lt;- json |&gt; \n  spread_all() |&gt; \n  enter_object(resultados) |&gt; \n  gather_array() |&gt; \n  spread_all() |&gt; \n  enter_object(classificacoes) |&gt; \n  gather_array() |&gt; \n  spread_all()\n\n# Extrai somente a coluna com os grupos de idade\ntblage &lt;- classificacao |&gt; \n  select(contains(\"93\")) |&gt;\n  as_tibble() |&gt; \n  unite(\"age_group\", everything(), na.rm = TRUE) |&gt; \n  filter(age_group != \"\") |&gt; \n  distinct()\n\n# Cria um grid com todas as classificações na ordem correta\ntblclass &lt;- expand_grid(\n  situacao_domicilio = \"urban\",\n  resp = \"Responsável pelo Domicilio\",\n  sex = c(\"male\", \"female\"),\n  age_group = tblage$age_group\n)\n\n# Junta as classificações com os valores \ntab &lt;- cbind(valores, tblclass)\n\n# Converte os dados para wide\ntab &lt;- tab |&gt; \n  pivot_longer(starts_with(\"431\"), names_to = \"code_nb\") |&gt; \n  select(code_nb, situacao_domicilio, resp, sex, age_group, value)\n\n# Converte os dados para uma tabela final\nrespnb &lt;- tab |&gt; \n  pivot_wider(\n    id_cols = c(\"code_nb\", \"age_group\"),\n    names_from = \"sex\",\n    values_from = \"value\"\n  ) |&gt; \n  mutate(total = male + female)|&gt; \n  group_by(code_nb) |&gt; \n  mutate(total_nb = sum(total)) |&gt; \n  ungroup() |&gt; \n  mutate(share_nb = total / total_nb * 100)\n\n\nA tabela abaixo mostra o resultado final do código. A tabela apresenta o número total de pessoas responsáveis por domicílios em grupos de idade quinquenais por sexo e por bairro de Porto Alegre. Além disso, a coluna total_nb traz o número total de pessoas responsáveis por domicílio no bairro e share_nb computa a participação relativa de cada faixa de idade no total do bairro.\n\n\n\n\n\n\ncode_nb\nage_group\nmale\nfemale\ntotal\ntotal_nb\nshare_nb\n\n\n\n\n4314902003\n20 a 24 anos\n383\n406\n789\n8757\n9\n\n\n4314902005\n20 a 24 anos\n3\n7\n10\n435\n2\n\n\n4314902007\n20 a 24 anos\n214\n245\n459\n9116\n5\n\n\n4314902069\n20 a 24 anos\n42\n62\n104\n2071\n5\n\n\n4314902009\n20 a 24 anos\n331\n359\n690\n13137\n5\n\n\n4314902072\n20 a 24 anos\n75\n57\n132\n3706\n4\n\n\n4314902068\n20 a 24 anos\n163\n144\n307\n8052\n4\n\n\n4314902013\n20 a 24 anos\n193\n177\n370\n6319\n6\n\n\n4314902012\n20 a 24 anos\n31\n19\n50\n2719\n2\n\n\n4314902004\n20 a 24 anos\n206\n227\n433\n13342\n3\n\n\n4314902032\n20 a 24 anos\n261\n285\n546\n12171\n4\n\n\n4314902022\n20 a 24 anos\n117\n105\n222\n6910\n3\n\n\n4314902071\n20 a 24 anos\n49\n29\n78\nNA\nNA\n\n\n4314902045\n20 a 24 anos\n70\n56\n126\n4190\n3\n\n\n4314902049\n20 a 24 anos\n71\n65\n136\n4365\n3\n\n\n\n\n\n\n\n\nApesar de claramente não ser o foco do post, pareceu um desperdício de esforço não fazer algo com esta informação. O código abaixo agrupa os dados um pouco mais e monta um mapa interativo dos domicílios por grupo de idade da pessoa responsável. Não seria difícil transformar isto num aplicativo para ver esta mesma info nas outras faixas de idade, mas isto fica para outro post.\n\n\nCode\nbairros &lt;- geobr::read_neighborhood(showProgress = FALSE)\n\nbairros_poa &lt;- bairros |&gt; \n  # Pega apenas os códigos dos bairros de POA\n  filter(code_muni == 4314902) |&gt; \n  # O código do bairro tem alguns números sobrando no meio\n  mutate(code_nb = paste0(\n    stringr::str_sub(code_neighborhood, 1, 7),\n    stringr::str_sub(code_neighborhood, 10, 12))\n    )\n\n\nrespnb &lt;- respnb |&gt;\n  separate(age_group, into = c(\"age_min\", \"age_max\"), sep = \" a \") |&gt; \n  mutate(\n    age_generation = case_when(\n      age_min &gt;= 25 & age_min &lt; 40 ~ \"25-39\",\n      age_min &gt;= 40 & age_min &lt; 60 ~ \"40-59\",\n      age_min &gt;= 60 & age_min &lt; 80 ~ \"60-79\",\n      age_min &gt;= 80 ~ \"80+\",\n      TRUE ~ \"&lt;25\"\n    )\n  )\n\nresp_grouped_nb &lt;- respnb |&gt; \n  group_by(code_nb, age_generation) |&gt; \n  summarise(pop = sum(total, na.rm = TRUE)) |&gt; \n  group_by(code_nb) |&gt; \n  mutate(total_nb = sum(pop)) |&gt; \n  ungroup() |&gt; \n  mutate(share_nb = pop / total_nb * 100)\n\nresp_ages &lt;- pivot_wider(\n  resp_grouped_nb,\n  id_cols = \"code_nb\",\n  names_from = \"age_generation\",\n  names_prefix = \"age_\",\n  values_from = \"share_nb\"\n)\n\nresp_ages &lt;- janitor::clean_names(resp_ages)\n\nresp_bairros &lt;- left_join(bairros_poa, resp_ages, by = \"code_nb\")\n\n\nO mapa é gerado pelo código abaixo. O mapa mostra o share de domicílios, em cada bairro, que são chefiados por “adultos-jovens”, isto é, de 25 a 39 anos. Nota-se que os bairros que tem maiores percentuais estão mais na periferia da cidade. Alguns bairros de classe alta como Moinhos de Vento, Três Figueiras e Vila Assunção tem os menores percentuais.\n\n\nCode\nlibrary(tmap)\nlibrary(tmaptools)\ntmap_mode(mode = \"view\")\n\nm &lt;- tm_shape(resp_bairros) +\n  tm_fill(\n    col = \"age_25_39\",\n    palette = \"inferno\",\n    alpha = 0.8,\n    style = \"jenks\",\n    n = 7,\n    id = \"name_neighborhood\",\n    title = \"Percentual de domicílios&lt;br&gt;chefiados por pessoas&lt;br&gt;de 25 a 39 anos (%).\",\n    popup.vars = c(\n      \"Menos de 25 anos\" = \"age_25\",\n      \"25 a 39 anos\" = \"age_25_39\",\n      \"40 a 59 anos\" = \"age_40_59\",\n      \"60 a 79 anos\" = \"age_60_79\",\n      \"80 anos ou mais\" = \"age_80\"),\n    popup.format = list(digits = 1),\n    legend.format = list(digits = 0)) +\n  tm_borders() +\n  tm_basemap(server = \"CartoDB.Positron\") +\n  tm_view(set.view = c(-51.179152, -30.025976, 13))\n\nleaf &lt;- tmap_leaflet(m)\nwidgetframe::frameWidget(leaf, width = \"100%\")"
  },
  {
    "objectID": "posts/general-posts/repost-mqo-teoria-assintotica/index.html",
    "href": "posts/general-posts/repost-mqo-teoria-assintotica/index.html",
    "title": "MQO - teoria assintótica",
    "section": "",
    "text": "Disclaimer\nEste é um repost antigo que fiz ainda na época do mestrado em economia. Apesar de intuitivo o código dos loops abaixo é muito ineficiente. De maneira geral, for-loops são melhores do que loops feitos com repeat; melhor ainda é montar funções e usar parallel::mclapply ou replicate. Além disso, é importante pre-definir o tamanho do objeto antes de um loop, e.g., x &lt;- vector(\"numeric\", length = 10000).\n\n\nMínimos Quadrados\nA maior parte dos resultados assintóticos dos estimadores de mínimos quadrados (MQO) são um misto da LGN, do TCL e de outros resultados de convergência como o método delta e o teorema de Slutsky. Um resultado simples que podemos visualizar através de uma simulação é a propriedade de não-viés dos estimadores de MQO. Em linhas gerais, desde que o termo de erro seja ortogonal às variáveis independentes, os estimadores de MQO não serão viesados, i.e., os estimadores \\(\\hat{\\beta}\\) vão convergir para os verdadeiros \\(\\beta\\). Dizer que eles são ortogonais costuma ser o mesmo que dizer que eles são independentes. Isto é, na prática queremos que a esperança condicional de \\(u_{t}\\), o erro, dado \\(x_{t}\\), as variáveis explicativas, seja nulo:\n\\[\n\\mathbb{E}(u_{t} | x_{t}) = 0\n\\]\nSuponha que o modelo verdadeiro (o processo gerador de dados) seja da forma:\n\\[\\begin{equation}\n    y_{t} = 5 + 2.5x_{1t} -1.75x_{2t} + 5x_{3t} + u_{t}\n\\end{equation}\\] onde \\[\\begin{align}\n    x_{1} & \\sim N(0,1)\\\\\n    x_{2} & \\sim \\text{Beta}(2, 5)\\\\\n    x_{3} & \\sim U(0,1)\\\\\n    u_{t} & \\sim N(0,1)\n\\end{align}\\]\nSe as variáveis \\(x_{1}, x_{2}\\) e \\(x_{3}\\) forem independentes de \\(u_{t}\\) então o modelo: \\[\\begin{equation}\n    y_{t} = \\beta_{0} + \\beta_{1}x_{1t} + \\beta_{2}x_{2t} + \\beta_{3}x_{3t}\n\\end{equation}\\] fornecerá estimativas consistentes para os betas.\n\n# Modelo verdadeiro (DGP)\nx1 &lt;- rnorm(n = 200)\nx2 &lt;- rbeta(n = 200, shape1 = 2, shape2 = 5)\nx3 &lt;- runif(n = 200)\ny &lt;- 5 + 2.5 * x1 - 1.75 * x2 + 5 * x3 + rnorm(200)\n\nsummary(fit &lt;- lm(y ~ x1 + x2 + x3))\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.40891 -0.71728 -0.03212  0.67974  2.85561 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.19288    0.18462  28.128  &lt; 2e-16 ***\nx1           2.55059    0.06534  39.035  &lt; 2e-16 ***\nx2          -1.79698    0.42362  -4.242 3.41e-05 ***\nx3           4.58214    0.24240  18.903  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9608 on 196 degrees of freedom\nMultiple R-squared:  0.9122,    Adjusted R-squared:  0.9109 \nF-statistic: 678.7 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nNote que os valores estimados estão bastante próximos dos valores verdadeiros. Podemos ver o impacto que o tamanho da amostra tem sobre as estimativas fazendo um loop para diferentes tamanhos. Para conseguir resultados mais consistentes podemos gerar os valores do DGP várias vezes. Abaixo os dados são gerados e estimados 100 vezes para diferentes amostras com \\(n = 10, 50, 10000\\).\n\npar(mfrow = c(2, 2))\ntabela &lt;- matrix(ncol = 4)\nfor (n in c(10, 50, 10000)) {\n    i &lt;- 0\n    X &lt;- matrix(ncol = 4, nrow = 100)\n    repeat{\n\n        x1 &lt;- rnorm(n) \n        x2 &lt;- rbeta(n, 2, 5)\n        x3 &lt;- runif(n)\n        y &lt;- 5 + 2.5 * x1 - 1.75 * x2 + 5 * x3 + rnorm(n)\n\n        fit &lt;- lm(y ~ x1 + x2 + x3)\n\n        coeficientes &lt;- coef(fit)\n\n        X[i + 1, ] &lt;- coeficientes\n\n        i &lt;- i + 1\n        if (i == 100) {break}\n    }\n\n    tabela &lt;- rbind(tabela, colMeans(X, na.rm = TRUE))\n\n    hist(X[, 1], freq = FALSE,\n         main = bquote(beta[0]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = 5, col = \"red\")\n    hist(X[, 2], freq = FALSE,\n         main = bquote(beta[1]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = 2.5, col = \"red\")\n    hist(X[, 3], freq = FALSE,\n         main = bquote(beta[2]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = -1.75, col = \"red\")\n    hist(X[, 4], freq = FALSE,\n         main = bquote(beta[3]~\", n = \"~.(n)), xlab = \"\")\n    abline(v = 5, col = \"red\")\n}\n\n\n\n\n\n\n\n\n\n\nA tabela abaixo mostra a média dos valores estimados para cada coeficiente\n\n\n\n\n\n\nb0 = 5\nb1 = 2,5\nb2 = -1,75\nb3 = 5\n\n\n\n\nn = 10\n5.092687\n2.563573\n-1.852012\n4.862016\n\n\nn = 50\n5.011683\n2.510650\n-1.792762\n5.019113\n\n\nn = 10000\n4.995774\n2.499198\n-1.736876\n5.003811\n\n\n\n\n\n\n\n\n\nMQO viesado\n\nProcesso não-estacionário\nUm dos casos em que os estimadores de MQO se tornam viesados acontece quando o processo é auto-regressivo e não-estacionário, isto é, quando o DGP é da forma\n\\[\\begin{equation}\n    y_{t} = \\phi y_{t-1} + u_{t}\n\\end{equation}\\]\nem que \\(|\\phi| \\geq 1\\). O código abaixo simula o modelo acima mil vezes para diferentes valores de \\(\\phi\\). Quando o processo é estacionário temos estimativas boas para o parâmetro, mas quando \\(\\phi = 1.1\\) as estimativas tornam-se muito ruins. Quando \\(\\phi = 1\\) temos um caso de raiz unitária que tem propriedades bastante específicas\n\nlibrary(dynlm)\npar(mfrow = c(2, 2))\nfor(phi in c(0.5, 0.9, 1, 1.1)){\n    j = 0\n    x &lt;- c()\n    \n    repeat{\n\n    y &lt;- 0\n    for(i in 1:99) { \n        y[i + 1] &lt;- phi * y[i] + rnorm(1)\n    }\n    \n    y &lt;- ts(y)\n    fit &lt;- dynlm(y ~ lag(y))\n    phi_hat &lt;- coef(fit)[2]\n    x &lt;- c(x, phi_hat)\n\n    j = j + 1\n    if (j == 1000) {break}\n    }\n\n    hist(x, freq = FALSE,\n         main = bquote(phi==.(phi)),\n         xlim = c(phi - 0.2, 1),\n         xlab = \"\")\n    abline(v = phi, col = \"red\")\n}\n\n\n\n\n\n\nErros ARMA\nQuando os erros do modelo são autocorrelacionados os estimadores de MQO perdem algumas de suas boas propriedades. Ainda assim, desde que o erro seja ortogonal aos regressores os estimadores continuaram sendo não-viesados. Há, contudo, um caso em que os estimadores de MQO são viesados: quando algum dos regressores independentes é uma defasagem da variável depedente.\nIsto acontece porque este tipo de autocorrelação implica que os regressores não são ortogonais aos erros. Tome o caso simples em que o erro \\(u_{t}\\) segue um processo AR(1) da forma\n\\[\\begin{equation}\n    u_{t} = \\phi u_{t-1} + \\eta_{t}\n\\end{equation}\\] em que \\(\\eta_{t}\\) é ruído branco. E o modelo é \\[\\begin{equation}\n    y_{t} = \\beta_{0} + \\beta_{1}y_{t-1} + u_{t}\n\\end{equation}\\] Agora note que \\[y_{t - 1} = \\beta_{0} + \\beta_{1}y_{t - 2} + u_{t - 1}\\] Reescrevendo a expressão do erro e substituindo na equação acima temos que: \\[y_{t - 1} = \\beta_{0} + \\beta_{1}y_{t - 2} + \\frac{u_{t} - \\eta_{t}}{\\phi}\\] logo \\(y_{t - 1} \\propto u_{t}\\), isto é, \\(y_{t - 1}\\) é correlacionado com o termo de erro \\(u_{t}\\).\nPara exemplificar este resultado, o código abaixo simula a seguinte série 1000 vezes para diferentes valores de \\(\\phi\\). O termo de erro segue um processo AR(1). Note como as estimativas \\(\\hat{\\phi}\\) são bastante ruins. Para remediar este problema teríamos que levar em conta a estrutura autoregressiva do erro no modelo.\n\\[\\begin{align}\n    y_{t} & = \\phi y_{t-1} + u_{t}\\\\\n    u_{t} & = 0.7u_{t-1} + \\eta_{t}\\\\\n\\end{align}\\]\n\npar(mfrow = c(2, 2))\n\nfor(phi in c(0.25, 0.5, 0.9, 1)) {\n    j &lt;- 0\n    x &lt;- c()\n    \n    repeat{\n\n    y &lt;- 0\n    erro &lt;- arima.sim(n = 100, model = list(ar = .7))\n\n    for(i in 1:99) { y[i + 1] &lt;- phi*y[i] + erro[i] }\n    \n    y &lt;- ts(y)\n\n    fit &lt;- dynlm(y ~ lag(y))\n    \n    phi_hat &lt;- coef(fit)[2]\n\n    x &lt;- c(x, phi_hat)\n\n    j = j + 1\n    if (j == 1000) {break}\n    }\n\n    hist(x, main = bquote(phi==.(phi)), xlim = c(phi - 0.2, 1), xlab = \"\")\n    abline(v = phi, col = \"red\")\n\n}\n\n\n\n\nEste resultado não muda assintoticamente, isto é, não importa qual o tamanho da amostra: as estimativas de \\(\\hat{\\phi}\\) serão viesadas. Na verdade, os resultados vão piorando à medida que cresce o tamanho da amostra. Abaixo mostro isto para o caso de \\(\\phi = 0.5\\) e n = \\(100, 200, 500, 10000\\).\n\npar(mfrow = c(2, 2))\nphi &lt;- 0.5\nfor(n in c(100, 200, 500, 10000)){\n    j = 0\n    x &lt;- c()\n    \n    repeat{\n\n    y &lt;- 0\n    erro &lt;- arima.sim(n = n, model = list(ar = .7))\n\n    for(i in 1:(n-1)) { y[i + 1] &lt;- phi*y[i] + erro[i] }\n    \n    y &lt;- ts(y)\n\n    fit &lt;- dynlm(y ~ lag(y))\n    \n    phi_hat &lt;- coef(fit)[2]\n\n    x &lt;- c(x, phi_hat)\n\n    j = j + 1\n    if (j == 1000) {break}\n    }\n\n    hist(x,\n         freq = FALSE,\n         main = bquote(phi==.(phi)~\", n = \"~.(n)),\n         xlim = c(0, 1),\n         xlab = \"\")\n    abline(v = phi, col = \"red\")\n\n}\n\n\n\n\nConsiderando a estrutura autoregressiva do modelo chegamos em estimativas melhores para os parâmetros. O código abaixo faz um ARMAX, isto é, uma estimativa de mínimos quadrados com erros ARMA. O modelo segue o processo \\[\\begin{align}\n    y_{t} & = \\beta_{0} + \\beta_{1}y_{t-1} + u_{t}\\\\\n    u_{t} & = \\beta_{2} u_{t-1} + \\eta_{t}\\\\\n\\end{align}\\]\nonde os valores foram definidos como \\(\\beta_{0} = 0\\), \\(\\beta_{1} = 0.5\\) e \\(\\beta_{2} = 0.7\\).\n\npar(mfrow = c(2, 2))\nn &lt;- 1000\nparam &lt;- matrix(nrow = n, ncol = 3)\nj &lt;- 0\nrepeat {\n\n    y &lt;- 0\n    erro &lt;- arima.sim(n = n, model = list(ar = .7))\n\n    for(i in 1:(n-1)) {\n        y[i + 1] &lt;- 0.5 * y[i] + erro[i + 1]\n    }\n    \n    y &lt;- ts(y)\n    ly &lt;- lag(y, -1)\n    x &lt;- ts.intersect(y, ly, dframe = TRUE)\n    fit &lt;- arima(x$y, order = c(1, 0, 0), xreg = x$ly)\n    param[j + 1, ] &lt;- coef(fit)\n\n    j = j + 1\n    if (j == 1000) {break}\n}\n\nhist(param[, 2], main = bquote(beta[0]==0~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0, col = \"red\")\nhist(param[, 1], main = bquote(beta[1]==0.5~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0.5, col = \"red\")\nhist(param[, 3], main = bquote(beta[2]==0.7~\", n = 1000\"), xlab = \"\", freq = F)\nabline(v = 0.7, col = \"red\")"
  },
  {
    "objectID": "posts/general-posts/precos-imoveis-demografia/index.html",
    "href": "posts/general-posts/precos-imoveis-demografia/index.html",
    "title": "Preços de Imóveis e Demografia",
    "section": "",
    "text": "Já vi em alguns lugares uma suposta ligação entre fatores demográficos e tendências de longo prazo no mercado imobiliário. Intuitivamente, alguns dos principais motivadores para comprar ou vender um imóvel estão ligados a fatores demográficos: nascimentos, casamentos, divórcios ou óbitos.\nEste tipo de análise omite fatores importantes como renda, condições de financiamento e o contexto geral da economia. Ainda assim, fiquei curioso para ver se havia algum padrão entre tendências demográficas mais simples e o comportamento dos preços."
  },
  {
    "objectID": "posts/general-posts/precos-imoveis-demografia/index.html#todos-os-países",
    "href": "posts/general-posts/precos-imoveis-demografia/index.html#todos-os-países",
    "title": "Preços de Imóveis e Demografia",
    "section": "Todos os países",
    "text": "Todos os países\nEsta análise é ainda bastante preliminar. Ainda que a demografia seja um motor para a demanda imobiliária, outros fatores como oferta de moradia e condições de crédito são importantes demais parecem serem omitidos.\nO gráfico abaixo compara a população em 2010/2020 com os preços em 2010/2020. Os países ao lado direito do gráfico, são os países onde houve crescimento populacional. Os países na parte de cima do gráfico são os países onde houve crescimento real do preço dos imóveis. Na média da amostra, destacada como WLD, houve crescimento de ambos.\nPaíses que estão muito para cima como Chile (CHL), Índia (IND) e Estônia (EDT) estão com imóveis muito “caros”. Já em países com França e Finlândia tanto a população como o nível de preço dos imóveis cresceram muito pouco. No caso da Grécia, tanto a população como o preço dos imóveis diminuiu nos últimos dez anos.\nO gráfico não me surpreendeu muito, mas esperava que os EUA estivessem mais para cima no gráfico e que o Brasil estivesse ao menos do lado positivo do eixo-x. Isso me sugere que a impressão de que os imóveis no Brasil são ou estão caros tem muito mais a ver com a baixa renda da população.\n\n\nCode\np5 &lt;- \n  ggplot(\n    data = na.omit(tbl_wide),\n    aes(x = index_pop, y = index_house)\n    ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, color = colors[1]) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(aes(label = iso3c, color = highlight)) +\n  scale_x_continuous(breaks = seq(-10, 30, 5)) +\n  scale_y_continuous(breaks = seq(-30, 90, 15)) +\n  scale_color_manual(values = c(\"black\", colors[1])) +\n  guides(color = \"none\") +\n  labs(\n    title = \"Real House Prices x Population (2010/20)\",\n    x = \"Population (2010/2020)\",\n    y = \"RPPI (2010/2020)\",\n    caption = \"Source: Real House Price Indexes (BIS), Population (UN).\") +\n  theme_vini\n\np5"
  },
  {
    "objectID": "posts/general-posts/precos-imoveis-demografia/index.html#footnotes",
    "href": "posts/general-posts/precos-imoveis-demografia/index.html#footnotes",
    "title": "Preços de Imóveis e Demografia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHong Kong: https://exame.com/economia/as-raizes-economicas-dos-protestos-de-hong-kong/↩︎"
  },
  {
    "objectID": "posts/general-posts/ols-com-matrizes/index.html",
    "href": "posts/general-posts/ols-com-matrizes/index.html",
    "title": "OLS com matrizes",
    "section": "",
    "text": "Uma forma instrutiva de entender o modelo de regressão linear é expressando ele em forma matricial. Os cursos introdutórios de econometria costumam omitir esta abordagem e expressam todas as derivações usando somatórios, deixando a abordagem matricial para cursos mais avançados. É bastante simples computar uma regressão usando apenas matrizes no R.\nDe fato, um dos objetos fundamentais do R é a e muitas das operações matriciais (decomposições, inversa, transposta, etc.) já estão implementadas em funções base. Uma var \\(k\\) .\nNeste post vou mostrar como fazer uma regressão linear usando somente matrizes no R. Além disso, vou computar algumas estatísticas típicas (t, F)\nO modelo linear é da forma\n\\[\ny_{t} = x^\\intercal_{t} \\beta + e_{t}\n\\]\nonde \\(x^\\intercal\\) é o vetor transposto de \\(x\\) . É importante sempre ter em mente a dimensão destes vetores. O vetor \\(y_{t}\\) é \\(n\\times1\\) onde \\(n\\) representa o número de observações na amostra. O vetor \\(\\beta\\) é \\(k\\times1\\) onde \\(k\\) é o número de regressores (ou variáveis explicativas). Como há \\(n\\) observações para cada uma dos \\(k\\) regressores, \\(x\\) é \\(k\\times1\\) ; o detalhe é que \\(x = (1 \\, \\,x_{1} \\, \\dots \\,x_{k-1})\\) , onde cada \\(x_{i}\\) é \\(n\\times 1\\) e \\(1\\) é um vetor de uns \\(n\\times1\\) . Finalmente, \\(e_{t}\\) é \\(n\\times1\\) . Temos então que:\n\\[\n\\begin{pmatrix}\ny_{1} \\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}=\n\\begin{pmatrix}\n1\\\\\nx_{1}\\\\\n\\vdots\\\\\nx_{k-1}\n\\end{pmatrix}^\\intercal\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k-1}\n\\end{pmatrix}+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n\\]\nonde:\n\\[\n\\begin{bmatrix}\nx_{1} & = (x_{11} & x_{12} & x_{13} & \\dots & x_{1n})\\\\\nx_{2} & = (x_{21} & x_{22} & x_{23} & \\dots & x_{2n})\\\\\nx_{3} & = (x_{31} & x_{32} & x_{33} & \\dots & x_{3n})\\\\\n\\vdots\\\\\nx_{k-1} & = (x_{(k-1)1} & x_{(k-1)2} & x_{(k-1)3} & \\dots & x_{(k-1)n})\n\\end{bmatrix}\n\\]\nQueremos encontrar o vetor \\(\\hat{\\beta}\\) que minimiza o a soma do quadrado dos erros, isto é, que minimiza\n\\[\nS(\\beta) = \\sum_{t = 1}^{T}(y_{t} - x^\\intercal_{t}\\beta)^{2}\n\\]\nEncontramos o ponto crítico derivando a expressão acima e igualando-a a zero. O resultado é o conhecido estimador de mínimos quadrados:\n\\[\n\\hat{\\beta} = \\left ( \\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} \\right )^{-1} \\sum_{t = 1}^{T}x_{t}y_{t}\n\\]\nPara reescrever as equações acima usando matrizes usamos os seguintes fatos:\n\\[\n\\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} = X^\\intercal X\n\\]\nonde \\(X\\) é uma matriz \\(n \\times k\\)\n\\[\n\\sum_{t = 1}^{T}x_{t}y_{t} = X^\\intercal y\n\\]\nonde \\(X^\\intercal y\\) é \\(k \\times 1\\) . Lembre-se que uma hipótese do modelo linear é de que \\(X\\) é uma matriz de posto completo, logo \\(X^\\intercal X\\) possui inversa e podemos escrever:\n\\[\n\\hat{\\beta} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]"
  },
  {
    "objectID": "posts/general-posts/ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "href": "posts/general-posts/ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "title": "OLS com matrizes",
    "section": "Exemplo: salário (Wooldridge)",
    "text": "Exemplo: salário (Wooldridge)\nComo exemplo vou usar um exemplo clássico de livro texto de econometria: uma regressão de salário (rendimento) contra algumas variáveis explicativas convencionais: anos de educação, sexo, anos de experiência, etc. As bases de dados do livro Introductory Econometrics estão disponíveis no pacote wooldridge. O código abaixo carrega a base de dados .\n\nlibrary(wooldridge)\n# Carrega a base\ndata(wage2)\n# Remove os valores ausentes (NAs)\nsal &lt;- na.omit(wage2)\n\n\n\n\n\n\n\n\nwage\nhours\nIQ\nKWW\neduc\nexper\ntenure\nage\nmarried\nblack\nsouth\nurban\nsibs\nbrthord\nmeduc\nfeduc\nlwage\n\n\n\n\n1\n769\n40\n93\n35\n12\n11\n2\n31\n1\n0\n0\n1\n1\n2\n8\n8\n6.645091\n\n\n3\n825\n40\n108\n46\n14\n11\n9\n33\n1\n0\n0\n1\n1\n2\n14\n14\n6.715383\n\n\n4\n650\n40\n96\n32\n12\n13\n7\n32\n1\n0\n0\n1\n4\n3\n12\n12\n6.476973\n\n\n5\n562\n40\n74\n27\n11\n14\n5\n34\n1\n0\n0\n1\n10\n6\n6\n11\n6.331502\n\n\n7\n600\n40\n91\n24\n10\n13\n0\n30\n0\n0\n0\n1\n1\n2\n8\n8\n6.396930\n\n\n9\n1154\n45\n111\n37\n15\n13\n1\n36\n1\n0\n0\n0\n2\n3\n14\n5\n7.050990\n\n\n10\n1000\n40\n95\n44\n12\n16\n16\n36\n1\n0\n0\n1\n1\n1\n12\n11\n6.907755\n\n\n11\n930\n43\n132\n44\n18\n8\n13\n38\n1\n0\n0\n0\n1\n1\n13\n14\n6.835185\n\n\n14\n1318\n38\n119\n24\n16\n7\n2\n28\n1\n0\n0\n1\n3\n1\n10\n10\n7.183871\n\n\n15\n1792\n40\n118\n47\n16\n9\n9\n34\n1\n0\n0\n1\n1\n1\n12\n12\n7.491087\n\n\n\n\n\n\n\nA base traz 663 observações de 17 variáveis. A função str é útil para entender a estrutura dos dados.\n\n# Dimensão da base (# linhas  # colunas)\ndim(sal)\n\n[1] 663  17\n\n# Descrição da base\nstr(sal)\n\n'data.frame':   663 obs. of  17 variables:\n $ wage   : int  769 825 650 562 600 1154 1000 930 1318 1792 ...\n $ hours  : int  40 40 40 40 40 45 40 43 38 40 ...\n $ IQ     : int  93 108 96 74 91 111 95 132 119 118 ...\n $ KWW    : int  35 46 32 27 24 37 44 44 24 47 ...\n $ educ   : int  12 14 12 11 10 15 12 18 16 16 ...\n $ exper  : int  11 11 13 14 13 13 16 8 7 9 ...\n $ tenure : int  2 9 7 5 0 1 16 13 2 9 ...\n $ age    : int  31 33 32 34 30 36 36 38 28 34 ...\n $ married: int  1 1 1 1 0 1 1 1 1 1 ...\n $ black  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ south  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urban  : int  1 1 1 1 1 0 1 0 1 1 ...\n $ sibs   : int  1 1 4 10 1 2 1 1 3 1 ...\n $ brthord: int  2 2 3 6 2 3 1 1 1 1 ...\n $ meduc  : int  8 14 12 6 8 14 12 13 10 12 ...\n $ feduc  : int  8 14 12 11 8 5 11 14 10 12 ...\n $ lwage  : num  6.65 6.72 6.48 6.33 6.4 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n - attr(*, \"na.action\")= 'omit' Named int [1:272] 2 6 8 12 13 19 20 21 31 36 ...\n  ..- attr(*, \"names\")= chr [1:272] \"2\" \"6\" \"8\" \"12\" ...\n\n\nO modelo proposto é o abaixo:\n\\[\n\\text{lwage}_{t} = \\beta_{0} + \\beta_{1}\\text{educ}_{t} + \\beta_{2}\\text{exper}_{t} + \\beta_{3}\\text{exper}^{2}_{t} + \\beta_{4}\\text{tenure}_{t} + \\beta{5}\\text{married}_{t} + u_{t}\n\\]\nonde:\n\nlwage = logaritmo natural do salário\neduc = anos de educação\nexper = anos de experiência (trabalhando)\ntenure = anos trabalhando com o empregador atual\nmarried = dummy (1 = casado, 0 = não-casado)\n\nHá 6 coeficientes para estimar logo \\(k = 6\\) . Além disso, como há \\(663\\) observações temos que \\(n = 663\\) . A matriz de “dados” é da forma:\n\\[\nX = \\begin{bmatrix}\n1 & 12 & 11 & 121 & 2 & 1\\\\\\\\\n1 & 14 & 11 & 121 & 9 & 1\\\\\\\\\n1 & 12 & 13 & 169 & 7 & 1\\\\\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\n1 & 13 & 10 & 100 & 3 & 1\n\\end{bmatrix}_{663\\times 6}\n\\]\nO código abaixo monta a matriz \\(X\\) acima. As funções head e tail podem ser usadas para verificar as primeiras e últimas linhas da matriz. Vale esclarecer dois pontos sobre o código a ser usado neste post. Primeiro, quando se cria um objeto usando o operador &lt;- pode-se forçar o R a imprimir o seu valor colocando a expressão entre parêntesis. Por exemplo, teste (x &lt;- 3). O segundo ponto é que o operador de multiplicação matricial é %*%.\n\n# Define alguns valores úteis: \n## N = número de observações\n## k = número de regressores\n## const = vetor com 1^\\intercals (uns)\nN &lt;- 663; k &lt;- 6; const &lt;- rep(1, 663)\n# Monta a matriz de observações da regressão\nX &lt;- cbind(const, sal$educ, sal$exper, sal$exper^2, sal$tenure, sal$married)\nX &lt;- as.matrix(X)\n# Define o nome das colunas da matriz de observações\ncolnames(X) &lt;- c(\"const\", \"educ\", \"exper\", \"exper2\", \"tenure\", \"married\")\n# Função para verificar as primeiras linhas da matriz X\nhead(X)\n\n     const educ exper exper2 tenure married\n[1,]     1   12    11    121      2       1\n[2,]     1   14    11    121      9       1\n[3,]     1   12    13    169      7       1\n[4,]     1   11    14    196      5       1\n[5,]     1   10    13    169      0       0\n[6,]     1   15    13    169      1       1\n\n# Função para verificar as últimas linhas da matriz X\ntail(X)\n\n       const educ exper exper2 tenure married\n[658,]     1   12     9     81      2       1\n[659,]     1   16     8     64     10       1\n[660,]     1   12    11    121      3       1\n[661,]     1   12     9     81      3       1\n[662,]     1   16    10    100      9       1\n[663,]     1   13    10    100      3       1\n\n\nLembrando que o problema de mínimos quadrados é de encontrar os valores de \\(\\beta\\) que minimizam a soma dos erros ao quadrado.\n\\[\n\\underset{\\beta}{\\text{Min }} e^\\intercal e\n\\]\nAbrindo mais a expressão acima:\n\\[\n\\begin{align}\n  e^\\intercal e & = (y - X\\beta )^\\intercal(y - X\\beta ) \\\\\\\\\n      & = y^\\intercal y - y^\\intercal X\\beta - \\beta ^\\intercal X^\\intercal y + \\beta ^\\intercal X^\\intercal X \\beta \\\\\\\\\n      & = y^\\intercal y - 2 y^\\intercal X \\beta + \\beta^\\intercal X^\\intercal X \\beta\n\\end{align}\n\\]\nDerivando em relação a \\(\\beta\\) e igualando a zero chega-se no estimador de MQO\n\\[\n\\beta_{\\text{MQO}} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]\nO código abaixo computa \\(\\beta_{\\text{MQO}}\\) . Note que os parêntesis por fora da expressão forçam o R a imprimir o valor do objeto. Além disso, como estamos multiplicando matrizes/vetores usamos %*%.\n\n# Define o vetor y (log do salário)\ny &lt;- sal$lwage\n# Computa a estimativa para os betas\n(beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y)\n\n                [,1]\nconst   5.3563606713\neduc    0.0767258959\nexper   0.0104985672\nexper2  0.0002881339\ntenure  0.0091039254\nmarried 0.2002468574\n\n\nOs valores estimados dos betas são reportados na tabela abaixo.\n\ntabela &lt;- as.data.frame(round(beta, 4))\ncolnames(tabela) &lt;- c(\"Coeficiente estimado\")\nround(beta, 4) %&gt;%\n  kable(align = \"c\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\nconst\n5.3564\n\n\neduc\n0.0767\n\n\nexper\n0.0105\n\n\nexper2\n0.0003\n\n\ntenure\n0.0091\n\n\nmarried\n0.2002"
  },
  {
    "objectID": "posts/general-posts/ols-com-matrizes/index.html#resíduo-e-variância",
    "href": "posts/general-posts/ols-com-matrizes/index.html#resíduo-e-variância",
    "title": "OLS com matrizes",
    "section": "Resíduo e variância",
    "text": "Resíduo e variância\nO resíduo do modelo é simplesmente a diferença entre o observado \\(y_{t}\\) e o estimado \\(\\hat{y_{t}}\\) . Isto é,\n\\[\n\\hat{e}_{t} = y_{t} - \\hat{y}_{t} = y_{t} - x_{t}^\\intercal\\hat{\\beta}\n\\]\nou, de forma equivalente,\n\\[\n\\hat{e} = y - X\\hat{\\beta}\n\\]\n\n# Computa o resíduo da regressão\nu_hat &lt;- y - X %*% beta\n\nUsando o histograma pode-se visualizar a distribuição dos resíduos.\n\nhist(u_hat, breaks = 30, freq = FALSE, main = \"Histograma dos resíduos\")\n\n\n\n\nO estimador da variância é dado por:\n\\[\n\\hat{\\sigma}^{2} = \\frac{1}{N-k}\\sum_{t = 1}^{N}\\hat{e}_{t}^{2}\n\\]\nSubstituindo os valores calculados acima chegamos em:\n\\[\n\\hat{\\sigma}^{2} = \\frac{\\hat{e}^\\intercal\\hat{e}}{N-k} = 0.1403927\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html",
    "title": "ARMA: um exemplo simples",
    "section": "",
    "text": "Neste post apresento como estimar e diagnosticar um modelo ARMA simples no R. O modelo ARMA decompõe uma série em função de suas observações passadas e de “passados” (às vezes chamados de inovações). O ARMA(1, 1) com constante tem a seguinte forma.\n\\[\ny_{t} = \\phi_{0} +\\phi_{1}y_{t-1} + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t - 1}\n\\]\nonde ambas as condições de estacionaridade como de invertibilidade devem ser atendidas. Assume-se que o termo \\(\\varepsilon_{t}\\) é um ruído branco. De maneira mais geral, um modelo ARMA(p, q) tem a forma:\n\\[\ny_{t} = \\phi_{0} +\\phi_{1}y_{t-1} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t - 1} + \\dots + \\theta_{q}\\varepsilon_{t- q}\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-1",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-1",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 1",
    "text": "Modelo 1\nVou primeiro estimar o ARMA(1, 2) usando a função arima() do R.\n\n(m1 &lt;- arima(train, order = c(1, 0, 2)))\n#&gt; \n#&gt; Call:\n#&gt; arima(x = train, order = c(1, 0, 2))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1     ma1     ma2  intercept\n#&gt;       0.2324  0.0971  0.1623     0.0084\n#&gt; s.e.  0.2294  0.2247  0.0958     0.0012\n#&gt; \n#&gt; sigma^2 estimated as 0.0001038:  log likelihood = 566.91,  aic = -1123.82\n\nA estimativa tem a forma: \\[\n  y_{t} = \\underset{(0.0012)}{0.0084} + \\underset{(0.2324)}{0.2294}y_{t - 1} + \\underset{(0.2247)}{0.0971}\\epsilon_{t-1} + \\underset{(0.0958)}{0.1623}\\epsilon_{t-2}\n\\] Note que os erros-padrão de \\(\\hat{\\phi_{1}}\\) e \\(\\hat{\\theta_{1}}\\) são bastante elevados. De fato, um teste-t revela que estes coeficientes não são significantes.\n\nDiagnóstico de resíduos\nPode-se verificar os resíduos do modelo de muitas formas. Idealmente, quer-se que os resíduos não apresentem autocorrelação alguma. Uma forma gráfica de ver isto é usando a função lag1.plot que apresenta gráficos de dispersão entre o resíduo \\(u_{t}\\) contra suas defasagens \\(u_{t-1}, u_{t-2}, \\dots\\). Abaixo faço isto para as primieras quatro defasagens.\n\nresiduos &lt;- resid(m1)\nlag1.plot(residuos, max.lag = 4)\n\n\n\n\n\n\n\n\nNa prática, é mais conveniente analisar diretamente os gráficos da FAC e da FACP dos resíduos.\n\nacf2(residuos)\n\n\n\n\n\n\n\n#&gt;      [,1]  [,2] [,3]  [,4]  [,5]  [,6]  [,7]  [,8] [,9] [,10] [,11] [,12] [,13]\n#&gt; ACF  0.01 -0.01    0 -0.09 -0.12 -0.03 -0.04 -0.05 0.04  0.06  0.07 -0.12 -0.07\n#&gt; PACF 0.01 -0.01    0 -0.09 -0.12 -0.03 -0.05 -0.06 0.02  0.04  0.06 -0.15 -0.09\n#&gt;      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]\n#&gt; ACF  -0.03 -0.12  0.07  0.06  0.06  0.02  0.09  -0.1  0.00 -0.07 -0.04\n#&gt; PACF -0.02 -0.11  0.06  0.02  0.04 -0.02  0.05  -0.1  0.01 -0.05 -0.01\n\nUm teste basatante usual para verificar a presença de autocorrelação nos resíduos é o Ljung-Box. Para computá-lo uso a função Box.test() do pacote tseries. Note que é preciso suplementar o argumento fitdf com o número de parâmetros estimados do modelo. Isto serve para corrigir a estatística do teste. A escolha da ordem do teste é um tanto arbitrária e, na prática, seria recomendado fazer o teste para várias ordens diferentes. O código abaixo computa a estatística do teste para uma defasagem igual a 8.\nNote o uso do argumento fitdf que leva em conta o número de parâmetros estimados no modelo. Segundo o p-valor, não temos evidência para rejeitar a hipótese nula de que os resíduos não são conjuntamente autocorrelacionados. Isto é, temos evidência de que o modelo está bem ajustado pois os resíduos parecem se comportar como ruído branco.\n\nlibrary(tseries)\nBox.test(residuos, type = \"Ljung-Box\", lag = 8, fitdf = 4)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  residuos\n#&gt; X-squared = 4.9486, df = 4, p-value = 0.2926\n\nNa prática, é bom repetir o teste para várias defasagens diferentes. A tabela abaixo resume os valores do teste para várias ordens de defasagem. Vale lembrar que o teste Ljung-Box tende a não-rejeitar H0 para defasagens muito elevadas.\n\n\n\n\n\nDefasagem\nEstatística de teste\nP-valor\n\n\n\n\n8\n4.9486\n0.2926\n\n\n9\n5.2251\n0.3890\n\n\n10\n5.8669\n0.4383\n\n\n11\n6.7991\n0.4501\n\n\n12\n9.7788\n0.2809\n\n\n13\n10.8054\n0.2893\n\n\n14\n10.9313\n0.3629\n\n\n15\n13.6377\n0.2537\n\n\n16\n14.5903\n0.2646\n\n\n17\n15.2158\n0.2941\n\n\n18\n15.8735\n0.3212\n\n\n19\n15.9294\n0.3868\n\n\n20\n17.4548\n0.3568\n\n\n\n\n\n\n\n\n\nUsando o pacote astsa\nUma forma bastante conveniente de trabalhar com modelos ARMA é com a função sarima do pacote astsa. Esta função apresenta automaticamente algumas valiosas informações para o diagnóstico dos resíduos: o gráfico do ACF, o gráfico qq-plot (para verificar a normalidade dos resíduos) e os p-valores do teste Ljung-Box para várias ordens de defasagem.\nA saída abaixo reúne quatro gráficos. O primeiro deles apresenta o resíduo normalizado (ou escalado). Este gráfico não deve apresentar um padrão claro. O segundo gráfico é a FAC do resíduo: idealmente, nenhuma defasagem deve ser significativa neste gráfico. Ao lado da FAC temos o QQ-plot: se todos os pontos caem sobre a linha azul temos evidência de que os resíduos são normalmente distribuídos.\nPor fim, o último gráfico mostra o p-valor do teste Ljung-Box (já ajustado pelo número de parâmetros do modelo estimado) para diferentes defasagens. A linha tracejada em azul indica o valor 0.05. Idealmente, todos os pontos devem estar acima desta linha.\n\nsarima(train, p = 1, d = 0, q = 2)\n#&gt; initial  value -4.507231 \n#&gt; iter   2 value -4.512000\n#&gt; iter   3 value -4.582718\n#&gt; iter   4 value -4.583615\n#&gt; iter   5 value -4.583727\n#&gt; iter   6 value -4.583754\n#&gt; iter   7 value -4.583896\n#&gt; iter   8 value -4.583941\n#&gt; iter   9 value -4.583957\n#&gt; iter  10 value -4.583958\n#&gt; iter  10 value -4.583958\n#&gt; final  value -4.583958 \n#&gt; converged\n#&gt; initial  value -4.586016 \n#&gt; iter   2 value -4.586020\n#&gt; iter   3 value -4.586022\n#&gt; iter   4 value -4.586023\n#&gt; iter   5 value -4.586025\n#&gt; iter   6 value -4.586026\n#&gt; iter   7 value -4.586027\n#&gt; iter   8 value -4.586027\n#&gt; iter   9 value -4.586028\n#&gt; iter   9 value -4.586028\n#&gt; iter   9 value -4.586028\n#&gt; final  value -4.586028 \n#&gt; converged\n\n\n\n\n\n\n\n#&gt; $fit\n#&gt; \n#&gt; Call:\n#&gt; arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n#&gt;     xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n#&gt;     optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1     ma1     ma2   xmean\n#&gt;       0.2324  0.0971  0.1623  0.0084\n#&gt; s.e.  0.2294  0.2247  0.0958  0.0012\n#&gt; \n#&gt; sigma^2 estimated as 0.0001038:  log likelihood = 566.91,  aic = -1123.82\n#&gt; \n#&gt; $degrees_of_freedom\n#&gt; [1] 175\n#&gt; \n#&gt; $ttable\n#&gt;       Estimate     SE t.value p.value\n#&gt; ar1     0.2324 0.2294  1.0130  0.3125\n#&gt; ma1     0.0971 0.2247  0.4319  0.6664\n#&gt; ma2     0.1623 0.0958  1.6946  0.0919\n#&gt; xmean   0.0084 0.0012  6.7491  0.0000\n#&gt; \n#&gt; $AIC\n#&gt; [1] -6.278312\n#&gt; \n#&gt; $AICc\n#&gt; [1] -6.277028\n#&gt; \n#&gt; $BIC\n#&gt; [1] -6.189279"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-2",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-2",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 2",
    "text": "Modelo 2\nEstimo também o modelo ARMA(1, 1). A análise de resíduos é análoga à apresentada acima.\n\nm2 &lt;- arima(train, order = c(1, 0, 1))"
  },
  {
    "objectID": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-3",
    "href": "posts/general-posts/repost-arma-exemplo-simples/index.html#modelo-3",
    "title": "ARMA: um exemplo simples",
    "section": "Modelo 3",
    "text": "Modelo 3\nPara o terceiro modelo uso o método automático do auto.arima(). A função escolhe o AR(1) com constante como melhor modelo para representar os dados. Note que a na inspeação visual também tínhamos verificado que o AR(1) seria um possível candidato.\n\n(m3 &lt;- auto.arima(train))\n#&gt; Series: train \n#&gt; ARIMA(1,0,0) with non-zero mean \n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1    mean\n#&gt;       0.3568  0.0084\n#&gt; s.e.  0.0695  0.0012\n#&gt; \n#&gt; sigma^2 = 0.0001066:  log likelihood = 565.52\n#&gt; AIC=-1125.04   AICc=-1124.91   BIC=-1115.48\n\n\nSeleção\nPara escolher o melhor modelo pode-se usar algum critério de informação. Abaixo comparo os modelos segundo os critérios AIC, AICc (AIC corrigido) e BIC (Bayesian Information Criterion). Na prática, não é comum que os três critérios escolham o mesmo modelo; em particular, o BIC penaliza o número de parâmetros mais fortemente que o AIC. Neste caso específico, os três critérios escolhem o AR(1).\n\n\n\n\n\n\nAIC\nAICc\nBIC\n\n\n\n\nARMA(1, 2)\n-1123.818\n-1124.908\n-1115.483\n\n\nARMA(1, 1)\n-1123.563\n-1124.908\n-1115.483\n\n\nAR(1)\n-1125.045\n-1124.908\n-1115.483\n\n\n\n\n\n\n\n\n\nPrevisão\nPara gerar previsões usamos a função forecast() (outra opção é usar a função base predict()). Abaixo computo previsões para os três modelos acima mais um modelo ingênuo que será usado como bench-mark. O modelo ingênuo é simplesmente um random-walk que prevê sempre o valor anterior, isto é, \\(\\hat{y_{T+1}} = y_{T}\\).\n\nprev1 &lt;- forecast(m1, h = length(teste))\nprev2 &lt;- forecast(m2, h = length(teste))\nprev3 &lt;- forecast(m3, h = length(teste))\nprev4 &lt;- forecast(naive(train, h = length(teste)), h = length(teste))\n\nerros &lt;- t(matrix(c(teste - prev1$mean,\n                    teste - prev2$mean,\n                    teste - prev3$mean,\n                    teste - prev4$mean),\n                    ncol = 4))\nerros &lt;- as.data.frame(erros)\ncolnames(erros) &lt;- paste(\"t =\", as.numeric(time(teste)))\nrow.names(erros) &lt;- c(\"ARMA(1, 2)\",\n                      \"ARMA(1, 1)\",\n                      \"AR(1)\",\n                      \"Y[t+1] = Y[t]\")\n\nOs erros de previsão são apresentados na tabela abaixo.\n\n\n\n\n\n\n\nt = 1992\nt = 1992.25\nt = 1992.5\nt = 1992.75\nt = 1993\nt = 1993.25\nt = 1993.5\nt = 1993.75\nt = 1994\nt = 1994.25\nt = 1994.5\nt = 1994.75\nt = 1995\nt = 1995.25\nt = 1995.5\nt = 1995.75\nt = 1996\nt = 1996.25\nt = 1996.5\nt = 1996.75\nt = 1997\nt = 1997.25\nt = 1997.5\nt = 1997.75\nt = 1998\nt = 1998.25\nt = 1998.5\nt = 1998.75\nt = 1999\nt = 1999.25\nt = 1999.5\nt = 1999.75\nt = 2000\nt = 2000.25\nt = 2000.5\nt = 2000.75\nt = 2001\nt = 2001.25\nt = 2001.5\nt = 2001.75\nt = 2002\nt = 2002.25\nt = 2002.5\n\n\n\n\nARMA(1, 2)\n0.0017385\n0.0009996\n-0.0011832\n0.0047685\n-0.0075880\n-0.0034300\n-0.0032720\n0.0041505\n0.0009645\n0.0048053\n-0.0031499\n0.0038158\n-0.0038534\n-0.0059706\n-0.0026298\n0.0011636\n-0.0008728\n0.0066953\n-0.0041940\n0.0035309\n0.0012488\n0.0064862\n0.0012950\n-0.0021395\n0.0069883\n-0.0032794\n-0.0002956\n0.0083934\n0.0023188\n-0.0031434\n0.0033963\n0.0099058\n-0.0030742\n0.0039295\n-0.0078546\n-0.0045499\n-0.0119065\n-0.0103021\n-0.0110772\n0.0006393\n0.0005900\n-0.0074360\n0.0023173\n\n\nARMA(1, 1)\n0.0015678\n0.0011472\n-0.0010609\n0.0048359\n-0.0075556\n-0.0034158\n-0.0032667\n0.0041516\n0.0009637\n0.0048036\n-0.0031520\n0.0038136\n-0.0038557\n-0.0059730\n-0.0026323\n0.0011611\n-0.0008752\n0.0066928\n-0.0041964\n0.0035284\n0.0012464\n0.0064838\n0.0012925\n-0.0021420\n0.0069858\n-0.0032818\n-0.0002980\n0.0083910\n0.0023164\n-0.0031458\n0.0033939\n0.0099034\n-0.0030766\n0.0039271\n-0.0078570\n-0.0045523\n-0.0119090\n-0.0103046\n-0.0110796\n0.0006369\n0.0005876\n-0.0074384\n0.0023149\n\n\nAR(1)\n0.0013079\n0.0009620\n-0.0011721\n0.0047728\n-0.0075918\n-0.0034379\n-0.0032817\n0.0041400\n0.0009537\n0.0047944\n-0.0031608\n0.0038049\n-0.0038643\n-0.0059815\n-0.0026408\n0.0011526\n-0.0008837\n0.0066843\n-0.0042049\n0.0035199\n0.0012379\n0.0064753\n0.0012840\n-0.0021505\n0.0069773\n-0.0032903\n-0.0003065\n0.0083825\n0.0023079\n-0.0031543\n0.0033854\n0.0098949\n-0.0030851\n0.0039186\n-0.0078655\n-0.0045608\n-0.0119175\n-0.0103131\n-0.0110881\n0.0006284\n0.0005791\n-0.0074469\n0.0023064\n\n\nY[t+1] = Y[t]\n0.0024552\n0.0025186\n0.0005306\n0.0065276\n-0.0058184\n-0.0016579\n-0.0014994\n0.0059232\n0.0027372\n0.0065781\n-0.0013772\n0.0055886\n-0.0020806\n-0.0041978\n-0.0008571\n0.0029363\n0.0009000\n0.0084680\n-0.0024213\n0.0053036\n0.0030215\n0.0082589\n0.0030677\n-0.0003668\n0.0087610\n-0.0015066\n0.0014772\n0.0101662\n0.0040916\n-0.0013707\n0.0051690\n0.0116786\n-0.0013015\n0.0057023\n-0.0060818\n-0.0027771\n-0.0101338\n-0.0085294\n-0.0093044\n0.0024120\n0.0023628\n-0.0056632\n0.0040900\n\n\n\n\n\n\n\nPode-se melhor comparar a performance das previsões usando alguma medida agregada de erro. Duas medidas bastante comuns são o Erro Absoluto Médio (EAM) e o Erro Quadrático Médio. A primeira toma o módulo da diferença entre o previsto (\\(\\hat{y}\\)) e o observado (\\(y\\)) e tira uma média, enquanto a última toma a diferença quadrática. Formalmente, para um horizonte de previsão \\(h\\) começando na última observação \\(T\\):\n\\[\\begin{align}\n\\text{EAM} & = \\frac{1}{h}\\sum_{i = T + 1}^{T + h} |y_{i} - \\hat{y}_{i}| \\\\\n\\text{EQM} & = \\frac{1}{h}\\sum_{i = T + 1}^{T + h} (y_{i} - \\hat{y}_{i})^2\n\\end{align}\\]\nA tabela abaixo compara os modelos segundo estas medidas de erro.\n\n\n\n\n\n\n\nEAM\nEQM\n\n\n\n\nARMA(1, 2)\n0.0042173\n0.0042062\n\n\nARMA(1, 1)\n0.0000268\n0.0000268\n\n\nAR(1)\n0.0042143\n0.0043644\n\n\nY[t+1] = Y[t]\n0.0000268\n0.0000280\n\n\n\n\n\n\n\nNem sempre é fácil comparar estas medidas, i.e., verificar se elas são estatisticamente significantes. Pode-se usar o teste Diebold-Mariano para comparar estas medidas de erro, mas é importante frisar que ele contém uma série de hipóteses sobre a distribuição dos erros. A função dm.test do pacote forecast faz este teste e mais informações sobre ele podem ser encontradas usando ?dm.test.\nPode-se visualizar as previsões usando as funções autoplot() e autolayer() do pacote forecast. Abaixo mostro os resultados para o modelo AR(1) e também para o modelo ingênuo. note como o erros-padrão deste último cresce muito rapidamente (pois a variância de um processo random-walk cresce sem limite).\n\nlibrary(ggplot2)\n\nautoplot(prev2, include = 50) +\n  autolayer(teste) +\n  theme_bw()\n\n\n\n\n\n\n\n\nautoplot(prev3, include = 50) +\n  autolayer(teste) +\n  labs(x = \"\", y = \"(%)\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nautoplot(prev4, include = 50) +\n  autolayer(teste) +\n  labs(x = \"\", y = \"(%)\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html",
    "href": "posts/general-posts/repost-gapminder/index.html",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "",
    "text": "Este é um post antigo que estou repostando porque ainda pode ser útil como um tutorial básico para R. Fiz apenas algumas modificações no código para deixar ele"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#preparativos",
    "href": "posts/general-posts/repost-gapminder/index.html#preparativos",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "Preparativos",
    "text": "Preparativos\n\nCarregando os pacotes\nMuitos já devem estar familiarizados com a apresentação do historiador Hans Rosling sobre a evolução da expectativa de vida e do PIB per capita dos países em torno do mundo. Este post vai mostrar como usar o ggplot2 e o tidyverse para explorar estes dados. Pode-se acessar uma versão simplificada da base de dados pelo pacote gapminder.\n\n# Tutorial Gapminder\n\n# Pacotes\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(kableExtra)\nlibrary(ggrepel)\nlibrary(showtext)\n\n# Carregar fontes\nsysfonts::font_add_google(\"Jost\", \"Jost\")\nshowtext::showtext_auto()\n# Carregar os dados\ndata(\"gapminder\")\ndata(\"continent_colors\")\n\n\n\nChecagem dos dados\nDe início é sempre importante verificar se há problemas com os dados. Como estamos usando uma base que já foi tratada é de se esperar que tudo esteja em ordem. Tipicamente, queremos verificar quantas observações ausentes (NAs) existem; se as variáveis estão no formato correto (ex: variáveis de texto como factor, números como numeric, etc.). Neste caso, além destas checagens também vamos criar uma nova variável que é o log do PIB per capita.\n\n# Checagens iniciais #\n# Verifica se há valores ausentes\ngapminder %&gt;%\n  summarise(across(everything(), ~sum(is.na(.x))))\n\n# A tibble: 1 × 6\n  country continent  year lifeExp   pop gdpPercap\n    &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1       0         0     0       0     0         0\n\n# Informações gerais sobre os dados\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n# Nome das variáveis minúsculas\nnames(gapminder) &lt;- tolower(names(gapminder))\n# Tranformações #\n# Computa o log do pib per capita\ngapminder &lt;- mutate(gapminder, lgdppc = log10(gdppercap))\n\n\n\nTema dos gráficos\nPara manter o padrão das visualizações pode-se criar um tema personalizado. A maneira mais simples de fazer isto é a partir de um tema padrão do ggplot2 mas é possível começar do zero. Aqui, por simplicidade, começo com o tema bw e apenas mudo a posição da legenda e o tamanho e a fonte do texto que será plotado nos eixos e no título do gráfico. Além disso, como o nome dos eixos vai ser repetido muitas vezes defino uma lista com o nome mais comum deles.\n\n# Tema customizado #\ntheme_vini &lt;- theme_bw() +\n  theme(\n    text = element_text(family = \"Jost\", size = 12, colour = \"gray20\"),\n    plot.title = element_text(size = 16),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n    )\n\n# Nomes que serão usados várias vezes para os eixos\nnomes &lt;- list(\n  title = \"Expectativa de vida e PIB per capita\",\n    x = \"Log do PIB per capita (US$ 2010)\",\n    y = \"Expectativa de vida ao nascer\",\n    fonte = \"Fonte: Gapmineder (www.gapminder.org) e World Bank Open Data.\"\n  )"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#visualizando",
    "href": "posts/general-posts/repost-gapminder/index.html#visualizando",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "Visualizando",
    "text": "Visualizando\n\nCorrelações\nNote que não temos dados para todos os anos do período. Os dados estão disponíveis de cinco em cinco anos começando em 1952 e terminando em 2007. Podemos começar com um gráfico de dispersão para ver a relação entre a “economia” (PIB per capita) e a “qualidade da saúde/vida” (expectativa de vida ao nascer) de um país. Apenas como exemplo incluo também uma linha de regressão quadrática no gráfico.\n\nunique(gapminder$year)\n\n [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\n\n\n\ngap2007 &lt;- filter(gapminder, year == 2007)\n\nggplot(gap2007, aes(lgdppc, lifeexp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    subtitle = \"Relação entre expectativa de vida ao nascer e o logaritmo do PIB per capita (fit linear)\",\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nPodemos nos valer de outras informações disponíveis na base para alterar atributos estéticos do gráfico. O tamanho de cada círculo pode refletir o tamanho da população daquele país; a cor do círculo, por sua vez, pode representar o continente daquele país. Por conveniência uso as cores pré-definidas do pacote gapminder. Além disso também podemos destacar o nome de alguns países usando o pacote ggrepel.\n\ndestaque &lt;- c(\n  \"Australia\", \"Argentina\", \"Brazil\", \"Chile\", \"India\", \"Nigeria\", \n  \"Sudan\", \"Taiwan\", \"Mozambique\", \"Angola\", \"Vietnam\"\n  )\n\ngap_highlight &lt;- gap2007 %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  mutate(sel = ifelse(country %in% destaque, country, \"\"))\n\nggplot(gap_highlight, aes(lgdppc, lifeexp)) +\n  geom_point(aes(size = pop, colour = continent), alpha = .75) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  geom_text_repel(\n    aes(label = sel),\n    family = \"Jost\",\n    force = 20,\n    max.overlaps = 30,\n    size = 5\n    ) + \n  labs(\n    title = paste(nomes$title, \"(2007)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  scale_color_manual(values = continent_colors, name = \"\") +\n  scale_size_continuous(range = c(1, 20)) +\n  guides(size = FALSE) + \n  theme_vini\n\n\n\n\n\n\n\n\nO gráfico acima é um retrato do momento, mas pode ser interessante entender como estas variáveis se comportaram ao longo do tempo.\n\n\nExpectativa de vida (média mundial)\n\n# Calcula a expectativa de vida média\ngap_life &lt;- gapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(media = mean(lifeexp))\n\nggplot(gap_life, aes(year, media)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = paste(nomes$title, \"(média mundial)\"),\n    caption = nomes$fonte,\n    x = nomes$x,\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\nExpectativa de vida (todos os países)\nPodemos desagregar a análise acima por país. É claro que fica difícil discernir um país específico, mas pode-se ver uma tendência geral de crescimento, ainda que haja alguns outliers. A maior parte das quedas significativas pode ser relacionada com alguma guerra. O país cuja expectativa de vida cai bruscamente no começo dos anos 90, por exemplo, é a Ruanda, que vivia uma guerra civil nesta época.\n\nggplot(gapminder, aes(year, lifeexp, group = country, colour = continent)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 30, to = 80, by = 10)) +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\nSe nos atermos somente ao nível de continente a visualização fica mais simples.\n\ngap_life_continent &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(expec_media = mean(lifeexp))\n\nggplot(gap_life_continent, aes(year, expec_media, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"Expectativa de vida média (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$y\n    ) +\n  theme_vini\n\n\n\n\n\n\n\n\n\n\nPIB per capita (continentes)\nPodemos fazer o mesmo para o log do PIB per capita. Aqui o gráfico é feito usando o log do PIB per capita, mas no eixo indico o valor equivalente em dólares para facilitar a interpretação.\n\n# Calcula o PIB per capita médio por continente a cada ano\ngap_gdp &lt;- gapminder %&gt;%\n  group_by(year, continent) %&gt;%\n  summarise(avg_gdp = mean(gdppercap))\n\nggplot(gap_gdp, aes(year, avg_gdp, colour = continent)) +\n  geom_line() +\n  geom_point() +\n  # Escala log\n  scale_y_log10() +\n  scale_color_brewer(palette = 6, type = \"qual\", name = \"\") +\n  labs(\n    title = \"PIB per capita médio (1952/2007)\",\n    caption = nomes$fonte,\n    x = \"\",\n    y = nomes$x\n    ) +\n  theme_vini"
  },
  {
    "objectID": "posts/general-posts/repost-gapminder/index.html#analisando-os-dados",
    "href": "posts/general-posts/repost-gapminder/index.html#analisando-os-dados",
    "title": "Repost: Expectativa de vida e Crescimento Econômico",
    "section": "Analisando os dados",
    "text": "Analisando os dados\nAntes de começar a análise tabular dos dados vale definir uma função simples para apresentar as tabelas.\n\nprint_table &lt;- function(df, ...) {\n  \n  str_title &lt;- function(string) {\n    \n    x &lt;- stringr::str_replace_all(string, \"_\", \" \")\n    x &lt;- stringr::str_to_title(x)\n    return(x)\n    \n  }\n  \n  knitr::kable(df, col.names = str_title(colnames(df)), ...) %&gt;%\n    kableExtra::kable_styling(\n      full_width = FALSE,\n      bootstrap_options = c(\"hover\", \"condensed\")\n    )\n  \n}\n\nPodemos encontrar fatos interessantes simplesmente agregando e reorganizando os dados. No gráfico anterior vimos que o continente com maior PIB per capita médio é a Oceania. Curiosamente, a base inclui somente dois países na Oceania: Austrália e Nove Zelândia.\n\n# Note que na Oceania os dados só incluem Autrália e Nova Zelândia\ngapminder %&gt;%\n  filter(continent == \"Oceania\") %&gt;%\n  count(country) %&gt;%\n  print_table()\n\n\n\n\nCountry\nN\n\n\n\n\nAustralia\n12\n\n\nNew Zealand\n12\n\n\n\n\n\n\n\nPodemos encontrar os países que mais cresceram (em termos absolutos e relativos) durante o período observado. Note como há vários países asiáticos listados qual seja a métrica escolhida.\n\n# Países que mais cresceram no período da amostra em termos absolutos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = last(gdppercap) - first(gdppercap)) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(caption = \"Países que mais cresceram de 1952 a 2007\")\n\n\nPaíses que mais cresceram de 1952 a 2007\n\n\nCountry\nGrowth\n\n\n\n\nSingapore\n44828.04\n\n\nNorway\n39261.77\n\n\nHong Kong, China\n36670.56\n\n\nIreland\n35465.72\n\n\nAustria\n29989.42\n\n\nUnited States\n28961.17\n\n\nIceland\n28913.10\n\n\nJapan\n28439.11\n\n\nNetherlands\n27856.36\n\n\nTaiwan\n27511.33\n\n\n\n\n\n\n\n\n# Países que mais cresceram no período da amostra em termos relativos\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = (last(gdppercap) / first(gdppercap) - 1) * 100) %&gt;%\n  slice_max(growth, n = 10) %&gt;%\n  print_table(\n    caption = \"Países que mais cresceram de 1952 a 2007 (%)\",\n    digits = 2\n    )\n\n\nPaíses que mais cresceram de 1952 a 2007 (%)\n\n\nCountry\nGrowth\n\n\n\n\nEquatorial Guinea\n3135.54\n\n\nTaiwan\n2279.41\n\n\nKorea, Rep.\n2165.51\n\n\nSingapore\n1936.30\n\n\nBotswana\n1376.65\n\n\nHong Kong, China\n1200.57\n\n\nChina\n1138.39\n\n\nOman\n1120.64\n\n\nThailand\n884.22\n\n\nJapan\n884.04\n\n\n\n\n\n\n\nA mesma análise também pode ser feita para a expectativa de vida. Adicionalmente também podemos encontrar qual foi a maior variação (negativa) entre um ponto observado e outro dentro da amostra.\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = last(lifeexp) - first(lifeexp)) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 0)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n38\n\n\nVietnam\n34\n\n\nIndonesia\n33\n\n\nSaudi Arabia\n33\n\n\nLibya\n31\n\n\nKorea, Rep.\n31\n\n\nNicaragua\n31\n\n\nWest Bank and Gaza\n30\n\n\nYemen, Rep.\n30\n\n\nGambia\n29\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(life_growth = (last(lifeexp) / first(lifeexp) - 1) * 100) %&gt;%\n  slice_max(life_growth, n = 10) %&gt;%\n  print_table(digits = 2)\n\n\n\n\nCountry\nLife Growth\n\n\n\n\nOman\n101.29\n\n\nGambia\n98.16\n\n\nYemen, Rep.\n92.63\n\n\nIndonesia\n88.56\n\n\nVietnam\n83.73\n\n\nSaudi Arabia\n82.51\n\n\nNepal\n76.41\n\n\nIndia\n73.11\n\n\nLibya\n73.10\n\n\nNicaragua\n72.28\n\n\n\n\n\n\n\n\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  mutate(\n    life_diff = lifeexp - lag(lifeexp),\n    life_abs  = abs(lifeexp - lag(lifeexp))) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(life_abs)) %&gt;%\n  select(country, year, life_diff) %&gt;%\n  slice(1:10) %&gt;%\n  print_table(digits = 1)\n\n\n\n\nCountry\nYear\nLife Diff\n\n\n\n\nRwanda\n1992\n-20.4\n\n\nCambodia\n1982\n19.7\n\n\nChina\n1967\n13.9\n\n\nZimbabwe\n1997\n-13.6\n\n\nRwanda\n1997\n12.5\n\n\nLesotho\n2002\n-11.0\n\n\nSwaziland\n2002\n-10.4\n\n\nBotswana\n1997\n-10.2\n\n\nCambodia\n1977\n-9.1\n\n\nNamibia\n2002\n-7.4"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html",
    "href": "posts/general-posts/emv-no-r/index.html",
    "title": "EMV no R",
    "section": "",
    "text": "A estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é consistente (converge para o valor verdadeiro), normalmente assintótico (distribuição assintórica segue uma normal padrão) e eficiente (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.\nA intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros \\(\\mu\\) e \\(\\sigma\\) de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido “gerada” por uma normal).\nHá vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no R. Neste post vou me ater apenas a dois pacotes: o optimx e o maxLik. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.\nVale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função optim do R. Os dois pacotes simplesmente trazem algumas comodidades.\n\nlibrary(maxLik)\nlibrary(optimx)\n# Para reproduzir os resultados\nset.seed(33)"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-optimx",
    "href": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-optimx",
    "title": "EMV no R",
    "section": "Usando o pacote optimx",
    "text": "Usando o pacote optimx\nA função optim já é bastante antiga e um novo pacote, chamado optimx, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são optimx e optimr. Mais informações sobre o pacote podem ser encontradas aqui.\nA sintaxe das funções é muito similar à sintaxe original do optim. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead\n\nsummary(fit &lt;- optimx(par = 1, fn = ll_pois, x = amostra))\n\n                p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nNelder-Mead 4.9375 2202.837     32     NA    NA        0   NA   NA 0.001\nBFGS        4.9380 2202.837     34      9    NA        0   NA   NA 0.003\n\n\nUma das principais vantagens do optimx é a possibilidade de usar vários métodos de otimização numérica numa mesma função.\n\nfit &lt;- optimx(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n  method = c(\"nlm\", \"BFGS\", \"Rcgmin\", \"nlminb\")\n  )\n\nfit\n\n             p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nnlm    4.937998 2202.837     NA     NA     8        0   NA   NA 0.001\nBFGS   4.938000 2202.837     34      9    NA        0   NA   NA 0.002\nRcgmin 4.938000 2202.837     15     10    NA        0   NA   NA 0.002\nnlminb 4.938000 2202.837     10     12     9        0   NA   NA 0.002\n\n\nComo este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos."
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-maxlik",
    "href": "posts/general-posts/emv-no-r/index.html#usando-o-pacote-maxlik",
    "title": "EMV no R",
    "section": "Usando o pacote maxLik",
    "text": "Usando o pacote maxLik\nA função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, não é preciso montar a função com sinal de menos como fizemos acima; segundo, ela já calcula erros-padrão e estatísticas-t dos coeficientes estimados. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas aqui.\nPara usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o dpois que implementa a função densidade da Poisson. O argumento log = TRUE retorna as probabilidades \\(p\\) como \\(log(p)\\).\n\nll_pois &lt;- function(x, theta) {\n    ll &lt;- dpois(x, theta, log = TRUE)\n    return(sum(ll))\n}\n\nO comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.\n\nsummary(fit &lt;- maxLik(ll_pois, start = 1, x = amostra))\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 8 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07617   64.83  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função optimx. Derivando a função de log-verossimilhança:\n\\[\n\\begin{align}\n  \\frac{\\partial \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda} & = \\frac{1}{\\lambda}\\sum_{k = 1}^{n}x_{k} - n \\\\\n  \\frac{\\partial^2 \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda^2} & = -\\frac{1}{\\lambda^2}\\sum_{k = 1}^{n}x_{k}\n\\end{align}\n\\]\nO código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.\n\ngrad_pois &lt;- function(x, theta) {\n  (1 / theta) * sum(x) - length(x)\n  }\n\nhess_pois &lt;- function(x, theta) {\n    -(1 / theta^2) * sum(x)\n}\n\nfit2 &lt;- maxLik(\n  ll_pois,\n  grad = grad_pois,\n  hess = hess_pois,\n  start = 1,\n  x = amostra\n  )\n\nsummary(fit2)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 7 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07027   70.27  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#consistência",
    "href": "posts/general-posts/emv-no-r/index.html#consistência",
    "title": "EMV no R",
    "section": "Consistência",
    "text": "Consistência\nVamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição \\(N(2, 3)\\); computamos as estimativas para \\(\\mu\\) e \\(\\sigma\\) e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.\n\nSimular uma amostra segundo uma distribuição.\nEstimata os parâmetros da distribuição.\nCalcula a variância assintótica dos estimadores.\nRepete 5000 vezes os passos 1-3.\n\nO código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.\n\nr &lt;- 5000\nn &lt;- 1000\n\nestimativas &lt;- matrix(ncol = 4, nrow = r)\n\nfor(i in 1:r) {\n    x &lt;- rnorm(n = n, mean = 2, sd = 3)\n    \n    fit &lt;- optimr(\n      par = c(1, 1),\n      fn = ll_norm,\n      method = \"BFGS\",\n      hessian = TRUE\n      )\n    # Guarda o valor estimado do parâmetro\n    estimativas[i, 1:2] &lt;- fit$par\n    estimativas[i, 3:4] &lt;- diag(n * solve(fit$hess))\n}\n\nA consistência dos estimadores \\(\\hat{\\theta}_{MV}\\) significa que eles aproximam os valores verdadeiros do parâmetros \\(\\theta_{0}\\) à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande \\(\\mathbb{N} \\to \\infty\\) então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros \\(\\hat{\\theta}_{\\text{MV}} \\to \\theta_{0}\\)\nO código abaixo calcula a média das estimativas para cada parâmetro - lembrando que \\(\\mu_{0} = 2\\) e que \\(\\sigma_{0} = 3\\). Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).\n\n# | fig-width: 10\npar(mfrow = c(1, 2))\n# Consistência dos estimadores de MV\nmu &lt;- estimativas[, 1]; sigma &lt;- estimativas[, 2]\nmean(mu)\n\n[1] 2.000883\n\nmean(sigma)\n\n[1] 2.997335\n\nhist(mu, main = bquote(\"Estimativas para \"~~mu), freq = FALSE, xlim = c(1.5, 2.5))\nabline(v = 2, col = \"indianred\")\nhist(sigma, main = bquote(\"Estimativas para \"~~sigma), freq = FALSE, xlim = c(2.7, 3.3))\nabline(v = 3, col = \"indianred\")"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#normalmente-assintótico",
    "href": "posts/general-posts/emv-no-r/index.html#normalmente-assintótico",
    "title": "EMV no R",
    "section": "Normalmente assintótico",
    "text": "Normalmente assintótico\nDizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:\n\\[\nz_{\\theta} = \\sqrt{N}\\frac{\\hat{\\theta}_{MV} - \\theta}{\\sqrt{\\text{V}_{ast}}} \\to \\mathbb{N}(0, 1)\n\\]\nonde \\(\\text{V}_{ast}\\) é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\nNo loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\n# Normalidade assintótica\n\n# Define objetos para facilitar a compreensão\nmu_hat &lt;- estimativas[, 1]\nsigma_hat &lt;- estimativas[, 2]\nvar_mu_hat &lt;- estimativas[, 3]\nvar_sg_hat &lt;- estimativas[, 4]\n\n# Centra a estimativa\nmu_centrado &lt;- mu_hat - 2 \nsigma_centrado &lt;- sigma_hat - 3\n# Computa z_mu z_sigma\nmu_normalizado &lt;- sqrt(n) * mu_centrado / sqrt(var_mu_hat)\nsigma_normalizado &lt;- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)\n\n\n# Monta o gráfico para mu\n\n# Eixo x\ngrid_x &lt;- seq(-3, 3, 0.01)\n\nhist(\n  mu_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[mu]),\n  freq = FALSE,\n  xlim = c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[mu]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n\n\n\n\n\n\n\n\n\n# Monta o gráfico para sigma2\nhist(\n  sigma_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[sigma]),\n  freq = FALSE,\n  xlim =c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[sigma]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")"
  },
  {
    "objectID": "posts/general-posts/emv-no-r/index.html#escolha-de-valores-inicias",
    "href": "posts/general-posts/emv-no-r/index.html#escolha-de-valores-inicias",
    "title": "EMV no R",
    "section": "Escolha de valores inicias",
    "text": "Escolha de valores inicias\nComo comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.\nO exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.\n\n# sensível a escolha de valores inicias\nx &lt;- rnorm(n = 1000, mean = 15, sd = 4)\nfit &lt;- optim(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n\n$par\n[1] 618.6792 962.0739\n\n$value\n[1] 7984.993\n\n$counts\nfunction gradient \n     107      100 \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]\n[1,]  0.001070703 -0.0013531007\n[2,] -0.001353101  0.0001884928\n\n\nNote que as estimativas estão muito distantes dos valores corretos \\(\\mu = 15\\) e \\(\\sigma = 4\\). Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.\nO código abaixo usa os momentos empíricos como valores inicias para \\(\\mu\\) e \\(\\sigma\\):\n\\[\n\\begin{align}\n  \\mu_{inicial} & = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\\\\n  \\sigma_{inicial} & = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} (x_{i} - \\mu_{inicial})^2}\n\\end{align}\n\\]\n\n(chute_inicial &lt;- c(mean(x), sqrt(var(x))))\n\n[1] 14.859702  3.930849\n\n(est &lt;- optimx(par = chute_inicial, fn = ll_norm))\n\n                  p1       p2    value fevals gevals niter convcode kkt1 kkt2\nNelder-Mead 14.85997 3.929097 2787.294     47     NA    NA        0 TRUE TRUE\nBFGS        14.85970 3.928884 2787.294     15      2    NA        0 TRUE TRUE\n            xtime\nNelder-Mead 0.001\nBFGS        0.001\n\n\nAgora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função optimx se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.\nNote como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método bobyqa retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos nlminb e Nelder-Mead ambos retornam os valores corretos.\n\n# Usando outros métodos numéricos\noptimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = c(\"BFGS\", \"Nelder-Mead\", \"CG\", \"nlminb\", \"bobyqa\")\n  )\n\n                   p1         p2    value fevals gevals niter convcode  kkt1\nBFGS        618.67917 962.073907 7984.993    107    100    NA        1  TRUE\nNelder-Mead  14.85571   3.929621 2787.294     83     NA    NA        0  TRUE\nCG           46.43586 628.570987 7358.601    204    101    NA        1  TRUE\nnlminb       14.85970   3.928883 2787.294     23     47    19        0  TRUE\nbobyqa       15.20011   8.993240 3211.556    109     NA    NA        0 FALSE\n             kkt2 xtime\nBFGS        FALSE 0.006\nNelder-Mead  TRUE 0.001\nCG          FALSE 0.008\nnlminb       TRUE 0.001\nbobyqa      FALSE 0.036\n\n\nVale notar também alguns detalhes técnicos da saída. Em particular, convcode == 0 significa que o otimizador conseguiu convergir com sucesso, enquanto convcode == 1 indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.\nJá o kkt1 e kkt2 verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte necessária do teorema enquanto a segunda condição verifica a parte suficiente. Note que o bobyqa falha em ambas as condições (pois ele não é feito para este tipo de problema).\nOs métodos que retornam os melhores valores, o Nelder-Mead e nlminb são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.\nMais detalhes sobre os métodos podem ser encontrados na página de ajuda da função ?optimx."
  },
  {
    "objectID": "posts/shiny-apps/ifdm.html",
    "href": "posts/shiny-apps/ifdm.html",
    "title": "IDH dos municípios do Brasil",
    "section": "",
    "text": "Sobre o aplicativo\n\n\n\n\n\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. Os quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nA lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque. O dashboard também mostra a evolução dos indicadores de desenvolvimento humano ao longo do tempo. Nesta comparação fica evidente o impacto da Crise de 2014-16.\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas.\nEste aplicativo foi construído no R usando {shiny} e {shinydashboardplus}\nPara acessar o código do aplicativo confira o repositório no GitHub."
  },
  {
    "objectID": "ggplot2-tutorial.html",
    "href": "ggplot2-tutorial.html",
    "title": "ggplot2: Do básico ao intermediário",
    "section": "",
    "text": "Introdução\n\n\n\n\n\nComeçe por aqui\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nApêndice: manipular para enxergar\n\n\n\n\n\nEste post revisa as principais funções para manipulação de dados do Tidyverse. O material serve mais como referência e apoio ao tutorial de ggplot2 do que como introdução ao assunto.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: gráfico de dispersão\n\n\n\n\n\nO gráfico de dispersão mapeia pares de pontos num plano bidimensional. A principal utilidade deste tipo de gráfico é deixar evidente a correlação entre as duas variáveis escolhidas.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: gráfico de coluna\n\n\n\n\n\nUm gráfico do colunas é uma ferramenta de visualização poderosa e versátil para visualizar a diferença de valores entre classes e também a evolução de valores ao longo do tempo.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: histograma\n\n\n\n\n\nUm histograma serve para visualizar a distribuição de um conjunto de dados. É uma visualização estatística poderosa para entender o comportamento dos dados\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentos: gráfico de linha\n\n\n\n\n\nGráficos de linha são frequentemente usados para representar séries de tempo, isto é, valores que mudam ao longo do tempo. Estes gráficos revelam a evolução de uma variável ao longo do tempo.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEstético: Destacando informação\n\n\n\n\n\nUm gráfico deve ser autoexplicativo. Neste post, discutiremos três estratégias simples para realçar informações em um gráfico: usar linhas com geom_vline(), geom_hline() e geom_abline() para destacar eixos ou informações numéricas; realçar seções do gráfico com geom_rect(); e destacar informações numéricas e texto usando geom_text() e annotate().\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEstético: Escalas e Cores\n\n\n\n\n\nEscalas, legendas e cores são elementos essenciais numa boa visualização. Este post apresenta a lógica das funções que controlam as escalas do gráfico e as suas cores com diversos exemplos.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "",
    "text": "A taxa de juros é talvez a variável macroeconômica mais importante para se observar quando se pensa em financiamento imobiliário. Quanto maior for a taxa de juros, mais “caro” fica o financiamento habitacional. Ou seja, mais difícil fica de comprar um imóvel.\nO financiamento imobiliário nada mais é do que um empréstimo que uma família contrai com o sistema financeiro (um banco); este empréstimo é uma dívida que a família deve repagar em parcelas mensais e sobre cada pagamento incide um valor de juros.\n\n\nA taxa de juros tem um efeito geral sobre a economia, mas o seu efeito é mais notável para o consumidor na hora de fazer compras grandes, de longo prazo: caso de um automóvel ou de um imóvel. Uma taxa menor significa que fica mais “barato” tomar crédito, enquanto uma taxa maior significa o contrário.\nQuem define a taxa de juros “geral” da economia, a taxa SELIC, é o Cômite de Política Monetária (COPOM). A SELIC é, na verdade, uma meta de taxa de juros, que o Banco Central do Brasil (BCB) deve perseguir. O COPOM se reúne periodicamente para definir a taxa SELIC; na ocasião mais recente, no início de agosto, decidiu-se reduzir a taxa de 13,75% a.a. para 13,25% a.a. - a primeira queda depois de um ciclo de alta de 3 anos.\nTipicamente, a taxa de juros é a ferramenta de política monetária que se usa para controlar a inflação: quando a taxa de inflação aumenta muito, o COPOM decide aumentar a taxa de juros. Foi isto o que aconteceu em 2014-16 e também em 2021-23.\n\n\n\n\n\n\n\n\n\nO gráfico acima mostra as oscilações recentes da SELIC. No final de 2016 encerrou-se um longo ciclo de alta que começou em reação à escalada da inflação em 2014. A taxa, então, chegou a cair até 2% a.a. no final de 2020, acompanhando a tendência internacional de taxas de juros baixíssimas, muito próximas de 0% a.a. A volta da inflação com a pandemia levou o COPOM a agressivamente aumentar a taxa SELIC nos meses seguintes. A taxa manteve-se estável em 13,75% a.a. desde agosto do ano passado.\nA taxa SELIC influencia as demais taxas de juros da economia indiretamente. Como será mostrado mais adiante, há várias taxas de financiamento imobiliário disponíveis."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#um-pouco-sobre-juros",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#um-pouco-sobre-juros",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "",
    "text": "A taxa de juros tem um efeito geral sobre a economia, mas o seu efeito é mais notável para o consumidor na hora de fazer compras grandes, de longo prazo: caso de um automóvel ou de um imóvel. Uma taxa menor significa que fica mais “barato” tomar crédito, enquanto uma taxa maior significa o contrário.\nQuem define a taxa de juros “geral” da economia, a taxa SELIC, é o Cômite de Política Monetária (COPOM). A SELIC é, na verdade, uma meta de taxa de juros, que o Banco Central do Brasil (BCB) deve perseguir. O COPOM se reúne periodicamente para definir a taxa SELIC; na ocasião mais recente, no início de agosto, decidiu-se reduzir a taxa de 13,75% a.a. para 13,25% a.a. - a primeira queda depois de um ciclo de alta de 3 anos.\nTipicamente, a taxa de juros é a ferramenta de política monetária que se usa para controlar a inflação: quando a taxa de inflação aumenta muito, o COPOM decide aumentar a taxa de juros. Foi isto o que aconteceu em 2014-16 e também em 2021-23.\n\n\n\n\n\n\n\n\n\nO gráfico acima mostra as oscilações recentes da SELIC. No final de 2016 encerrou-se um longo ciclo de alta que começou em reação à escalada da inflação em 2014. A taxa, então, chegou a cair até 2% a.a. no final de 2020, acompanhando a tendência internacional de taxas de juros baixíssimas, muito próximas de 0% a.a. A volta da inflação com a pandemia levou o COPOM a agressivamente aumentar a taxa SELIC nos meses seguintes. A taxa manteve-se estável em 13,75% a.a. desde agosto do ano passado.\nA taxa SELIC influencia as demais taxas de juros da economia indiretamente. Como será mostrado mais adiante, há várias taxas de financiamento imobiliário disponíveis."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#exemplo-guiado",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#exemplo-guiado",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "Exemplo Guiado",
    "text": "Exemplo Guiado\n\nO empréstimo\nVamos simular o financiamento de um imóvel de R$450.000. Supondo um LTV de 80%, o valor da entrada é de R$90.000 e o valor a ser financiado, portanto, é de R$360.000. Num contrato de 30 anos, o valor da amortização é de:\n\\[\nA = \\frac{R\\$360.000}{R\\$360} = R\\$1.000\n\\]\nVamos começar assumindo que a taxa de juros seja de 10% a.a. A tabela abaixo mostra o fluxo de pagamentos do primeiro ano do empréstimo. Note como o valor da amortização é sempre o mesmo. À medida que a dívida vai sendo paga, o valor cobrado de juros também diminui e, por conseguinte, diminui também o valor da parcela mensal.\n\n\n\n\n\n\n  Fluxo de pagamento do financiamento de um imóvel de 450.000 (SAC).\n  \n    \n    \n      Período (mês)\n      Amortização\n      Juros\n      Parcela\n      Dívida\n    \n  \n  \n    1\nR$1.000\nR$2.870,69\nR$3.870,69\nR$360.000\n    2\nR$1.000\nR$2.862,72\nR$3.862,72\nR$359.000\n    3\nR$1.000\nR$2.854,74\nR$3.854,74\nR$358.000\n    4\nR$1.000\nR$2.846,77\nR$3.846,77\nR$357.000\n    5\nR$1.000\nR$2.838,79\nR$3.838,79\nR$356.000\n    6\nR$1.000\nR$2.830,82\nR$3.830,82\nR$355.000\n    7\nR$1.000\nR$2.822,85\nR$3.822,85\nR$354.000\n    8\nR$1.000\nR$2.814,87\nR$3.814,87\nR$353.000\n    9\nR$1.000\nR$2.806,90\nR$3.806,90\nR$352.000\n    10\nR$1.000\nR$2.798,92\nR$3.798,92\nR$351.000\n    11\nR$1.000\nR$2.790,95\nR$3.790,95\nR$350.000\n    12\nR$1.000\nR$2.782,98\nR$3.782,98\nR$349.000\n  \n  \n  \n\n\n\n\nAo longo dos 360 meses do financiamento, o valor dos juros e da parcela vão diminuindo até que a dívida tenha sido totalmente paga. Note como no início do financiamento, a parcela mensal está na faixa de R$3800, mas já no final está próxima de R$1000.\n\n\n\n\n\n\n\n\n\n\n\nRenda necessária\nAgora podemos responder uma dúvida importante: qual a renda necessária para financiar este imóvel? Cada banco ou instituição financeira usa regras próprias para decidir se libera ou não o valor do financiamento imobiliário. Uma regra de bolso comum é de que o valor da parcela inicial não pode ser maior do que 30% da renda do requerente.\nNo exemplo acima, o valor da primeira parcela é de R$3870. A renda mínima necessária (RMN) para estar elegível a este financiamento é:\n\\[\nRMN = \\frac{R\\$ 3.870,69}{0,3} = R\\$ 12.902,3\n\\]\nComo que este resultado final depende da taxa de juros? Podemos simular o mesmo financiamento para diferentes taxas juros e calcular novamente a renda mínima necessária. A tabela abaixo mostra como a renda varia para valores de taxa de juros de 7% a 12%. É notável como a taxa de juros tem impacto direto no poder de compra e capacidade de pagamento das famílias. A uma taxa favorável de 7%, é necessário ter uma renda de R$10 mil para ser aprovado no financimento; já a uma taxa de 12% é necessário ter quase R$15 mil.\n\n\n\n\n\n\n  Renda mínima necessária para financiar um imóvel de R$450.000 a diferentes taxas de juros.\n  \n    \n    \n      Juros (% a.a)\n      Renda Mínima\n    \n  \n  \n    7,00%\nR$10.118,31\n    7,50%\nR$10.587,24\n    8,00%\nR$11.054,17\n    8,50%\nR$11.519,13\n    9,00%\nR$11.982,12\n    9,50%\nR$12.443,17\n    10,00%\nR$12.902,30\n    10,50%\nR$13.359,52\n    11,00%\nR$13.814,85\n    11,50%\nR$14.268,30\n    12,00%\nR$14.719,88"
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#o-impacto-do-aumento-dos-juros",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#o-impacto-do-aumento-dos-juros",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "O impacto do aumento dos juros",
    "text": "O impacto do aumento dos juros\nNos últimos anos vimos uma alta significativa das taxas de juros. A taxa média de financiamento habitacional era próxima de 7% em 2020 e subiu para 11,6%. Considerando o imóvel do exemplo acima, seria necessário um aumento de mais de R$4000 na renda para conseguir comprar o mesmo imóvel - sem levar em conta o aumento de preço do imóvel.\nVamos retomar o exemplo do imóvel de R$450.000 acima. Mantendo este preço constante, podemos calcular qual a renda necessária para financiar este mesmo imóvel à medida que a taxa de juros foi aumentando. Por simplicidade, uso a taxa de juros média mensal em cada período3.\nO gráfico abaixo apresenta, a cada mês, a renda necessária para financiar um imóvel de R$450.0004. No ponto mais baixo da taxa, seria necessário R$8.550 (a uma taxa de 6,63%) para ser aprovado num financiamento; já no ponto mais recente, seria necessário R$12.600 (a uma taxa de 11,6%).\n\n\n\n\n\n\n\n\n\nA análise acima olha somente o impacto do aumento dos juros e não leva em consideração o aumento médio do preços dos imóveis durante este período. Segundo o IGMI-R (Abrainc/FGV), de janeiro de 2018 a abril 2023, houve um aumento médio de 58% no preços dos imóveis. A tabela abaixo mostra a variação acumulada do IGMI-R em cada ano. Nota-se como os preços aumentam significativamente a partir de 2020.\n\n\n\n\n\n\n  Variação acumulada do IGMI-R por ano.\n  \n    \n    \n      Ano\n      Var. Acum. (%)\n    \n  \n  \n    2018\n0.64%\n    2019\n4.11%\n    2020\n10.28%\n    2021\n16.25%\n    2022\n15.06%\n    2023\n2.52%\n  \n  \n  \n\n\n\n\nO gráfico abaixo refaz o experimento acima, mas leva em conta também o aumento médio do preços dos imóveis. Em cada mês vê-se a renda necessária para financiar um imóvel médio, que em janeiro de 2018 valia R\\$450.000.\nFica evidente como a combinação simultânea de aumento de juros e de preços tornou os imóveis menos acessíveis. Em abril de 2023, seria necessário uma renda em torno de R\\$20.000 para financiar o mesmo imóvel5.\n\n\n\n\n\n\n\n\n\nA análise omite ainda um fator: o crescimento médio da renda ao longo do tempo. No Brasil, o salário mínimo é indexado à variação da inflação e, de maneira geral, quando a economia vai bem a renda média costuma crescer. Deixo esta última etapa da análise de acessibilidade financeira para outro post.\nAlém disso, o programa habitacional do Brasil, o Minha Casa Minha Vida (MCMV) oferece empréstimos com taxas mais atrativas do que as taxas médias de mercado, conforme a renda da família e o preço do imóvel6. Como o programa foi revisto recentemente, vou dedicar um post somente ao MCMV e como ele deve impactar a acessibilidade à moradia no Brasil."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#o-caminho-futuro",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#o-caminho-futuro",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "O caminho futuro",
    "text": "O caminho futuro\nNa última semana o COPOM decidiu reduzir a taxa SELIC em 0.5 p.p., diminuindo a taxa de 13,75% para 13,25%. Esta foi a primeira queda desde que se iniciou o ciclo de altas no início de 2021. Espera-se que o Banco Central agora entre num ciclo de queda de taxa de juros que devem se estabilizar em torno de 8,5% no longo prazo.\nComo se viu na análise acima, as oscilações de SELIC eventualmente traduzem-se em mudanças nas taxas do financiamento imobiliário. Além da queda na taxa de juros, os índices de preços imobiliários, como o IGMI-R e o IVGR, começam a apontar para uma relativa estabilidade nos preços dos imóveis. Esta combinação deve aumentar o poder de compras das famílias e melhorar a acessibilidade financeira à moradia.\nHá um último componente, da equação da acessibilidade à moradia, que ficou inexplorado neste post: a renda das famílias. Evidentemente, um aumento da renda média das famílias permite que elas tenham acesso a imóveis melhores e mais caros. Além disso, para imóveis com ticket menores, o MCMV oferece condições mais favoráveis de financiamento. Deixo esta discussão, contudo, para um outro momento."
  },
  {
    "objectID": "posts/general-posts/2023-08-juros-affordability/index.html#footnotes",
    "href": "posts/general-posts/2023-08-juros-affordability/index.html#footnotes",
    "title": "O impacto dos juros na demanda imobiliário",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEm abril de 2023, cerca de 95% do estoque de contratos de financiamentos imobiliários para pessoas físicas era indexado pela TR. O IPCA era utilizado em cerca de 2% e outros indexadores eram utilizados em 2,2% dos contratos. Apenas 0,8% dos contratos eram pré-fixados.↩︎\nAlguns exemplos incluem: cadastro positivo, regulamentação das fintechs de crédito, regulamentação das Letras Imobiliárias Garantidas (LIGs), portabilidade de crédito. Para mais informações veja a Agenda BC#.↩︎\nA taxa de juros média do financiamento habitacional para pessoas físicas toma as cinco taxas apresentadas e pondera elas pelo volume de crédito. Assim, as principais linhas (FGTS e SFH) têm maior peso.↩︎\nPor simplicidade, suponho um financiamento estilo SAC com LTV de 70% e prazo de 360 meses. Para ser aprovado no financiamento, suponho que o valor da primeira parcela não possa ser maior do que 30% da renda familiar bruta.↩︎\nVale notar que o IGMI-R é um índice de preços hedônico então ele provê um índice de preços “ajustado pela qualidade”, isto é, um quality adjusted price index. Assim, este aumento de preços não reflete meramente uma mudança no mix de imóveis disponíveis no mercado.↩︎\nO preço de R\\$450.000 não foi escolhido ao acaso já que ele supera o teto atual do Minha Casa Minha Vida e não estaria elegível ao programa.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html",
    "title": "Fundamentos: gráfico de linha",
    "section": "",
    "text": "Gráficos de linha são frequentemente usados para representar séries de tempo, isto é, valores que mudam ao longo do tempo. Estes gráficos revelam a evolução de uma variável ao longo do tempo. O ggplot oferece alguma variedade de funções para este fim, mas a mais comum é a geom_line(). Este geom exige argumentos tanto para o eixo-x como para o eixo-y. Em geral, o eixo-x representa o tempo e o eixo-y o valor da variável de interesse.\n\n\n\n\n\n\n\n\n\nNeste post vamos aprender a montar gráficos de linha usando o ggplot2 no R. Primeiro vamos fazer um exemplo simples para enxergar a dinâmica da taxa de poupança nos EUA. Depois, vamos entender como customizar o gráfico.\nAlém disso, vamos importar séries do Banco Central para fazer alguns exemplos aplicados usando o pacote GetBCBData."
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#ggplot2",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#ggplot2",
    "title": "Fundamentos: gráfico de linha",
    "section": "ggplot2",
    "text": "ggplot2\nO pacote ggplot2 segue uma sintaxe bastante consistente, que permite “somar” elementos visuais sobre um mesmo gráfico. Isto permite que se crie uma infinidade de gráficos complexos a partir de elementos simples. Os elementos visuais são todos chamados por funções geom_*. Neste primeiro exemplo vamos focar na função geom_line() que desenha linhas. Estes elementos são todos somados de maneira intuitiva usando o sinal de soma +.\nEssencialmente, temos os seguintes elementos principais:\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nEstes elementos são combinados numa sintaxe recorrente. A função ggplot tipicamente tem apenas dois elementos: data e aes. O argumento data indica o nome da base de dados. Já a função aes é a que indica como transformar os dados (as colunas da base de dados) em elementos visuais. Nos casos mais simples, esta função serve para indicar qual é o nome da variável x e qual é o nome da variável y.\nAdicionamos uma função geom nesta chamada inicial como no exemplo abaixo.\n\nggplot(data = dados, aes(x = varivel_x, y = variavel_y)) +\n  geom_line()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#gráfico-de-linha-1",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#gráfico-de-linha-1",
    "title": "Fundamentos: gráfico de linha",
    "section": "Gráfico de linha",
    "text": "Gráfico de linha\nPara construir um gráfico de linha precisamos, em geral, de apenas dois arguementos: um argumento x que é o nome da variável no eixo-x (comumemente, o tempo) e um argumento y que é o nome da variável no eixo-y (comumemente, a variável numérica).\nVamos utilizar a base economics que é carregada conjuntamente com o pacote ggplot2. Esta base traz a evolução de algumas variáveis econômicas ao longo do tempo.\n\n# Carrega a base de dados (caso ainda não tenha feito)\ndata(\"economics\")\n# Visualiza as primeiras linhas da base de dados\nhead(economics)\n\n# A tibble: 6 × 6\n  date         pce    pop psavert uempmed unemploy\n  &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 1967-07-01  507. 198712    12.6     4.5     2944\n2 1967-08-01  510. 198911    12.6     4.7     2945\n3 1967-09-01  516. 199113    11.9     4.6     2958\n4 1967-10-01  512. 199311    12.9     4.9     3143\n5 1967-11-01  517. 199498    12.8     4.7     3066\n6 1967-12-01  525. 199657    11.8     4.8     3018\n\n\nNote que a primeira coluna à esquerda (date) traz as datas. A coluna psavert é a taxa de poupança individual, mensurada em proporção à renda disponível. Podemos visualizar como se altera a taxa de poupança ao longo do tempo.\n\nggplot(data = economics, aes(x = date, y = psavert)) +\n  geom_line()\n\n\n\n\n\n\n\n\nVamos quebrar o código acima em detalhes. Primeiro, usamos a função ggplot() para declarar que queremos fazer um gráfico. Colocamos os argumentos data e aes dentro desta função. O argumento data deve ser o nome da nossa base de dados: neste caso, data = economics.\nO argumento aes é o que transforma as variáveis (as colunas da base de dados) em elementos visuais. Neste caso ele vai transformar o tempo e a taxa de poupança em algum elemento visual. Escolhemos aes(x = date, y = psavert).\nEspecificamos qual deve ser o elemento visual somando a função geom_line() no código inicial. Esta função indica que queremos um gráfico de linhas.\nUnindo todas estes elementos temos um código enxuto que plota o gráfico. Vemos que a taxa de poupança nos EUA apresenta uma tendência de queda a partir da segunda metade dos anos 1970 que é revertida somente após a primeira metade dos anos 2000.\n\nggplot(\n  # Especifica o nome da base de dados\n  data = economics,\n  # Indica como devem ser mapeadas as variáveis nos eixos x e y\n  aes(x = date, y = psavert)) +\n  # Especifica que queremos um gráfico de linha\n  geom_line()\n\n\n\n\n\n\n\n\n\nOpções estéticas\nPodemos customizar um gráfico de ggplot modificando os seus elementos estéticos. Um elemento estético pode assumir dois tipos de valor: constante ou variável. Um valor constante é um número ou texto, enquanto uma variável é uma coluna da nossa base de dados.\nHá quatro opções estéticas básicas para gráficos de linha: color, alpha, linewidth e linetype.\n\ncolor - Define a cor da linha\nalpha - Define o nível de transparência da linha\nlinewidth - Define a espessura da linha\nlinetype - Define o tracejado da linha\n\n\n\nCores\nUtilizamos o argumento color dentro da função geom_line para variar a cor da linha no gráfico. Pode-se escolher a cor da linha tanto por nome como por código hexadecimal. Por padrão, a função geom_line utiliza color = \"black\". No exemplo abaixo utilizo um tom de azul chamado \"steelblue\". Uma lista completa de cores (por nome) está disponível aqui.\n\n# Gráfico de linha\nggplot(data = economics, aes(x = date, y = psavert)) + \n  # Altera a cor da linha\n  geom_line(color = \"steelblue\")\n\n\n\n\n\n\n\n\nTambém é possível chamar as cores via código hexadecimal como no exemplo abaixo.\n\n# Gráfico de linha\nggplot(data = economics, aes(x = date, y = psavert)) +\n  # Altera a cor da linha\n  geom_line(color = \"#e76f51\")\n\n\n\n\n\n\n\n\n\n\nAlpha\nO argumento alpha controla o nível de transparência da cor e varia de 0 a 1, em que alpha = 0 é perfeitamente transparente e alpha = 1 é nada transparente. Por padrão, a função geom_line define que alpha = 1. Na prática, são raros os casos em que vale alterar este argumento. No exemplo abaixo definimos alpha = 0.5.\n\nggplot(data = economics, aes(x = date, y = psavert)) +\n  geom_line(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nEspessura da linha\nO argumento linewidth controla a espessura da linha. No exemplo abaixo defino linewidth = 1 dentro da função geom_line(). Na prática, não recomendo escolher um valor de linewidth acima de 1 pois a linha se torna muito espessa.\n\nggplot(data = economics, aes(x = date, y = psavert)) + \n  geom_line(linewidth = 1)\n\n\n\n\n\n\n\n\nO gráfico abaixo compara alguns tamanhos diferentes de espessura de linha. Note que é possível especificar valores não-inteiros como 0,5. Para evitar de repetir o mesmo código várias vezes, guardo a chamada inicial do ggplot em um objeto chamado p (de plot).\n\np &lt;- ggplot(data = economics, aes(x = date, y = psavert))\n\n# Espessura = 0.5\np + geom_line(linewidth = 0.5)\n# Espessura = 1\np + geom_line(linewidth = 1)\n# Espessura = 2\np + geom_line(linewith = 2)\n# Espessura = 5\np + geom_line(linewidth = 5)\n\n\n\n\n\n\n\n\n\n\n\n\nTracejado da linha\nO argumento linetype controla o tracejado da linha. Por padrão, a função geom_line() utiliza linetype = 1, mas podemos escolher valores distintos. No exemplo abaixo usamos linetype = 2.\n\nggplot(data = economics, aes(x = date, y = psavert)) + \n  geom_line(linetype = 2)\n\n\n\n\n\n\n\n\nAbaixo pode-se ver a diferença entre cada uma delas."
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#combinando-elementos",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#combinando-elementos",
    "title": "Fundamentos: gráfico de linha",
    "section": "Combinando elementos",
    "text": "Combinando elementos\nComo foi aludido no início do post, parte da mágica do ggplot é de poder somar elementos ao mesmo gráfico. Isto permite uma grande flexibilidade na hora de montar nossas visualizações. Uma combinação bastante efetiva é juntar um gráfico de linha com um gráfico de pontos.\nPrimeiro, vamos importar a série do consumo de energia elétrica residencial no Brasil. Podemos importar séries de tempo do site do Banco Central do Brasil usando a função gbcbd_get_series do pacote GetBCBData.\nPara importar uma série de tempo precisamos informar apenas: (1) o código da série pelo argumento id; e (2) a data de início da extração pelo argumento first.date (no formato YYYY-MM-DD).\n\n# Import a série de consumo de energia elétrica residencial\nconsumo &lt;- gbcbd_get_series(id = 1403, first.date = as.Date(\"2014-01-01\"))\n\nO gráfico abaixo combina um geom_line() com um geom_point() para representar tanto os pontos de observação como a linha que une os pontos.\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha da série\n  geom_line() +\n  # Pontos sinalizando as observações\n  geom_point()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#começando-o-gráfico-do-zero",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#começando-o-gráfico-do-zero",
    "title": "Fundamentos: gráfico de linha",
    "section": "Começando o gráfico do zero",
    "text": "Começando o gráfico do zero\nOs gráficos de linha gerados pelo ggplot2 não começam do zero no eixo-y. Isto pode ser um problema e, visualmente, criar a ilusão de que há mais volatilidade nos dados do que realmente existe.\nHá muitas formas de forçar o gráfico a iniciar no zero. Uma das mais efetivas é incluir uma linha horizontal no eixo (\\(y = 0\\)). Pode-se desenhar esta linha horizontal com a função geom_hline() (onde o “hline” sinaliza “horizontal line”, linha horizontal).\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha horizontal y = 0\n  geom_hline(yintercept = 0) +\n  # Linha da série\n  geom_line()\n\n\n\n\n\n\n\n\nOutra forma de chegar no mesmo resultado é manipular o eixo-y para forçá-lo a iniciar no zero. No exemplo abaixo utilizamos a função scale_y_continuous(). Esta função controla todos os aspectos visuais da escala no eixo-y. O sufixo _continuous indica que a variável no eixo-y é contínua.\nPara redefinir os limites do eixo-y variamos o argumento limits que aceita um par de valores. O primeiro valor é o limite inferior e o segundo valor é o limite superior. Por padrão ambos são definidos como NA.\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha da série\n  geom_line() +\n  # Manipular o eixo-y para inicar no zero\n  scale_y_continuous(limits = c(0, NA))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#séries-de-tempo-em-degraus",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#séries-de-tempo-em-degraus",
    "title": "Fundamentos: gráfico de linha",
    "section": "Séries de tempo em degraus",
    "text": "Séries de tempo em degraus\nAlgumas séries de tempo variam de maneira “descontínua” no tempo. É o caso, por exemplo, da taxa SELIC, que é periodicamente definida como uma meta a ser perseguida pelo Banco Central. Neste caso, faz sentido ressaltar isto fazendo um gráfico que varia em “degraus”.\nA função geom_step() faz este tipo de gráfico e possui os mesmos argumentos que a função geom_line(). O gráfico abaixo mostra a variação da taxa SELIC (meta) nos últimos anos.\n\n# Importa a série diária da SELIC (meta) anualizada\nselic &lt;- gbcbd_get_series(id = 1178, first.date = as.Date(\"2016-01-01\"))\n\nggplot(data = selic, aes(x = ref.date, y = value)) +\n  geom_step()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#usando-cores-para-representar-variáveis",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#usando-cores-para-representar-variáveis",
    "title": "Fundamentos: gráfico de linha",
    "section": "Usando cores para representar variáveis",
    "text": "Usando cores para representar variáveis\nComo de costume, as características estéticas do gráfico podem refletir grupos de variáveis. No caso de gráficos de linha é comum querer representar séries de tempo distintas com cores diferentes.\nVamos importar três séries de tempo: o consumo de energia elétrica comercial (1402), residencial (1403) e industrial (1404). Todas as séries são mensais e são importadas a partir de janeiro de 2014.\n\n# Importa as séries mensais do consumo de energia elétrica\nseries &lt;- gbcbd_get_series(\n  id = c(1402, 1403, 1404),\n  first.date = as.Date(\"2014-01-01\")\n  )\n\nOs dados são automaticamente formatados no padrão longitudinal, em que cada linha representa uma observação no tempo de uma série em particular. Este é o formato ideal para trabalhar com visualizações em ggplot, onde os dados estão “empilhados”.\nA coluna ref.date indica a data, a coluna series.name identifica o id da série e a coluna value representa o valor desta observação.\n\n\n\n\n\nref.date\nvalue\nid.num\nseries.name\n\n\n\n\n2014-01-01\n7745\n1402\nid = 1402\n\n\n2014-01-01\n11798\n1403\nid = 1403\n\n\n2014-01-01\n14537\n1404\nid = 1404\n\n\n2014-02-01\n8204\n1402\nid = 1402\n\n\n2014-02-01\n11879\n1403\nid = 1403\n\n\n2014-02-01\n15107\n1404\nid = 1404\n\n\n\n\n\n\n\nVamos mapear a coluna series.name nas cores das linhas. Para especificar isto utilizamos a função aes, que serve para transformar dados em elementos visuais. Da mesma forma como mapeamos a coluna ref.date e value numa linha, agora mapeamos series.name nas cores do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Indica que a variável series.name deve ser mapeada nas cores das linhas\n    aes(color = series.name))\n\n\n\n\n\n\n\n\nAs escolhas de cores padrão nem sempre são satisfatórias. Para modificar as cores utilizamos a função scale_color_manual() que também nos dá controle sobre a legenda do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_manual(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Cores das linhas\n    values = c(\"#264653\", \"#e9c46a\", \"#e76f51\"),\n  )\n\n\n\n\n\n\n\n\nVale notar que existem vários pacotes e funções com cores pré-definidas que simplificam o processo manual de escolher as cores. O exemplo mais simples é o scale_color_brewer() que utiliza as paletas de cores do Color Brewer.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_brewer(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Tipo (\"qual\" - qualitativo, \"div\" - divergente ou \"seq\" - sequencial)\n    type = \"qual\",\n    # Escolha da paleta\n    palette = 6\n  )\n\n\n\n\n\n\n\n\nPor padrão, a legenda das cores é colocada no lado direito do gráfico. Para modificar a sua posição usamos a função theme().\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  scale_color_brewer(\n    name = \"Consumo de energia\",\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    type = \"qual\",\n    palette = 6) +\n  theme(\n    # Define a posição da legenda (top, bottom, right ou left)\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nDa mesma forma que modificamos a cor das linhas, podemos variar outros aspectos estéticos como, por exemplo, o tracejado da linha de cada série. Este tipo de modificação pode ser útil no caso de uma publicação que vai ser visualizada ou impressa em escala de cinza.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Cada série é desenhada com um tracejado diferente\n    aes(linetype = series.name)\n    ) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )\n\n\n\n\n\n\n\n\nPor fim, podemos modificar elementos estéticos de dois geoms no mesmo gráfico. No exemplo abaixo tanto o tracejado da linha como o formato do ponto variam segundo a série.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(linetype = series.name)) +\n  geom_point(aes(shape = series.name)) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  ) +\n  scale_shape_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#renomeando-os-eixos-do-gráfico",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#renomeando-os-eixos-do-gráfico",
    "title": "Fundamentos: gráfico de linha",
    "section": "Renomeando os eixos do gráfico",
    "text": "Renomeando os eixos do gráfico\nÉ muito importante que um gráfico seja o mais auto-explicativo possível. Para isso precisamos inserir informações relevantes como título, subtítulo e fonte.\nA função labs() permite facilmente renomear os eixos do gráfico. Os argumentos principais são os abaixo.\n\ntitle - título do gráfico\nsubtitle - subtítulo do gráfico\nx - título do eixo-x (horizontal)\ny - título do eixo-y (vertical)\ncaption - legenda abaixo do gráfico (em geral, a fonte)\n\nNo caso de gráficos de linha é muito comum omitir o nome do eixo-x, pois ele geralmente representa o tempo. Para omitir o nome de qualquer eixo basta defini-lo como NULL.\nNovamente, utilizamos o sinal de soma para adicionar estes elementos ao gráfico.\n\nggplot(data = consumo, aes(x = ref.date, y = value)) +\n  # Linha da série\n  geom_line() +\n  # Define os elementos textuais do gráfico\n  labs(\n    title = \"Aumento gradual na demanda por energia elétrica\",\n    subtitle = \"Consumo residencial de energia elétrica no Brasil.\",\n    # Omite o título do eixo-x\n    x = NULL,\n    y = \"GWh\",\n    caption = \"Fonte: Eletrobras.\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#resumo",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#resumo",
    "title": "Fundamentos: gráfico de linha",
    "section": "Resumo",
    "text": "Resumo\nNeste post aprendemos o básico da estrutura sintática do ggplot e conseguimos montar gráficos de linha sofisticados usando poucas linhas de código. Em qualquer gráfico temos três elementos básicos\n\nDados - nossa tabela de dados.\nFunção aes() - que transforma os dados em objetos visuais.\nObjeto geométrico (geom) - que escolhe qual o formato destes objetos visuais.\n\nAlguns pontos de destaque:\n\nUtilize cores diferentes para séries distintas usando aes(color = ...).\nCombine elementos como linhas e pontos somando os “geoms”.\nComece o gráfico no zero utilizando ou geom_hline(yintercept = 0) ou scale_y_continuous(limits=c(0,NA)).\n\nSeguindo esta lógica e somando os objetos podemos criar belos gráficos.\n\n# Importa as séries mensais do consumo de energia elétrica\nseries &lt;- gbcbd_get_series(\n  id = c(1402, 1403, 1404),\n  first.date = as.Date(\"2019-01-01\")\n  )\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_hline(yintercept = 0) +\n  geom_line(\n    aes(color = series.name),\n    linewidth = 1) +\n  geom_point(\n    aes(color = series.name),\n    size = 2) +\n  scale_color_manual(\n    name = \"Consumo de Energia\",\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    values = c(\"#264653\", \"#e9c46a\", \"#e76f51\")) +\n  labs(\n    title = \"Recuperação do consumo de energia pós-pandemia\",\n    subtitle = \"Consumo mensal de energia elétrica segundo a finalidade.\",\n    x = NULL,\n    y = \"(GWh)\",\n    caption = \"Fonte: Eletrobras.\") +\n  theme(\n    legend.position = \"bottom\"\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/4-grafico-de-linha.html#mapeando-elementos-estéticos",
    "href": "posts/ggplot2-tutorial/4-grafico-de-linha.html#mapeando-elementos-estéticos",
    "title": "Fundamentos: gráfico de linha",
    "section": "Mapeando elementos estéticos",
    "text": "Mapeando elementos estéticos\n\nCores\nComo de costume, as características estéticas do gráfico podem refletir grupos de variáveis. No caso de gráficos de linha é comum querer representar séries de tempo distintas com cores diferentes.\nVamos importar três séries de tempo: o consumo de energia elétrica comercial (1402), residencial (1403) e industrial (1404). Todas as séries são mensais e são importadas a partir de janeiro de 2014.\n\n# Importa as séries mensais do consumo de energia elétrica\nseries &lt;- gbcbd_get_series(\n  id = c(1402, 1403, 1404),\n  first.date = as.Date(\"2014-01-01\")\n  )\n\nOs dados são automaticamente formatados no padrão longitudinal, em que cada linha representa uma observação no tempo de uma série em particular. Este é o formato ideal para trabalhar com visualizações em ggplot, onde os dados estão “empilhados”.\nA coluna ref.date indica a data, a coluna series.name identifica o id da série e a coluna value representa o valor desta observação.\n\n\n\n\n\nref.date\nvalue\nid.num\nseries.name\n\n\n\n\n2014-01-01\n7745\n1402\nid = 1402\n\n\n2014-01-01\n11798\n1403\nid = 1403\n\n\n2014-01-01\n14537\n1404\nid = 1404\n\n\n2014-02-01\n8204\n1402\nid = 1402\n\n\n2014-02-01\n11879\n1403\nid = 1403\n\n\n2014-02-01\n15107\n1404\nid = 1404\n\n\n\n\n\n\n\nVamos mapear a coluna series.name nas cores das linhas. Para especificar isto utilizamos a função aes, que serve para transformar dados em elementos visuais. Da mesma forma como mapeamos a coluna ref.date e value numa linha, agora mapeamos series.name nas cores do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Indica que a variável series.name deve ser mapeada nas cores das linhas\n    aes(color = series.name))\n\n\n\n\n\n\n\n\nAs escolhas de cores padrão nem sempre são satisfatórias. Para modificar as cores utilizamos a função scale_color_manual() que também nos dá controle sobre a legenda do gráfico.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_manual(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Cores das linhas\n    values = c(\"#264653\", \"#e9c46a\", \"#e76f51\"),\n  )\n\n\n\n\n\n\n\n\nVale notar que existem vários pacotes e funções com cores pré-definidas que simplificam o processo manual de escolher as cores. O exemplo mais simples é o scale_color_brewer() que utiliza as paletas de cores do Color Brewer.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  # Controla as cores e a legenda\n  scale_color_brewer(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    # Tipo (\"qual\" - qualitativo, \"div\" - divergente ou \"seq\" - sequencial)\n    type = \"qual\",\n    # Escolha da paleta\n    palette = 6\n  )\n\n\n\n\n\n\n\n\nPor padrão, a legenda das cores é colocada no lado direito do gráfico. Para modificar a sua posição usamos a função theme().\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(color = series.name)) +\n  scale_color_brewer(\n    name = \"Consumo de energia\",\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\"),\n    type = \"qual\",\n    palette = 6) +\n  theme(\n    # Define a posição da legenda (top, bottom, right ou left)\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nTipo de linha\nDa mesma forma que modificamos a cor das linhas, podemos variar outros aspectos estéticos como, por exemplo, o tracejado da linha de cada série. Este tipo de modificação pode ser útil no caso de uma publicação que vai ser visualizada ou impressa em escala de cinza.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(\n    # Cada série é desenhada com um tracejado diferente\n    aes(linetype = series.name)\n    ) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )\n\n\n\n\n\n\n\n\nPor fim, podemos modificar elementos estéticos de dois geoms no mesmo gráfico. No exemplo abaixo tanto o tracejado da linha como o formato do ponto variam segundo a série.\n\nggplot(data = series, aes(x = ref.date, y = value)) +\n  geom_line(aes(linetype = series.name)) +\n  geom_point(aes(shape = series.name)) +\n  # Controla a legenda das séries\n  scale_linetype_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  ) +\n  scale_shape_discrete(\n    # Título da legenda (opcional)\n    name = \"Consumo de energia\",\n    # Nome das séries (opcional)\n    labels = c(\"Comercial\", \"Residencial\", \"Industrial\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2022-08-recife/index.html",
    "href": "posts/general-posts/2022-08-recife/index.html",
    "title": "Weekly Viz: Recife em mapas",
    "section": "",
    "text": "Semana que vem vou estar no Recife a trabalho e para conhecer um pouco da cidade resolvi tirar um tempo para fazer alguns mapas da cidade.\n\n\n\n\n\n\n\nOs dados dos dois primeiros mapas, das principais vias e dos corpos d’água, são do OpenStreetMap. Pode-se ver, por exemplo, a Rodovia Mario Covas, que corta a cidade de norte a sul e o famoso Rio Capibaribe que atravessa a cidade.\nPara o terceiro mapa puxei os dados do Ciclomapa. Apesar do título, eu incluí tanto ciclovias, ciclofaixas e ciclorrotas.\nOs mapas da segunda linha foram todos feitos com dados do projeto de Acesso a Oportunidades do IPEA. Os dados de população e renda são do Censo de 2010 e espacialmente interpolados com os hexágonos H3, da Uber. Para facilitar a visualização usei o algoritmo de Jenks para agrupar os dados em 7 grupos. Além disso, usei uma transformação log tanto na renda como na população para reduzir um pouco da variância nos dados.\nA renda da cidade é visivelmente concentrada na região sul-sudeste, próxima do litoral e na região centro-leste, no que me parece ser a região dos bairros Madalena e Boa Vista. Já a densidade populacional não segue um padrão simples; a população está espalhada por toda a cidade.\nComo medida de acesso a empregos usei o percentual de oportunidades de emprego acessíveis a 15 minutos de carro em horário de pico. Segundo os dados do Censo, a maior parte dos deslocamentos casa-trabalho na RM de Recife são de menos de uma hora (83,4%). Assim, 15 minutos de carro (em horário de pico) me parece ser um “luxo” que as pessoas estão dispostas a pagar e que deve se refletir, em algum nível, no preço dos imóveis.\nO mapa sugere que Recife é uma cidade monocêntrica, com a maior parte dos empregos concentrada na região central. Interessante notar que, apesar da distância geográfica, a região sul, de Boa Viagem, Pina, etc. continua com indicadores de acessibilidade relativamente altos.\nPara tentar mensurar a verticalização calculei a proporção de apartamentos em relação ao total de domicílios em cada região. Novamente os dados vem do Censo e são agrupados a nível de setor censitário. Eu faço uma interpolação espacial simples com os hexágonos H3, na mesma resolução 9 do projeto do IPEA, para manter o padrão. Grosso modo, parece que as regiões de rendas mais altas coincidem com as regiões mais verticalizadas.\nPor fim, os dados de anúncios e preço provém de anúncios online de venda de imóveis ativos entre janeiro e junho de 2023. Por simplicidade, eu escolhi trabalhar apenas com anúncios de apartamentos e removi algumas observações discrepantes para conseguir um valor médio mais razoável. O preço médio observado na cidade neste período foi de R$7.900, com a maior parte das observações caindo dentro do intervalo R$3.800-R$12.700."
  },
  {
    "objectID": "posts/general-posts/2022-08-recife/index.html#detalhes-do-mapa",
    "href": "posts/general-posts/2022-08-recife/index.html#detalhes-do-mapa",
    "title": "Weekly Viz: Recife em mapas",
    "section": "",
    "text": "Os dados dos dois primeiros mapas, das principais vias e dos corpos d’água, são do OpenStreetMap. Pode-se ver, por exemplo, a Rodovia Mario Covas, que corta a cidade de norte a sul e o famoso Rio Capibaribe que atravessa a cidade.\nPara o terceiro mapa puxei os dados do Ciclomapa. Apesar do título, eu incluí tanto ciclovias, ciclofaixas e ciclorrotas.\nOs mapas da segunda linha foram todos feitos com dados do projeto de Acesso a Oportunidades do IPEA. Os dados de população e renda são do Censo de 2010 e espacialmente interpolados com os hexágonos H3, da Uber. Para facilitar a visualização usei o algoritmo de Jenks para agrupar os dados em 7 grupos. Além disso, usei uma transformação log tanto na renda como na população para reduzir um pouco da variância nos dados.\nA renda da cidade é visivelmente concentrada na região sul-sudeste, próxima do litoral e na região centro-leste, no que me parece ser a região dos bairros Madalena e Boa Vista. Já a densidade populacional não segue um padrão simples; a população está espalhada por toda a cidade.\nComo medida de acesso a empregos usei o percentual de oportunidades de emprego acessíveis a 15 minutos de carro em horário de pico. Segundo os dados do Censo, a maior parte dos deslocamentos casa-trabalho na RM de Recife são de menos de uma hora (83,4%). Assim, 15 minutos de carro (em horário de pico) me parece ser um “luxo” que as pessoas estão dispostas a pagar e que deve se refletir, em algum nível, no preço dos imóveis.\nO mapa sugere que Recife é uma cidade monocêntrica, com a maior parte dos empregos concentrada na região central. Interessante notar que, apesar da distância geográfica, a região sul, de Boa Viagem, Pina, etc. continua com indicadores de acessibilidade relativamente altos.\nPara tentar mensurar a verticalização calculei a proporção de apartamentos em relação ao total de domicílios em cada região. Novamente os dados vem do Censo e são agrupados a nível de setor censitário. Eu faço uma interpolação espacial simples com os hexágonos H3, na mesma resolução 9 do projeto do IPEA, para manter o padrão. Grosso modo, parece que as regiões de rendas mais altas coincidem com as regiões mais verticalizadas.\nPor fim, os dados de anúncios e preço provém de anúncios online de venda de imóveis ativos entre janeiro e junho de 2023. Por simplicidade, eu escolhi trabalhar apenas com anúncios de apartamentos e removi algumas observações discrepantes para conseguir um valor médio mais razoável. O preço médio observado na cidade neste período foi de R$7.900, com a maior parte das observações caindo dentro do intervalo R$3.800-R$12.700."
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html",
    "href": "posts/general-posts/2022-08-firjan-app/index.html",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório\n\n\n\n\nO app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16.\n\n\n\n\nUm caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html#introdução",
    "href": "posts/general-posts/2022-08-firjan-app/index.html#introdução",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Este ano decidi aprender Shiny para construir aplicativos de dados. Shiny é um pacote que facilita a criação de aplicativos e que permite combinar linguagens (como HTML, CSS, R). Como primeiro projeto, optei por fazer um dashboard simples, que mostrasse os dados de desenvolvimento humano dos municípios brasileiros.\nA curva de aprendizado do Shiny é dura no início; o lado bom é acaba-se forçando a aprender o básico de HTML e CSS no processo de montar o aplicativo. Como o código, em R, se relaciona com o aplicativo também muda: tudo tem que ser planejado em termos de front-end e back-end. Organizei a estrutura do aplicativo quase como se fosse um pacote de R e também usei o renv para garantir que tudo continue funcionando bem.\nNão conheço material de referência em português, mas posso recomendar o livro Mastering Shiny de Hadley Wickham.\nLinks:\n\nAplicativo\nRepositório"
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html#sobre-o-app",
    "href": "posts/general-posts/2022-08-firjan-app/index.html#sobre-o-app",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "O app foi feito em Shiny, usando o pacote shinydashboardplus, e todos os códigos estão no repositório do Github.\nEste aplicativo permite visualizar os dados do Índice Firjan de Desenvolvimento Municipal (IFDM) num dashboard. O IFDM tem metodologia similar ao popular Índice de Desenvolvimento Humano (IDH) da ONU; contudo, o IFDM abrange um número maior de variáveis. Além disso, o IFDM é calculado anualmente enqunto o IDH é calculado apenas uma vez a cada dez anos.\n\n\n\n\n\nA interpretação do IFDM é bastante simples: quanto maior, melhor. O mapa interativo permite escolher uma cidade e compará-la com a realidade do seu estado. Também é possível fazer uma comparação regional ou nacional, alterando o campo ‘Comparação Geográfica’, mas note que isto pode levar algum tempo para carregar. A lista de cidades está ordenada pelo tamanho da população, então as principais cidades tem maior destaque.\nO usuário pode escolher o tipo de mapa, paleta de cores e o número de grupos para produzir diferentes tipos de visualização. O dashboard contém um pequeno box com explicações sobre a metodologia do IFDM, a classificação dos dados, e sobre como utilizar o app.\nOs quatro gráficos que aparecem abaixo do mapa ajudam a contextualizar a cidade.\nNa primeira linha há dois gráficos estáticos: um painel de histogramas e uma espécie de gráfico de linha. Em ambos os casos, o objetivo é ajudar a contextualizar o município em relação à base de comparação. No caso acima, vê-se como São Paulo está entre os maiores IDHs do estado, mas que tem um escore relativamente ruim no critério de educação.\n\nA segunda linha mostra gráficos interativos, feitos com {plotly}. O gráfico da esquerda compara os números do município contra os valores médios do Brasil. Já o gráfico da esquerda mostra a evolução temporal de todas as variáveis do IFDM. Nesta comparação fica evidente o impacto da Crise de 2014-16."
  },
  {
    "objectID": "posts/general-posts/2022-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "href": "posts/general-posts/2022-08-firjan-app/index.html#alguns-exemplos-de-uso",
    "title": "Shiny Dashboard: IDH municípios",
    "section": "",
    "text": "Um caso interessante que surgiu nos dados foi Porto Alegre. Em geral, a capital do estado, e a sua região metropolitana concentram cidades com os maiores níveis de desenvolvimento humano do seu respectivo estado. Isto não vale para a Região Metropolitana de Porto Alegre; as cidades com os melhores indicadores de desenvolvimento humano no RS estão concentrados na “Serra Gaúcha”, no entorno de Caxias do Sul e Bento Gonçalves.\n\n\n\n\n\nOutro caso interessante é de Belo Horizonte e do estado de Minas Gerais como um todo. Em Minas Gerais há cidades com alto padrão de desenvolvimento como Uberlândia e Uberaba e cidade com baixíssimo desenvolvimento como Minas Novas e Santa Helena de Minas."
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html",
    "title": "Estético: Destacando informação",
    "section": "",
    "text": "Um gráfico deve ser o mais autosuficiente possível; a imagem deve explicar-se por si mesma. Existem algumas ferramentas adicionais que pode-se usar para atingir este objetivo. Neste post vou discutir três estratégias simples para destacar uma informação no gráfico.\nEspecificamente, vamos ver como:\n\nUsar linhas para destacar os eixos ou informações numéricas com geom_vline(), geom_hline() e geom_abline()\nDestacar partes do gráfico com geom_rect()\nDestacar informações numéricas e texto usando geom_text() e annotate()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#básico",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#básico",
    "title": "Estético: Destacando informação",
    "section": "Básico",
    "text": "Básico\nJá sabemos como desenhar linhas usando tanto o geom_line() como o geom_path(). Estas funções exigem dois a três arguemntos que especficam as coordenadas da linha e de que forma a linha deve unir estas coordenadas.\nSuponha, por exemplo, que se queira desenhar uma linha reta no eixo-x. Para isto precisamos estruturar um data.frame, informar as coordenadas e então chamar geom_line().\nNote como no código abaixo, não preciso repetir o valor y = 0 já que o tibble “recicla” este número para que ele tenha o mesmo comprimento (length) que o vetor x.\n\ndf &lt;- tibble(\n  x = c(1, 10),\n  y = 0\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line()\n\n\n\n\n\n\n\n\nO procedimento acima é nada prático, exige a criação de um data.frame apartado dos dados e as linhas ainda tem comprimento fixo. Para desenhar linhas retas arbitrárias com maior facilidade há três funções:\n\ngeom_vline - Desenha linhas verticais\ngeom_hline - Desenha linhas horizontais\ngeom_abline - Desenha linhas retas especificando o intercepto e a inclinação.\n\n\nggplot() +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\nA função geom_abline() segue a equação da reta \\(y = ax + b\\) onde \\(a\\) é a inclinação da reta (slope) e \\(b\\) é o intercepto (intercept) o ponto onde a linha cruza o eixo vertical.\n\nggplot() +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_abline(slope = 1, intercept = 0)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#desenhando-eixos-num-gráfico",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#desenhando-eixos-num-gráfico",
    "title": "Estético: Destacando informação",
    "section": "Desenhando eixos num gráfico",
    "text": "Desenhando eixos num gráfico\nO uso mais imediato destas funções é de desenhar eixos num gráfico. O gráfico de dispersão abaixo mostra a relação entre a taxa de desmprego e a taxa de poupança, usando a base economics. A inclusão do eixo-x e eixo-y ajudam a melhor contextualizar os dados.\n\nggplot(economics, aes(uempmed, psavert)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#destacando-informações-com-linhas",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#destacando-informações-com-linhas",
    "title": "Estético: Destacando informação",
    "section": "Destacando informações com linhas",
    "text": "Destacando informações com linhas\nTambém pode-se usar linhas para destacar algum tipo de informação no gráfico. Nos exemplos abaixo mostro como destacar o valor médio e mediano ou os quintis.\n\nMédia e mediana num gráfico\nNo gráfico abaixo, mostro a distribuição mensal de listings de imóveis (anúncios de imóveis) em Houston, TX. No código eu uso geom_hline() para exibir uma linha reta no eixo-x e geom_vline() para destacar os valores médio e mediano. A linha tracejada em vermelho indica o valor médio, enquanto a linha tracejada em amarelo indica o valor mediano.\n\nsubhousing &lt;- filter(txhousing, city == \"Houston\")\n\nggplot(subhousing, aes(x = listings)) +\n  geom_histogram(bins = 6, color = \"white\", fill = \"#1B6F88\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(\n    xintercept = mean(subhousing$listings, na.rm = TRUE),\n    color = \"#9e2a2b\",\n    linewidth = 1,\n    linetype = 2) +\n  geom_vline(\n    xintercept = median(subhousing$listings, na.rm = TRUE),\n    color = \"#e09f3e\",\n    linewidth = 1,\n    linetype = 2\n    )\n\n\n\n\n\n\n\n\n\n\nQuintis ou grupos\nPara desenhar múltiplas linhas basta passar o argumento como um vetor. No caso abaixo eu calculo os quintis do taxa de poupança da base economics e sobreponho estas linhas ao histograma.\n\nquintil &lt;- quantile(economics$psavert, probs = c(0.2, 0.4, 0.6, 0.8))\n\nggplot(economics, aes(x = psavert)) +\n  geom_histogram(bins = 15, color = \"white\") +\n  geom_vline(xintercept = quintil)\n\n\n\n\n\n\n\n\nPor fim, pode-se também passar as linhas como um argumento estético em color. No código abaixo eu calculo o número total de listings por ano; no gráfico, cada ano é exibido numa linha com cor diferente.\n\nlisting &lt;- subhousing %&gt;%\n  filter(year &lt; 2015) %&gt;%\n  summarise(total_listing = sum(listings, na.rm = TRUE), .by = \"year\")\n\nggplot(listing) +\n  geom_vline(aes(color = year, xintercept = total_listing)) +\n  scale_color_viridis_c()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#pib-e-ciclos-de-recessão",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#pib-e-ciclos-de-recessão",
    "title": "Estético: Destacando informação",
    "section": "PIB e ciclos de recessão",
    "text": "PIB e ciclos de recessão\nVamos aplicar esta função num exemplo mais interessante. Primeiro vamos baixar a série do PIB brasileiro1 do site do Banco Central do Brasil usando a função gbcbd_get_series(). O código abaixo, além de baixar a série, também faz um gráfico de linha simples.\n\n# Importa a série do PIB\npib &lt;- gbcbd_get_series(22109, first.date = as.Date(\"1995-01-01\"))\n\nggplot(pib, aes(ref.date, value)) +\n  geom_line()\n\n\n\n\n\n\n\n\nPara mapearmos os ciclos de recessão uso as definições da CODACE2. A tabela indica o início e o final de cada recessão, junto com uma sigla que identifica cada recessão. Eu crio esta tabela usando a função tribble(). Esta função é, essencialmente, equivalente a um data.frame() convencional, mas permite colocar os dados num formato que fica mais legível3.\n\ncodace &lt;- tribble(\n            ~rec_start,               ~rec_end, ~label,\n  #-------------------#----------------------#----------#\n  as.Date(\"1998-01-01\"), as.Date(\"1999-03-31\"), \"FHC-1\",\n  as.Date(\"2001-04-01\"), as.Date(\"2001-12-31\"), \"FHC-2\",\n  as.Date(\"2003-01-01\"), as.Date(\"2003-06-30\"), \"LULA\",\n  as.Date(\"2008-10-01\"), as.Date(\"2009-03-31\"), \"GFR\",\n  as.Date(\"2014-04-01\"), as.Date(\"2016-12-31\"), \"DILMA\",\n  as.Date(\"2020-01-01\"), as.Date(\"2020-06-30\"), \"COVID\"\n)\n\nO código abaixo gera o mesmo gráfico do PIB, mas agora temos retângulos sombreados destacando cada uma das recessões da história econômica recente do Brasil.\nComo estou mapeando dados de dois objetos distintos, eu coloco o argumento data dentro da respectiva função geom(). Isto não é obrigatório, mas acredito que o código fica melhor estruturado desta maneira.\nNote o uso do Inf para garantir que o retângulo se estenda verticalmente no gráfico de maneira indefinida. Além disso, uso o argumento alpha para deixar a região mais transparente.\n\nggplot() +\n  geom_line(\n    data = pib,\n    aes(x = ref.date, y = value)\n  ) +\n  geom_rect(\n    data = codace,\n    aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = Inf, group = label),\n    alpha = 0.4\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#usando-geom_text",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#usando-geom_text",
    "title": "Estético: Destacando informação",
    "section": "Usando geom_text",
    "text": "Usando geom_text\nQuando queremos trabalhar com texto como uma variável num data.frame é mais adequado usar a função geom_text(). Esta função tem os memos três argumentos obrigatórios da função annotate():\n\nx - Especifica a posição do texto no eixo-x\ny - Especifica a posição do texto no eixo-y\nlabel - Texto a ser plotado\n\nAlém do básico, também pode-se alterar diversos elementos estéticos usando:\n\ncolor - Para alterar a cor do texto.\nfamily e fontface - Para alterar a fonte do texto ou deixar ele negrito, itálico, etc.\nsize - Para alterar o tamnho do texto.\nalpha - Para alterar a transparência do texto.\nangle, vjust, hjust - Para ajustar o alinhamento horizontal e vertical do texto.\n\nO código abaixo refaz o gráfico acima, mas ao invés de pontos, agora vê-se o nome do modelo de cada automóvel.\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_text(aes(label = rownames(mtcars)), size = 3)\n\n\n\n\n\n\n\n\nNo gráfico acima, o eixo-y (mpg) é “milhas por galão”, isto é, quantas milhas o carro consegue percorrer por galão de combustível enquanto o eixo-x (wt) é o peso do veículo em milhares de libras. Assim, o Toyota Corolla é um carro leve e eficiente, enquanto o Chrysler Imperial é um carro pesado e ineficiente.\nUm problema frequente deste tipo de gráfico é a sobreposição dos nomes. De fato, vale a pena considerar encurtar os nomes ou usar abreviações para atenuar este tipo de problema. Outra opção é usar a extensão ggrepel5.\n\nExemplo: Estados e Aluguel\nA tabela abaixo reúne alguns fatos da PNADC/A mais recente sobre o percentual de apartamentos, em relação ao total de domicílios, e o percentual de imóveis alugados, também em relação ao total de domicílios.\n\nrented &lt;- tibble::tribble(\n  ~abbrev_state, ~share_apto, ~share_rented, ~name_region, \n  #------------#------------#--------------#------------#      \n           \"RO\",           8,          11.1, \"Norte\", \n           \"AC\",        7.47,          6.93, \"Norte\",\n           \"AM\",        15.8,          7.26, \"Norte\",\n           \"RR\",        14.3,          11.7, \"Norte\", \n           \"PA\",        4.53,          6.39, \"Norte\",\n           \"AP\",        10.7,          6.34, \"Norte\",\n           \"TO\",        3.17,          11.7, \"Norte\", \n           \"MA\",        4.06,          5.74, \"Nordeste\",\n           \"PI\",        4.08,          5.15, \"Nordeste\",\n           \"CE\",        9.85,          9.68, \"Nordeste\",\n           \"RN\",        9.27,          10.8, \"Nordeste\", \n           \"PB\",        11.9,          9.18, \"Nordeste\",\n           \"PE\",        10.4,          9.98, \"Nordeste\",\n           \"AL\",        6.32,          10.0, \"Nordeste\", \n           \"SE\",        12.0,            11, \"Nordeste\",   \n           \"BA\",        12.0,          7.61, \"Nordeste\",\n           \"MG\",        14.0,          10.5, \"Sudeste\", \n           \"ES\",        21.7,          10.7, \"Sudeste\", \n           \"RJ\",        26.8,          10.7, \"Sudeste\", \n           \"SP\",        19.4,          12.7, \"Sudeste\", \n           \"PR\",        11.9,          11.7, \"Sul\", \n           \"SC\",          17,          11.8, \"Sul\", \n           \"RS\",        16.5,          8.46, \"Sul\",\n           \"MS\",        3.67,          11.9, \"Centro Oeste\", \n           \"MT\",        3.29,          12.7, \"Centro Oeste\", \n           \"GO\",        9.88,          13.6, \"Centro Oeste\", \n           \"DF\",        35.4,          17.8, \"Centro Oeste\"\n)\n\nPode-se visualizar este gráfico usando a sigla de cada estado e os números na tabela como a posição do texto. Assim, fica evidente como o DF tem uma proporção grande de apartamentos e de domicílios alugados. Maranhão e Piauí, por outro lado, tem poucos apartamentos e poucos domicílios alugados. Já Goiás tem um percentual relativamente grande de unidades alugadas, apesar de ter quase 90% de casas.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state),\n    size = 4\n    )\n\n\n\n\n\n\n\n\nPode-se, naturalmente, mapear uma das variáveis categóricas num elemento estético. No gráfico abaixo, cada região é destaca com uma cor distinta.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state, color = as.factor(name_region)),\n    size = 4\n    ) +\n  scale_color_brewer(name = \"Região\", type = \"qual\", palette = 6) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nPor fim, vale notar que existe uma função equivalente chamada geom_label. A única diferença desta função é que ela coloca o texto dentro de um pequeno quadrado com fundo branco.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_label(\n    aes(label = abbrev_state, color = as.factor(name_region)),\n    size = 4\n    ) +\n  scale_color_brewer(name = \"Região\", type = \"qual\", palette = 6) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nTrocando a fonte\nComo citado acima, é possível modificar a fonte do texto usando o argumento family. As fontes disponíveis no R dependem do sistema operacional em que ele está rodando. Para verificar as fontes disponíveis no seu computador você pode tentar uma das duas opções abaixo\n\n#&gt; Instale o pacote sysfonts se necessário\ninstall.packages(\"sysfonts\")\nlibrary(sysfonts)\n\nsysfonts::font_families()\n#&gt; [1] \"sans\"  \"serif\" \"mono\"\n#&gt; \n\n#&gt; Ou use a função system\navailable_fonts &lt;- system(\"fc-list : family\", intern = TRUE)\navailable_fonts &lt;- available_fonts[order(available_fonts)]\n#&gt; Mostra as primeiras dez fontes encontradas no sistema\navailable_fonts[1:10]\n\n#&gt;  [1] \".Al Bayan PUA\"                                            \n#&gt;  [2] \".Al Nile PUA\"                                             \n#&gt;  [3] \".Al Tarikh PUA\"                                           \n#&gt;  [4] \".Apple Color Emoji UI\"                                    \n#&gt;  [5] \".Apple SD Gothic NeoI,Apple SD 산돌고딕 Neo\"              \n#&gt;  [6] \".Aqua Kana,.Aqua かな\"                                    \n#&gt;  [7] \".Aqua Kana,.Aqua かな,.Aqua Kana Bold,.Aqua かな ボールド\"\n#&gt;  [8] \".Arial Hebrew Desk Interface\"                             \n#&gt;  [9] \".Baghdad PUA\"                                             \n#&gt; [10] \".Beirut PUA\"  \n\nO gráfico abaixo mostra um exemplo de como modificar a fonte do texto. A depender do seu sistema operacional o código pode não funcionar.\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state),\n    family = \"mono\"\n    )\n\n\n\n\n\n\n\n\nUma solução mais consistente é de usar o pacote showtext. Tenho um post explicando como este pacote funciona. O código abaixo importa a fonte Montserrat do Google Fonts e aplica no gráfico.\n\nlibrary(showtext)\nsysfonts::font_add_google(\"Montserrat\", \"Montserrat\")\nshowtext::showtext_auto()\n\nggplot(rented, aes(x = share_apto, y = share_rented)) +\n  geom_text(\n    aes(label = abbrev_state),\n    family = \"Montserrat\"\n    )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#combinando-texto-com-outros-gráficos",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#combinando-texto-com-outros-gráficos",
    "title": "Estético: Destacando informação",
    "section": "Combinando texto com outros gráficos",
    "text": "Combinando texto com outros gráficos\nComo de costume, um dos grandes diferenciais do ggplot2 é a possibilidade de adicionar elementos aos gráficos sequencialmente. Podemos resgatar nosso exemplo do PIB com os ciclos de recessão.\nNo código abaixo, uso um pouco de álgebra para plotar o texto no meio da região sombreada.\n\nggplot() +\n  geom_hline(yintercept = 100) +\n  geom_line(\n    data = pib,\n    aes(x = ref.date, y = value)\n  ) +\n  geom_rect(\n    data = codace,\n    aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = Inf, group = label),\n    alpha = 0.4\n  ) +\n  geom_text(\n    data = codace,\n    aes(x = (rec_end - rec_start) / 2 + rec_start, y = 180, label = label),\n    size = 3\n  )\n\n\n\n\n\n\n\n\n\nExemplo: IPCA\nOutra combinação popular é de texto com colunas. No gráfico abaixo mostro a variação anual do IPCA com o valor percentual indicado em cima de cada coluna. Note como a função geom_text() recebe y + 0.5 como posição vertical.\n\nipca &lt;- tibble::tribble(\n  ~ano, ~ipca,\n  2013, 5.91,\n  2014, 6.41,\n  2015, 10.67,\n  2016, 6.29,\n  2017, 2.95,\n  2018, 3.75,\n  2019, 4.31,\n  2020, 4.52,\n  2021, 10.06,\n  2022, 5.79\n)\n\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  #&gt; Desenha a coluna com o valor do IPCA\n  geom_col() + \n  #&gt; Linha horizontal no eixo-e\n  geom_hline(yintercept = 0) +\n  #&gt; Número em cima de cada coluna\n  geom_text(aes(y = ipca + 0.5, label = ipca)) +\n  #&gt; Força o eixo-x a imprimir todos os valores\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nUma maneira alternativa de chegar no mesmo resultado é usando o argumento nudge_y, que cria um pequeno deslocamento no eixo-y.\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  #&gt; Usa nudge_y ao invés de ipca + 0.5\n  geom_text(aes(y = ipca, label = ipca), nudge_y = 0.5) +\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nAs funções round, paste, paste0 e format podem ser muito úteis para melhor apresentar resultados numéricos6. Esta funções servem para arredondar números, concatenar strings e formatar números (usando ponto para separar milhar, vírgula para separar decimal, etc.). No código abaixo mostro um passo-a-passo de como usar estas funções para formatar o número em cima de cada coluna.\n\nipca &lt;- ipca %&gt;%\n  mutate(\n    # Arredonda até a primeira casa decimal\n    ipca_label = round(ipca, 1),\n    # Usa vírgula como separador de decimal\n    ipca_label = format(ipca_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    ipca_label = paste0(ipca_label, \"%\")\n  )\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  geom_text(aes(y = ipca + 0.5, label = ipca_label)) +\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\n\n\nExemplo: Demografia\nA tabela abaixo mostra a representatividade de cada grande grupo demográfico na população brasileira aos longo dos anos. Aqui, usam-se as seguintes definições: população jovem (14 anos ou menos); população adulta (de 15 a 64 anos); população idosa (65 anos ou mais). Os valores estão em percentuais.\n\ntbl_demographics &lt;- tribble(\n  ~year, ~young, ~adult, ~elder,\n  #----#-------#-------#-------#\n   1950, 0.4246, 0.5515, 0.0239,\n   1960, 0.4400, 0.5338, 0.0261,\n   1970, 0.4284, 0.5400, 0.0315,\n   1980, 0.3859, 0.5763, 0.0377,\n   1990, 0.3545, 0.6012, 0.0442,\n   2000, 0.3011, 0.6442, 0.0545,\n   2010, 0.2499, 0.6817, 0.0683,\n   2020, 0.2099, 0.6986, 0.0914\n)\n\nAntes de visualizar os dados é preciso converter esta tabela para um formato longitudinal, onde cada linha representa um ano-grupo, isto é, cada linha indica o share da população de determinado grupo da população num determinado ano. Para isto uso a função pivot_longer(). Além disso eu transformo a coluna age_group num factor ordenado para melhorar a visualização.\nNo exemplo abaixo eu também ajusto o dado da coluna share: eu multiplico o valor por 100, arredondo até a primeira casa decimal, uso a vírgula como separador de decimal e coloco um sinal de “%” após o número.\n\ndemo &lt;- tbl_demographics %&gt;%\n  pivot_longer(\n    cols = young:elder,\n    names_to = \"age_group\",\n    values_to = \"share\"\n  ) %&gt;%\n  mutate(\n    age_group = factor(age_group, levels = c(\"elder\", \"adult\", \"young\")),\n    # Arredonda na primeira casa decimal\n    share_label = round(share * 100, 1),\n    # Usa vírgula como separador de decimal\n    share_label = format(share_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    share_label = paste0(share_label, \"%\")\n    )\n\nPara montar o gráfico eu utilizo o argumento position_stack(), já apresentado no post introdutório sobre gráficos de coluna. A função geom_text() aceita o mesmo argumento; note que coloco o valor vjust = 0.5. Este valor indica que desejo que o texto fica exatamente no meio da coluna, no seu respectivo grupo.\nOutras opções seriam: vjust = 0 para colocar o valor na base da coluna ou vjust = 1 para colocar o valor no topo da coluna.\n\nggplot(demo, aes(x = year, y = share, fill = age_group)) +\n  geom_col(position = position_stack()) +\n  geom_text(aes(label = share_label), position = position_stack(vjust = 0.5)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nExemplo: carros (de novo…)\nPor fim, monto mais um exemplo de como combinar gráficos de coluna com texto. No gráfico abaixo eu mostro a eficiência (milhas por galão) de um subconjunto de carros (os que tem 4 cilindradas). Além disso, eu ordeno o nome dos carros pela variável mpg.\n\ntbl_cars &lt;- mtcars %&gt;%\n  filter(cyl == 4) %&gt;%\n  mutate(\n    name_car = rownames(.),\n    name_car = factor(name_car),\n    name_car = forcats::fct_reorder(name_car, mpg)\n    ) %&gt;%\n  select(name_car, mpg)\n\nUsando o geom_text() eu consigo plotar o nome do modelo dentro da coluna assim como o respectivo valor de mpg. Note como uso fontface = 'bold' para deixar o valor numérico em negrito.\n\nggplot(tbl_cars, aes(x = name_car, y = mpg)) +\n  geom_col(fill = \"#186177\") +\n  geom_text(\n    aes(y = 1, label = name_car),\n    color = \"#F9F7F3\",\n    hjust = 0) +\n  geom_text(\n    aes(y = mpg - 2, label = mpg),\n    color = \"#F9F7F3\",\n    fontface = \"bold\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nInfelizmente, o gráfico acaba sobrecarregado com informação redundante, mas veremos como controlar os aspectos estéticos do gráfico em si (e.g. cor do fundo, texto nos eixos, etc.) num post futuro."
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#footnotes",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#footnotes",
    "title": "Estético: Destacando informação",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMais especificamente, a série trimestral do PIB dessazonalizado a preços de mercado com valores encadeados com base no primeiro trimestre de 1995.↩︎\nO Comitê de Datação de Ciclos Econômicos (CODACE) organizado pela Fundação Getúlio Vargas (FGV) se reúne periodicamente para datar os ciclos econômicos brasileiros.↩︎\nEste esforço adicional é feito por motivos didáticos. Na prática não é necessário usar a função tribble.↩︎\nPara mais informações sobre esta base consulte help(\"mtcars\").↩︎\nO pacote {ggrepel} cria pequenos deslocamentos na posição do texto e setas para melhor dispor os dados. Para mais informações veja o site.↩︎\nOutra opção interessante é função glue::glue().↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-pib-e-ciclos",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-pib-e-ciclos",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: PIB e ciclos",
    "text": "Exemplo: PIB e ciclos\nPodemos resgatar nosso exemplo do PIB com os ciclos de recessão.\nNo código abaixo, uso um pouco de álgebra para plotar o texto no meio da região sombreada.\n\nggplot() +\n  geom_hline(yintercept = 100) +\n  geom_line(\n    data = pib,\n    aes(x = ref.date, y = value)\n  ) +\n  geom_rect(\n    data = codace,\n    aes(xmin = rec_start, xmax = rec_end, ymin = -Inf, ymax = Inf, group = label),\n    alpha = 0.4\n  ) +\n  geom_text(\n    data = codace,\n    aes(x = (rec_end - rec_start) / 2 + rec_start, y = 180, label = label),\n    size = 3\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-ipca",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-ipca",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: IPCA",
    "text": "Exemplo: IPCA\nPodemos também mesclar um gráfico de colunas com texto. A tabela traz os valores do IPCA acumulados ano a ano desde 2013.\n\nipca &lt;- tibble::tribble(\n  ~ano, ~ipca,\n  2013, 5.91,\n  2014, 6.41,\n  2015, 10.67,\n  2016, 6.29,\n  2017, 2.95,\n  2018, 3.75,\n  2019, 4.31,\n  2020, 4.52,\n  2021, 10.06,\n  2022, 5.79\n)\n\nNo gráfico abaixo mostro a variação anual do IPCA com o valor percentual indicado em cima de cada coluna. Note como a função geom_text() recebe y + 0.5 como posição vertical.\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  #&gt; Desenha a coluna com o valor do IPCA\n  geom_col() + \n  #&gt; Linha horizontal no eixo-e\n  geom_hline(yintercept = 0) +\n  #&gt; Número em cima de cada coluna\n  geom_text(aes(y = ipca + 0.5, label = ipca)) +\n  #&gt; Força o eixo-x a imprimir todos os valores\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nUma maneira alternativa de chegar no mesmo resultado é usando o argumento nudge_y, que cria um pequeno deslocamento no eixo-y.\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  #&gt; Usa nudge_y ao invés de ipca + 0.5\n  geom_text(aes(y = ipca, label = ipca), nudge_y = 0.5) +\n  scale_x_continuous(breaks = 2013:2022)\n\n\n\n\n\n\n\n\nAs funções round, paste, paste0 e format podem ser muito úteis para melhor apresentar resultados numéricos6. Esta funções servem para arredondar números, concatenar strings e formatar números (usando ponto para separar milhar, vírgula para separar decimal, etc.). No código abaixo mostro um passo-a-passo de como usar estas funções para formatar o número em cima de cada coluna.\n\nipca &lt;- ipca %&gt;%\n  mutate(\n    # Arredonda até a primeira casa decimal\n    ipca_label = round(ipca, 1),\n    # Usa vírgula como separador de decimal\n    ipca_label = format(ipca_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    ipca_label = paste0(ipca_label, \"%\")\n  )\n\nggplot(ipca, aes(x = ano, y = ipca)) +\n  geom_col() + \n  geom_hline(yintercept = 0) +\n  geom_text(aes(y = ipca + 0.5, label = ipca_label)) +\n  scale_x_continuous(breaks = 2013:2022)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-demografia",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-demografia",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: Demografia",
    "text": "Exemplo: Demografia\nA tabela abaixo mostra a representatividade de cada grande grupo demográfico na população brasileira aos longo dos anos. Aqui, usam-se as seguintes definições: população jovem (14 anos ou menos); população adulta (de 15 a 64 anos); população idosa (65 anos ou mais). Os valores estão em percentuais.\n\ntbl_demographics &lt;- tribble(\n  ~year, ~young, ~adult, ~elder,\n  #----#-------#-------#-------#\n   1950, 0.4246, 0.5515, 0.0239,\n   1960, 0.4400, 0.5338, 0.0261,\n   1970, 0.4284, 0.5400, 0.0315,\n   1980, 0.3859, 0.5763, 0.0377,\n   1990, 0.3545, 0.6012, 0.0442,\n   2000, 0.3011, 0.6442, 0.0545,\n   2010, 0.2499, 0.6817, 0.0683,\n   2020, 0.2099, 0.6986, 0.0914\n)\n\nAntes de visualizar os dados é preciso converter esta tabela para um formato longitudinal, onde cada linha representa um ano-grupo, isto é, cada linha indica o share da população de determinado grupo da população num determinado ano. Para isto uso a função pivot_longer(). Além disso eu transformo a coluna age_group num factor ordenado para melhorar a visualização.\nNo exemplo abaixo eu também ajusto o dado da coluna share: eu multiplico o valor por 100, arredondo até a primeira casa decimal, uso a vírgula como separador de decimal e coloco um sinal de “%” após o número.\n\ndemo &lt;- tbl_demographics %&gt;%\n  pivot_longer(\n    cols = young:elder,\n    names_to = \"age_group\",\n    values_to = \"share\"\n  ) %&gt;%\n  mutate(\n    age_group = factor(age_group, levels = c(\"elder\", \"adult\", \"young\")),\n    # Arredonda na primeira casa decimal\n    share_label = round(share * 100, 1),\n    # Usa vírgula como separador de decimal\n    share_label = format(share_label, decimal.mark = \",\"),\n    # Coloca '%' no final do string\n    share_label = paste0(share_label, \"%\")\n    )\n\nPara montar o gráfico eu utilizo o argumento position_stack(), já apresentado no post introdutório sobre gráficos de coluna. A função geom_text() aceita o mesmo argumento; note que coloco o valor vjust = 0.5. Este valor indica que desejo que o texto fica exatamente no meio da coluna, no seu respectivo grupo.\nOutras opções seriam: vjust = 0 para colocar o valor na base da coluna ou vjust = 1 para colocar o valor no topo da coluna.\n\nggplot(demo, aes(x = year, y = share, fill = age_group)) +\n  geom_col(position = position_stack()) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = share_label), position = position_stack(vjust = 0.5)) +\n  scale_x_continuous(breaks = seq(1950, 2020, 10)) +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-carros-de-novo",
    "href": "posts/ggplot2-tutorial/5-grafico-text.html#exemplo-carros-de-novo",
    "title": "Estético: Destacando informação",
    "section": "Exemplo: carros (de novo…)",
    "text": "Exemplo: carros (de novo…)\nPor fim, monto mais um exemplo de como combinar gráficos de coluna com texto. No gráfico abaixo eu mostro a eficiência (milhas por galão) de um subconjunto de carros (os que tem 4 cilindradas). Além disso, eu ordeno o nome dos carros pela variável mpg.\n\ntbl_cars &lt;- mtcars %&gt;%\n  filter(cyl == 4) %&gt;%\n  mutate(\n    name_car = rownames(.),\n    name_car = factor(name_car),\n    name_car = forcats::fct_reorder(name_car, mpg)\n    ) %&gt;%\n  select(name_car, mpg)\n\nUsando o geom_text() eu consigo plotar o nome do modelo dentro da coluna assim como o respectivo valor de mpg. Note como uso fontface = 'bold' para deixar o valor numérico em negrito.\n\nggplot(tbl_cars, aes(x = name_car, y = mpg)) +\n  geom_col(fill = \"#186177\") +\n  geom_hline(yintercept = 0) +\n  geom_text(\n    aes(y = 1, label = name_car),\n    color = \"#F9F7F3\",\n    hjust = 0) +\n  geom_text(\n    aes(y = mpg - 2, label = mpg),\n    color = \"#F9F7F3\",\n    fontface = \"bold\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nInfelizmente, o gráfico acaba sobrecarregado com informação redundante, mas veremos como controlar os aspectos estéticos do gráfico em si (e.g. cor do fundo, texto nos eixos, etc.) num post futuro."
  },
  {
    "objectID": "posts/general-posts/2022-09-brasilia/index.html",
    "href": "posts/general-posts/2022-09-brasilia/index.html",
    "title": "Mapa de altitude de ruas de Brasília",
    "section": "",
    "text": "Inspirado num antigo post de BlakeRMills, criador do pacote {MetBrewer}, criei um mapa com a altitude das ruas em Brasília.\nEm breve farei um post com tutorial detalhado e também pretendo replicar este tipo de mapa para outras cidades interessantes. O código para replicar o gráfico está abaixo.\n\n\nCode\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(osmdata)\nlibrary(purrr)\nsf::sf_use_s2(FALSE)\nsysfonts::font_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nshowtext::showtext_auto()\n\nurl = \"https://pt.wikipedia.org/wiki/Lista_de_municípios_do_Brasil_por_população\"\n\ntab = xml2::read_html(url) |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(2)\n\nas_numeric_char = Vectorize(function(x) {\n  ls = stringr::str_extract_all(x, \"[[:digit:]]\")\n  y = paste(ls[[1]], collapse = \"\")\n  as.numeric(y)\n})\n\nclean_tab = tab |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    code_muni = codigo_ibge,\n    name_muni = municipio,\n    rank = posicao,\n    name_state = unidade_federativa,\n    pop = populacao\n  ) |&gt; \n  filter(name_muni != \"Brasil\") |&gt; \n  mutate(\n    code_muni = as.numeric(code_muni),\n    pop = as_numeric_char(pop),\n    rank = rank(-pop)\n  )\n\ntop20 = slice_max(clean_tab, pop, n = 20)\n\nget_border = function(city) {\n  \n  #&gt; Encontra o código do município\n  code_muni = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(code_muni) |&gt; \n    unique()\n  \n  stopifnot(length(code_muni) == 1)\n  \n  #&gt; Baixa o shapefile do município\n  border = geobr::read_municipality(code_muni, showProgress = FALSE)\n  \n  return(border)\n}\n\nget_streets = function(city, border) {\n  \n  #&gt; Encontra o nome da Unidade Federativa\n  nome_uf = top20 |&gt; \n    filter(name_muni == city) |&gt; \n    pull(name_state)\n  #&gt; Monta o nome do local\n  name_place = stringr::str_glue(\"{city}, {nome_uf}, Brazil\")\n  #&gt; Monta a query\n  place = opq(bbox = getbb(name_place))\n  \n  #&gt; Importa todas as principais vias da cidade\n  # streets = add_osm_feature(\n  #   place,\n  #   key = \"highway\",\n  #   value = c(\n  #     \"motorway\", \"primary\", \"motorway_link\", \"primary_link\",\n  #     \"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\",\n  #     \"residential\"\n  #     )\n  # )\n  \n  streets = add_osm_feature(place, key = \"highway\")\n  \n  #&gt; Converte o dado\n  streets = streets %&gt;%\n    osmdata_sf() %&gt;%\n    .$osm_lines %&gt;%\n    select(osm_id, name) %&gt;%\n    st_transform(crs = 4674)\n  \n  #&gt; Enconrtra a intersecção entre as estradas e o limites do município\n  streets_border = st_intersection(streets, border)\n  \n  # out = list(streets = streets, streets_border = streets_border)\n  \n  return(streets_border)\n  \n}\n\nget_elevation = function(border, z = 8) {\n  \n  altitude = elevatr::get_elev_raster(border, z = z, clip = \"bbox\")\n  altitude = raster::rasterToPolygons(altitude)\n  altitude = st_as_sf(altitude)\n  names(altitude)[1] = \"elevation\"\n  \n  altitude = st_transform(altitude, crs = 4674)\n  altitude = suppressWarnings(st_intersection(altitude, border))\n  altitude = filter(altitude, st_is_valid(altitude))\n  \n  return(altitude)\n  \n}\n\nadd_jenks_breaks = function(shp, k = 7, round = TRUE, r = 0) {\n  #&gt; Classifica os dados de altitude em k grupos segundo o algo. de Jenks\n  jbreaks = BAMMtools::getJenksBreaks(shp$elevation, k = k)\n  #&gt; Arredonda os números para chegar numa legenda menos quebrada\n  if (round) {\n    jbreaks = round(jbreaks, r)\n  }\n  #&gt; Cria a coluna 'jenks_group' que classifica cada valor num grupo\n  shp = mutate(shp, jenks_group = cut(elevation, jbreaks))\n  \n  #&gt; Verifica se todas as observações tem um grupo\n  check = any(is.na(shp$jenks_group))\n  if (check) {\n    warning(\"Some observations have failed to be grouped\")\n  }\n  \n  #&gt; Transforma os groups em legendas\n  labels = get_jenks_labels(jbreaks)\n  \n  #&gt; Retorna o output numa lista\n  out = list(shp = shp, labels = labels)\n  return(out)\n  \n}\n\nget_jenks_labels = function(x) {\n  labels = paste(x, x[-1], sep = \"–\")\n  labels[1] = paste(\"&lt;\", x[2])\n  labels[length(labels)] = paste(\"&gt;\", max(x))\n  return(labels)\n}\n\nget_streets_altitude = function(altitude, streets) {\n  \n  stopifnot(any(colnames(altitude) %in% \"jenks_group\"))\n  \n  #&gt; Get all groups\n  groups = levels(altitude$jenks_group)\n  \n  #&gt; For each group get the full polygon and join with streets\n  join_streets = function(group) {\n    \n    poly = altitude %&gt;%\n      filter(jenks_group == group) %&gt;%\n      st_union(.) %&gt;%\n      st_as_sf() %&gt;%\n      st_make_valid()\n    \n    joined = suppressWarnings(st_intersection(streets, poly))\n    \n    return(joined)\n    \n  }\n  #&gt; Apply the function to all groups\n  street_levels = purrr::map(groups, join_streets)\n  #&gt; Bind all results together\n  out = bind_rows(street_levels, .id = \"level\")\n  \n  return(out)\n  \n}\n\nmap_plot = function(shp, labels, title, showtext = TRUE) {\n  \n  colors = viridis::plasma(n = length(labels) + 1)\n  colors = colors[-length(colors)]\n  \n  font = ifelse(showtext == TRUE, \"Roboto Condensed\", \"sans\")\n  \n  plot =\n    ggplot(data = shp) +\n    geom_sf(aes(color = level, fill = level), linewidth = 0.2) +\n    scale_color_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    scale_fill_manual(\n      name = \"Altitude\",\n      labels = labels,\n      values = colors) +\n    guides(fill = guide_legend(nrow = 1), color = guide_legend(nrow = 1)) +\n    ggtitle(title) +\n    ggthemes::theme_map() +\n    coord_sf() +\n    theme(\n      plot.title = element_text(\n        size = 30,\n        hjust = 0.5,\n        family = font\n      ),\n      legend.title = element_text(\n        size = 20,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.text = element_text(\n        size = 14,\n        family = font,\n        color = \"gray10\"\n      ),\n      legend.position = \"top\",\n      legend.direction = \"horizontal\",\n      plot.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      panel.background = element_rect(color = NA, fill = \"#f6eee3\"),\n      legend.background = element_rect(color = NA, fill = \"#f6eee3\")\n    )\n  \n  return(plot)\n  \n}\n\nmap_altitude = function(city, k, z) {\n  \n  #&gt; Importa o shape do limite do município\n  message(\"Importando os limites do município: \", city)\n  city_border = get_border(city)\n  #&gt; Importa as principais vias da cidade e junta com o limite do muni\n  message(\"Importando as vias.\")\n  city_street = get_streets(city, city_border)\n  #&gt; Importa a altitude da cidade\n  message(\"Importando a altitude.\")\n  city_elevation = suppressMessages(get_elevation(city_border, z = z))\n  #&gt; Classifica a altitude em grupos\n  message(\"Classificando e juntando os shapefiles.\")\n  jenks = add_jenks_breaks(city_elevation, k = k)\n  city_elevation = jenks[[\"shp\"]]\n  labels = jenks[[\"labels\"]]\n  #&gt; Junta a altitude (agrupada) com as vias\n  city_street_elevation = get_streets_altitude(city_elevation, city_street)\n  \n  #&gt; Monta o mapa final\n  message(\"Gerando o mapa final.\")\n  plot = map_plot(city_street_elevation, labels = labels, title = city)\n  message(\"Feito.\")\n  #&gt; Retorna o output numa lista\n  out = list(\n    shp = city_street_elevation,\n    streets = city_street,\n    elevation = city_elevation,\n    plot = plot\n  )\n  \n  return(out)\n  \n}\n\nexport_citymap = function(city, w = 14, h = 16, ...) {\n  \n  plot = map_altitude(city, ...)$plot\n  \n  if (is.numeric(city)) {\n    name_city = cities_brasil |&gt; \n      filter(code_muni == city) |&gt; \n      pull(name_muni)\n  } else if (is.character(city)) {\n    name_city = city\n  }\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.svg\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n  \n  name_file = glue::glue(\n    \"elevation_{janitor::make_clean_names(name_city)}.png\"\n  )\n  \n  ggsave(\n    here::here(\"graphics/altitude\", name_file),\n    plot,\n    width = w,\n    height = h\n  )\n}\n\nsafe_export = purrr::safely(export_citymap)\n\nparams = tribble(\n  ~city,       ~z, ~k,\n  #----------------#---#---#\n  \"São Paulo\",       8,  7,\n  \"Rio de Janeiro\",  9,  7,\n  \"Brasília\",        8,  7,\n  \"Fortaleza\",      11,  7,\n  \"Salvador\",       10,  7,\n  \"Belo Horizonte\", 10,  7,\n  \"Manaus\",          6,  7,\n  \"Curitiba\",       10,  7,\n  \"Recife\",         10,  7,\n  \"Goiânia\",         8,  7,\n  \"Porto Alegre\",    8,  7,\n  \"Belém\",           9,  7,\n  \"Guarulhos\",       9,  7,\n  \"Campinas\",        9,  7,\n  \"São Luís\",        9,  7,\n  \"Maceió\",         11,  8,\n  \"Campo Grande\",    7,  7,\n  \"São Gonçalo\",     8,  7,\n  \"Teresina\",        10,  7,\n  \"João Pessoa\",     9,  7,\n  \"Joinville\",       9,  7\n  )\n\n#&gt; Exportar todas as cidades listadas acima\n#pmap(params, safe_export)\n#&gt; Exportar uma cidade em particular\n#safe_export(\"Brasília\", z = 8, k = 7)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html",
    "title": "Estético: Escalas e Cores",
    "section": "",
    "text": "Este post é o primeiro da série de tutoriais que não vai tratar de um tipo de gráfico ou, mais especificamente, de um tipo de geom. Escalas, no contexto do ggplot2 são tanto os eixos do gráfico como as suas cores, quando usa-se a função aes para mapear alguma variável em cores. As funções scale_* controlam todos os aspectos das escalas, incluindo as cores e as suas respectivas legendas.\nAlguns aspectos mais detalhados das escalas, como a fonte do texto, o tamanho, etc. são controlados por uma função mais específica theme. Em alguns exemplos deste post eu apresento como utilizar esta função, mas uma apresentação mais formal fica postergada para outro momento. A função theme é talvez a mais burocrática e complexa do pacote ggplot2.\nO código abaixo lista os pacotes necessários para acompanhar este post.\n\n#&gt; Os pacotes necessários para acompanhar este post\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(readr)\n#&gt; Importa os dados limpos da Zona OD.\npod &lt;- read_csv(\"https://github.com/viniciusoike/restateinsight/raw/main/posts/ggplot2-tutorial/table_pod.csv\")\n#&gt; Seleciona apenas zonas de São Paulo com população acima de zero\npod &lt;- filter(pod, code_muni == 36, pop &gt; 0)\n\nPara este post, além das bases de dados que já acompanham o pacote ggplot2 também uso a base da Pesquisa Origem e Destino (POD) de São Paulo, de 2018. A POD é uma pesquisa feita pela companhia Metrô de São Paulo que compila informações sobre os deslocamentos diários da população da Região Metropolitana de São Paulo. Além de dados sobre mobilidade, a POD também reúne uma série de informações socioeconômicas. Os dados são agregados a nível de Zona Origem e Destino ou Zona OD; as zonas OD são subconjuntos de distritos de São Paulo e das demais cidades da Região Metropolitana e, na prática, são como bairros de cada cidade1.\nEu mesmo tratei a base e as funções utilizadas para montar esta tabela estão disponíveis no repositório {tidypod} do meu GitHub.\nNeste post vou explorar apenas as zonas OD da cidade de São Paulo, excluindo as zonas “não-residenciais”, como a Cidade Universitária (campus da USP)."
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#tipos-de-variáveis",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#tipos-de-variáveis",
    "title": "Estético: Escalas e Cores",
    "section": "Tipos de variáveis",
    "text": "Tipos de variáveis\nO ggplot, grosso modo, divide as variáveis em contínuas e discretas. As variáveis contínuas, em geral, são numéricas e podem assumir qualquer valor; já as variáveis discretas costumam ser “categóricas” e são “contáveis”. O preço de um imóvel, por exemplo, é uma variável contínua. Já uma variável que categoriza um imóvel entre “casa” e “apartamento” é uma variável discreta.\nEsta lógica é aplicada nas funções que controlam os eixos x e y de um gráfico.\n\nscale_y_continuous()\nscale_y_discrete()\nscale_x_continuous()\nscale_x_discrete()\n\nUma lógica muito similar se aplicas às principais funções que controlam as cores e a legenda de cores de um gráfico:\n\nscale_color_continuous()\nscale_color_discrete()\nscale_fill_continuous()\nscale_fill_discrete()\n\nMesmo quando modificamos outros aspectos estéticos do gráfico como size e alpha temos:\n\nscale_alpha_continuous()\nscale_alpha_discrete()\nscale_size_continuous()\nscale_size_discrete()\n\n\nVariáveis contínuas\nUma variável contínua costuma representar algum número. No R há várias formas de armazenar números, mas isto não costuma ser muito relevante para a tarefa de visualização dos dados. Na maior parte dos casos, basta garantir que a coluna numérica em questão seja um número usando is.numeric() ou as.numeric().\nComo mencionado acima, exemplos comuns de variáveis contínuas são: preço de um imóvel, salário de um indivíduo, a taxa de inflação num mês, etc.\n\n\nVariáveis discretas\nUma variável discreta costuma representar uma categoria. No R existe uma classe especial de variável para armazenar este tipo de dado chamada factor. Um factor é um vetor de texto ou de números que segue uma ordem. Além de ter uma ordem, cada elemento pode ter um label.\n\nfactor(x = c(...), levels = c(...), labels = c(...))\n\nPara se definir um factor basta usar a função homônima. Note que na ausência de uma ordem explicitamente definida, o R organiza o vetor em ordem alfabética. Se, ao invés de um vetor de texto tivéssemos usado um vetor de números, eles teriam sido ordenados no sentido ascedente (do menor para o maior).\nPara acessar a ordem do factor pode-se usar a função order() ou, mais especificamente, a função levels().\n\n#&gt; Criando um factor \nmedalhas &lt;- factor(c(\"ouro\", \"prata\", \"bronze\"))\n#&gt; [1] ouro   prata  bronze\n#&gt; Levels: bronze ouro prata\n\n#&gt; Conferindo a ordem dos elementos\norder(medalhas)\n#&gt; [1] 3 1 2\nlevels(medalhas)\n#&gt; [1] \"bronze\" \"ouro\"   \"prata\" \n\nO código abaixo recria o factor, deixando mais explícito a estrutura deste tipo de objeto. Note que o argumento levels e labels não precisam ser repetidos.\n\n#&gt; Criando um factor \nmedalhas &lt;- factor(\n  c(\"ouro\", \"prata\", \"bronze\", \"bronze\", \"ouro\", \"bronze\"),\n  levels = c(\"bronze\", \"prata\", \"ouro\"),\n  labels = c(\"Bronze\", \"Prata\", \"Ouro\")\n  )\n\nmedalhas\n#&gt; [1] Ouro   Prata  Bronze Bronze Ouro   Bronze\n#&gt; Levels: Bronze Prata Ouro\n\nTrabalhar com factors pode ser uma tarefa bastante frustrante. Neste sentido, recomendo muito o uso do pacote forcats, que provê uma série de funções fct_* que facilitam muito a manipulação deste tipo de objeto. Os exemplos abaixo mostram algumas das funções mais úteis deste pacote.\n\ndf &lt;- data.frame(\n  x = c(5, 2, 3),\n  y = factor(c(\"bronze\", \"prata\", \"ouro\"))\n)\n\n#&gt; Troca a ordem do factor segundo algum outro vetor\nfct_reorder(df$y, df$x)\n#&gt; [1] bronze prata  ouro  \n#&gt; Levels: prata ouro bronze\n\n#&gt; Troca os labels do factor usando uma função\nfct_relabel(df$y, toupper)\n#&gt; [1] BRONZE PRATA  OURO  \n#&gt; Levels: BRONZE OURO PRATA\n\n#&gt; Troca os labels do factor manualmente\nfct_recode(df$y, \"Bronze\" = \"bronze\")\n#&gt; [1] Bronze prata  ouro  \n#&gt; Levels: Bronze ouro prata\n\n#&gt; Conta a ocorrência de cada factor\nfct_count(df$y)\n#&gt; A tibble: 3 × 2\n#&gt;   f          n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 bronze     1\n#&gt; 2 ouro       1\n#&gt; 3 prata      1\n\nPor fim, vale comentar brevemente sobre uma particularidade de um factor criado a partir de uma variável numérica. Para converter um factor de texto em character basta usar as.character(x). Para converter de volta um factor de números é preciso usar as.numeric(as.character(x)).\n\nx &lt;- c(1, 10, 2, 5, 1)\ny &lt;- as.factor(x)\n\n#&gt; Por padrão, as.numeric retorna a ordem do factor. Equivalente a order()\nas.numeric(y)\n#&gt; [1] 1 4 2 3 1\n\n#&gt; Para converter de volta no número original\nas.numeric(as.character(y))\n#&gt; [1]  1 10  2  5  1"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#escalas-o-básico",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#escalas-o-básico",
    "title": "Estético: Escalas e Cores",
    "section": "Escalas: o básico",
    "text": "Escalas: o básico\n\nEscalas Contínuas\nVamos começar com um gráfico simples que mostra a renda domiciliar média da Zona OD no eixo-x e o número médio de carros por domicílio no eixo-y.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNo caso do gráfico acima, ambas as variáveis são contínuas, portanto, para alterar algum dos eixos usa-se as funções scale_x_continuous() e scale_y_continuous(). Estas funções tem 5 argumentos principais: name, breaks, labels, limits e expand.\nO argumento name define o título do eixo. Alternativamente, pode-se usar a função labs, como fizemos em posts anteriores.\n\n#&gt; Define o título de cada eixo usando as funções scale\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(name = \"Renda Domiciliar Média (R$)\") +\n  scale_y_continuous(name = \"Automóveis por domicílio\")\n\n#&gt; Define o título de cada eixo usando a função labs\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  labs(x = \"Renda Domiciliar Média (R$)\", y = \"Automóveis por domicílio\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara controlar as ‘quebras’ do eixo-x (os pontos onde aparece cada número) usa-se o argumento breaks.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 20000, 2500))\n\n\n\n\n\n\n\n\nPor padrão, o número exibido no gráfico é igual ao argumento fornecido em breaks, mas pode-se alterar isto usando labels. Para ser mais preciso: breaks define a posição onde o labels vai ser exibido.\nNo exemplo abaixo uso o fato do salário mínimo, à época da pesquisa, ser de R$954.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(\n    breaks = 954 * c(2, 4, 6, 10, 15),\n    labels = stringr::str_glue(\"{c(2, 4, 6, 10, 15)} S.M.\")\n    )\n\n\n\n\n\n\n\n\nO argumento labels pode ser qualquer texto, desde que ele tenha o mesmo número de elementos que o argumento breaks. O pacote scales oferece algumas funções label_* pré-definidas que auxiliam a formatar as escalas. O exemplo abaixo mostra como usar a função label_dollar para formatar o eixo-x.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(0, 20000, 2500),\n    labels = label_dollar(big.mark = \".\")\n    )\n\n\n\n\n\n\n\n\nNa minha experiência, as funções mais úteis do pacote são:\n\nlabel_number(): usado para formar números de maneira geral\nlabel_percent(): usado para formatar números expressados percentualmente\nlabel_dollar(): usado para formatar números que representam dinheiro\n\nPara seguir o padrão brasileiro, utiliza-se big.mark = \".\" e decimal.mark = \",\".\nPara dar um “zoom-in” no gráfico pode-se alterar o argumento limits. Este argumento recebe um par de números para definir o número máximo e mínimo que deve ser plotado. Para deixar o eixo “livre” basta definir o valor como NA.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(limits = c(3000, 5000)) +\n  scale_y_continuous(limits = c(NA, 1))\n\n\n\n\n\n\n\n\nPor fim, o argumento expand diminui ou aumenta a distância entre o gráfico e o limite dos eixos. A aplicação mais comum disso é para reduzir o espaço em branco que “sobra” em alguns gráficos.\nO par de histogramas abaixo mostra a distribuição do número médio de automóveis por domicílio entre as zonas OD.\n\nggplot(pod, aes(x = car_rate)) +\n  geom_histogram(bins = 12, color = \"white\")\n\nggplot(pod, aes(x = car_rate)) +\n  geom_histogram(bins = 12, color = \"white\") +\n  scale_x_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEscalas discretas\nEscalas discretas funcionam praticamente da mesma forma. Uma distinção importante é que, no caso de uma variável discreta ou categórica, o eixo - por padrão - vai plotar o label da variável.\nO código abaixo encontra os 5 distritos mais populosos de São Paulo.\n\n#&gt; Seleciona os cinco distritos mais populosos\ndstr_pop &lt;- pod |&gt; \n  #&gt; Soma a variável pop em cada distrito\n  summarise(total_pop = sum(pop), .by = \"name_district\") |&gt; \n  #&gt; Encontra os cinco valores mais elevados \n  slice_max(total_pop, n = 5) |&gt; \n  #&gt; Converte a variável `name_district` para factor\n  mutate(name_district = factor(name_district))\n\nggplot(dstr_pop, aes(x = name_district, y = total_pop)) +\n  geom_col()\n\n\n\n\n\n\n\n\nCaso não se queira alterar o tipo da variável é possível definir os labels diretamente na função scale_x_discrete().\n\nggplot(dstr_pop, aes(x = name_district, y = total_pop)) +\n  geom_col() +\n  scale_x_discrete(\n    labels = c(\"Capão Redondo\", \"Grajaú\", \"Jd. Ângela\", \"Jd. São Luis\",\n               \"Sapopemba\")\n    )\n\n\n\n\n\n\n\n\nPor fim, vale notar que o argumento labels aceita uma função. Neste caso, fornece-se apenas o nome da função, sem argumentos explícitos. O gráfico abaixo mostra quatro exemplos.\n\nbase_plot &lt;- ggplot(dstr_pop, aes(x = name_district, y = total_pop)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n#&gt; Texto maísculo\nbase_plot + scale_x_discrete(labels = stringr::str_to_upper)\n#&gt; Texto minúsculo\nbase_plot + scale_x_discrete(labels = stringr::str_to_lower)\n#&gt; Texto em formato de 'título'\nbase_plot + scale_x_discrete(labels = stringr::str_to_title)\n#&gt; Quebras de linha automáticas no texto\nbase_plot + scale_x_discrete(labels = \\(x) stringr::str_wrap(x, width = 8))\n\n\n\n\n\n\n\n\n\n\nNo exemplo acima, utiliza-se a função stringr::str_to_upper para converter o texto do eixo x para maiúsculo.\nNo caso de uma função que precisa de um argumento adicional, como é o caso da função stringr::str_wrap é preciso criar uma ‘função anônima’. Uma função anônima funciona exatamente como uma função convencional e permite que um argumento adicional seja inserido.\nA sintaxe para definir uma função anônima é simplesmente function(x) g(x, ...) onde g(x) é alguma função qualquer. Por exemplo. function(x) mean(x, na.rm = TRUE). Alternativamente, é possível definir uma função anônima simplesmente com \\(x)."
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#um-pouco-mais-de-escalas",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#um-pouco-mais-de-escalas",
    "title": "Estético: Escalas e Cores",
    "section": "Um pouco mais de escalas",
    "text": "Um pouco mais de escalas\nAlém das funções scale_* há também algumas mais específicas. O código abaixo apresenta a função scale_x_log10() que, como o nome sugere, aplica uma transformação log na variável x.\nUma das vantagens de usar esta função, ao invés de transformar os dados usando a função log(), ou mesmo de usar trans = 'log' dentro de scale_x_continuous(), é que as quebras do eixo-x fiquem num formato num formato mais bonito como se vê abaixo.\nDe maneira geral, a função scale_x_log10() é útil quando há variância crescente nos dados ou a variável segue algum tipo de crescimento exponencial.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_log10()\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutra função interessante que eventualmente pode ser útil é a scale_*_reverse() que inverte a direção dos dados. Esta função é muito mais prática do que, por exemplo, trocar o sinal do dado original e aí utilizar o argumento labels para ajustar os números.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_count() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nPor fim, outra função interessante é a scale_*_binned(), que ajuda a discretizar uma varíalvel contínua. Ela funciona de maneira similar a um historgrama, argupando uma variável contínua em grupos: isto facilita a observação de padrões nos dados. No caso do gráfico abaixo, vê-se que o grupo mais comum é de zonas com renda entre R\\$4000 e R\\$6000 com número médio de automóveis por domicílios entre 0,6 e 0,8.\n\nggplot(pod, aes(x = income_avg, y = car_rate)) +\n  geom_count() +\n  scale_x_binned() +\n  scale_y_binned()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#datas",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#datas",
    "title": "Estético: Escalas e Cores",
    "section": "Datas",
    "text": "Datas\nUm dado em formato de data é significativamente mais complicado do que as variáveis contínuas e discretas que vimos acima. Existem inúmeros formatos distintos para se apresentar datas que variam de local para local; além disso, pode ser necessário lidar com fusos horários, feriados, anos bissextos, etc.\nIdealmente, toda coluna com datas deve sempre estar no formato YYYY-MM-DD, isto é, 2014-01-15 (15 de janeiro de 2014). Para converter um character em Date basta usar a função as.Date. Se a data não estiver no formato YYYY-MM-DD será necessário especificar o formato usando o argumento format. Para casos mais complexos recomendo o uso do pacote {lubridate}.\n\nx &lt;- c(\"2014-01-01\", \"2014-02-01\")\ny &lt;- c(\"01/01/2014\", \"01/02/2014\")\n\nas.Date(x)\n#&gt; [1] \"2014-01-01\" \"2014-02-01\"\nas.Date(y, format = \"%d/%m/%Y\")\n#&gt; [1] \"2014-01-01\" \"2014-02-01\"\n\nComo datas têm uma classe especial elas, por conseguinte, têm também algumas funções dedicadas. A função scale_x_date tem dois argumentos principais2:\n\ndate_breaks: que aceita valores como “1 year”, “3 months”, etc.\ndate_labels: que aceita valores como “%Y%m%d”, “%Y%b”, etc.\n\nVou apresentar esta função de maneira breve, diretamente atráveis de exemplos. O exemplo abaixo mostra o funcionamento geral desta função.\n\nggplot(economics, aes(x = date, y = psavert)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"5 year\", date_labels = \"%Y\")\n\n\n\n\n\n\n\n\nO próximo exemplo mostra como exibir o número do ano junto com a nome abreviado do mês. Note como o uso de \\n quebra a linha no eixo-x.\n\nprices_austin &lt;- txhousing %&gt;%\n  filter(city == \"Austin\", year %in% 2007:2011) %&gt;%\n  mutate(date = lubridate::make_date(year, month))\n\nggplot(prices_austin, aes(x = date, y = sales)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y\\n%b\")\n\n\n\n\n\n\n\n\nPara escolher datas específicas, usa-se o argumento convencional breaks.\n\ndatas &lt;- c(as.Date(\"2007-01-01\"), as.Date(\"2008-07-30\"), as.Date(\"2010-03-01\"))\n\nggplot(prices_austin, aes(x = date, y = sales)) +\n  geom_line() +\n  scale_x_date(breaks = datas, date_labels = \"%Y\\n%b\")\n\n\n\n\n\n\n\n\nO exemplo abaixo pula alguns passos, já que ainda não se apresentou formalmente a função theme. Essencialmente, a função theme diminui o número de linhas verticais no fundo do gráfico e gira o texto do eixo-x em 90 graus.\n\nprices_austin10 &lt;- txhousing %&gt;%\n  filter(city == \"Austin\", year %in% 2010:2011) %&gt;%\n  mutate(date = lubridate::make_date(year, month))\n\nggplot(prices_austin10, aes(x = date, y = sales)) +\n  geom_line() +\n  scale_x_date(\n    breaks = seq(as.Date(\"2010-01-01\"), as.Date(\"2011-12-01\"), by = \"month\"),\n    date_labels = \"%Y-%m\"\n    ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\n\n\n\n\nA tabela abaixo resume como funciona a codificação dos padrões de datas. Vale notar que os inputs %b e %B usam o nome dos meses definidos ou pelo sistema operacional ou pelo padrão de locale do R. O mesmo vale para %A e outras que imprimem algum texto.\nPara consultar o padrão do seu computador veja Sys.getlocale(). Para trocar o padrão usa-se Sys.setlocale(), mas recomendo evitar este tipo de comando a não ser que você saiba o que está fazendo. É importante reforçar que não basta somente definir um date_labels apropriado, é preciso também que o dado esteja no formato correto.\n\nCódigos de datas\n\n\nInput\nLabel\n\n\n\n\n%Y\nAno completo (e.g. 2010) (0000-9999)\n\n\n%y\nAno abreviação (e.g. 10) (00-99)\n\n\n%m\nMês em número (e.g. 11) (01-12)\n\n\n%b\nMês abreviação (e.g. Jan) (Jan-Dec)\n\n\n%B\nMês completo (e.g. January) (January-December)\n\n\n%d\nDia do mês (e.g. 02) (01-31)\n\n\n%a\nDia da semana, abreviação (Mon-Sun)\n\n\n%A\nDia da semana, completo (Monday-Sunday)\n\n\n%I\nHora, no padrão 12 horas (01-12)\n\n\n%H\nHora, no padrão 24 horas (00-23)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#cores-o-básico",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#cores-o-básico",
    "title": "Estético: Escalas e Cores",
    "section": "Cores: o básico",
    "text": "Cores: o básico\n\nVariáveis discretas\nNovamente, vamos começar com um exemplo simples. O código abaixo seleciona algumas Zonas OD e ranqueia elas segundo a variável prop_educ_superior que é o percentual de indivíduos (naquela Zona) com ensino superior.\n\nzonas &lt;- c(\"Vila Mariana\", \"Paraíso\", \"Saúde\", \"Jabaquara\", \"Grajaú\")\n\nsubpod &lt;- pod %&gt;%\n  filter(name_zone %in% zonas) %&gt;%\n  mutate(\n    name_zone = factor(name_zone),\n    name_zone = fct_reorder(name_zone, prop_educ_superior)\n    )\n\nUsando o argumento aes sabemos que é possível mapear uma cor diferente para cada uma das regiões como no gráfico abaixo.\n\nggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone))\n\n\n\n\n\n\n\n\nPara trocar as cores usamos a função scale_fill_manual. Note que uso a função scale_fill_* pois quero trocar as cores do fill. Vale notar que, neste caso, a função scale_fill_discrete atingiria o mesmo resultado.\n\ncores &lt;- c(\"#C0D1B6\", \"#A3C9A8\", \"#84B59F\", \"#69A297\", \"#50808E\")\n\nggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) +\n  scale_fill_manual(values = cores)\n\n\n\n\n\n\n\n\nNote que os mesmos argumentos name, labels, etc. continuam valendo.\n\nggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) +\n  scale_fill_manual(\n    name = \"ZONAS\",\n    values = cores,\n    labels = toupper\n    )\n\n\n\n\n\n\n\n\nPara controlar os detalhes da legenda de cores usa-se a função guide em conjunto com a função guide_legend. Os exemplos abaixo mostram algumas das customizações possíveis. Para mais detalhes vale conferir a página de ajuda da função guide_legend.\n\nbase_plot &lt;- ggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) + \n  scale_fill_manual(name = \"Zona\", values = cores) +\n  labs(x = NULL, y = NULL) +\n  scale_x_discrete(labels = c(\"GRA\", \"JAB\", \"SAU\", \"PAR\", \"VMR\"))\n\n#&gt; Remove a legenda\nbase_plot + guides(fill = \"none\")\n#&gt; Posiciona o título da legenda em baixo\nbase_plot + guides(fill = guide_legend(title.position = \"bottom\"))\n#&gt; Centraliza o texto da legenda\nbase_plot + guides(fill = guide_legend(label.hjust = 0.5))\n#&gt; Muda a disposição da legenda\nbase_plot + guides(fill = guide_legend(nrow = 1, label.position = \"top\"))\n\n\n\n\n\n\n\n\n\n\nComo mencionado acima, pode-se usar funções com cores pré-definidas. Abaixo mostro alguns exemplos.\n\nbase_plot &lt;- ggplot(subpod, aes(x = name_zone, y = prop_educ_superior)) +\n  geom_col(aes(fill = name_zone)) +\n  labs(x = NULL, y = NULL) +\n  scale_x_discrete(labels = c(\"GRA\", \"JAB\", \"SAU\", \"PAR\", \"VMR\"))\n\n#&gt; Usando ColorBrewer\nbase_plot + scale_fill_brewer(type = \"qual\", palette = 4)\n#&gt; Escala de cinza\nbase_plot + scale_fill_grey()\n#&gt; Usando Ghibli, cores do filme Princesa Mononoke\nbase_plot + ghibli::scale_fill_ghibli_d(\"MononokeMedium\")\n#&gt; Usando MetBrewer, cores do artista Hokusai\nbase_plot + MetBrewer::scale_fill_met_d(\"Hokusai1\")\n\n\n\n\n\n\n\n\n\n\nPor fim, mostro um exemplo onde as cores podem tanto ajudar a enxergar tendências de longo prazo como também algum tipo de padrão sazonal nos dados. O gráfico de linha abaixo mostra o preço mediano de venda de imóveis na cidade de Austin a cada mês durante o período 2000-2014.\nRepare no uso da função scale_x_continuous para mostrar o nome abreviado de cada mês no eixo-x e o uso de as.factor para garantir que o R interprete a variável year como categórica e não como contínua.\n\naustin &lt;- filter(txhousing, city == \"Austin\", year &lt; 2015)\n\nggplot(austin, aes(x = month, y = median)) +\n  geom_line(aes(color = as.factor(year))) +\n  scale_x_continuous(breaks = 1:12, labels = month.abb) +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\nVariáveis contínuas\nA mesma lógica apresentada acima se aplica a variáveis contínuas. Infelizmente, algumas das aplicações mais úteis de escalas de cores contínuas envolvem tipos de gráficos que ainda não vimos, como geom_tile() e geom_sf().\nO exemplo abaixo mostra a relação entre renda e escolaridade, olhando especificamente para a taxa de indivíduos com ensino superior. Note o uso da função scale_y_log10.\nA função scale_color_continuous não é particularmente útil pois, ao contrário do caso discreto, em que podíamos escolher as cores manualmente usando a função scale_color_discrete ou scale_color_manual, não existe uma maneira de “escolher” as cores no caso contínuo.\nO problema acontece porque teríamos que escolher “infinitas” cores, ou melhor, todo um gradiente de cores. Para definir um gradiente de cores com facilidade, usamos a função scale_color_gradient. Esta função define um gradiente a partir dos argumentos low e high.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  scale_color_gradient(low = \"blue\", high = \"orange\")\n\n\n\n\n\n\n\n\nPara construir um gradiente “divergente”, que destaca valores longe da média, por exemplo, pode-se usar scale_color_gradient2. O exemplo abaixo usa a função scale para “normalizar” a densidade populacional.\nO gráfico agora destaca em azul/roxo as Zonas com alta densidade populacional e em vermelho as Zonas com baixa densidade populacional; as Zonas com densidade próximas da média ficam em branco.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = scale(pop_density))) +\n  scale_y_log10() +\n  scale_color_gradient2(limits = c(-4, 4))\n\n\n\n\n\n\n\n\nUma função muito útil para trabalhar este tipo de dado é a scale_*_distiller(), que cria gradientes de cores a partir das paletas de cores do ColorBrewer, que vimos anteriormente. A cor de cada ponto no gráfico abaixo ilustra a densidade populacional de cada Zona em que os pontos mais escuros são as Zonas mais densas.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  scale_color_distiller(palette = 3, direction = 1)\n\n\n\n\n\n\n\n\nA principal vantagem de trabalhar com esta função é a facilidade em testar e gerar boas escalas de cores. Os exemplos abaixo mostram algumas das aplicações. Para conhecer mais vale consultar ?scale_color_distiller.\n\nbase_plot &lt;- ggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  guides(color = \"none\")\n\n#&gt; Seleciona uma paleta de cores usando número e tipo\nbase_plot + scale_color_distiller(palette = 1, type = \"sequential\")\n#&gt; Seleciona uma paleta de cores usando uma abreviação (BluesGreen)\nbase_plot + scale_color_distiller(palette = \"BuGn\")\n#&gt; Seleciona uma paleta de cores usando uma abreviação (YellowOrangeRed)\nbase_plot + scale_color_distiller(palette = \"YlOrRd\")\n#&gt; Seleciona uma paleta de cores e inverte a direção\nbase_plot + scale_color_distiller(palette = 1, direction = 1)\n\n\n\n\n\n\n\n\n\n\nNovamente, os argumentos das funções scale_* são compartilhados. No exemplo abaixo, mostro como usar breaks dentro de scale_color_distiller.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density)) +\n  scale_y_log10() +\n  scale_color_distiller(\n    name = \"Dens. Pop\",\n    breaks = seq(50, 400, 50),\n    palette = 3,\n    direction = 1\n    )\n\n\n\n\n\n\n\n\nA função scale_*_binned permite uma visualização um pouco mais simplificada de dados contínuos ao agrupar eles em grupos. Esta função não permite customizar a escolha dos grupos, mas, em geral, ela funciona muito bem. Em casos mais complexos, vale mais a pena agrupar os dados antes de visualizá-los, e então tratá-los como dados discretos. Outra opção é usar uma função que permita maior controle como scale_color_steps ou scale_color_gradient.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density), alpha = 0.7) +\n  scale_y_log10() +\n  scale_color_binned(type = \"viridis\")\n\n\n\n\n\n\n\n\nA função scale_color_binned tem apenas duas opções de cores: \"gradient\" e \"viridis\". Contudo, ela também aceita escalas customizadas de outras funções. No exemplo abaixo mostro como utilizar uma escala do MetBrewer.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density), alpha = 0.7) +\n  scale_y_log10() +\n  scale_color_binned(type = \\(x) MetBrewer::scale_color_met_c(name = \"VanGogh3\"))\n\n\n\n\n\n\n\n\nPor fim, para controlar os detalhes da legenda, novamente usa-se a função guides mas agora em conjunto com guide_colorbar.\n\nggplot(pod, aes(x = prop_educ_superior, y = income_avg)) +\n  geom_point(aes(color = pop_density), alpha = 0.7) +\n  scale_y_log10() +\n  scale_color_binned(type = \"viridis\") +\n  guides(color = guide_colorbar(reverse = TRUE))\n\n\n\n\n\n\n\n\nO {ggplot2} é um pacote muito flexível, oferece todo tipo de visualização imaginável. Como resultado, podemos (como já vimos em vários casos) gerar todo tipo de gráfico sem sentido. O exemplo abaixo mostra novamente a série da taxa de poupança, mas agora a cor da linha é proporcional à taxa de desemprego naquele mês.\n\nggplot(economics, aes(x = date, y = psavert)) +\n  geom_line(aes(color = unemploy/pop)) +\n  scale_color_viridis_c()"
  },
  {
    "objectID": "posts/ggplot2-tutorial/6-scales-labels.html#footnotes",
    "href": "posts/ggplot2-tutorial/6-scales-labels.html#footnotes",
    "title": "Estético: Escalas e Cores",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVale lembrar, que a cidade de São Paulo não possui uma divisão oficial de bairros, apenas de distritos e subprefeituras.↩︎\nExiste também a função scale_y_date mas datas quase sempre são apresentadas no eixo-x.↩︎\nNão conheço boas referências sobre teoria de cores em português. Atualmente, o ChatGPT pode fornecer boas paletas de cores: “Quero uma paleta de cores profissional com tons de laranja. Esta paleta de cores será utilizada dentro do R. Quero o retorno em vetores de tamanhos de 3 a 8 elementos com cores em formato hexadecimal. Retorne os vetores numa lista.”.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Aqueles que acompanham as venturas da capital gaúcha bem devem saber da polêmica Avenida Castelo Branco, que dá entrada à cidade, para quem vem da Região Metropolitana. Em 2014, a câmara de vereadores de Porto Alegre aprovou a mudança da Av. Castelo Branco para Av. da Legalidade e da Democracia. A Avenida da Legalidade, como ficou conhecida, durou pouco tempo: em 2018 a justiça considerou inválida a lei que alterou o nome da avenida e ela voltou a se chamar Castelo Branco, agora Av. Presidente Castelo Branco, como se querendo debochar da mudança anterior.\nNa minha vivência na cidade, sempre reparei na quantidade de ruas e avenidas que homenageavam não somente os presidentes da ditadura militar, mas membros do exército em geral. A principal via do boêmio bairro da Cidade Baixa chama-se Rua General Lima e Silva; a avenida que liga a Protásio com a 24, Av. Coronel Lucas de Oliveira; um longo trecho da terceira perimetral, Av. Coronel Aparício Borges; no querido Bom Fim, General João Telles; no Centro Histórico, Gen. Vitorino, Gen. Andrade Neves, Gen. Câmara, Gen. Salustiano, Cel. Fernando Machado e tantos outros.\nParti então para a empreitada de classificar todas as ruas da cidade e entender quais os nomes que formam os logradouros de Porto Alegre. O resultado está no mapa abaixo. Para ver em mais detalhes, selecione “Abrir imagem em nova aba” e use o zoom.\n\n\n\n\n\n\n\n\nA tarefa foi mais difícil do que esperava. De imediato, nota-se que a maior parte das ruas têm nome de pessoas, cerca de 65%. O restante das ruas ou são “coisas”, como Av. Icaraí, Av. Ipiranga, etc. ou são ruas sem nome como Beco A, Rua 1, etc. Estas ruas sem nome concentram-se ou em aglomerados subnormais (favelas) ou em condomínios fechados. Por fim, temos ruas que têm nomes de outros lugares, como a Av. Nova York, Rua Europa, etc.\nO exército realmente tem muitas ruas, em torno de 185, ou 3% do total; é mais do que o nome de ruas que têm os políticos (35) e até mesmo do que as figuras religiosas que têm 155. Há menos ruas com nomes de políticos do que eu esperava; as poucas ruas, contudo, costumam ser bastante importantes, como a Av. João Pessoa, Av. Getúlio Vargas e Av. Protásio Alves. Nossos padres, freis e papas têm muitas ruas, mas a maioria delas são pequenas; a exceção é a Av. Padre Cacique, na Zona Sul.\n\n\n\n\n\nComo mencionei acima, o Centro Histórico concentra muitas ruas com nomes de generais, coroneis e marechais.\n\n\n\n\n\nAs ruas sem nome são bastante aglomeradas espacialmente. Na sua maioria, estas ruas são de condomínios fechados ou de aglomerados subnormais. A foto abaixo mostra a esquina da Rua C com a Rua Oito, no bairro Bom Jesus. As ruas em volta seguem o mesmo padrão: Rua 11, Rua Doze, Rua P, etc.\n\n\n\n\n\nAlgumas partes da cidade parecem temáticas. Na Zona Norte, por exemplo, no Bairro São Geraldo, há um enorme número de ruas contíguas com nomes de locais: Av. Pará, Av. Amazonas, Av. Bahia, Av. Ceará, Av. França, Av. Madrid, etc. No Partenon, a Rua Chile faz esquina com a R. Valparaíso e é paralela com a R. Buenos Aires.\n\n\n\n\n\nOs nomes de personalidades históricas estão espalhados pela cidade e não parece haver um padrão. Os escritores Machado de Assis, Graciliano Ramos, Eça de Queiroz, Olavo Bilac e outros estão cada um em um canto. Já Raimundo Correa, Alfonso Celso e Vicente de Carvalho (todos poetas) estão reunidos no entorno da Praça Japão; durante um trecho, também, Castro Alves e Casemiro de Abreu são paralelos. A Av. Érico Veríssimo, na sua longa extensão, faz esquina com a Olavo Bilac e eventualmente encontra-se com a Av. José de Alencar.\nMuitas ruas têm nomes de profissões, como R. Doutor Flores ou R. Professor Fitzgerald. A profissão mais comum é a de doutor (cerca de 120 ruas) seguida por professor (cerca de 80 ruas). Também temos bastante engenheiros, como na Av. Engenheiro Alfredo Corrêa Daudt. No meio das ruas temos até uma Av. Economista Nilo Wulff, no Bairro Restinga, que foi um dos criados do Conselho de Economia do Rio Grande do Sul. As ruas com nomes de profissão estão espalhadas por toda a cidade. No Bairro Tristeza, na Zona Sul, há várias delas reunidas, várias ruas com nome de Doutor.\n\n\n\n\n\n\n\n\nClassificar o nome das ruas é uma tarefa nada trivial. Primeiro, porque as categorias que eu me propus a usar não são mututamente excludentes. A Av. Senador Salgado Filho, por exemplo, é nome de um município, refere-se a uma profissão (senador), é nome de um político e, de maneira geral, é nome de uma personalidade histórica.\nA Av. Bento Gonçalves também é um pouco ambígua, afinal é o nome de um município importante; neste caso, contudo, tanto a avenida como a cidade homenageam Bento Gonçalves da Silva, tenente-coronel, líder da Revolução Farroupilha. Além disso, a Lei Ordinária No 1 de março de 1936, deixa evidente que o homenageado era o General. Infelizmente, são poucas as ruas e avenidas que estão propriamente documentadas online. Em alguns casos não há registros de quem era a pessoa homenageada ou há mesmo várias pessoas com nome similar.\nPor fim, algumas ruas são simplesmente difíceis de classificar. A Travessa Azevedo, por exemplo. Quem terá sido este Azevedo? Na dúvida, ficou como personalidade histórica.\nPara a minha surpesa algumas ruas tem nomes duplicados. A BR-290 que liga a cidade ao litoral tem o nome Estrada Marechal Osório no Google Maps, mas tem nome Rodovia Osvaldo Aranha no OpenStreetMaps. Até onde me lembro sempre ouvi as pessoas chamando ela de “freeway”. Não consegui encontrar algum tipo de material oficial na prefeitura para sanar a dúvida.\nNão encontrei nenhum tipo de “dicionário” ou lista com personalidades gaúchas. Sem uma boa lista que junta nomes com descrições fica difícil aplicar algum tipo de metodologia que envolva aprendizado de máquina. A classificação foi feita com uso extensivo de regex; a revisão foi na base de tentativa e erro, pois há incontáveis exceções a todo tipo de regra. Na minha classificação as categorias “coringa” são “personalidade histórica” e “coisa”. Até por isso, estas são as duas categorias com maior chance de estar superestimadas.\n\n\n\nComo sempre, o código para fazer o mapa segue abaixo:\n\n\nCode\n# Classificar o nome das ruas de Poa #\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggtext)\nsysfonts::font_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext::showtext_auto()\n\n# Import Data -------------------------------------------------------------\n\n## geobr -------------------------------------------------------------------\n\n# City border\npoa &lt;- geobr::read_municipality(4314902)\n\n## osmdata -----------------------------------------------------------------\n\n# Define bbox\nbbox &lt;- getbb(\"Porto Alegre Brazil\")\n# Base query\nqr &lt;- opq(bbox)\n\n# Add feature requests to query\n\n# All roads\nqr_roads &lt;- add_osm_feature(qr, key = \"highway\")\n\n# Only big roads\nqr_big_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\")\n)\n\n# Only medium roads\nqr_med_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\")\n)\n\n# Only small roads\nqr_small_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"residential\", \"living_street\", \"unclassified\", \"service\",\n            \"footway\")\n)\n\n# Download\nroads &lt;- osmdata_sf(q = qr_roads)\nroads &lt;- roads$osm_lines\nroads &lt;- st_transform(roads, crs = 4674)\nroads &lt;- st_join(roads, poa)\nroads &lt;- filter(roads, !is.na(code_muni))\n\n\nbig_streets &lt;- osmdata_sf(q = qr_big_streets)\nmed_streets &lt;- osmdata_sf(q = qr_med_streets)\nsmall_streets &lt;- osmdata_sf(q = qr_small_streets)\n\n#&gt; Get street names\nsnames &lt;- roads$name\n\n#&gt; Remove duplicate names and missing values\nsnames &lt;- unique(snames)\nsnames &lt;- na.omit(snames)\n\n# Convert to tibble\ndictionary &lt;- tibble(\n  street_name = snames\n)\n\n# Streets - classify ------------------------------------------------------\n\n#&gt; Group into categories: historical personalities, names of other cities or \n#&gt; states, jobs, holiday, religious figure, army, nameless\n\n#&gt; Get names of all cities and states in Brazil\nmuni &lt;- sidrar::get_sidra(4709, geo = \"City\", variable = 93)\n\nname_muni &lt;- muni |&gt; \n  janitor::clean_names() |&gt; \n  filter(valor &gt; 1e5) |&gt; \n  tidyr::separate(municipio, into = c(\"name_muni\", \"abb\"), sep = \" - \") |&gt; \n  pull(name_muni) |&gt; \n  unique()\n \nstate &lt;- geobr::read_state()\nstate_name &lt;- state$name_state\n\nstate_lower &lt;- c(\n  \"Rio Grande do Sul\", \"Rio de Janeiro\", \"Rio Grande do Norte\",\n  \"Mato Grosso do Sul\"\n  )\n\nstate_name &lt;- c(state_name, state_lower)\n\ngeonames &lt;- c(name_muni, state_name)\ngeostring &lt;- paste(glue::glue(\"({geonames})\"), collapse = \"|\")\n\n#&gt; Common titles for army position, liberal professions, and religious\n#&gt; personalities\n\nposto_exercito &lt;- c(\n  \"Marechal\", \"Almirante\", \"General\", \"Comandante\", \"Coronel\", \"Cabo\",\n  \"Capitão\", \"Brigadeiro\", \"Tenente\", \"(Castello Branco)\",\n  \"(Costa e Silva)\", \"(Ernesto Geisel)\", \"PM\", \"Major\"\n  )\n\n#&gt; Common prefixes for job titles\nprofissao &lt;- c(\n  \"Engenheir\", \"Doutor\", \"Profess\", \"Desembargador\", \"Economista\", \"Jornalista\", \"Escrivão\", \"Contabilista\"\n  )\n\n#&gt; Common names for religious figures\nsantidade &lt;- c(\"Frei\", \"Santo\", \"Santa\", \"São\", \"Padre\", \"Papa\", \"Reverendo\")\n\npolitico &lt;- c(\"Vereador\", \"Deputado\", \"Governador\", \"Senador\", \"Presidente\")\n\n#&gt; Collapse strings\nposto_exercito &lt;- paste(posto_exercito, collapse = \"|\")\nprofissao &lt;- paste(profissao, collapse = \"|\")\nsantidade &lt;- paste(paste0(santidade, \" \"), collapse = \"|\")\npolitico &lt;- paste(politico, collapse = \"|\")\n\n# Proxy for closed condominiums / favelas\n\ncardinais &lt;- c(\n  \"Um\", \"Dois\", \"Três\", \"Quatro\", \"Cinco\", \"Seis\", \"Sete\", \"Oito\",\n  \"Nove\", \"Dez\", \"Onze\", \"Doze\", \"Treze\", \"Catorze\", \"Quatorze\",\n  \"Quinze\", \"Dezesseis\", \"Dezesete\", \"Dezoito\", \"Dezenove\", \"Vinte\",\n  \"Vinte e Um\", \"Vinte e Dois\", \"Vinte e Três\", \"Vinte e Quatro\",\n  \"Vinte e Cinco\", \"Vinte e Seis\", \"Vinte e Sete\", \"Vinte e Oito\",\n  \"Vinte e Nove\", \"Trinta\", \"Quarenta\", \"Cinquenta\", \"Sessenta\",\n  \"Setenta\", \"Oitenta\", \"Noventa\", \"Cem\"\n  )\n\ncardinais &lt;- paste(paste0(\"(Rua \", cardinais, \")\"), collapse = \"|\")\n\n#&gt; \"nameless\" streets\nname_vias &lt;- c(\n  \"Alameda\", \"Avenida\", \"Acesso\", \"Beco\", \"Caminho\", \"Passagem\", \"Via\", \"Viela\"\n)\n\nrx_vias &lt;- paste(name_vias, \"[A-Z0-9]+[A-Z]?$\")\n\nrua_sem_nome &lt;- c(\"[0-9].+\", rx_vias, cardinais)\n\nrua_sem_nome &lt;- paste(glue::glue(\"({rua_sem_nome})\"), collapse = \"|\")\n\nrua_sem_nome &lt;- paste(rua_sem_nome, cardinais, sep = \"|\")\n\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = case_when(\n      str_count(street_name, \"\\\\w+\") &gt; 2 ~ \"Personalidade Histórica\",\n      str_count(street_name, \"\\\\w+\") &lt;= 2 ~ \"Coisa\",\n      TRUE ~ \"Outro\"\n    ),\n    class = case_when(\n      str_detect(street_name, geostring) ~ \"Nome de Cidade/UF\",\n      str_detect(street_name, politico) ~ \"Político\",\n      str_detect(street_name, posto_exercito) ~ \"Exército\",\n      str_detect(street_name, profissao) ~ \"Profissão\",\n      str_detect(street_name, santidade) ~ \"Figura Religiosa\",\n      str_detect(street_name, \"[0-9] de\") ~ \"Feriado\",\n      str_detect(street_name, rua_sem_nome) ~ \"Rua sem nome\",\n      TRUE ~ class\n    )\n  )\n\npers &lt;- c(\n  \"Carlos Gomes\", \"Protásio Alves\", \"Salgado Filho\", \"Mário Tavares Haussen\",\n  \"Donário Braga\", \"Joracy Camargo\", \"Goethe\", \"Mozart\", \"Schiller\",\n  \"Edgar Pires de Castro\", \"Plínio Brasil Milano\", \"Santos Dumont\"\n  )\n\nexercito &lt;- c(\n  \"Bento Gonçalves\", \"Luís Carlos Prestes\", \"Presidente Castello Branco\",\n  \"João Antônio da Silveira\"\n  )\n\npoliticos &lt;- c(\n  \"Getúlio Vargas\", \"João Pessoa\", \"Protásio Alves\", \"Loureiro da Silva\"\n  )\n\ncoisas &lt;- c(\n  \"Azenha\", \"Rua da Conceição\", \"Túnel da Conceição\", \"Elevada da Conceição\",\n  \"Viaduto da Conceição\", \"Ipiranga\", \"Beira Rio\", \"Rio Jacuí\", \" Banco\",\n  \"Lago das \", \"Lago do\", \"Rio dos Frades\", \"Rio Pardo\", \"Rio Claro\",\n  \"Rio dos Sinos\", \"Rio Negro\", \"Rio Grande\", \"Rio Tejo\", \"Rio Maria\",\n  \"Rio Verde\", \"Rio Solimoes\", \"Rio Xingu\", \"Rio Tapajos\", \"Avenida da Cavalhada\",\n  \"Estrada do Varejão\", \"Estrada da Taquara\", \"Sarandi\"\n  )\n\nlocais &lt;- c(\n  \"Chicago\", \"Madri\", \"Nova York\", \"Nova Zelândia\", \"Quito\", \"Brasil\",\n  \"Europa\", \"Estados Unidos\", \"Japão\", \"Itália\", \"Polônia\", \"Caracas\",\n  \"Buenos Aires\"\n)\n\npers &lt;- paste(glue::glue(\"({pers})\"), collapse = \"|\")\ncoisas &lt;- paste(glue::glue(\"({coisas})\"), collapse = \"|\")\nlocais &lt;- paste(glue::glue(\"({locais})\"), collapse = \"|\")\npoliticos &lt;- paste(glue::glue(\"({politicos})\"), collapse = \"|\")\nexercito &lt;- paste(glue::glue(\"({exercito})\"), collapse = \"|\")\n\n# Ajustes manuais\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = if_else(str_detect(street_name, pers), \"Personalidade Histórica\", class),\n    class = if_else(str_detect(street_name, coisas), \"Coisa\", class),\n    class = if_else(str_detect(street_name, locais), \"Nome de Cidade/UF\", class),\n    class = if_else(str_detect(street_name, politicos), \"Político\", class),\n    class = if_else(str_detect(street_name, exercito), \"Exército\", class)\n  )\n\ndictionary |&gt; \n  count(class, sort = TRUE) |&gt; \n  mutate(share = n / sum(n) * 100)\n\n# Streets ---------------------------------------------------------------\n\ns1 &lt;- big_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns2 &lt;- med_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns3 &lt;- small_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\n# Plot ------------------------------------------------------------------\ncores &lt;- c(\n  \"coisa\" = \"#33a02c\",\n  \"exercito\" = \"#b15928\",\n  \"feriado\" = \"#fb9a99\",\n  \"religioso\" = \"#e41a1c\",\n  \"nome_cidade\" = \"#1f77b4\",\n  \"personalidade\" = \"#ff7f00\",\n  \"politico\" = \"#a6cee3\",\n  \"profissao\" = \"#984ea3\",\n  \"sem_nome\" = \"#e78ac3\"\n  )\n\nbg_color &lt;- \"#F5F5F5\"\n\np1 &lt;- ggplot() +\n  geom_sf(\n    data = filter(s1, !is.na(class)),\n    aes(color = class),\n    linewidth = 0.8,\n    key_glyph = draw_key_rect\n  ) +\n  geom_sf(\n    data = filter(s2, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.35\n  ) +\n  geom_sf(\n    data = filter(s3, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.15\n  ) +\n  coord_sf(\n    ylim = c(-30.124, -29.9691),\n    xlim = c(-51.265, -51.135)\n  ) +\n  scale_colour_manual(name = \"\", values = unname(cores)) +\n  labs(\n    title = \"**Origem do Nome de Ruas em Porto Alegre**\",\n    caption = \"Fonte: OSM. Cores: ColorBrewer. Autor: @viniciusoike\"\n  ) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = bg_color, colour = bg_color),\n    panel.background = element_rect(fill = bg_color, colour = bg_color),\n    #&gt; Plot Margin\n    plot.margin = margin(t = 1.5, r = 0.5, l = 0.5, b = 1, unit = \"cm\"),\n    #&gt; Textual elements\n    text = element_text(family = \"Gill Sans\", size = 8),\n    plot.title = element_markdown(\n      family = \"Gill Sans\",\n      size = 20,\n      hjust = 0.5\n    ),\n    panel.border = element_rect(colour = \"gray20\", fill = \"transparent\"),\n    #&gt; Legend \n    legend.position = c(0.16, 0.8),\n    #legend.position = \"right\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.box.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.text = element_text(size = 8),\n    legend.margin = margin(t = 0.5, b = 0.5, unit = \"cm\")\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#uma-avenida-polêmica",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#uma-avenida-polêmica",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Aqueles que acompanham as venturas da capital gaúcha bem devem saber da polêmica Avenida Castelo Branco, que dá entrada à cidade, para quem vem da Região Metropolitana. Em 2014, a câmara de vereadores de Porto Alegre aprovou a mudança da Av. Castelo Branco para Av. da Legalidade e da Democracia. A Avenida da Legalidade, como ficou conhecida, durou pouco tempo: em 2018 a justiça considerou inválida a lei que alterou o nome da avenida e ela voltou a se chamar Castelo Branco, agora Av. Presidente Castelo Branco, como se querendo debochar da mudança anterior.\nNa minha vivência na cidade, sempre reparei na quantidade de ruas e avenidas que homenageavam não somente os presidentes da ditadura militar, mas membros do exército em geral. A principal via do boêmio bairro da Cidade Baixa chama-se Rua General Lima e Silva; a avenida que liga a Protásio com a 24, Av. Coronel Lucas de Oliveira; um longo trecho da terceira perimetral, Av. Coronel Aparício Borges; no querido Bom Fim, General João Telles; no Centro Histórico, Gen. Vitorino, Gen. Andrade Neves, Gen. Câmara, Gen. Salustiano, Cel. Fernando Machado e tantos outros.\nParti então para a empreitada de classificar todas as ruas da cidade e entender quais os nomes que formam os logradouros de Porto Alegre. O resultado está no mapa abaixo. Para ver em mais detalhes, selecione “Abrir imagem em nova aba” e use o zoom."
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-o-mapa",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-o-mapa",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "A tarefa foi mais difícil do que esperava. De imediato, nota-se que a maior parte das ruas têm nome de pessoas, cerca de 65%. O restante das ruas ou são “coisas”, como Av. Icaraí, Av. Ipiranga, etc. ou são ruas sem nome como Beco A, Rua 1, etc. Estas ruas sem nome concentram-se ou em aglomerados subnormais (favelas) ou em condomínios fechados. Por fim, temos ruas que têm nomes de outros lugares, como a Av. Nova York, Rua Europa, etc.\nO exército realmente tem muitas ruas, em torno de 185, ou 3% do total; é mais do que o nome de ruas que têm os políticos (35) e até mesmo do que as figuras religiosas que têm 155. Há menos ruas com nomes de políticos do que eu esperava; as poucas ruas, contudo, costumam ser bastante importantes, como a Av. João Pessoa, Av. Getúlio Vargas e Av. Protásio Alves. Nossos padres, freis e papas têm muitas ruas, mas a maioria delas são pequenas; a exceção é a Av. Padre Cacique, na Zona Sul.\n\n\n\n\n\nComo mencionei acima, o Centro Histórico concentra muitas ruas com nomes de generais, coroneis e marechais.\n\n\n\n\n\nAs ruas sem nome são bastante aglomeradas espacialmente. Na sua maioria, estas ruas são de condomínios fechados ou de aglomerados subnormais. A foto abaixo mostra a esquina da Rua C com a Rua Oito, no bairro Bom Jesus. As ruas em volta seguem o mesmo padrão: Rua 11, Rua Doze, Rua P, etc.\n\n\n\n\n\nAlgumas partes da cidade parecem temáticas. Na Zona Norte, por exemplo, no Bairro São Geraldo, há um enorme número de ruas contíguas com nomes de locais: Av. Pará, Av. Amazonas, Av. Bahia, Av. Ceará, Av. França, Av. Madrid, etc. No Partenon, a Rua Chile faz esquina com a R. Valparaíso e é paralela com a R. Buenos Aires.\n\n\n\n\n\nOs nomes de personalidades históricas estão espalhados pela cidade e não parece haver um padrão. Os escritores Machado de Assis, Graciliano Ramos, Eça de Queiroz, Olavo Bilac e outros estão cada um em um canto. Já Raimundo Correa, Alfonso Celso e Vicente de Carvalho (todos poetas) estão reunidos no entorno da Praça Japão; durante um trecho, também, Castro Alves e Casemiro de Abreu são paralelos. A Av. Érico Veríssimo, na sua longa extensão, faz esquina com a Olavo Bilac e eventualmente encontra-se com a Av. José de Alencar.\nMuitas ruas têm nomes de profissões, como R. Doutor Flores ou R. Professor Fitzgerald. A profissão mais comum é a de doutor (cerca de 120 ruas) seguida por professor (cerca de 80 ruas). Também temos bastante engenheiros, como na Av. Engenheiro Alfredo Corrêa Daudt. No meio das ruas temos até uma Av. Economista Nilo Wulff, no Bairro Restinga, que foi um dos criados do Conselho de Economia do Rio Grande do Sul. As ruas com nomes de profissão estão espalhadas por toda a cidade. No Bairro Tristeza, na Zona Sul, há várias delas reunidas, várias ruas com nome de Doutor."
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-fazer-o-mapa",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#sobre-fazer-o-mapa",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Classificar o nome das ruas é uma tarefa nada trivial. Primeiro, porque as categorias que eu me propus a usar não são mututamente excludentes. A Av. Senador Salgado Filho, por exemplo, é nome de um município, refere-se a uma profissão (senador), é nome de um político e, de maneira geral, é nome de uma personalidade histórica.\nA Av. Bento Gonçalves também é um pouco ambígua, afinal é o nome de um município importante; neste caso, contudo, tanto a avenida como a cidade homenageam Bento Gonçalves da Silva, tenente-coronel, líder da Revolução Farroupilha. Além disso, a Lei Ordinária No 1 de março de 1936, deixa evidente que o homenageado era o General. Infelizmente, são poucas as ruas e avenidas que estão propriamente documentadas online. Em alguns casos não há registros de quem era a pessoa homenageada ou há mesmo várias pessoas com nome similar.\nPor fim, algumas ruas são simplesmente difíceis de classificar. A Travessa Azevedo, por exemplo. Quem terá sido este Azevedo? Na dúvida, ficou como personalidade histórica.\nPara a minha surpesa algumas ruas tem nomes duplicados. A BR-290 que liga a cidade ao litoral tem o nome Estrada Marechal Osório no Google Maps, mas tem nome Rodovia Osvaldo Aranha no OpenStreetMaps. Até onde me lembro sempre ouvi as pessoas chamando ela de “freeway”. Não consegui encontrar algum tipo de material oficial na prefeitura para sanar a dúvida.\nNão encontrei nenhum tipo de “dicionário” ou lista com personalidades gaúchas. Sem uma boa lista que junta nomes com descrições fica difícil aplicar algum tipo de metodologia que envolva aprendizado de máquina. A classificação foi feita com uso extensivo de regex; a revisão foi na base de tentativa e erro, pois há incontáveis exceções a todo tipo de regra. Na minha classificação as categorias “coringa” são “personalidade histórica” e “coisa”. Até por isso, estas são as duas categorias com maior chance de estar superestimadas."
  },
  {
    "objectID": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#código",
    "href": "posts/general-posts/2023-09-ruas-porto-alegre/index.html#código",
    "title": "Weekly Viz: Ruas de Porto Alegre",
    "section": "",
    "text": "Como sempre, o código para fazer o mapa segue abaixo:\n\n\nCode\n# Classificar o nome das ruas de Poa #\n\nlibrary(osmdata)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggtext)\nsysfonts::font_add(\"Gill Sans\", \"GillSans.ttc\")\nshowtext::showtext_auto()\n\n# Import Data -------------------------------------------------------------\n\n## geobr -------------------------------------------------------------------\n\n# City border\npoa &lt;- geobr::read_municipality(4314902)\n\n## osmdata -----------------------------------------------------------------\n\n# Define bbox\nbbox &lt;- getbb(\"Porto Alegre Brazil\")\n# Base query\nqr &lt;- opq(bbox)\n\n# Add feature requests to query\n\n# All roads\nqr_roads &lt;- add_osm_feature(qr, key = \"highway\")\n\n# Only big roads\nqr_big_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\")\n)\n\n# Only medium roads\nqr_med_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\")\n)\n\n# Only small roads\nqr_small_streets &lt;- add_osm_feature(\n  qr,\n  key = \"highway\",\n  value = c(\"residential\", \"living_street\", \"unclassified\", \"service\",\n            \"footway\")\n)\n\n# Download\nroads &lt;- osmdata_sf(q = qr_roads)\nroads &lt;- roads$osm_lines\nroads &lt;- st_transform(roads, crs = 4674)\nroads &lt;- st_join(roads, poa)\nroads &lt;- filter(roads, !is.na(code_muni))\n\n\nbig_streets &lt;- osmdata_sf(q = qr_big_streets)\nmed_streets &lt;- osmdata_sf(q = qr_med_streets)\nsmall_streets &lt;- osmdata_sf(q = qr_small_streets)\n\n#&gt; Get street names\nsnames &lt;- roads$name\n\n#&gt; Remove duplicate names and missing values\nsnames &lt;- unique(snames)\nsnames &lt;- na.omit(snames)\n\n# Convert to tibble\ndictionary &lt;- tibble(\n  street_name = snames\n)\n\n# Streets - classify ------------------------------------------------------\n\n#&gt; Group into categories: historical personalities, names of other cities or \n#&gt; states, jobs, holiday, religious figure, army, nameless\n\n#&gt; Get names of all cities and states in Brazil\nmuni &lt;- sidrar::get_sidra(4709, geo = \"City\", variable = 93)\n\nname_muni &lt;- muni |&gt; \n  janitor::clean_names() |&gt; \n  filter(valor &gt; 1e5) |&gt; \n  tidyr::separate(municipio, into = c(\"name_muni\", \"abb\"), sep = \" - \") |&gt; \n  pull(name_muni) |&gt; \n  unique()\n \nstate &lt;- geobr::read_state()\nstate_name &lt;- state$name_state\n\nstate_lower &lt;- c(\n  \"Rio Grande do Sul\", \"Rio de Janeiro\", \"Rio Grande do Norte\",\n  \"Mato Grosso do Sul\"\n  )\n\nstate_name &lt;- c(state_name, state_lower)\n\ngeonames &lt;- c(name_muni, state_name)\ngeostring &lt;- paste(glue::glue(\"({geonames})\"), collapse = \"|\")\n\n#&gt; Common titles for army position, liberal professions, and religious\n#&gt; personalities\n\nposto_exercito &lt;- c(\n  \"Marechal\", \"Almirante\", \"General\", \"Comandante\", \"Coronel\", \"Cabo\",\n  \"Capitão\", \"Brigadeiro\", \"Tenente\", \"(Castello Branco)\",\n  \"(Costa e Silva)\", \"(Ernesto Geisel)\", \"PM\", \"Major\"\n  )\n\n#&gt; Common prefixes for job titles\nprofissao &lt;- c(\n  \"Engenheir\", \"Doutor\", \"Profess\", \"Desembargador\", \"Economista\", \"Jornalista\", \"Escrivão\", \"Contabilista\"\n  )\n\n#&gt; Common names for religious figures\nsantidade &lt;- c(\"Frei\", \"Santo\", \"Santa\", \"São\", \"Padre\", \"Papa\", \"Reverendo\")\n\npolitico &lt;- c(\"Vereador\", \"Deputado\", \"Governador\", \"Senador\", \"Presidente\")\n\n#&gt; Collapse strings\nposto_exercito &lt;- paste(posto_exercito, collapse = \"|\")\nprofissao &lt;- paste(profissao, collapse = \"|\")\nsantidade &lt;- paste(paste0(santidade, \" \"), collapse = \"|\")\npolitico &lt;- paste(politico, collapse = \"|\")\n\n# Proxy for closed condominiums / favelas\n\ncardinais &lt;- c(\n  \"Um\", \"Dois\", \"Três\", \"Quatro\", \"Cinco\", \"Seis\", \"Sete\", \"Oito\",\n  \"Nove\", \"Dez\", \"Onze\", \"Doze\", \"Treze\", \"Catorze\", \"Quatorze\",\n  \"Quinze\", \"Dezesseis\", \"Dezesete\", \"Dezoito\", \"Dezenove\", \"Vinte\",\n  \"Vinte e Um\", \"Vinte e Dois\", \"Vinte e Três\", \"Vinte e Quatro\",\n  \"Vinte e Cinco\", \"Vinte e Seis\", \"Vinte e Sete\", \"Vinte e Oito\",\n  \"Vinte e Nove\", \"Trinta\", \"Quarenta\", \"Cinquenta\", \"Sessenta\",\n  \"Setenta\", \"Oitenta\", \"Noventa\", \"Cem\"\n  )\n\ncardinais &lt;- paste(paste0(\"(Rua \", cardinais, \")\"), collapse = \"|\")\n\n#&gt; \"nameless\" streets\nname_vias &lt;- c(\n  \"Alameda\", \"Avenida\", \"Acesso\", \"Beco\", \"Caminho\", \"Passagem\", \"Via\", \"Viela\"\n)\n\nrx_vias &lt;- paste(name_vias, \"[A-Z0-9]+[A-Z]?$\")\n\nrua_sem_nome &lt;- c(\"[0-9].+\", rx_vias, cardinais)\n\nrua_sem_nome &lt;- paste(glue::glue(\"({rua_sem_nome})\"), collapse = \"|\")\n\nrua_sem_nome &lt;- paste(rua_sem_nome, cardinais, sep = \"|\")\n\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = case_when(\n      str_count(street_name, \"\\\\w+\") &gt; 2 ~ \"Personalidade Histórica\",\n      str_count(street_name, \"\\\\w+\") &lt;= 2 ~ \"Coisa\",\n      TRUE ~ \"Outro\"\n    ),\n    class = case_when(\n      str_detect(street_name, geostring) ~ \"Nome de Cidade/UF\",\n      str_detect(street_name, politico) ~ \"Político\",\n      str_detect(street_name, posto_exercito) ~ \"Exército\",\n      str_detect(street_name, profissao) ~ \"Profissão\",\n      str_detect(street_name, santidade) ~ \"Figura Religiosa\",\n      str_detect(street_name, \"[0-9] de\") ~ \"Feriado\",\n      str_detect(street_name, rua_sem_nome) ~ \"Rua sem nome\",\n      TRUE ~ class\n    )\n  )\n\npers &lt;- c(\n  \"Carlos Gomes\", \"Protásio Alves\", \"Salgado Filho\", \"Mário Tavares Haussen\",\n  \"Donário Braga\", \"Joracy Camargo\", \"Goethe\", \"Mozart\", \"Schiller\",\n  \"Edgar Pires de Castro\", \"Plínio Brasil Milano\", \"Santos Dumont\"\n  )\n\nexercito &lt;- c(\n  \"Bento Gonçalves\", \"Luís Carlos Prestes\", \"Presidente Castello Branco\",\n  \"João Antônio da Silveira\"\n  )\n\npoliticos &lt;- c(\n  \"Getúlio Vargas\", \"João Pessoa\", \"Protásio Alves\", \"Loureiro da Silva\"\n  )\n\ncoisas &lt;- c(\n  \"Azenha\", \"Rua da Conceição\", \"Túnel da Conceição\", \"Elevada da Conceição\",\n  \"Viaduto da Conceição\", \"Ipiranga\", \"Beira Rio\", \"Rio Jacuí\", \" Banco\",\n  \"Lago das \", \"Lago do\", \"Rio dos Frades\", \"Rio Pardo\", \"Rio Claro\",\n  \"Rio dos Sinos\", \"Rio Negro\", \"Rio Grande\", \"Rio Tejo\", \"Rio Maria\",\n  \"Rio Verde\", \"Rio Solimoes\", \"Rio Xingu\", \"Rio Tapajos\", \"Avenida da Cavalhada\",\n  \"Estrada do Varejão\", \"Estrada da Taquara\", \"Sarandi\"\n  )\n\nlocais &lt;- c(\n  \"Chicago\", \"Madri\", \"Nova York\", \"Nova Zelândia\", \"Quito\", \"Brasil\",\n  \"Europa\", \"Estados Unidos\", \"Japão\", \"Itália\", \"Polônia\", \"Caracas\",\n  \"Buenos Aires\"\n)\n\npers &lt;- paste(glue::glue(\"({pers})\"), collapse = \"|\")\ncoisas &lt;- paste(glue::glue(\"({coisas})\"), collapse = \"|\")\nlocais &lt;- paste(glue::glue(\"({locais})\"), collapse = \"|\")\npoliticos &lt;- paste(glue::glue(\"({politicos})\"), collapse = \"|\")\nexercito &lt;- paste(glue::glue(\"({exercito})\"), collapse = \"|\")\n\n# Ajustes manuais\ndictionary &lt;- dictionary |&gt;\n  mutate(\n    class = if_else(str_detect(street_name, pers), \"Personalidade Histórica\", class),\n    class = if_else(str_detect(street_name, coisas), \"Coisa\", class),\n    class = if_else(str_detect(street_name, locais), \"Nome de Cidade/UF\", class),\n    class = if_else(str_detect(street_name, politicos), \"Político\", class),\n    class = if_else(str_detect(street_name, exercito), \"Exército\", class)\n  )\n\ndictionary |&gt; \n  count(class, sort = TRUE) |&gt; \n  mutate(share = n / sum(n) * 100)\n\n# Streets ---------------------------------------------------------------\n\ns1 &lt;- big_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns2 &lt;- med_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\ns3 &lt;- small_streets$osm_lines |&gt;\n  st_transform(crs = 4674) |&gt;\n  left_join(dictionary, by = c(\"name\" = \"street_name\"))\n\n# Plot ------------------------------------------------------------------\ncores &lt;- c(\n  \"coisa\" = \"#33a02c\",\n  \"exercito\" = \"#b15928\",\n  \"feriado\" = \"#fb9a99\",\n  \"religioso\" = \"#e41a1c\",\n  \"nome_cidade\" = \"#1f77b4\",\n  \"personalidade\" = \"#ff7f00\",\n  \"politico\" = \"#a6cee3\",\n  \"profissao\" = \"#984ea3\",\n  \"sem_nome\" = \"#e78ac3\"\n  )\n\nbg_color &lt;- \"#F5F5F5\"\n\np1 &lt;- ggplot() +\n  geom_sf(\n    data = filter(s1, !is.na(class)),\n    aes(color = class),\n    linewidth = 0.8,\n    key_glyph = draw_key_rect\n  ) +\n  geom_sf(\n    data = filter(s2, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.35\n  ) +\n  geom_sf(\n    data = filter(s3, !is.na(class)),\n    aes(color = class),\n    key_glyph = draw_key_rect,\n    linewidth = 0.15\n  ) +\n  coord_sf(\n    ylim = c(-30.124, -29.9691),\n    xlim = c(-51.265, -51.135)\n  ) +\n  scale_colour_manual(name = \"\", values = unname(cores)) +\n  labs(\n    title = \"**Origem do Nome de Ruas em Porto Alegre**\",\n    caption = \"Fonte: OSM. Cores: ColorBrewer. Autor: @viniciusoike\"\n  ) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = bg_color, colour = bg_color),\n    panel.background = element_rect(fill = bg_color, colour = bg_color),\n    #&gt; Plot Margin\n    plot.margin = margin(t = 1.5, r = 0.5, l = 0.5, b = 1, unit = \"cm\"),\n    #&gt; Textual elements\n    text = element_text(family = \"Gill Sans\", size = 8),\n    plot.title = element_markdown(\n      family = \"Gill Sans\",\n      size = 20,\n      hjust = 0.5\n    ),\n    panel.border = element_rect(colour = \"gray20\", fill = \"transparent\"),\n    #&gt; Legend \n    legend.position = c(0.16, 0.8),\n    #legend.position = \"right\",\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.box.background = element_rect(fill = bg_color, colour = bg_color),\n    legend.text = element_text(size = 8),\n    legend.margin = margin(t = 0.5, b = 0.5, unit = \"cm\")\n  )"
  },
  {
    "objectID": "press.html",
    "href": "press.html",
    "title": "Imprensa",
    "section": "",
    "text": "Em certas ocasiões, sou convidado para participar de matérias sobre mercado imobiliário na imprensa. Em outros casos, estudos que fiz ganham repercussão nos meios de comunicação. Aqui destaco algumas das publicações mais relevantes.\n\n\n\n[BBC] Nômades digitais e aluguel em dólar: por que moradores estão sendo expulsos de seus bairros na América Latina\n[BBC] Cuánto se han disparado los alquileres en las mayores ciudades de América Latina y qué posibilidades hay de que bajen\n[BBC] O que explica disparada de aluguéis nas maiores cidades da América Latina?\n[BBC] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[CBN] Entenda o movimento de alta dos aluguéis e de queda no valor de compra de imóveis\n[Folha de SP] Aluguel de apartamento com 1 quarto cai em SP, diz pesquisa\n[InvestNews] Aluguel em SP e RJ tem a maior alta nos últimos 3 anos, segundo Quinto Andar\n[Forbes] Aluguéis em SP sobem mais do que a inflação no 1ºtrimestre\n[G1] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[G1] Mercado de imóveis de luxo cresce no Brasil e preços passam dos R$ 40 milhões; veja apartamentos por dentro\n[G1 RS] Microapartamentos: aluguel médio de imóvel com até 30m² em Porto Alegre é de R$ 860\n[Rede Minas TV] A BUSCA POR MICROAPARTAMENTOS É REFLEXO DE UMA NOVA DINÂMICA SOCIAL\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[Veja SP] São Paulo teve aumento de 15,5% no aluguel em 2022\n\n\n\n\n\n[Abecip] Rio é a 6ª capital mais cara para comprar imóvel na América Latina\n[El Economista] CDMX quiere ser el hogar de los nómadas digitales con ayuda de Airbnb\n[Estadão] Preço de Aluguel de Microapartamentos em SP bate recorde após 6a alta seguida\n[Folha de SP] Tamanho mínimo de casa vai de 8 a 25 metros quadrados em capitais brasileiras\n[G1] Comprar imóvel é mais barato no Brasil em relação à América Latina, mas valor ainda pesa no bolso das famílias, diz pesquisa\n[Imobi Report] Em alta ou em queda? Entenda o momento dos microapartamentos\n[Real Estate Market] CDMX y Buenos Aires tienen el alquiler más caro en LATAM\n[R7] Porto Alegre, Curitiba, Salvador e BH são as cidades mais baratas para morar na América Latina\n[Nacion MX] ESTUDIO EN LATAM NOS DA PANORAMA DE REAL ESTATE EN MÉXICO\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[QA] Mercado residencial na América Latina: QuintoAndar faz estudo inédito em 12 cidades de seis países\n[QA] Tendências do mercado imobiliário: o que a economia nos aponta?\n\n\n\n\n\n[Folha de SP] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP, diz pesquisa\n[Abecip] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP\n[G1 SP] Casal com dois filhos e renda mediana em SP só consegue comprar imóvel com mais de 40m² longe do Centro"
  },
  {
    "objectID": "press.html#section",
    "href": "press.html#section",
    "title": "Imprensa",
    "section": "",
    "text": "[BBC] Nômades digitais e aluguel em dólar: por que moradores estão sendo expulsos de seus bairros na América Latina\n[BBC] Cuánto se han disparado los alquileres en las mayores ciudades de América Latina y qué posibilidades hay de que bajen\n[BBC] O que explica disparada de aluguéis nas maiores cidades da América Latina?\n[BBC] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[CBN] Entenda o movimento de alta dos aluguéis e de queda no valor de compra de imóveis\n[Folha de SP] Aluguel de apartamento com 1 quarto cai em SP, diz pesquisa\n[InvestNews] Aluguel em SP e RJ tem a maior alta nos últimos 3 anos, segundo Quinto Andar\n[Forbes] Aluguéis em SP sobem mais do que a inflação no 1ºtrimestre\n[G1] O que explica alta do aluguel residencial acima da inflação e o que esperar em 2023\n[G1] Mercado de imóveis de luxo cresce no Brasil e preços passam dos R$ 40 milhões; veja apartamentos por dentro\n[G1 RS] Microapartamentos: aluguel médio de imóvel com até 30m² em Porto Alegre é de R$ 860\n[Rede Minas TV] A BUSCA POR MICROAPARTAMENTOS É REFLEXO DE UMA NOVA DINÂMICA SOCIAL\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[Veja SP] São Paulo teve aumento de 15,5% no aluguel em 2022"
  },
  {
    "objectID": "press.html#section-1",
    "href": "press.html#section-1",
    "title": "Imprensa",
    "section": "",
    "text": "[Abecip] Rio é a 6ª capital mais cara para comprar imóvel na América Latina\n[El Economista] CDMX quiere ser el hogar de los nómadas digitales con ayuda de Airbnb\n[Estadão] Preço de Aluguel de Microapartamentos em SP bate recorde após 6a alta seguida\n[Folha de SP] Tamanho mínimo de casa vai de 8 a 25 metros quadrados em capitais brasileiras\n[G1] Comprar imóvel é mais barato no Brasil em relação à América Latina, mas valor ainda pesa no bolso das famílias, diz pesquisa\n[Imobi Report] Em alta ou em queda? Entenda o momento dos microapartamentos\n[Real Estate Market] CDMX y Buenos Aires tienen el alquiler más caro en LATAM\n[R7] Porto Alegre, Curitiba, Salvador e BH são as cidades mais baratas para morar na América Latina\n[Nacion MX] ESTUDIO EN LATAM NOS DA PANORAMA DE REAL ESTATE EN MÉXICO\n[Valor] Incorporadoras perdem margem e lucro e veem 2023 difícil\n[QA] Mercado residencial na América Latina: QuintoAndar faz estudo inédito em 12 cidades de seis países\n[QA] Tendências do mercado imobiliário: o que a economia nos aponta?"
  },
  {
    "objectID": "press.html#section-2",
    "href": "press.html#section-2",
    "title": "Imprensa",
    "section": "",
    "text": "[Folha de SP] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP, diz pesquisa\n[Abecip] Apenas os 10% mais ricos podem comprar imóvel acima de R$ 600 mil em SP\n[G1 SP] Casal com dois filhos e renda mediana em SP só consegue comprar imóvel com mais de 40m² longe do Centro"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html",
    "title": "Pipes",
    "section": "",
    "text": "A partir da versão 4.1.0, o R passou a oferecer o operador |&gt; chamado de pipe (literalmente, cano). Este operador foi fortemente inspirado no operador homônimo %&gt;% do popular pacote magrittr. Neste post, explico como utilizar o pipe nativo e como ele difere do pipe do magrittr.\nO operador pipe (em ambos os casos), essencialmente, ordena uma função composta.\nLembrando um pouco sobre funções compostas: a expressão abaixo mostra a aplicação de três funções onde primeiro aplica-se a função f sobre x, depois a função g e, por fim, a função h. Lê-se a função de dentro para fora.\n\\[\nh(g(f(x))) = \\dots\n\\]\nPara tornar o exemplo mais concreto considere o exemplo abaixo onde calcula-se a média geométrica de uma sequência de números aleatórios.\n\nx &lt;- rnorm(n = 100, mean = 10)\n#&gt; Calcula a média geométrica\nexp(log(mean(x)))\n\n[1] 9.933438\n\n\nUsando a mesma notação acima, aplica-se primeiro a função mean (f), depois a função log (g) e, por fim, a função exp (h). Usando o operador pipe, pode-se reescrever a expressão da seguinte forma.\n\nx |&gt; mean() |&gt; log() |&gt; exp()\n\n[1] 9.933438\n\n\nNote que o resultado da função vai sendo “carregado” da esquerda para a direita sucessivamente. Para muitos usuários, a segunda sintaxe é mais intuitiva e/ou fácil de ler. No segundo código a ordem em que o nome das funções aparecem coincide com a ordem da sua aplicação.\nPor fim, note que o uso de várias funções numa mesma linha de código também nos poupa de ter de criar objetos intermediários como no exemplo abaixo.\n\nmedia_x &lt;- mean(x)\nlog_media &lt;- log(media_x)\nmedia_geometrica_x &lt;- exp(log_media)\n\nmedia_geometrica_x\n\n[1] 9.933438\n\n\nOs exemplos acima funcionaram sem problemas porque usou-se o operador pipe para “abrir” uma função composta. O argumento de cada função subsequente é o resultado da função antecedente: funciona como uma linha de montagem, em que cada nova etapa soma-se ao resultado da etapa anterior.\nQuando o resultado da função anterior não vai diretamente no primeiro argumento da função subsequente, precisa-se usar o operador _ (underline). Este operador serve como um placeholder: indica onde que o resultado da etapa anterior deve entrar. No exemplo abaixo, uso o placeholder para colocar a base de dados filtrada no argumento data dentro da função lm.\n\ncarros_4 &lt;- subset(mtcars, cyl == 4)\nfit &lt;- lm(mpg ~ wt, data = carros_4)\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _)\n\nPor fim, temos o caso das funções anônimas. Uma função anônima é simplesmente uma função sem nome que é chamada uma única vez. Infelizmente, a sintaxe de um pipe com uma função anônima é bastante carregada.\n\nobjeto |&gt; (\\(...) {define função})()\n\nO exemplo repete o código acima, mas agora usa uma função anônima para pegar o R2 ajustado da regressão.\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _) |&gt; \n  (\\(x) {summary(x)$adj.r.squared})()\n\n\n\n\nImagine agora que se quer calcular o erro absoluto médio de uma regressão. Lembre-se que o EAM é dado por\n\\[\n\\text{EAM} = \\frac{1}{N}\\sum_{i = 1}^{N}|e_{i}|\n\\]\nonde \\(e_{i}\\) é o resíduo da regressão. O código abaixo mostra como fazer isto usando pipes.\n\n#&gt; Estima uma regressão qualquer\nfit &lt;- lm(mpg ~ wt, data = mtcars)\n\n#&gt; Calcula o erro absoluto médio\nfit |&gt; residuals() |&gt; abs() |&gt; mean()\n\n[1] 2.340642\n\n\nNote, contudo, que a situação fica um pouco mais complicada no caso em que se quer calcular a raiz do erro quadrado médio.\n\\[\n\\text{REQM} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^{N}(e_{i})^2}\n\\]\nNa sintaxe convencional temos\n\nsqrt(mean(residuals(fit)^2))\n\n[1] 2.949163\n\n\nO problema é que a exponenciação acontece via um operador e não uma função. Nenhuma dos exemplos abaixo funciona.\n\nfit |&gt; residuals() |&gt; ^2 |&gt; mean() |&gt; sqrt()\n\nError: &lt;text&gt;:1:23: unexpected '^'\n1: fit |&gt; residuals() |&gt; ^\n                          ^\n\n\n\nfit |&gt; residuals()^2 |&gt; mean() |&gt; sqrt()\n\nError: function '^' not supported in RHS call of a pipe\n\n\nPara chegar no mesmo resultado, novamente precisa-se usar uma sintaxe bastante esotérica que envolve passar o resultado de residuals para uma função anônima.\n\nfit |&gt; residuals() |&gt; (\\(y) {sqrt(mean(y^2))})()\n\n[1] 2.949163\n\n\n\n\n\nAssim, apesar de muito útil, o operador pipe tem suas limitações. O operador sempre espera encontrar uma função à sua direita; a única maneira de seguir |&gt; com um operador é criando uma função anônima, cuja sintaxe é um pouco carregada. Pode-se resumir os principais fatos sobre o operador pipe:\n\nSimplifica funções compostas. Na expressão x |&gt; f |&gt; g o operador |&gt; aplica a função f sobre o objeto x usando x como argumento de f. Depois, aplica a função g sobre o resultado de f(x). Isto é equivalente a g(f(x)).\nEvita a definição de objetos intermediários. O uso de pipes evita que você precise “salvar” cada passo intermediário da aplicação de funções. Isto deixa seu espaço de trabalho mais limpo e também consome menos memória.\nPlaceholder. Quando o objeto anterior não serve como o primeiro argumento da função subsequente, usa-se o placeholder para indicar onde ele deve ser inserido. x |&gt; f(y = 2, data = _).\nFunção anônima. Em casos mais complexos, é necessário montar uma função anônima usando x |&gt; (\\(y) {funcao})().\n\n\n\n\n\n\nO uso mais comum de pipes é junto com funções do tidyverse, que foram desenvolvidas com este intuito.\n\nlibrary(tidyverse)\n\nAs funções do tidyverse (quase) sempre recebem um data.frame como primeiro argumento; isto facilita a construção de código usando pipe, pois basta encadear as funções em sequência.\n\nfiltered_df &lt;- filter(mtcars, wt == 2)\ngrouped_df &lt;- group_by(filtered_df, cyl)\ntbl &lt;- summarise(grouped_df, avg = mean(mpg), count = n())\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg))\n\nA leitura do código fica mais “gramatical”: pegue o objeto mtcars filtre as linhas onde wt &gt; 2 depois agrupe pela variável cyl e, por fim, tire uma média de mpg.\nPode-se terminar um pipe com uma chamada para um plot em ggplot2 para uma rápida visualização dos resultados\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg)) |&gt; \n  ggplot(aes(x = as.factor(cyl), y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNão se recomenda fazer longas sequências de pipes, pois o código pode acabar muito confuso para quem está lendo. O exemplo abaixo mostra justamente isto.\n\nlibrary(realestatebr)\n\nabecip &lt;- get_abecip_indicators(cached = TRUE)\n\nabecip$units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) |&gt; \n  filter(category == \"units\", type != \"total\") |&gt; \n  ggplot(aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nO código abaixo mostra um exemplo de como quebrar o código acima em passos distintos. Além de ficar mais organizado, o código salva objetos úteis como a tabela agrupada por trimestre, antes de se aplicar o filtro de unidades. A tabela final também fica salva num objeto, permitindo que se faça outros gráficos e análises com estes dados.\n\nunits &lt;- abecip$units\n\n#&gt; Converte em long e agrega os dados por trimestre\ntab_quarter &lt;- units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) \n\n#&gt; Filtra apenas dados de unidades e retira o 'total'\ntab_units &lt;- tab_quarter |&gt; \n  filter(category == \"units\", type != \"total\")\n\n#&gt; Faz o gráfico\nggplot(tab_units, aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\nO pacote sf também funciona bem com pipes pois há vários casos em que se quer aplicar múltiplas funções num mesmo objeto.\n\n# Transforma um data.frame num objeto espacial (pontos)\n# depois faz a interseção dos pontos num polígono e\n# por fim limpa as geometrias\n\ndat |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  st_join(poly) |&gt; \n  filter(!is.na(gid)) |&gt; \n  st_make_valid()\n\nO exemplo abaixo é emprestado do pacote censobr e mostra como combinar a manipulação de dados do dplyr com objetos espaciais manipulados via sf.\n\nlibrary(censobr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(mapview)\n\npop &lt;- read_population(\n  year = 2010,\n  columns = c(\"code_weighting\", \"abbrev_state\", \"V0010\")\n  )\n\ndf &lt;- pop |&gt;\n      filter(abbrev_state == \"RJ\") |&gt;\n      group_by(code_weighting) |&gt;\n      summarise(total_pop = sum(V0010)) |&gt;\n      collect()\n\nareas &lt;- read_weighting_area(3304557, showProgress = FALSE)\n\nareas |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_make_valid() |&gt; \n  left_join(df, by = \"code_weighting\") |&gt; \n  mapview(zcol = \"total_pop\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#introdução",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#introdução",
    "title": "Pipes",
    "section": "",
    "text": "A partir da versão 4.1.0, o R passou a oferecer o operador |&gt; chamado de pipe (literalmente, cano). Este operador foi fortemente inspirado no operador homônimo %&gt;% do popular pacote magrittr. Neste post, explico como utilizar o pipe nativo e como ele difere do pipe do magrittr.\nO operador pipe (em ambos os casos), essencialmente, ordena uma função composta.\nLembrando um pouco sobre funções compostas: a expressão abaixo mostra a aplicação de três funções onde primeiro aplica-se a função f sobre x, depois a função g e, por fim, a função h. Lê-se a função de dentro para fora.\n\\[\nh(g(f(x))) = \\dots\n\\]\nPara tornar o exemplo mais concreto considere o exemplo abaixo onde calcula-se a média geométrica de uma sequência de números aleatórios.\n\nx &lt;- rnorm(n = 100, mean = 10)\n#&gt; Calcula a média geométrica\nexp(log(mean(x)))\n\n[1] 9.933438\n\n\nUsando a mesma notação acima, aplica-se primeiro a função mean (f), depois a função log (g) e, por fim, a função exp (h). Usando o operador pipe, pode-se reescrever a expressão da seguinte forma.\n\nx |&gt; mean() |&gt; log() |&gt; exp()\n\n[1] 9.933438\n\n\nNote que o resultado da função vai sendo “carregado” da esquerda para a direita sucessivamente. Para muitos usuários, a segunda sintaxe é mais intuitiva e/ou fácil de ler. No segundo código a ordem em que o nome das funções aparecem coincide com a ordem da sua aplicação.\nPor fim, note que o uso de várias funções numa mesma linha de código também nos poupa de ter de criar objetos intermediários como no exemplo abaixo.\n\nmedia_x &lt;- mean(x)\nlog_media &lt;- log(media_x)\nmedia_geometrica_x &lt;- exp(log_media)\n\nmedia_geometrica_x\n\n[1] 9.933438\n\n\nOs exemplos acima funcionaram sem problemas porque usou-se o operador pipe para “abrir” uma função composta. O argumento de cada função subsequente é o resultado da função antecedente: funciona como uma linha de montagem, em que cada nova etapa soma-se ao resultado da etapa anterior.\nQuando o resultado da função anterior não vai diretamente no primeiro argumento da função subsequente, precisa-se usar o operador _ (underline). Este operador serve como um placeholder: indica onde que o resultado da etapa anterior deve entrar. No exemplo abaixo, uso o placeholder para colocar a base de dados filtrada no argumento data dentro da função lm.\n\ncarros_4 &lt;- subset(mtcars, cyl == 4)\nfit &lt;- lm(mpg ~ wt, data = carros_4)\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _)\n\nPor fim, temos o caso das funções anônimas. Uma função anônima é simplesmente uma função sem nome que é chamada uma única vez. Infelizmente, a sintaxe de um pipe com uma função anônima é bastante carregada.\n\nobjeto |&gt; (\\(...) {define função})()\n\nO exemplo repete o código acima, mas agora usa uma função anônima para pegar o R2 ajustado da regressão.\n\nmtcars |&gt; \n  subset(cyl == 4) |&gt; \n  lm(mpg ~ wt, data = _) |&gt; \n  (\\(x) {summary(x)$adj.r.squared})()"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#limitações",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#limitações",
    "title": "Pipes",
    "section": "",
    "text": "Imagine agora que se quer calcular o erro absoluto médio de uma regressão. Lembre-se que o EAM é dado por\n\\[\n\\text{EAM} = \\frac{1}{N}\\sum_{i = 1}^{N}|e_{i}|\n\\]\nonde \\(e_{i}\\) é o resíduo da regressão. O código abaixo mostra como fazer isto usando pipes.\n\n#&gt; Estima uma regressão qualquer\nfit &lt;- lm(mpg ~ wt, data = mtcars)\n\n#&gt; Calcula o erro absoluto médio\nfit |&gt; residuals() |&gt; abs() |&gt; mean()\n\n[1] 2.340642\n\n\nNote, contudo, que a situação fica um pouco mais complicada no caso em que se quer calcular a raiz do erro quadrado médio.\n\\[\n\\text{REQM} = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^{N}(e_{i})^2}\n\\]\nNa sintaxe convencional temos\n\nsqrt(mean(residuals(fit)^2))\n\n[1] 2.949163\n\n\nO problema é que a exponenciação acontece via um operador e não uma função. Nenhuma dos exemplos abaixo funciona.\n\nfit |&gt; residuals() |&gt; ^2 |&gt; mean() |&gt; sqrt()\n\nError: &lt;text&gt;:1:23: unexpected '^'\n1: fit |&gt; residuals() |&gt; ^\n                          ^\n\n\n\nfit |&gt; residuals()^2 |&gt; mean() |&gt; sqrt()\n\nError: function '^' not supported in RHS call of a pipe\n\n\nPara chegar no mesmo resultado, novamente precisa-se usar uma sintaxe bastante esotérica que envolve passar o resultado de residuals para uma função anônima.\n\nfit |&gt; residuals() |&gt; (\\(y) {sqrt(mean(y^2))})()\n\n[1] 2.949163"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#resumo",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#resumo",
    "title": "Pipes",
    "section": "",
    "text": "Assim, apesar de muito útil, o operador pipe tem suas limitações. O operador sempre espera encontrar uma função à sua direita; a única maneira de seguir |&gt; com um operador é criando uma função anônima, cuja sintaxe é um pouco carregada. Pode-se resumir os principais fatos sobre o operador pipe:\n\nSimplifica funções compostas. Na expressão x |&gt; f |&gt; g o operador |&gt; aplica a função f sobre o objeto x usando x como argumento de f. Depois, aplica a função g sobre o resultado de f(x). Isto é equivalente a g(f(x)).\nEvita a definição de objetos intermediários. O uso de pipes evita que você precise “salvar” cada passo intermediário da aplicação de funções. Isto deixa seu espaço de trabalho mais limpo e também consome menos memória.\nPlaceholder. Quando o objeto anterior não serve como o primeiro argumento da função subsequente, usa-se o placeholder para indicar onde ele deve ser inserido. x |&gt; f(y = 2, data = _).\nFunção anônima. Em casos mais complexos, é necessário montar uma função anônima usando x |&gt; (\\(y) {funcao})()."
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#aplicações-comuns",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#aplicações-comuns",
    "title": "Pipes",
    "section": "",
    "text": "O uso mais comum de pipes é junto com funções do tidyverse, que foram desenvolvidas com este intuito.\n\nlibrary(tidyverse)\n\nAs funções do tidyverse (quase) sempre recebem um data.frame como primeiro argumento; isto facilita a construção de código usando pipe, pois basta encadear as funções em sequência.\n\nfiltered_df &lt;- filter(mtcars, wt == 2)\ngrouped_df &lt;- group_by(filtered_df, cyl)\ntbl &lt;- summarise(grouped_df, avg = mean(mpg), count = n())\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg))\n\nA leitura do código fica mais “gramatical”: pegue o objeto mtcars filtre as linhas onde wt &gt; 2 depois agrupe pela variável cyl e, por fim, tire uma média de mpg.\nPode-se terminar um pipe com uma chamada para um plot em ggplot2 para uma rápida visualização dos resultados\n\nmtcars |&gt; \n  filter(wt &gt; 2) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(mpg)) |&gt; \n  ggplot(aes(x = as.factor(cyl), y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNão se recomenda fazer longas sequências de pipes, pois o código pode acabar muito confuso para quem está lendo. O exemplo abaixo mostra justamente isto.\n\nlibrary(realestatebr)\n\nabecip &lt;- get_abecip_indicators(cached = TRUE)\n\nabecip$units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) |&gt; \n  filter(category == \"units\", type != \"total\") |&gt; \n  ggplot(aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nO código abaixo mostra um exemplo de como quebrar o código acima em passos distintos. Além de ficar mais organizado, o código salva objetos úteis como a tabela agrupada por trimestre, antes de se aplicar o filtro de unidades. A tabela final também fica salva num objeto, permitindo que se faça outros gráficos e análises com estes dados.\n\nunits &lt;- abecip$units\n\n#&gt; Converte em long e agrega os dados por trimestre\ntab_quarter &lt;- units |&gt; \n  pivot_longer(-date) |&gt; \n  mutate(yq = lubridate::floor_date(date, unit = \"quarter\")) |&gt; \n  group_by(yq, name) |&gt; \n  summarise(total = sum(value)) |&gt; \n  separate(name, into = c(\"category\", \"type\")) \n\n#&gt; Filtra apenas dados de unidades e retira o 'total'\ntab_units &lt;- tab_quarter |&gt; \n  filter(category == \"units\", type != \"total\")\n\n#&gt; Faz o gráfico\nggplot(tab_units, aes(x = yq, y = total, fill = type)) +\n  geom_area() +\n  scale_fill_brewer() +\n  theme_light() +\n  theme(legend.position = \"top\")\n\n\n\n\nO pacote sf também funciona bem com pipes pois há vários casos em que se quer aplicar múltiplas funções num mesmo objeto.\n\n# Transforma um data.frame num objeto espacial (pontos)\n# depois faz a interseção dos pontos num polígono e\n# por fim limpa as geometrias\n\ndat |&gt; \n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |&gt; \n  st_join(poly) |&gt; \n  filter(!is.na(gid)) |&gt; \n  st_make_valid()\n\nO exemplo abaixo é emprestado do pacote censobr e mostra como combinar a manipulação de dados do dplyr com objetos espaciais manipulados via sf.\n\nlibrary(censobr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(mapview)\n\npop &lt;- read_population(\n  year = 2010,\n  columns = c(\"code_weighting\", \"abbrev_state\", \"V0010\")\n  )\n\ndf &lt;- pop |&gt;\n      filter(abbrev_state == \"RJ\") |&gt;\n      group_by(code_weighting) |&gt;\n      summarise(total_pop = sum(V0010)) |&gt;\n      collect()\n\nareas &lt;- read_weighting_area(3304557, showProgress = FALSE)\n\nareas |&gt; \n  st_transform(crs = 4326) |&gt; \n  st_make_valid() |&gt; \n  left_join(df, by = \"code_weighting\") |&gt; \n  mapview(zcol = \"total_pop\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-pipes-in-r/index.html#footnotes",
    "href": "posts/general-posts/2023-09-pipes-in-r/index.html#footnotes",
    "title": "Pipes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUma quantidade enorme de pacotes utiliza o magrittr como dependência. Veja a página do CRAN.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html",
    "title": "Apêndice: manipular para enxergar",
    "section": "",
    "text": "Este tutorial, ao contrário da série “ggplot2: do básico ao intermediário” já assume que se tenha um entendimento razoável de R. O material aqui serve mais para consultar/relembrar ou aprender um truque novo. Da maneira como está escrito, não é adequado para uma primeira leitura. Grosso modo, a referência aqui é Hadley (2017) capítulos 4, 5, 10-12."
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#tabelas",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#tabelas",
    "title": "Apêndice: manipular para enxergar",
    "section": "Tabelas",
    "text": "Tabelas\nO objeto central da análise de dados é o data.frame. Um data.frame é uma tabela bidimensional que contém informações: em geral, cada coluna é uma variável diferente e cada linha é uma observação. Este objeto possui propriedades bastante simples:\n\nComprimento fixo. O número de linhas de um data.frame é fixo, assim todas as colunas têm o mesmo comprimento.\nHomogeneidade. Cada coluna de um data.frame é homogênea, isto é, contém um dado de um único tipo. Assim, uma mesma coluna não pode misturar um string e um número, um factor e um string, etc.\nNomes. Cada coluna tem um nome (único e idiomático). Este nome é utilizado para fazer refrência a esta coluna.\n\nEstas três características garantem a funcionalidade e consistência de um data.frame. O comprimento fixo e a homogeneidade, em particular, tornam este tipo de objeto muito conveniente e previsível.\n\nConstruindo tabelas\nPara construir um data.frame basta chamar a função homônima e declarar as suas colunas seguindo as três propriedades acima. Nos exemplos abaixo, ao invés da função data.frame vou utilizar a função tibble que é, essencialmente, equivalente, mas que possui algumas pequenas vantagens. No restante do texto as palavras tibble e data.frame serão utilizadas como sinônimas.\nNo primeiro exemplo crio uma tabela com três linhas e duas colunas.\n\ndados &lt;- tibble(\n  cidade = c(\"Porto Alegre\", \"São Paulo\", \"Salvador\"),\n  pop22 = c(1.332, 11.451, 2.418)\n)\n\nPara visualizar o resultado basta chamar o objeto por nome ou usar a função print. Uma das vantanges do tibble é de mostrar a classe de cada coluna, onde chr indica character (caractere), isto é, um string e dbl indica double, isto é, um número3.\n\ndados\n\n# A tibble: 3 × 2\n  cidade       pop22\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Porto Alegre  1.33\n2 São Paulo    11.5 \n3 Salvador      2.42\n\n\nPode-se também criar a tabela a partir de vetores/objetos previamente declarados.\n\ncidades &lt;- c(\"Porto Alegre\", \"São Paulo\", \"Salvador\")\npopulacao &lt;- c(1.332, 11.451, 2.418)\n\ndados &lt;- tibble(\n  nome_cidade = cidades,\n  pop22 = populacao\n)\n\nQuando alguma das colunas não tiver o mesmo comprimento das demais, o R vai tentar “reciclar” os valores desta coluna. Em geral, isto vai causar um erro, mas em alguns casos pode funcionar. No caso abaixo o valor \"Brasil\" (de comprimento unitário) é repetido três vezes para “caber” dentro da tabela.\n\ndados &lt;- tibble(\n  cidade = c(\"Porto Alegre\", \"São Paulo\", \"Salvador\"),\n  pop22 = c(1.332, 11.451, 2.418),\n  pais = \"Brasil\"\n)\n\ndados\n\n# A tibble: 3 × 3\n  cidade       pop22 pais  \n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; \n1 Porto Alegre  1.33 Brasil\n2 São Paulo    11.5  Brasil\n3 Salvador      2.42 Brasil\n\n\n\n\nPropriedades de tabelas\nToda coluna de um data.frame possui nomes. Para acessar os nomes usa-se names.\n\nnames(dados)\n#&gt; [1] \"cidade\" \"pop22\"  \"pais\"\n\nOs nomes das colunas sempre devem ser únicos. Aqui, há uma pequena vantagem em utilizar o tibble. Mesmo no caso em que se tenta criar uma tabela com nomes idênticos, a função data.frame evita que isto acontece, mas emite nenhum tipo de alerta sobre o que está acontecendo.\n\ntab &lt;- data.frame(\n  a = c(1, 2, 3),\n  a = c(\"a\", \"b\", \"c\")\n)\n\ntab\n\n  a a.1\n1 1   a\n2 2   b\n3 3   c\n\n\nA função tibble é um pouco mais exigente e retorna um erro neste caso.\n\ntab &lt;- tibble(\n  a = c(1, 2, 3),\n  a = c(\"a\", \"b\", \"c\")\n)\n\nError in `tibble()`:\n! Column name `a` must not be duplicated.\nUse `.name_repair` to specify repair.\nCaused by error in `repaired_names()`:\n! Names must be unique.\n✖ These names are duplicated:\n  * \"a\" at locations 1 and 2.\n\n\nPara extrair uma coluna de um data.frame temos duas opções. A mais simples e direta é utilizar o operador $ e chamar o nome da coluna como se fosse um objeto. A segunda opção é utilizar [[ e chamar o nome da coluna como um string4.\n\n#&gt; Extraindo uma coluna \n\ndados$cidade\n#&gt; [1] \"Porto Alegre\" \"São Paulo\"    \"Salvador\"\n\ndados[[\"cidade\"]]\n#&gt; [1] \"Porto Alegre\" \"São Paulo\"    \"Salvador\"\n\n\n\nImportando tabelas\nRaramente vamos declarar todas as observações de uma tabela. Na prática, é muito mais comum importar uma tabela de alguma fonte externa como de uma planilha de Excel ou de um arquivo csv. Para cada tipo de arquivo existe uma função read_* diferente. Importar dados costuma ser uma tarefa frustrante por três motivos:\n\nHá muitos arquivos para se importar.\nÉ difícil fazer o R encontrar o arquivo.\nOs arquivos têm problemas (valores corrompidos, linhas vazias, etc.)\n\nOs dois primeiros problemas são simples de se resolver. Pode-se importar múltiplos arquivos ao mesmo tempo usando um loop; importar todos os arquivos dentro de uma mesma pasta é trivial, desde que os arquivos sigam o mesmo padrão.\nGarantir que o R consiga encontrar os arquivos também é simples. Idealmente, todos os arquivos externos devem estar organizados dentro de uma pasta chamada dados ou data e deve-se chamar estes dados usando funções read_*. Uma boa prática é sempre usar “caminhos relativos” ao invés de caminhos absolutos.\n\n#&gt; Ruim\ndat &lt;- read_csv(\"/Users/viniciusoike/Documents/GitHub/projeto/data/income.csv\")\n#&gt; Bom \ndat &lt;- read_csv(\"data/income.csv\")\n#&gt; Ainda melhor\ndat &lt;- read_csv(here::here(\"data/income.csv\"))\n\nO terceiro problema é muito mais complexo e vai exigir mais conhecimento e prática. Em geral, resolve-se a maior parte dos problemas usando algum dos argumentos dentro da função read_* como:\n\nskip: Pula as primeiras k linhas.\nna: Define quais valores devem ser interpretados como valores ausentes.\ncol_types: Permite que se declare explicitamente qual o tipo de dado (numérico, data, texto) que está armazenado em cada coluna.\ncol_names ou name_repair: O primeiro permite que se declare explicitamente o nome que cada coluna vai ter dentro do R enquanto o segundo permite que se use uma função que renomeia as colunas.\nlocale: Permite selecionar diferentes tipos de padrão de local. Em geral, usa-se locale = locale(\"pt_BR\").\nrange: Este argumento só vale no caso de planilhas de Excel e permite que se importe uma seleção específica da planilha (e.g. “D4:H115”)\n\nO código abaixo mostra um exemplo particularmente tenebroso. O título da segunda coluna inclui símbolos como $ e /; as datas estão em português com o mês escrito por extenso e em formato dia-mês-ano; os números usam a vírgula (ao invés do ponto) para separar o decimal e o valor ausente é sinalizado com “X”.\n\n#&gt; Input de um csv sujo\ndados &lt;-\n'Data; Valor (R$/m2)\n\"01-maio-2020\";22,3\n\"01-junho-2020\";21,5\n\"06-julho-2021\";X\n\"07-novembro-2022\";22'\n\n#&gt; Lendo o arquivo\ndf &lt;- read_delim(\n  #&gt; Substitui esta linha pelo 'path' até o csv\n  I(dados),\n  delim = \";\",\n  #&gt; Usa , como separador decimal; lê meses em português (e.g. maio, junho, etc.)\n  locale = locale(decimal_mark = \",\", date_names = \"pt\", date_format = \"%d-%B-%Y\"),\n  #&gt; Interpreta X como valores ausentes (NA)\n  na = \"x\",\n  #&gt; Renomeia as colunas\n  name_repair = janitor::clean_names\n  )"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#rename",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#rename",
    "title": "Apêndice: manipular para enxergar",
    "section": "rename",
    "text": "rename\nPara renomear as colunas de data.frame usa-se a função rename (renomear) com rename(tbl, novo_nome = velho_nome). Vale lembrar que para checar os nomes da tabela usa-se names(tbl).\n\n#&gt; Renomear colunas\ntbl_renamed &lt;- rename(tbl, codigo_municipio = code_muni, pop = population)\n\nOs nomes das colunas de um data.frame devem\n\nSer únicos (não-duplicados) e evitar caracteres maiúsculos.\nNão devem incluir caracteres especiais (e.g. !*&@%), nem começar com um número ou caractere especial.\nEvitar espaços em branco, que devem ser substituídos por _ ou omitidos (e.g. PIB Agro deve ser reescrito como pibAgro ou pib_agro.\n\nTambém é possível renomear colunas com auxílio de um vetor ou lista e usando ou all_of (todos) ou any_of (algum/alguns). O exemplo abaixo mostra a lógica geral: temos um vetor que indica o novo nome e o antigo nome de cada coluna que se quer trocar. Caso se queira trocar exatamente todos os nomes indicados no vetor usa-se all_of (mais rigoroso), caso se queira trocar todos os nomes indicados no vetor, ignorando os casos que não batem com nomes de colunas existentes, usa-se any_of.\n\nnew_names &lt;- c(\n  \"codigo_municipio\" = \"code_muni\",\n  \"pop\" = \"population\",\n  \"pop_rate\" = \"population_growth_rate\"\n  )\n\ntbl_renamed &lt;- rename(tbl, all_of(new_names))\n\nNo exemplo abaixo incluo uma “nova coluna” chamada unit que desejo renomear para unidade. Usando any_of o retorno é exatamente igual ao caso acima, pois a função ignora a coluna inexistente unit.\n\nnew_names &lt;- c(\n  \"codigo_municipio\" = \"code_muni\",\n  \"pop\" = \"population\",\n  \"pop_rate\" = \"population_growth_rate\",\n  \"unidade\" = \"unit\"\n)\n\ntbl_renamed &lt;- rename(tbl, any_of(new_names))\n\nJá a função all_of é mais rigorosa e retorna um erro indicando que a coluna unit não existe.\n\ntbl_renamed &lt;- rename(tbl, all_of(new_names))\n\nError in `all_of()`:\n! Can't rename columns that don't exist.\n✖ Column `unit` doesn't exist.\n\n\nO uso de um vetor externo para renomear colunas é conveniente não somente porque permite melhor organizar o código; na prática, este vetor externo funciona como um dicionário de variáveis que pode ser inclusive utilizado para tratar várias bases de dados ou mesmo em outros códigos. Além disso, tratar o nome das colunas como strings é útil pois nos permite transformar este dado mais facilmente.\nPor fim, pode-se aplicar uma função para renomear as colunas usando rename_with.\n\ntbl_renamed &lt;- rename_with(tbl, toupper)\nnames(tbl_renamed)\n\n [1] \"CODE_MUNI\"              \"NAME_MUNI\"              \"CODE_STATE\"            \n [4] \"NAME_STATE\"             \"ABBREV_STATE\"           \"CODE_REGION\"           \n [7] \"NAME_REGION\"            \"POPULATION\"             \"POPULATION_GROWTH\"     \n[10] \"POPULATION_GROWTH_RATE\" \"CITY_AREA\"              \"POPULATION_DENSITY\"    \n[13] \"HOUSEHOLDS\"             \"DWELLERS_PER_HOUSEHOLD\" \"PIB\"                   \n[16] \"PIB_SHARE_UF\"           \"PIB_TAXES\"              \"PIB_ADDED_VALUE\"       \n[19] \"PIB_AGRICULTURE\"        \"PIB_INDUSTRIAL\"         \"PIB_SERVICES\"          \n[22] \"PIB_GOVMT_SERVICES\"    \n\n\nUma dica final para rapidamente renomear colunas é a função janitor::clean_names que obedece aos três princípios elencados acima. Ela pode ser utilizada diretamente num data.frame como se vê no exemplo abaixo.\n\ntest_df &lt;- as.data.frame(matrix(ncol = 6))\nnames(test_df) &lt;- c(\"firstName\", \"ábc@!*\", \"% successful (2009)\",\n                    \"REPEAT VALUE\", \"REPEAT VALUE\", \"\")\njanitor::clean_names(test_df)\n#&gt;   first_name abc percent_successful_2009 repeat_value repeat_value_2  x\n#&gt; 1         NA  NA                      NA           NA             NA NA"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#select",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#select",
    "title": "Apêndice: manipular para enxergar",
    "section": "select",
    "text": "select\nA função select serve para selecionar colunas num data.frame, permitindo-se trabalhar com uma versão menor dos dados. Similarmente à função rename é possível selecionar colunas diretamente select(tbl, coluna_1, coluna_2, …) ou selecionando os nomes via um vetor de strings com auxílio de any_of of all_of.\n\n#&gt; Seleciona diretamente as colunas\nsel_tbl &lt;- select(tbl, code_muni, pib, pib_agriculture)\n\n#&gt; Cria um vetor de nomes\ncolunas &lt;- c(\"code_muni\", \"pib\", \"pib_agriculture\")\n#&gt; Seleciona as colunas baseado no vetor\nsel_tbl &lt;- select(tbl, all_of(colunas))\n\nsel_tbl\n\n# A tibble: 5,570 × 3\n   code_muni     pib pib_agriculture\n       &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n 1   1100015  570272          203394\n 2   1100023 2818049          199723\n 3   1100031  167190           81177\n 4   1100049 2519353          236215\n 5   1100056  600670           94758\n 6   1100064  366931           88923\n 7   1100072  268381          155648\n 8   1100080  261978           80684\n 9   1100098  666331          137802\n10   1100106  984586           59384\n# ℹ 5,560 more rows\n\n\nPara remover uma coluna, basta usar o sinal de menos na frente do nome.\n\n#&gt; Seleciona diretamente as colunas\nsel_tbl &lt;- select(tbl, -pib, -city_area, -population_density)\n\n#&gt; Cria um vetor de nomes\ncolunas &lt;- c(\"pib\", \"city_area\", \"population_density\")\n#&gt; Seleciona as colunas baseado no vetor\nsel_tbl &lt;- select(tbl, -all_of(colunas))\n\nsel_tbl\n\n# A tibble: 5,570 × 19\n   code_muni name_muni             code_state name_state abbrev_state\n       &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       \n 1   1100015 Alta Floresta D'Oeste         11 Rondônia   RO          \n 2   1100023 Ariquemes                     11 Rondônia   RO          \n 3   1100031 Cabixi                        11 Rondônia   RO          \n 4   1100049 Cacoal                        11 Rondônia   RO          \n 5   1100056 Cerejeiras                    11 Rondônia   RO          \n 6   1100064 Colorado do Oeste             11 Rondônia   RO          \n 7   1100072 Corumbiara                    11 Rondônia   RO          \n 8   1100080 Costa Marques                 11 Rondônia   RO          \n 9   1100098 Espigão D'Oeste               11 Rondônia   RO          \n10   1100106 Guajará-Mirim                 11 Rondônia   RO          \n   code_region name_region population population_growth population_growth_rate\n         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n 1           1 Norte            21495             -2897                  -1.05\n 2           1 Norte            96833              6480                   0.58\n 3           1 Norte             5363              -950                  -1.35\n 4           1 Norte            86895              8321                   0.84\n 5           1 Norte            15890             -1139                  -0.58\n 6           1 Norte            15663             -2928                  -1.42\n 7           1 Norte             7519             -1264                  -1.29\n 8           1 Norte            12627             -1051                  -0.66\n 9           1 Norte            29397               668                   0.19\n10           1 Norte            39386             -2270                  -0.47\n   households dwellers_per_household pib_share_uf pib_taxes pib_added_value\n        &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1       7695                   2.79         1.11     35109          535163\n 2      34768                   2.77         5.46    295656         2522393\n 3       1967                   2.73         0.32      7237          159953\n 4      31919                   2.71         4.88    274451         2244902\n 5       5873                   2.69         1.16     89923          510747\n 6       5991                   2.61         0.71     24075          342856\n 7       2840                   2.64         0.52     10200          258181\n 8       4161                   3.01         0.51      9276          252702\n 9      10463                   2.8          1.29     58285          608046\n10      11803                   3.3          1.91    139461          845125\n   pib_agriculture pib_industrial pib_services pib_govmt_services\n             &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n 1          203394          20716       150192             160860\n 2          199723         404752      1207405             710513\n 3           81177           5438        28667              44671\n 4          236215         275537      1157344             575806\n 5           94758          23582       276755             115652\n 6           88923          24322       118529             111082\n 7          155648          10847        34749              56937\n 8           80684           6205        48318             117495\n 9          137802          54521       213513             202211\n10           59384          41650       443005             301086\n# ℹ 5,560 more rows\n\n\nPode-se também selecionar várias colunas ao mesmo tempo se elas estiverem em sequência (uma ao lado da outra). O código abaixo, por exemplo, seleciona a coluna code_muni e todas as colunas entre pib e pib_added_value (inclusive).\n\nsel_tbl &lt;- select(tbl, code_muni, pib:pib_added_value)\n\nsel_tbl\n\n# A tibble: 5,570 × 5\n   code_muni     pib pib_share_uf pib_taxes pib_added_value\n       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1   1100015  570272         1.11     35109          535163\n 2   1100023 2818049         5.46    295656         2522393\n 3   1100031  167190         0.32      7237          159953\n 4   1100049 2519353         4.88    274451         2244902\n 5   1100056  600670         1.16     89923          510747\n 6   1100064  366931         0.71     24075          342856\n 7   1100072  268381         0.52     10200          258181\n 8   1100080  261978         0.51      9276          252702\n 9   1100098  666331         1.29     58285          608046\n10   1100106  984586         1.91    139461          845125\n# ℹ 5,560 more rows\n\n\nTambém é possível renomear e selecionar ao mesmo tempo.\n\nsel_tbl &lt;- select(tbl, codigo_municipio = code_muni, pib)\n\nsel_tbl\n\n# A tibble: 5,570 × 2\n   codigo_municipio     pib\n              &lt;dbl&gt;   &lt;dbl&gt;\n 1          1100015  570272\n 2          1100023 2818049\n 3          1100031  167190\n 4          1100049 2519353\n 5          1100056  600670\n 6          1100064  366931\n 7          1100072  268381\n 8          1100080  261978\n 9          1100098  666331\n10          1100106  984586\n# ℹ 5,560 more rows\n\n\nPor fim, existem algumas funções auxiliares que facilitam a seleção de múltiplas colunas. Vou apresentar apenas três destas funções:\n\nstarts_with/ends_with - selecionam colunas que começam ou terminam com determindo string.\nwhere - seleciona colunas de uma determinada classe (numeric, factor, etc.)\nmatches - seleciona colunas com base num match, usando regex. Esta função é mais geral e engloba as duas primeiras.\n\nO código abaixo mostra alguns exemplos simples. Vou omitir as saídas do código, max experimente reproduzir o resultado.\n\n#&gt; Seleciona code_muni mais as colunas que começam com 'pib'\nselect(tbl, code_muni, starts_with(\"pib\"))\n#&gt; Seleciona as colunas que terminam com 'muni'\nselect(tbl, ends_with(\"muni\"))\n#&gt; Seleciona code_muni mais as colunas que contêm números\nselect(tbl, code_muni, where(is.numeric))\n\n#&gt; Seleciona as colunas que tem o padrão '_texto_'\nselect(tbl, matches(\"_[a-z].+_\"))\n#&gt; Selciona todas as colunas que começam com 'pib'\nselect(tbl, matches(\"^pib\"))\n#&gt; Seleciona todas as colunas que terminam com 'muni'\nselect(tbl, matches(\"muni$\"))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#filter",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#filter",
    "title": "Apêndice: manipular para enxergar",
    "section": "filter",
    "text": "filter\nA função filter serve para filtrar as linhas de um data.frame segundo alguma condição lógica.\n\nfiltered_tbl &lt;- filter(tbl, population_growth &lt; 0)\n\nfiltered_tbl\n\n# A tibble: 2,399 × 22\n   code_muni name_muni                code_state name_state abbrev_state\n       &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       \n 1   1100015 Alta Floresta D'Oeste            11 Rondônia   RO          \n 2   1100031 Cabixi                           11 Rondônia   RO          \n 3   1100056 Cerejeiras                       11 Rondônia   RO          \n 4   1100064 Colorado do Oeste                11 Rondônia   RO          \n 5   1100072 Corumbiara                       11 Rondônia   RO          \n 6   1100080 Costa Marques                    11 Rondônia   RO          \n 7   1100106 Guajará-Mirim                    11 Rondônia   RO          \n 8   1100114 Jaru                             11 Rondônia   RO          \n 9   1100130 Machadinho D'Oeste               11 Rondônia   RO          \n10   1100148 Nova Brasilândia D'Oeste         11 Rondônia   RO          \n   code_region name_region population population_growth population_growth_rate\n         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n 1           1 Norte            21495             -2897                  -1.05\n 2           1 Norte             5363              -950                  -1.35\n 3           1 Norte            15890             -1139                  -0.58\n 4           1 Norte            15663             -2928                  -1.42\n 5           1 Norte             7519             -1264                  -1.29\n 6           1 Norte            12627             -1051                  -0.66\n 7           1 Norte            39386             -2270                  -0.47\n 8           1 Norte            50591             -1414                  -0.23\n 9           1 Norte            30707              -428                  -0.12\n10           1 Norte            15679             -4195                  -1.96\n   city_area population_density households dwellers_per_household     pib\n       &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n 1      7067               3.04       7695                   2.79  570272\n 2      1314               4.08       1967                   2.73  167190\n 3      2783               5.71       5873                   2.69  600670\n 4      1451              10.8        5991                   2.61  366931\n 5      3060               2.46       2840                   2.64  268381\n 6      4987               2.53       4161                   3.01  261978\n 7     24857               1.58      11803                   3.3   984586\n 8      2944              17.2       18947                   2.66 1665068\n 9      8509               3.61      10841                   2.81  700317\n10      1703               9.21       5798                   2.7   403370\n   pib_share_uf pib_taxes pib_added_value pib_agriculture pib_industrial\n          &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n 1         1.11     35109          535163          203394          20716\n 2         0.32      7237          159953           81177           5438\n 3         1.16     89923          510747           94758          23582\n 4         0.71     24075          342856           88923          24322\n 5         0.52     10200          258181          155648          10847\n 6         0.51      9276          252702           80684           6205\n 7         1.91    139461          845125           59384          41650\n 8         3.23    189006         1476062          223881         195776\n 9         1.36     35830          664487          226106          35782\n10         0.78     26898          376472          123270          22561\n   pib_services pib_govmt_services\n          &lt;dbl&gt;              &lt;dbl&gt;\n 1       150192             160860\n 2        28667              44671\n 3       276755             115652\n 4       118529             111082\n 5        34749              56937\n 6        48318             117495\n 7       443005             301086\n 8       712461             343944\n 9       150365             252233\n10       100806             129836\n# ℹ 2,389 more rows\n\n\nOs principais opereadores lógicos no R:\n\n“Maior que”, “Menor que”: &gt;, &lt;, &gt;=, &lt;=\nE/ou: &, |\n“Negação”: !\n“Igual a”: ==\n“Dentro de”: %in%\n\nExistem alguns outros operadores, mas estes costumam resolver 95% dos casos. O exemplo abaixo mostra como filtrar linhas baseado num string. Note que quando se usa múltiplos strings é preciso usar o %in%.\n\nfilter(tbl, name_muni == \"São Paulo\")\n\n# A tibble: 1 × 22\n  code_muni name_muni code_state name_state abbrev_state code_region name_region\n      &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;      \n1   3550308 São Paulo         35 São Paulo  SP                     3 Sudeste    \n  population population_growth population_growth_rate city_area\n       &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;\n1   11451245            197742                   0.15      1521\n  population_density households dwellers_per_household       pib pib_share_uf\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1              7528.    4307693                   2.65 748759007         31.5\n  pib_taxes pib_added_value pib_agriculture pib_industrial pib_services\n      &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1 124349146       624409861           61896       58077784    520357969\n  pib_govmt_services\n               &lt;dbl&gt;\n1           45912212\n\nfilter(tbl, name_muni %in% c(\"São Paulo\", \"Rio de Janeiro\"))\n\n# A tibble: 2 × 22\n  code_muni name_muni      code_state name_state     abbrev_state code_region\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;\n1   3304557 Rio de Janeiro         33 Rio De Janeiro RJ                     3\n2   3550308 São Paulo              35 São Paulo      SP                     3\n  name_region population population_growth population_growth_rate city_area\n  &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;\n1 Sudeste        6211423           -109023                  -0.14      1200\n2 Sudeste       11451245            197742                   0.15      1521\n  population_density households dwellers_per_household       pib pib_share_uf\n               &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1              5175.    2437059                   2.53 331279902         44.0\n2              7528.    4307693                   2.65 748759007         31.5\n  pib_taxes pib_added_value pib_agriculture pib_industrial pib_services\n      &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1  59975014       271304888          105065       36666723    180098159\n2 124349146       624409861           61896       58077784    520357969\n  pib_govmt_services\n               &lt;dbl&gt;\n1           54434942\n2           45912212\n\n\nPara negar a igualdade, basta usar o operador !. No caso do operador %in% há duas maneiras válidas de negá-lo: pode-se colocar o ! no começo da expressão ou colocar a expressão inteira dentro de um parêntesis. Eu tendo a preferir a segunda sintaxe.\n\n#&gt; Remove todas as cidades da região Sudeste\nfilter(tbl, name_region != \"Sudeste\")\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(tbl, !name_region %in% c(\"Sudeste\", \"Norte\"))\n#&gt; Remove todas as cidades das regiões Sudeste e Norte\nfilter(tbl, !(name_region %in% c(\"Sudeste\", \"Norte\")))\n\nEm geral, não é preciso utilizar o E (&), já que pode-se colocar várias condições lógicas dentro de uma mesma chamada para função filter.\n\nfilter(\n  tbl,\n  name_region == \"Nordeste\",\n  !(name_state %in% c(\"Pernambuco\", \"Piauí\")),\n  !(name_muni %in% c(\"Natal\", \"Fortaleza\", \"Maceió\"))\n  )\n\nNo caso de relações de grandeza, pode-se colocar um número absoluto, mas também pode-se usar alguma função. No exemplo abaixo filtra-se apenas os municípios com PIB acima da média, por exemplo.\n\nfilter(tbl, pib &gt; mean(pib))\nfilter(tbl, population &gt;= 1000000)"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#arrange",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#arrange",
    "title": "Apêndice: manipular para enxergar",
    "section": "arrange",
    "text": "arrange\nA função arrange é talvez a mais simples e serve para rearranjar as linhas de um data.frame. Em geral, ela é utilizada mais para fins estéticos ou exploratórios, como para ordenar as cidades pelo maior PIB, ou menor população. Contudo, no caso de uma tabela que contenha séries de tempo, pode ser importante validar que as observações estão na ordem correta.\n\ntbl_arranged &lt;- arrange(tbl, pib)\n\ntbl_arranged\n\n# A tibble: 5,570 × 22\n   code_muni name_muni                  code_state name_state         \n       &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;              \n 1   2209450 Santo Antônio dos Milagres         22 Piauí              \n 2   2510659 Parari                             25 Paraíba            \n 3   3166600 Serra da Saudade                   31 Minas Gerais       \n 4   2414902 Viçosa                             24 Rio Grande Do Norte\n 5   2206308 Miguel Leão                        22 Piauí              \n 6   3147501 Passabém                           31 Minas Gerais       \n 7   2501153 Areia de Baraúnas                  25 Paraíba            \n 8   3115607 Cedro do Abaeté                    31 Minas Gerais       \n 9   5201207 Anhanguera                         52 Goiás              \n10   2504850 Coxixola                           25 Paraíba            \n   abbrev_state code_region name_region  population population_growth\n   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 PI                     2 Nordeste           2138                79\n 2 PB                     2 Nordeste           1720              -122\n 3 MG                     3 Sudeste             833                18\n 4 RN                     2 Nordeste           1822               204\n 5 PI                     2 Nordeste           1318                65\n 6 MG                     3 Sudeste            1600              -166\n 7 PB                     2 Nordeste           2005              -178\n 8 MG                     3 Sudeste            1081              -129\n 9 GO                     5 Centro Oeste        924               -96\n10 PB                     2 Nordeste           1824                53\n   population_growth_rate city_area population_density households\n                    &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n 1                   0.31        34              63.6         606\n 2                  -0.57       208               8.28        644\n 3                   0.18       336               2.48        337\n 4                   0.99        38              48.1         636\n 5                   0.42        93              14.1         406\n 6                  -0.82        94              17.0         610\n 7                  -0.71       114              17.6         673\n 8                  -0.94       283               3.82        449\n 9                  -0.82        56              16.6         358\n10                   0.25       174              10.5         734\n   dwellers_per_household   pib pib_share_uf pib_taxes pib_added_value\n                    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1                   3.53 16741         0.03       396           16345\n 2                   2.67 20839         0.03       903           19936\n 3                   2.46 21055         0          555           20500\n 4                   2.86 21254         0.03       739           20515\n 5                   3.23 21627         0.04      1454           20173\n 6                   2.61 21854         0          817           21037\n 7                   2.98 22128         0.03      1047           21082\n 8                   2.41 22133         0          636           21497\n 9                   2.58 22362         0.01      1312           21050\n10                   2.49 22544         0.03       742           21801\n   pib_agriculture pib_industrial pib_services pib_govmt_services\n             &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;\n 1             471            776         2545              12552\n 2            1888            769         4692              12587\n 3            5970            908         4249               9373\n 4             768            738         5355              13654\n 5             614           2042         5336              12181\n 6            2582            873         5838              11745\n 7             982            893         3735              15472\n 8            4116            875         5633              10872\n 9            2768            964         7164              10154\n10            1907           1218         4849              13827\n# ℹ 5,560 more rows\n\n\nO padrão da função é de sempre ordenar de maneira crescente (do menor para o maior). Para inverter este comportamento pode-se usar a função desc ou o sinal de menos.\n\n#&gt; Ordena as cidades por PIB em ordem decrescente (maior ao menor)\narrange(tbl, desc(pib))\n#&gt; Ordena as cidades por PIB em ordem decrescente (maior ao menor)\narrange(tbl, -pib)\n\nA função arrange ordena strings em ordem alfabética e Datas em ordem cronológica."
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#mutate",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#mutate",
    "title": "Apêndice: manipular para enxergar",
    "section": "mutate",
    "text": "mutate\nA função mutate cria novas colunas. Em geral, cria-se uma nova coluna com base nas colunas pré-existentes, mas a expressão é bastante geral na forma mutate(tbl, nova_coluna = …). Novamente, vou omitir as saídas para poupar espaço.\n\n#&gt; Cria uma coluna onde todas as entradas são iguais a 1\nmutate(tbl, id = 1)\n#&gt; Cria a coluna 'lpib' igual ao logaritmo natural do 'pib'\nmutate(tbl, lpib = log(pib))\n#&gt; Cria a coluna hh igual a 'household' dividido por 1 milhão\nmutate(tbl, hh = household / 1e6)\n\nUm fato conveniente da função mutate é que ela vai criando as colunas sequencialmente, assim é possível fazer diversas transformações numa mesma chamada à função. No caso abaixo, pode-se criar a variável lpibpc a partir das colunas lpib e lpop.\n\nmutate(tbl,\n  lpib = log(pib),\n  lpop = log(population),\n  #&gt; Criando uma variável a partir de duas colunas criadas anteriormente\n  lpibpc = lpib - lpop,\n  pibserv = pib_services + pib_govmt_services,\n  lpibs = log(pibserv)\n  )\n\nPor fim, é possível transformar múltiplas colunas simultaneamente usando a função across da seguinte maneira: across(colunas, função). Para indicar quais colunas quer-se transformar podemos usar a mesma lógica da função select: isto é, declarando o nome das colunas, usando col1:col2, ou mesmo uma função como starts_with/matches, etc.\n\ntbl |&gt; \n  mutate(across(pib:pib_services, log)) |&gt; \n  select(pib:pib_services)\n\n# A tibble: 5,570 × 7\n     pib pib_share_uf pib_taxes pib_added_value pib_agriculture pib_industrial\n   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n 1  13.3        0.104     10.5             13.2            12.2           9.94\n 2  14.9        1.70      12.6             14.7            12.2          12.9 \n 3  12.0       -1.14       8.89            12.0            11.3           8.60\n 4  14.7        1.59      12.5             14.6            12.4          12.5 \n 5  13.3        0.148     11.4             13.1            11.5          10.1 \n 6  12.8       -0.342     10.1             12.7            11.4          10.1 \n 7  12.5       -0.654      9.23            12.5            12.0           9.29\n 8  12.5       -0.673      9.14            12.4            11.3           8.73\n 9  13.4        0.255     11.0             13.3            11.8          10.9 \n10  13.8        0.647     11.8             13.6            11.0          10.6 \n   pib_services\n          &lt;dbl&gt;\n 1         11.9\n 2         14.0\n 3         10.3\n 4         14.0\n 5         12.5\n 6         11.7\n 7         10.5\n 8         10.8\n 9         12.3\n10         13.0\n# ℹ 5,560 more rows\n\n\n\n#&gt; Aplica uma transformação log em todas as colunas entre pib e pib_services\nmutate(tbl, across(pib:pib_services, log))\n#&gt; Aplica uma transformação log em todas as colunas que começam com pib\nmutate(tbl, across(starts_with(\"pib\"), log))\n#&gt; Divide por pib e multiplica por 100 todas as colunas entre pib_taxes e\n#&gt; pib_govmt_services\nmutate(tbl, across(pib_taxes:pib_govmt_services, ~.x / pib * 100))"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#summarise-e-group_by",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#summarise-e-group_by",
    "title": "Apêndice: manipular para enxergar",
    "section": "summarise e group_by",
    "text": "summarise e group_by\n\nUso básico\nAs funções summarise5 e group_by são (quase) sempre utilizadas em conjunto e servem para resumir ou “sumarizar” os dados. A função group_by agrupa os dados segundo alguma coluna. No caso da nossa base de cidades, poderíamos agrupar os dados por estado ou região, por exemplo. A função summarise aplica transformações nestes dados agrupados: pode-se, por exemplo, calcular a população total de cada estado, o PIB per capita médio de cada região, etc.\nA tabela abaixo calcula a população total de cada região e ordena os dados, de maneira decrescente, segundo a população. A partir de agora começo a usar mais o pipe nos códigos.\n\ntbl |&gt; \n  #&gt; Agrupa por região\n  group_by(name_region) |&gt; \n  #&gt; Soma o total da população (dentro de cada região)\n  summarise(pop = sum(population)) |&gt; \n  #&gt; Rearranja o resultado final\n  arrange(desc(pop))\n\n# A tibble: 5 × 2\n  name_region       pop\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Sudeste      84847187\n2 Nordeste     54644582\n3 Sul          29933315\n4 Norte        17349619\n5 Centro Oeste 16287809\n\n\n\n\nUm pouco mais de group_by\nA função group_by agrupa os dados de um tibble e permite que se faça operações sobre estes grupos. Pode-se, por exemplo, calcular o share da população de cada cidade, dentro do seu estado usando mutate. No caso abaixo, eu calculo o share percentual e mostro o resultado para a capital paulista. Vê-se que a capital tem cerca de 11,45 milhão de habitantes, equivalente a 25,78% da população do estado.\n\ntbl_share_pop &lt;- tbl |&gt; \n  group_by(name_state) |&gt; \n  mutate(pop_share = population / sum(population) * 100)\n\ntbl_share_pop |&gt; \n  filter(name_muni == \"São Paulo\") |&gt; \n  select(name_muni, population, pop_share)\n\n# A tibble: 1 × 4\n# Groups:   name_state [1]\n  name_state name_muni population pop_share\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 São Paulo  São Paulo   11451245      25.8\n\n\nSimilarmente, pode-se combinar outras funções como filter. O código abaixo filtra somente as cidades que possuem população acima da média do seu estado. Vale comparar os resultados e entender as diferenças entre cada uma das tabelas\n\n#&gt; Filtra cidades que possuem população acima da média do seu estado\ntbl_pop_grouped &lt;- tbl |&gt; \n  group_by(name_state) |&gt; \n  filter(population &gt; mean(population))\n\n#&gt; Filtra cidades que possuem população acima da média do país\ntbl_pop_ungrouped &lt;- tbl |&gt; \n  filter(population &gt; mean(population))\n\n\n\nUm pouco mais de summarise\nÉ possível fazer várias novas colunas num mesmo summarise. Assim como em mutate também é possível fazer transformações com colunas que foram criadas anteriormente na mesma função. No caso abaixo eu calculo o PIB e população totais de cada estado do nordeste e depois calculo o PIB per capita baseado nestes valores agregados.\n\ntbl |&gt; \n  filter(name_region == \"Nordeste\") |&gt; \n  group_by(name_state) |&gt; \n  summarise(\n    pib_uf = sum(pib) * 1000,\n    pop_uf = sum(population),\n    pibpc_uf = pib_uf / pop_uf\n    ) |&gt; \n  arrange(pibpc_uf)\n\n# A tibble: 9 × 4\n  name_state                pib_uf   pop_uf pibpc_uf\n  &lt;chr&gt;                      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Maranhão            106915961000  6775152   15781.\n2 Piauí                56391259000  3269200   17249.\n3 Paraíba              70292036000  3974495   17686.\n4 Ceará               166914529000  8791688   18985.\n5 Alagoas              63202350000  3127511   20209.\n6 Sergipe              45409659000  2209558   20551.\n7 Pernambuco          193307324000  9058155   21341.\n8 Bahia               305320808000 14136417   21598.\n9 Rio Grande Do Norte  71577110000  3302406   21674.\n\n\nA função summarise é bastante potente. O exemplo abaixo filtra as cidades de médio-grande porte (acima de 100.000 habitantes), agrupa os dados por estado e faz:\n\nO total (soma) da população (cidades com mais de 100.000 habitantes, por estado).\nCalcula a média da população (entre as cidades com mais de 100.000 habitantes, por estado).\nCalcula a população máxima (idem).\nCalcula os quintis da distribuição da população (idem).\nFaz uma regressão linear entre a população e o PIB (idem).\nFaz uma regressão linear entre a população e o PIB e extrai o R2 (idem).\nConta o número de cidades (idem).\n\n\ntbl_summary &lt;- tbl |&gt; \n  filter(population &gt; 100000) |&gt; \n  group_by(name_state) |&gt; \n  summarise(\n    pop_uf = sum(population),\n    pop_avg = mean(population),\n    pop_max = max(population),\n    pop_ntile = list(quantile(population, probs = c(0.2, 0.4, 0.6, 0.8))),\n    reg = list(lm(population ~ pib)),\n    reg_r2 = summary(lm(population ~ pib))$r.squared,\n    count = n()\n    ) |&gt; \n  arrange(desc(count))\n\nNem tudo o que fiz acima faz muito sentido, mas ilustra a capacidade da função summarise de gerar informação e a flexibilidade de um tibble para armazenar diferentes tipos de output. Note que as funções foram todas executadas dentro dos respectivos grupos.\nNo código abaixo pode-se verificar os quintis da população das cidades de Minas Gerais.\n\ntbl_summary |&gt; \n  filter(name_state == \"Minas Gerais\") |&gt; \n  pull(pop_ntile)\n\n[[1]]\n     20%      40%      60%      80% \n111694.6 129821.8 169058.0 333014.8 \n\n\nJá no código abaixo pode-se verificar o resultado da regressão entre população e PIB feita somente nos municípios grandes de São Paulo.\n\nreg_sp &lt;- filter(tbl_summary, name_state == \"São Paulo\")[[\"reg\"]]\nreg_sp &lt;- reg_sp[[1]]\n\nsummary(reg_sp)\n\n\nCall:\nlm(formula = population ~ pib)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-535435  -28068    8337   55289  240897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.330e+04  1.619e+04   3.911 0.000199 ***\npib         1.511e-02  1.851e-04  81.617  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 137300 on 76 degrees of freedom\nMultiple R-squared:  0.9887,    Adjusted R-squared:  0.9886 \nF-statistic:  6661 on 1 and 76 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nAdendos técnicos\nVale notar que a função group_by trasnforma um tibble num grouped_df. Para desfazer esta transformação é preciso usar ungroup. De fato, é uma boa prática sempre utilizar ungroup depois de usar um group_by. Por exemplo, no caso em que se calcula o share percentual da população de cada município em seu respectivo estado, é importante desagrupar os dados.\n\ntbl_share_pop &lt;- tbl |&gt; \n  group_by(name_state) |&gt; \n  mutate(pop_share = population / sum(population) * 100) |&gt; \n  ungroup()\n\nEsta prática serve para evitar erros potenciais e, infelizmente, é necessária em alguns casos, como quando se quer combinar duas bases distintas. Atualmente, existe uma sintaxe experimental, fortemente inspirada na sintaxe do data.table, que desagrupa os dados por padrão. No caso do código abaixo não é preciso utilizar ungroup().\nPessoalmente, gosto bastante desta sintaxe, mas como ela ainda está em fase experimental, é melhor esperar um pouco para utilizá-la.\n\ntbl_share_pop &lt;- tbl |&gt; \n  mutate(\n    pop_share = population / sum(population) * 100,\n    .by = \"name_state\")\n\nEste mesmo .by pode ser utilizado dentro de summarise, filter, etc."
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#contra-exemplos",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#contra-exemplos",
    "title": "Apêndice: manipular para enxergar",
    "section": "Contra-exemplos",
    "text": "Contra-exemplos\nVamos começar explorando alguns contra-exemplos de dados que não estão em formato “tidy”.\n\nVendas de casas e apartamentos\nA tabela abaixo segue um formato tipicamente encontrando em planilhas de Excel. A primeira coluna define: (vendas de) apartamentos, casas e o total. Cada coluna subsequente representa um mês diferente; os valores de cada linha representam o número de vendas de cada tipo em cada mês.\n\ndat &lt;- tibble(\n  nome = c(\"Apartamentos\", \"Casas\", \"Total\"),\n  `2022-01` = c(900, 100, 1000),\n  `2022-02` = c(850, 120, 970),\n  `2022-03` = c(875, 125, 1000),\n  `2022-04` = c(920, 100, 1020),\n)\n\ndat\n\n# A tibble: 3 × 5\n  nome         `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos       900       850       875       920\n2 Casas              100       120       125       100\n3 Total             1000       970      1000      1020\n\n\nNote que:\n\nCada coluna não é uma variável. A maior parte das colunas são datas, que deveriam estar todas numa única coluna.\nCada linha não é uma observação. Cada linha é uma série de valores de observações que varia mês a mês.\n\n\n\nVendas e Alugueis de apartamentos e casas\nEsta segunda tabela é uma versão piorada da versão acima.\n\ndat2 &lt;- tibble(\n  nome = c(\"Apartamentos\", \"Casas\", \"Total\", \"Apartamentos\", \"Casas\", \"Total\"),\n  tipo = c(\"Venda\", \"Venda\", \"Venda\", \"Aluguel\", \"Aluguel\", \"Aluguel\"),\n  `2022-01` = c(900, 100, 1000, 50, 100, 150),\n  `2022-02` = c(850, 120, 970, 60, 80, 140),\n  `2022-03` = c(875, 125, 1000, 70, 90, 160),\n  `2022-04` = c(920, 100, 1020, 50, 50, 100),\n)\n\ndat2\n\n# A tibble: 6 × 6\n  nome         tipo    `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos Venda         900       850       875       920\n2 Casas        Venda         100       120       125       100\n3 Total        Venda        1000       970      1000      1020\n4 Apartamentos Aluguel        50        60        70        50\n5 Casas        Aluguel       100        80        90        50\n6 Total        Aluguel       150       140       160       100\n\n\nNote que:\n\nO nome das colunas mistura variáveis e valores.\nA linha continua não sendo uma observação.\n\n\n\nTeste AB\nA tabela abaixo mostra um teste AB num formato (bem) problemático. No experimento, Bernardo e Álvares estão no grupo de tratamento, enquanto Fernando e Ricardo estão no grupo controle.\n\ndat3 &lt;- tibble(\n  id = c(\"Bernardo\", \"Álvares\", \"Fernando\", \"Ricardo\"),\n  controle = c(NA, NA, 7.2, 5.1),\n  tratamento = c(6.4, 5.5, NA, NA)\n)\n\ndat3\n\n# A tibble: 4 × 3\n  id       controle tratamento\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Bernardo     NA          6.4\n2 Álvares      NA          5.5\n3 Fernando      7.2       NA  \n4 Ricardo       5.1       NA  \n\n\nNote que:\n\nNem ‘controle’ e nem ‘tratamento’ são variáveis, já que são valores de uma mesma variável “qual grupo que está o indivíduo”.\n\nAlternativamente, pode-se ter:\n\ndat31 &lt;- tibble(\n  grupo = c(\"controle\", \"tratamento\"),\n  `Bernardo` = c(NA, 6.4),\n  `Álvares` = c(NA, 5.5),\n  `Fernando` = c(7.2, NA),\n  `Ricardo` = c(5.1, NA)\n)\n\ndat31\n\n# A tibble: 2 × 5\n  grupo      Bernardo Álvares Fernando Ricardo\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 controle       NA      NA        7.2     5.1\n2 tratamento      6.4     5.5     NA      NA  \n\n\nAgora ‘controle’ e ‘tratamento’ estão corretamente dentro de uma mesma coluna, mas cada coluna representa um indivíduo diferente e não uma variável. “Bernardo” não é uma variável e sim um valor (nome do indivíduo).\n\n\nCidades\nA tabela abaixo mostra dados hipóteticos de PIB e população de duas cidades.\n\ndat4 &lt;- tibble(\n  nome_cidade = c(\"São Paulo\", \"Porto Alegre\"),\n  pib_2020 = c(1000, 500),\n  pib_2021 = c(1200, 700),\n  pop_2020 = c(1100, 110),\n  pop_2021 = c(1200, 120)\n)\n\ndat4\n\n# A tibble: 2 × 5\n  nome_cidade  pib_2020 pib_2021 pop_2020 pop_2021\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 São Paulo        1000     1200     1100     1200\n2 Porto Alegre      500      700      110      120\n\n\nNote que:\n\nHá mais de uma variável por coluna: a coluna pib_2020 indica duas informações: o ano da observação e o que está sendo mensurado (PIB).\n\n\n\nCasas e Apartamentos\nNesta tabela o nome das colunas mistura variáveis e valores.\n\ndata &lt;- tibble(\n  cidade = c(\"A\", \"B\", \"C\"),\n  financiado_apto_2020 = c(1, 2, 3),\n  vista_apto_2020 = c(2, 2, 2),\n  permuta_casa_2020 = c(3, 1, 2),\n  vista_casa_2020 = c(1, 1, 1)\n)\n\ndata\n\n# A tibble: 3 × 5\n  cidade financiado_apto_2020 vista_apto_2020 permuta_casa_2020 vista_casa_2020\n  &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1 A                         1               2                 3               1\n2 B                         2               2                 1               1\n3 C                         3               2                 2               1"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#organizando",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#organizando",
    "title": "Apêndice: manipular para enxergar",
    "section": "Organizando",
    "text": "Organizando\nSendo bastante franco, acho difícil explicar a intuição por trás das funções pivot_longer e pivot_wider. No caso da primeira função tem-se:\n\ndat |&gt; \n  pivot_longer(\n    cols = ...,\n    #&gt; Argumentos opcionais\n    names_to = \"name\",\n    values_to = \"value\"\n  )\n\nonde cols indica quais colunas devem ser convertidas em formato longitudinal. Este argumento é bastante flexível e segue as mesmas regras da função select. Por exemplo:\n\ndat |&gt; pivot_longer(cols = -date)\ndat |&gt; pivot_longer(cols = starts_with(\"pib\"))\ndat |&gt; pivot_longer(cols = c(\"x1\", \"x2\"))\n\nJá a função pivot_wider é mais exigente:\n\ndat |&gt; \n  pivot_wider(\n    id_cols = ...,\n    names_from = ...,\n    values_from = ...\n  )\n\nO primeiro argumento indica qual coluna identifica unicamente os valores; o segundo argumento indica quais valores devem ser convertidos em colunas (variáveis); o terceiro argumento indica quais valores devem ser convertido em valores (sim, é isto mesmo). A função “desfaz” o que a pivot_longer “faz”, mas também pode fazer novas tabelas e também condensar informação. Pra piorar a situação, ambas as funções tem vários argumentos opcionais, que muitas vezes são super úteis.\nComo num jogo, explicar as regras em voz-alta parece torná-lo mais complicado do que é. A melhor dica que posso dar é que se pratique bastante. Alternativamente, considere também as funções:\n\ndata.table::melt ou reshape2::melt. É preciso escolher as colunas “identificadoras” e as colunas de “mensuração”. Eu penso no “id” como o que identifica unicamente cada linha e “measure” como o que identifica o que está sendo mensurado. Por exemplo: na tabela abaixo temos o preços de duas ações em dois dias distintos.\n\n\ntab &lt;- tibble(\n  data = c(as.Date(\"2023-05-04\"), as.Date(\"2023-05-05\")),\n  PETR4 = c(23.02, 24),\n  CYRE3 = c(15.54, 15.97)\n)\n\ntab\n\n# A tibble: 2 × 3\n  data       PETR4 CYRE3\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2023-05-04  23.0  15.5\n2 2023-05-05  24    16.0\n\n\nPara converter em formato “tidy” ou longitudinal, declara-se quais as colunas identificam a observação e quais as colunas indicam o que está sendo mensurado.\n\nreshape2::melt(tab, id.vars = \"data\", measure.vars = c(\"PETR4\", \"CYRE3\"))\n\n        data variable value\n1 2023-05-04    PETR4 23.02\n2 2023-05-05    PETR4 24.00\n3 2023-05-04    CYRE3 15.54\n4 2023-05-05    CYRE3 15.97\n\n\nNeste caso particular, é útil saber que o argumento measure.vars pode ser omitido\n\nreshape2::melt(tab, id.vars = \"data\")\n\n        data variable value\n1 2023-05-04    PETR4 23.02\n2 2023-05-05    PETR4 24.00\n3 2023-05-04    CYRE3 15.54\n4 2023-05-05    CYRE3 15.97\n\n\n\nO contrário destas funções é data.table::dcast ou reshape2::dcast, cuja sintaxe é um pouco mais estranha, mas bastante intuitiva.\n\n\nlong &lt;- reshape2::melt(tab, id.vars = \"data\")\n\nreshape2::dcast(long, data ~ variable)\n\n        data PETR4 CYRE3\n1 2023-05-04 23.02 15.54\n2 2023-05-05 24.00 15.97\n\n\n\ntidyr::gather que é a versão antiga de pivot_longer. Pessoalmente, sempre achei esta função bastante confusa, mas, ela talvez seja mais intuitiva para você.\n\n\ngather(tab, \"ticker\", \"price\", -data)\n\n# A tibble: 4 × 3\n  data       ticker price\n  &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 2023-05-04 PETR4   23.0\n2 2023-05-05 PETR4   24  \n3 2023-05-04 CYRE3   15.5\n4 2023-05-05 CYRE3   16.0\n\n\n\nlong &lt;- gather(tab, \"ticker\", \"price\", -data)\nspread(long, \"ticker\", \"price\")\n\n# A tibble: 2 × 3\n  data       CYRE3 PETR4\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2023-05-04  15.5  23.0\n2 2023-05-05  16.0  24  \n\n\nNeste caso simples, o par gather/spread parece bastante conveniente, mas em casos mais complexos esta sintaxe limitada torna-se bastante problemática.\n\nExemplo 1\n\ndat\n\n# A tibble: 3 × 5\n  nome         `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos       900       850       875       920\n2 Casas              100       120       125       100\n3 Total             1000       970      1000      1020\n\n\nEste caso é bem simples, pois todas as colunas, exceto a primeira, são uma única variável (as datas do mês). A função rename é usada somente para deixar mais evidente que a primeira coluna contém a tipologia do que foi vendido. O argumento values_to também é opcional.\n\ndat |&gt; \n  rename(tipologia = nome) |&gt; \n  pivot_longer(cols = -tipologia, names_to = \"data\", values_to = \"unidades\")\n\n# A tibble: 12 × 3\n   tipologia    data    unidades\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;\n 1 Apartamentos 2022-01      900\n 2 Apartamentos 2022-02      850\n 3 Apartamentos 2022-03      875\n 4 Apartamentos 2022-04      920\n 5 Casas        2022-01      100\n 6 Casas        2022-02      120\n 7 Casas        2022-03      125\n 8 Casas        2022-04      100\n 9 Total        2022-01     1000\n10 Total        2022-02      970\n11 Total        2022-03     1000\n12 Total        2022-04     1020\n\n\nNote que agora:\n\nCada linha é uma observação: 900 é o valor de apartamentos vendidos em 2022-01.\nCada coluna é uma variável: a primeira coluna define a ‘tipologia’, a segunda coluna define a ‘data’ e a terceira coluna é o número de ‘unidades’.\n\nPor fim, para ser mais completo pode-se converter a data num formato padrão.\n\ndat |&gt; \n  rename(tipologia = nome) |&gt; \n  pivot_longer(cols = -tipologia, names_to = \"data\", values_to = \"unidades\") |&gt; \n  mutate(data = readr::parse_date(data, format = \"%Y-%m\"))\n\n# A tibble: 12 × 3\n   tipologia    data       unidades\n   &lt;chr&gt;        &lt;date&gt;        &lt;dbl&gt;\n 1 Apartamentos 2022-01-01      900\n 2 Apartamentos 2022-02-01      850\n 3 Apartamentos 2022-03-01      875\n 4 Apartamentos 2022-04-01      920\n 5 Casas        2022-01-01      100\n 6 Casas        2022-02-01      120\n 7 Casas        2022-03-01      125\n 8 Casas        2022-04-01      100\n 9 Total        2022-01-01     1000\n10 Total        2022-02-01      970\n11 Total        2022-03-01     1000\n12 Total        2022-04-01     1020\n\n\n\n\nExemplo 2\n\ndat2\n\n# A tibble: 6 × 6\n  nome         tipo    `2022-01` `2022-02` `2022-03` `2022-04`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Apartamentos Venda         900       850       875       920\n2 Casas        Venda         100       120       125       100\n3 Total        Venda        1000       970      1000      1020\n4 Apartamentos Aluguel        50        60        70        50\n5 Casas        Aluguel       100        80        90        50\n6 Total        Aluguel       150       140       160       100\n\n\nO segundo exemplo é quase idêntico ao primeiro.\n\ndat2 |&gt; \n  rename(tipologia = nome, mercado = tipo) |&gt; \n  pivot_longer(\n    cols = -c(tipologia, mercado),\n    names_to = \"data\",\n    values_to = \"unidades\"\n    ) |&gt; \n  print(n = 24)\n\n# A tibble: 24 × 4\n   tipologia    mercado data    unidades\n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n 1 Apartamentos Venda   2022-01      900\n 2 Apartamentos Venda   2022-02      850\n 3 Apartamentos Venda   2022-03      875\n 4 Apartamentos Venda   2022-04      920\n 5 Casas        Venda   2022-01      100\n 6 Casas        Venda   2022-02      120\n 7 Casas        Venda   2022-03      125\n 8 Casas        Venda   2022-04      100\n 9 Total        Venda   2022-01     1000\n10 Total        Venda   2022-02      970\n11 Total        Venda   2022-03     1000\n12 Total        Venda   2022-04     1020\n13 Apartamentos Aluguel 2022-01       50\n14 Apartamentos Aluguel 2022-02       60\n15 Apartamentos Aluguel 2022-03       70\n16 Apartamentos Aluguel 2022-04       50\n17 Casas        Aluguel 2022-01      100\n18 Casas        Aluguel 2022-02       80\n19 Casas        Aluguel 2022-03       90\n20 Casas        Aluguel 2022-04       50\n21 Total        Aluguel 2022-01      150\n22 Total        Aluguel 2022-02      140\n23 Total        Aluguel 2022-03      160\n24 Total        Aluguel 2022-04      100\n\n\n\n\nExemplo 3\n\ndat3\n\n# A tibble: 4 × 3\n  id       controle tratamento\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Bernardo     NA          6.4\n2 Álvares      NA          5.5\n3 Fernando      7.2       NA  \n4 Ricardo       5.1       NA  \n\n\n\ndat3 |&gt; \n  pivot_longer(-id, names_to = \"grupo\", values_to = \"valor\")\n\n# A tibble: 8 × 3\n  id       grupo      valor\n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 Bernardo controle    NA  \n2 Bernardo tratamento   6.4\n3 Álvares  controle    NA  \n4 Álvares  tratamento   5.5\n5 Fernando controle     7.2\n6 Fernando tratamento  NA  \n7 Ricardo  controle     5.1\n8 Ricardo  tratamento  NA  \n\n\n\n\nExemplo 4\n\ndat4\n\n# A tibble: 2 × 5\n  nome_cidade  pib_2020 pib_2021 pop_2020 pop_2021\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 São Paulo        1000     1200     1100     1200\n2 Porto Alegre      500      700      110      120\n\n\nNeste exemplo pode-se separar as colunas facilmente usando names_sep e names_to.\n\ndat4 |&gt; \n  pivot_longer(\n    cols = -nome_cidade,\n    names_sep = \"_\",\n    names_to = c(\"variavel\", \"ano\"),\n    values_to = \"valor\"\n    )\n\n# A tibble: 8 × 4\n  nome_cidade  variavel ano   valor\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n1 São Paulo    pib      2020   1000\n2 São Paulo    pib      2021   1200\n3 São Paulo    pop      2020   1100\n4 São Paulo    pop      2021   1200\n5 Porto Alegre pib      2020    500\n6 Porto Alegre pib      2021    700\n7 Porto Alegre pop      2020    110\n8 Porto Alegre pop      2021    120\n\n\n\n\nExemplo 5\n\ndata\n\n# A tibble: 3 × 5\n  cidade financiado_apto_2020 vista_apto_2020 permuta_casa_2020 vista_casa_2020\n  &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1 A                         1               2                 3               1\n2 B                         2               2                 1               1\n3 C                         3               2                 2               1\n\n\nConfesso que só coloquei este exemplo aqui para mostrar que é possível usar dois pivot_longer em sequência para chegar num resultado útil.\n\ndata |&gt; \n  pivot_longer(\n    cols = -cidade,\n    names_sep = \"_\",\n    names_to = c(\".value\", \"tipologia\", \"ano\")\n  ) |&gt; \n  pivot_longer(\n    cols = financiado:permuta,\n    names_to = \"forma_pagamento\",\n    values_to = \"unidades\"\n  )\n\n# A tibble: 18 × 5\n   cidade tipologia ano   forma_pagamento unidades\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1 A      apto      2020  financiado             1\n 2 A      apto      2020  vista                  2\n 3 A      apto      2020  permuta               NA\n 4 A      casa      2020  financiado            NA\n 5 A      casa      2020  vista                  1\n 6 A      casa      2020  permuta                3\n 7 B      apto      2020  financiado             2\n 8 B      apto      2020  vista                  2\n 9 B      apto      2020  permuta               NA\n10 B      casa      2020  financiado            NA\n11 B      casa      2020  vista                  1\n12 B      casa      2020  permuta                1\n13 C      apto      2020  financiado             3\n14 C      apto      2020  vista                  2\n15 C      apto      2020  permuta               NA\n16 C      casa      2020  financiado            NA\n17 C      casa      2020  vista                  1\n18 C      casa      2020  permuta                2"
  },
  {
    "objectID": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#footnotes",
    "href": "posts/ggplot2-tutorial/apendix-manipular-enxergar.html#footnotes",
    "title": "Apêndice: manipular para enxergar",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara saber mais sobre pipes e a diferença entre o novo pipe nativo |&gt; e o pipe %&gt;% do magrittr veja meu post sobre o assunto.↩︎\nNo fundo, isto é ainda mais um incentivo para aprender inglês.↩︎\nA classe mais geral de números do R é a numeric. Aqui, “double” faz referência à precisão do número, que é um “double-precision value”, que equivale a uma precisão de 53 bits, com aplitude de \\(2\\times 10^{-308}\\) a \\(2\\times 10^{-308}\\). Em geral, a maioria dos números (com exceção de inteiros) é armazenada desta forma.↩︎\nDe fato, esta é a mesma sintaxe que se utiliza para extrair um elemento de uma lista. Isto acontece pois um data.frame é essencialmente, uma lista com um pouco mais de estrutura. Isto pode ser verificado usando typeof em um objeto data.frame.↩︎\nO dplyr também aceita a função summarize com ‘z’ ao invés de ‘s’. As funções são exatamente iguais e podem ser intercambiadas sem maiores problemas.↩︎"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html",
    "href": "posts/general-posts/2023-09-happiness/index.html",
    "title": "Life Satisfaction and GDP per capita",
    "section": "",
    "text": "In this tutorial post I will replicate this graph, from OurWorldInData (OWID) using ggplot2. The original graph is available at OWID website. The plot shows the correlation between GDP per capita and self-reported happiness. The income data comes from the World Bank and is in 2017 constant PPP dollars: this ensures the data is comparable across countries since it accounts for both inflation and different costs of living. The happiness data comes from the World Happiness Report.\n\nThis plot has several attractive features including how colors are used to represent continents and how size is used to show the population of each country.\nTo follow this tutorial make sure all of the packages below are installed.\n\n#&gt; Packages needed to replicate the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(showtext)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n\n\n\nThe first step is acquiring the data. Luckily, the csv data is readily available at the OWID website. For convenience I stored a smaller version of the dataset in my Github. The code below imports the data directly into the R session.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nThe data shows both the GDP per capita and the Happiness Indicator for several countries across many years. The code below selects only the most recent that is available for each country.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#adicionando-os-paises",
    "href": "posts/general-posts/2023-09-happiness/index.html#adicionando-os-paises",
    "title": "Satisfação com a vida e felicidade",
    "section": "Adicionando os paises",
    "text": "Adicionando os paises\n\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\nbase_plot &lt;- ggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_text_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#escalas-e-cores",
    "href": "posts/general-posts/2023-09-happiness/index.html#escalas-e-cores",
    "title": "Satisfação com a vida e felicidade",
    "section": "Escalas e cores",
    "text": "Escalas e cores\n\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\nbase_plot &lt;- base_plot +\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  scale_y_continuous(breaks = 3:7) +\n  scale_size_continuous(range = c(1, 15)) +\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  guides(\n    color = \"none\",\n    size = \"none\",\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#elementos-textuais",
    "href": "posts/general-posts/2023-09-happiness/index.html#elementos-textuais",
    "title": "Satisfação com a vida e felicidade",
    "section": "Elementos textuais",
    "text": "Elementos textuais\n\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\nbase_plot &lt;- base_plot +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#fonte",
    "href": "posts/general-posts/2023-09-happiness/index.html#fonte",
    "title": "Satisfação com a vida e felicidade",
    "section": "Fonte",
    "text": "Fonte\nOlhando o c’odigo fonte da p’agina\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nfont_add_google(\"Lato\", \"Lato\")\n\nshowtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#theme",
    "href": "posts/general-posts/2023-09-happiness/index.html#theme",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Theme",
    "text": "Theme\nThis step really does the magic.\nFor the theme, I use theme_minimal as a template, since its fairly similar to the OWID graphic. I start by removing the minor panel grids from the background and changing the major panel grids. Then, I alter the textual elements of the graphic and make minor tweaks to the legend and plot margins.\nAll of the text is in different shades of gray with exception of the title, the axis titles, and the legend text which are all in plain black. Figuring out the sizes of the text is mostly a trial and error process. Almost all of the text is small except the main title and the text on the axes.\n\nbase_plot +\n  theme_minimal() +\n  theme(\n    #&gt; Background grid\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(linetype = 3, color = \"#DDDDDD\"),\n    \n    #&gt; Text elements (change font)\n    text = element_text(family = \"Lato\"),\n    title = element_text(family = \"Lato\"),\n    #&gt; Caption, title, and subtitle\n    plot.caption = element_text(color = \"#777777\", hjust = 0, size = 8),\n    plot.title = element_text(\n      color = \"#444444\",\n      family = \"Playfair Display\",\n      size = 18),\n    plot.subtitle = element_text(color = \"#666666\", size = 11),\n    #&gt; Axis text\n    axis.title = element_text(color = \"#000000\", size = 9),\n    axis.text = element_text(color = \"#666666\", size = 12),\n    #&gt; Legend\n    legend.key.size = unit(5, \"pt\"),\n    legend.position = \"right\",\n    legend.text = element_text(size = 10),\n    #&gt; Margin\n    plot.margin = margin(rep(10, 4))\n  )"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#highlighting-the-countries",
    "href": "posts/general-posts/2023-09-happiness/index.html#highlighting-the-countries",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Highlighting the countries",
    "text": "Highlighting the countries\nThis part is mostly manual labor. I create a simple vector with the names of all countries highlighted in the original plot. This vector allows me to create a dummy variable that indicates whether the name of the country should be plotted or not. For increased flexibility I store this as an auxiliar tibble called dftext. In the end, this didn’t make much of a difference but it can be helpful in cases where one needs finer control over the text that is plotted.\nI ggrepel to avoid overlapping the text labels.\n\n#&gt; Countries to highlight\nsel_countries &lt;- c(\n  \"Ireland\", \"Qatar\", \"Hong Kong\", \"Switzerland\", \"United States\", \"France\",\n  \"Japan\", \"Costa Rica\", \"Russia\", \"Turkey\", \"China\", \"Brazil\", \"Indonesia\",\n  \"Iran\", \"Egypt\", \"Botswana\", \"Lebanon\", \"Philippines\", \"Bolivia\", \"Pakistan\",\n  \"Bangladesh\", \"Nepal\", \"Senegal\", \"Burkina Faso\", \"Ethiopia\", \"Tanzania\",\n  \"Democratic Republic of Congo\", \"Mozambique\", \" Somalia\", \"Chad\", \"Malawi\",\n  \"Burundi\", \"India\")\n\n#&gt; Auxiliar tibble with names of countries to highlight\ndftext &lt;- dat |&gt; \n  mutate(highlight = if_else(entity %in% sel_countries, entity, NA))\n\n#&gt; Creates a base plot with the bubbles plus the text labels\nbase_plot &lt;- ggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    ) +\n  ggrepel::geom_text_repel(\n    data = dftext,\n    aes(x = gdppc, y = life_satisfaction, label = highlight, color = continent),\n    size = 3\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#scales-and-colors",
    "href": "posts/general-posts/2023-09-happiness/index.html#scales-and-colors",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Scales and colors",
    "text": "Scales and colors\nThe x-axis of the original plot is in a logarithmic scale and its labels highlight specific values (1000, 2000, 5000, …, 100000). The numbers are formatted with a comma and dollar sign. The y-axis is much more straightforward since the numbers are simple integers ranging from 3 to 7.\nThe default size of the bubbles in ggplot is small so I use scale_size_continuous to increase them. I have no idea how to emulate the original size legend (the circle within a circle) so I omit it.\nBoth the interior color of the bubbles and the text follow a particular color scheme. I got the exact colors of the original plot by exporting it to SVG. By default, the legend key inherits its colors. So the legend shows slightly transparent round circles. To get solid colored squares I override this default behavior.\n\nxbreaks &lt;- c(3, 3.3, 3.7, 4, 4.3, 5)\nxlabels &lt;- c(1000, 2000, 5000, 10000, 20000, 100000)\nxlabels &lt;- paste0(\"$\", format(xlabels, big.mark = \",\", scientific = FALSE))\n\ncolors &lt;- c(\"#A2559C\", \"#00847E\", \"#4C6A9C\", \"#E56E5A\", \"#9A5129\", \"#883039\")\n\nbase_plot &lt;- base_plot +\n  #&gt; Adds labels to the log scale\n  scale_x_continuous(breaks = xbreaks, labels = xlabels) +\n  #&gt; Adds labels to the y-axis scale\n  scale_y_continuous(breaks = 3:7) +\n  #&gt; Increases the size of the bubbles\n  scale_size_continuous(range = c(1, 15)) +\n  #&gt; Adds colors to the bubbles and text labels\n  scale_fill_manual(name = \"\", values = colors) +\n  scale_color_manual(name = \"\", values = colors) +\n  #&gt; Removes the size and color legend\n  guides(\n    color = \"none\",\n    size = \"none\",\n    #&gt; Override default behaviour to get solid colors\n    fill = guide_legend(override.aes = list(shape = 22, alpha = 1))\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#text-elements",
    "href": "posts/general-posts/2023-09-happiness/index.html#text-elements",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Text elements",
    "text": "Text elements\nThe code below inserts the other textual elements of the plot.\n\n#&gt; Caption\ncaption &lt;- \"Source: World Happiness Report (2023), Data compiled from multiple sources by World Bank\\nNote: GDP per capita is expressed in international-$ at 2017 prices.\\nOurWorldInData.org/happiness-and-life-satisfacation/\"\n#&gt; Subtitle\nsubtitle &lt;- \"Self-reported life satisfaction is measured on a scale ranging from 0-10, where 10 is the highest possible life\\nsatisfaction. GDP per capita is adjusted for inflation and differences in the cost of living between countries.\"\n\n#&gt; Adds textual elements to base plot\nbase_plot &lt;- base_plot +\n  labs(\n    title = \"Self-reported life satisfaction vs. GDP per capita, 2022\",\n    subtitle = subtitle,\n    x = \"GDP per capita\",\n    y = \"Life satisfaction (country average; 0-10)\",\n    caption = caption\n    )\n\nbase_plot"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#font",
    "href": "posts/general-posts/2023-09-happiness/index.html#font",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Font",
    "text": "Font\nLooking at the source code of the page it seems that most of the text is displayed in Lato while the titles are displayed in Playfair Display. I import both fonts using font_add_google and load them using showtext.\n\nfont_add_google(\"Playfair Display\", \"Playfair Display\")\nfont_add_google(\"Lato\", \"Lato\")\n\nshowtext::showtext_auto()"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#bubbles",
    "href": "posts/general-posts/2023-09-happiness/index.html#bubbles",
    "title": "Life Satisfaction and GDP per capita",
    "section": "Bubbles",
    "text": "Bubbles\nThe most essential aspect of this plot is summarized in the code below. The plot shows each country as bubble, where the size of the bubble is proportional to its population. The position of each bubble shows the country’s GDP per capita (on the horizontal axis) and average life satisfaction (on the vertical axis). Finally, the color of each bubble corresponds to the continent of the country. Since many of the observations overlap the original plot uses a bit of transparency.\nNote that I use shape = 21 to get a special circle with two colors. The color argument controls the circle’s border while the fill argument controls the circle’s interior color.\n\nggplot(dat, aes(x = gdppc, y = life_satisfaction)) +\n  geom_point(\n    aes(fill = continent, size = pop),\n    color = \"#A5A9A9\",\n    alpha = 0.8,\n    shape = 21\n    )"
  },
  {
    "objectID": "posts/general-posts/2023-09-happiness/index.html#the-data",
    "href": "posts/general-posts/2023-09-happiness/index.html#the-data",
    "title": "Life Satisfaction and GDP per capita",
    "section": "",
    "text": "The first step is acquiring the data. Luckily, the csv data is readily available at the OWID website. For convenience I stored a smaller version of the dataset in my Github. The code below imports the data directly into the R session.\n\nowid &lt;- readr::read_csv(\n  \"https://github.com/viniciusoike/restateinsight/raw/main/static/data/gdp-vs-happiness.csv\"\n  )\n\nThe data shows both the GDP per capita and the Happiness Indicator for several countries across many years. The code below selects only the most recent that is available for each country.\n\n\nCode\ndat &lt;- owid |&gt; \n  janitor::clean_names() |&gt; \n  rename(\n    life_satisfaction = cantril_ladder_score,\n    gdppc = gdp_per_capita_ppp_constant_2017_international,\n    pop = population_historical_estimates\n  ) |&gt;\n  filter(!is.na(gdppc), !is.na(life_satisfaction)) |&gt; \n  mutate(gdppc = log10(gdppc)) |&gt; \n  group_by(entity) |&gt; \n  filter(year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(entity, pop, gdppc, life_satisfaction)\n\ndim_continent &lt;- owid |&gt; \n  select(entity, continent) |&gt; \n  filter(!is.na(continent), !is.na(entity)) |&gt; \n  distinct()\n\ndat &lt;- left_join(dat, dim_continent, by = \"entity\")"
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html",
    "title": "Preços de Imóveis e Demografia",
    "section": "",
    "text": "Já vi em alguns lugares uma suposta ligação entre fatores demográficos e tendências de longo prazo no mercado imobiliário. Intuitivamente, alguns dos principais motivadores para comprar ou vender um imóvel estão ligados a fatores demográficos: nascimentos, casamentos, divórcios ou óbitos.\nEste tipo de análise omite fatores importantes como renda, condições de financiamento e o contexto geral da economia. Ainda assim, fiquei curioso para ver se havia algum padrão entre tendências demográficas mais simples e o comportamento dos preços."
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html#todos-os-países",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html#todos-os-países",
    "title": "Preços de Imóveis e Demografia",
    "section": "Todos os países",
    "text": "Todos os países\nEsta análise é ainda bastante preliminar. Ainda que a demografia seja um motor para a demanda imobiliária, outros fatores como oferta de moradia e condições de crédito são importantes demais parecem serem omitidos.\nO gráfico abaixo compara a população em 2010/2020 com os preços em 2010/2020. Os países ao lado direito do gráfico, são os países onde houve crescimento populacional. Os países na parte de cima do gráfico são os países onde houve crescimento real do preço dos imóveis. Na média da amostra, destacada como WLD, houve crescimento de ambos.\nPaíses que estão muito para cima como Chile (CHL), Índia (IND) e Estônia (EDT) estão com imóveis muito “caros”. Já em países com França e Finlândia tanto a população como o nível de preço dos imóveis cresceram muito pouco. No caso da Grécia, tanto a população como o preço dos imóveis diminuiu nos últimos dez anos.\nO gráfico não me surpreendeu muito, mas esperava que os EUA estivessem mais para cima no gráfico e que o Brasil estivesse ao menos do lado positivo do eixo-x. Isso me sugere que a impressão de que os imóveis no Brasil são ou estão caros tem muito mais a ver com a baixa renda da população.\n\n\nCode\np5 &lt;- \n  ggplot(\n    data = na.omit(tbl_wide),\n    aes(x = index_pop, y = index_house)\n    ) +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2, color = colors[1]) +\n  geom_point(aes(color = highlight)) +\n  geom_text_repel(aes(label = iso3c, color = highlight)) +\n  scale_x_continuous(breaks = seq(-10, 30, 5)) +\n  scale_y_continuous(breaks = seq(-30, 90, 15)) +\n  scale_color_manual(values = c(\"black\", colors[1])) +\n  guides(color = \"none\") +\n  labs(\n    title = \"Real House Prices x Population (2010/20)\",\n    x = \"Population (2010/2020)\",\n    y = \"RPPI (2010/2020)\",\n    caption = \"Source: Real House Price Indexes (BIS), Population (UN).\") +\n  theme_vini\n\np5"
  },
  {
    "objectID": "posts/general-posts/repost-precos-imoveis-demografia/index.html#footnotes",
    "href": "posts/general-posts/repost-precos-imoveis-demografia/index.html#footnotes",
    "title": "Preços de Imóveis e Demografia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHong Kong: https://exame.com/economia/as-raizes-economicas-dos-protestos-de-hong-kong/↩︎"
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html",
    "href": "posts/general-posts/repost-aquecimento-global/index.html",
    "title": "Aquecimento Global",
    "section": "",
    "text": "Uma recente edição da revista inglesa The Economist exibe uma série de listras coloridas em sua capa. Elas formam um degradê que vai de um azul escuro até um vermelho intenso. Cada listra representa a temperatura de um ano e a linha do tempo vai desde o 1850 até o presente. A mensagem é bastante clara: o planeta esta cada ano mais quente e é nos anos recentes que estão concentradas as maiores altas de temperatura. Esta imagem é creditada a Ed Hawkings, editor do Climate Lab Book.\nPara ser preciso, a imagem não plota a temperatura de cada ano, mas sim o quanto cada ano se desvia da temperatura média do período 1971-2000. Isto é, anos acima dessa média têm um valor positivo, valores abaixo dessa média, valores negativos. Esta é uma forma bastante comum de representar este tipo de dado climático. De imediato, quando vi a imagem me ocorreu que seria bastante simples reproduzir uma versão aproximada dela usando o R."
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html#o-código",
    "href": "posts/general-posts/repost-aquecimento-global/index.html#o-código",
    "title": "Aquecimento Global",
    "section": "O código",
    "text": "O código\nO código necessário para gerar a imagem é bastante enxuto. Vou descrever em linhas gerais o que ele faz:\nPrimeiro carrego dois pacotes (linhas 1, 2), depois a série de temperatura (linha 3), faço algumas transformações nos dados (linhas 4, 5) e, por fim, ploto os dados (linhas 6, 7, 8). O resultado inicial já é bastante satisfatório e a partir destas poucas linhas de código pode-se chegar num resultado muito próximo ao da imagem original. Vale notar que a imagem fica um pouco diferente da original porque eu uso uma base de dados diferente.\n\n# Carrega pacotes\nlibrary(ggplot2)\nlibrary(astsa)\n# Carrega a base de dados 'globtemp'\ndata(\"globtemp\")\n# Converte o objeto para data.frame\ndf &lt;- data.frame(ano = as.numeric(time(globtemp)),\n temp = as.numeric(globtemp))\n\n# Monta o gráfico\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")"
  },
  {
    "objectID": "posts/general-posts/repost-aquecimento-global/index.html#os-detalhes-do-código",
    "href": "posts/general-posts/repost-aquecimento-global/index.html#os-detalhes-do-código",
    "title": "Aquecimento Global",
    "section": "Os detalhes do código",
    "text": "Os detalhes do código\nVou explicar cada linha de código para ser didático. O R funciona, grosso modo, como um repositório de pacotes: cada pacote contem funções e, às vezes, bases de dados. O primeiro pacote que carrego é o ggplot2. Ele serve para fazer visualizações de dados. O pacote astsa traz várias funções para fazer análise de séries de tempo, mas eu carrego ele somente para usar a base de dados globtemp, que traz informação sobre a temperatura anual da terra coletada pela NASA.\nO objeto globtemp é uma série de tempo (um objeto da classe ts), que tem alguns atributos especiais. Um deles pode ser acesado pela função time que extrai um vetor numérico com as datas desta série de tempo. No código abaixo mostro os primeiros dez valores do time(globtemp).\n\nclass(globtemp)\n\n[1] \"ts\"\n\ntime(globtemp)[1:10]\n\n [1] 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n\n\nPara extrair somente os valores da série, uso a função as.numeric, que converte o vetor de ts para numeric (numérico). Este tipo de função é bastante comum já que frequentemente é preciso trocar a classe de um objeto. O objetivo destes primeiros passos é de inserir as informações do globtemp num data.frame em que a data aparece na primeira coluna e os valores da série são armazenados na segunda coluna. O procedimento pode parecer um tanto trabalhoso (e acho que é mesmo), mas é o jeito. Um data.frame é como uma tabela com dados. Este é um objeto bastante típico em análise de dados e é necessário para usar a função ggplot que vai fazer o gráfico. Abaixo pode-se ver as primeiras linhas desta tabela.\n\nhead(df)\n\n   ano  temp\n1 1880 -0.20\n2 1881 -0.11\n3 1882 -0.10\n4 1883 -0.20\n5 1884 -0.28\n6 1885 -0.31\n\n\nAgora que tenho os dados no formato apropriado posso usar o ggplot. O argumento que pode ser um pouco confuso é o aes. Nele especifica-se quais dados serão mapeados no gráfico. Depois disso adicionamos um geom. Há vários tipos de geom (geom_line, geom_bar, geom_histogram, etc.) e cada um deles produz uma imagem diferente. O geom_tile faz um pequeno quadrado. Para que a função consiga desenhar o quadrado é preciso informar uma variável x e uma variável y. Além disso, também especifico fill = temp. O fill se refere à cor que vai preencher (fill) o quadrado. Como especifico fill = temp a cor do quadrado vai representar a variável temp (temperatura).\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile()\n\n\n\n\n\n\n\n\nO resultado é exatamente como o esperado, mas ainda é preciso mudar a escala de cores. Faço isto com o scale_fill_gradient2. Aqui cada termo tem um signficado: scale_fill pois estamos mudando a escala do fill (outra opção seria scale_color que muda a escala do color). scale_fill_gradient pois queremos um gradiente (degradê) de cores. Por fim, o 2 é adicionado no final pois queremos um escala que diferencie dois grupos distintos: temperaturas acima da média em vermelho, temperaturas abaixo da média em azul. A escala de cores é determinada pelos argumentos low, mid e high.\nOs valores negativos serão coloridos pelo low, os próximos de zero pelo mid e os valores grandes pelo high. Abaixo escrevo as cores em hexa-decimal, mas elas podem ser lidas, essencialmente, como: azul-escuro, cinza-azulado-claro e vermelho-escuro.\n\nggplot(data = df, aes(x = ano, y = 0, fill = temp)) +\n geom_tile() +\n scale_fill_gradient2(low = \"#104e8b\", mid = \"#b9d3ee\", high = \"#ff0000\")\n\n\n\n\n\n\n\n\nComo comentei acima, pode-se melhorar o gráfico acima adicionando outros elementos e detalhes. A versão final que fiz do gráfico fica no código abaixo.\n\n# Pacote para carregar fontes externas no R\n# Necessário para utilizar 'Georgia' no gráfico\nlibrary(extrafont)\n# Data.frames auxiliares para plotar as anotações de texto\ndf_aux_title &lt;- data.frame(x = 1930, y = 0, label = \"The Climate Issue\")\ndf_aux_anos &lt;- data.frame(\n  label = c(1880, 1920, 1960, 2000),\n  x = c(1890, 1925, 1960, 1995)\n  )\n\nggplot() +\n  geom_tile(data = df, aes(x = ano, y = 0, fill = temp)) +\n  geom_text(\n    data = df_aux_anos,\n    aes(x = x, y = 0, label = label),\n    vjust = 1.5,\n    colour = \"white\",\n    size = 6,\n    family = \"Georgia\") +\n  geom_text(\n    data = df_aux_title,\n    aes(x = 1950, y = 0.05, label = label),\n    family = \"Georgia\",\n    size = 11,\n    colour = \"white\") +\n  geom_hline(yintercept = 0, colour = \"white\", size = 1) +\n  scale_fill_gradientn(\n    colors = c(\"#213A82\", \"#3B60CE\", \"#8DA2E2\", \"#DE2E02\", \"#9d0208\")\n   ) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.background = element_rect(fill = NA),\n    plot.margin = margin(c(0, 0, 0, 0))\n    )"
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html",
    "href": "posts/general-posts/repost-arima-no-r/index.html",
    "title": "Séries de Tempo no R",
    "section": "",
    "text": "Neste post vou explorar um pouco das funções base do R para montar um modelo SARIMA. O R vem “pré-equipado” com um robusto conjunto de funções para lidar com séries de tempo. Inclusive, como se verá, existe uma class específica de objeto para trabalhar com séries de tempo."
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html#modelagem-sarima",
    "href": "posts/general-posts/repost-arima-no-r/index.html#modelagem-sarima",
    "title": "Séries de Tempo no R",
    "section": "Modelagem SARIMA",
    "text": "Modelagem SARIMA\nAqui a ideia é experimentar com alguns modelos simples. Em especial, o modelo que Box & Jenkins sugerem para a série é de um SARIMA (0, 1, 1)(0, 1, 1)[12] da forma\n\\[\n(1 - \\Delta)(1 - \\Delta^{12})y_{t} = \\varepsilon_{t} + \\theta_{1}\\varepsilon_{t_1} + \\Theta_{1}\\varepsilon_{t-12}\n\\]\nA metodologia correta para a análise seria primeiro fazer testes de raiz unitária para avaliar a estacionaridade da série. Mas só de olhar para as funções de autocorrelação e autocorrelação parcial, fica claro que há algum componente sazonal e que a série não é estacionária.\n\n\n\n\n\n\n\n\n\n\nTeste de raiz unitária\nApenas a título de exemplo, faço um teste Dickey-Fuller (ADF), bastante geral, com constante e tendência temporal linear. Para uma boa revisão metodológica de como aplicar testes de raiz unitária, em partiular o teste ADF, consulte o capítulo de séries não-estacionárias do livro Applied Econometric Time Series do Enders\nAqui, a escolha ótima do lag é feita usando o critério BIC (também conhecido como Critério de Schwarz). Não existe uma função que aplica o teste ADF no pacote base o R. A implementação é feita na função ur.df do pacote urca.\nA estatísitica de teste mais relevante é a tau3 e vê-se, surpreendentemente, que se rejeita a hipótese nula de raiz unitária. As estatísticas phi2 e phi3 são testes-F da significânica conjunta dos termos de constante e de tendência temporal. As estatísticas de teste são convencionais e seguem a notação do livro do Enders citado acima e também do clássico livro do Hamilton.\n\nlibrary(urca)\nadf_test &lt;- ur.df(y, type = \"trend\", selectlags = \"BIC\", lags = 13)\nsummary(adf_test)\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090139 -0.022382 -0.002417  0.021008  0.110003 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.8968049  0.3999031   2.243 0.026859 *  \nz.lag.1      -0.1809364  0.0842729  -2.147 0.033909 *  \ntt            0.0016886  0.0008609   1.961 0.052263 .  \nz.diff.lag1  -0.2588927  0.1134142  -2.283 0.024301 *  \nz.diff.lag2  -0.0986455  0.1070332  -0.922 0.358665    \nz.diff.lag3  -0.0379799  0.1045583  -0.363 0.717097    \nz.diff.lag4  -0.1392651  0.0981271  -1.419 0.158560    \nz.diff.lag5  -0.0283998  0.0963368  -0.295 0.768686    \nz.diff.lag6  -0.1326313  0.0889223  -1.492 0.138581    \nz.diff.lag7  -0.1096365  0.0865862  -1.266 0.208019    \nz.diff.lag8  -0.2348880  0.0829892  -2.830 0.005497 ** \nz.diff.lag9  -0.0926604  0.0843594  -1.098 0.274344    \nz.diff.lag10 -0.2053937  0.0789245  -2.602 0.010487 *  \nz.diff.lag11 -0.1081091  0.0786801  -1.374 0.172127    \nz.diff.lag12  0.6633101  0.0752086   8.820 1.54e-14 ***\nz.diff.lag13  0.3197783  0.0883636   3.619 0.000443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04011 on 114 degrees of freedom\nMultiple R-squared:  0.8781,    Adjusted R-squared:  0.8621 \nF-statistic: 54.75 on 15 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nValue of test-statistic is: -2.147 4.9781 3.4342 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n\n\nPela análise do correlograma do resíduo da regressão, fica claro que ainda há autocorrelação. Novamente, o mais correto seria aplicar o teste Ljung-Box sobre os resíduos para verificar a presença de autocorrelação conjunta nas primeiras k defasagens, mas este não é o foco deste post. Esta pequena digressão exemplifica como a aplicação destes testes em séries de tempo pode não ser tão direto/simples.\n\n\n\n\n\n\n\n\n\nOs gráficos abaixo mostram o correlograma da série após tirarmos a primeira diferença e a primeira diferença sazonal. A partir da análise destes correlogramas poderíamos inferir algumas especificações alternativas para modelos SARIMA e aí, poderíamos escolher o melhor modelo usando algum critério de informação.\nA metodologia Box & Jenkins de análise de séries de tempo tem, certamente, um pouco de arte e feeling. Não é tão imediato entender como devemos proceder e, na prática, faz sentido experimentar com vários modelos alternativos de ordem baixa como SARIMA(1, 1, 1)(0, 1, 1), SARIMA(2, 1, 0)(0, 1, 1), etc.\n\n\n\n\n\n\n\n\n\n\n\nOs modelos\nPara não perder muito tempo experimentando com vários modelos vou me ater a três modelos diferentes. Uma função bastante útil é a auto.arima do pacote forecast que faz a seleção automática do melhor modelo da classe SARIMA/ARIMA/ARMA.\nEu sei que o Schumway/Stoffer, autores do ótimo Time Series Analysis and Its Applications, tem um post crítico ao uso do auto.arima. Ainda assim, acho que a função tem seu mérito e costuma ser um bom ponto de partida para a sua análise. Quando temos poucas séries de tempo para analisar, podemos nos dar ao luxo de fazer a modelagem manualmente, mas quando há centenas de séries, é muito conveniente poder contar com o auto.arima.\nComo o auto.arima escolhe o mesmo modelo do Box & Jenkins eu experimento com uma especificação diferente. Novamente, a título de exemplo eu comparo ambos os modelos SARIMA com uma regressão linear simples que considera uma tendência temporal linear e uma série de dummies sazonais. O modelo é algo da forma\n\\[\ny_{t} = \\alpha_{0} + \\alpha_{1}t + \\sum_{i = 1}^{11}\\beta_{i}s_{i} + \\varepsilon_{t}\n\\]\nOnde \\(s_{i}\\) é uma variável indicadora igual a 1 se \\(t\\) corresponder ao mês \\(i\\) e igual a 0 caso contrário. Vale notar que não podemos ter uma dummy para todos os meses se não teríamos uma matriz de regressores com colinearidade perfeita!\nAqui vou contradizer um pouco o espírito do post novamente para usar o forecast. O ganho de conveniência vem na hora de fazer as previsões. Ainda assim, indico como estimar os mesmos modelos usando apenas funções base do R.\n\n# Usando o forecast\nlibrary(forecast)\nmodel1 &lt;- auto.arima(train)\nmodel2 &lt;- Arima(train, order = c(1, 1, 1), seasonal = c(1, 1, 0))\nmodel3 &lt;- tslm(train ~ trend + season)\n\n\n# Usando apenas funções base\nmodel2 &lt;- arima(\n  trains,\n  order = c(1, 1, 1),\n  seasonal = list(order = c(1, 1, 1), period = 12)\n)\n\n# Extrai uma tendência temporal linear \ntrend &lt;- time(train)\n# Cria variáveis dummies mensais\nseason &lt;- cycle(train)\nmodel3 &lt;- lm(train ~ trend + season)\n\nA saída dos modelos segue abaixo. As saídas dos modelos SARIMA não são muito interessantes. Em geral, não é muito comum avaliar nem a significância e nem o sinal dos coeficientes, já que eles não têm muito valor interpretativo. Uma coisa que fica evidente dos dois modelos abaixo é que o primeiro parece melhor ajustado aos dados pois tem valores menores em todos os critérios de informação considerados.\n\nmodel1\n\nSeries: train \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.3424  -0.5405\ns.e.   0.1009   0.0877\n\nsigma^2 = 0.001432:  log likelihood = 197.51\nAIC=-389.02   AICc=-388.78   BIC=-381\n\n\n\nmodel2\n\nSeries: train \nARIMA(1,1,1)(1,1,0)[12] \n\nCoefficients:\n         ar1      ma1     sar1\n      0.0668  -0.4518  -0.4426\ns.e.  0.3046   0.2825   0.0875\n\nsigma^2 = 0.001532:  log likelihood = 195.14\nAIC=-382.28   AICc=-381.89   BIC=-371.59\n\n\nJá o modelo de regressão linear tem uma saída mais interessante. Note que, por padrão, o primeiro mês foi omitido e seu efeito aparece no termo constante. Na tabela abaixo, vemos que há um efeito positivo e significativo, por exemplo, nos meses 6-8 (junho a agosto), que coincidem com o período de férias de verão no hemisfério norte. Já, em novembro (mês 11) parece haver uma queda na demanda por passagens aéreas.\nNote que o R quadrado da regressão é extremamente elevado e isso é um indício de que algo está errado. Isto, muito provavelmente é resultado da não-estacionaridade da série.\n\nbroom::tidy(model3)\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic   p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  4.71     0.0191    247.     2.81e-149\n 2 trend        0.0106   0.000145   73.2    3.09e- 93\n 3 season2     -0.0134   0.0245     -0.549  5.84e-  1\n 4 season3      0.120    0.0245      4.91   3.32e-  6\n 5 season4      0.0771   0.0245      3.15   2.14e-  3\n 6 season5      0.0675   0.0245      2.75   6.93e-  3\n 7 season6      0.191    0.0245      7.81   4.23e- 12\n 8 season7      0.288    0.0245     11.7    6.32e- 21\n 9 season8      0.278    0.0245     11.4    4.36e- 20\n10 season9      0.143    0.0245      5.82   6.13e-  8\n11 season10     0.00108  0.0245      0.0441 9.65e-  1\n12 season11    -0.141    0.0245     -5.77   7.97e-  8\n13 season12    -0.0248   0.0245     -1.01   3.14e-  1\n\n\nDe fato, olhando para a função de autocorrelação do resíduo do modelo de regressão linear, fica evidente que há autocorrelação. Uma forma de contornar isso seria de incluir um termo ARMA no termo de erro. Novamente, este não é o foco do post e vamos seguir normalmente.\n\nacf(resid(model3))"
  },
  {
    "objectID": "posts/general-posts/repost-arima-no-r/index.html#comparando-as-previsões",
    "href": "posts/general-posts/repost-arima-no-r/index.html#comparando-as-previsões",
    "title": "Séries de Tempo no R",
    "section": "Comparando as previsões",
    "text": "Comparando as previsões\nA maneira mais prática de trabalhar com vários modelos ao mesmo tempo é agregando eles em listas e aplicando funções nessas listas.\nAbaixo eu aplico a função forecast para gerar as previsões 24 períodos a frente nos três modelos. Depois, eu extraio somente a estimativa pontual de cada previsão.\n\nmodels &lt;- list(model1, model2, model3)\nyhat &lt;- lapply(models, forecast, h = 24)\nyhat_mean &lt;- lapply(yhat, function(x) x$mean)\n\nComparamos a performance de modelos de duas formas: (1) olhando para medidas de erro (o quão bem o modelo prevê os dados do test) e (2) olhando para critérios de informação (o quão bem o modelo se ajusta aos dados do train).\nOs critérios de informação têm todos uma interpretação bastante simples: quanto menor, melhor. Tipicamente, o AIC tende a escolher modelos sobreparametrizados enquanto o BIC tende a escolher modelos mais parcimoniosos.\nJá a comparação de medidas de erro não é tão simples. Pois ainda que um modelo tenha, por exemplo, um erro médio quadrático menor do que outro, não é claro se esta diferença é significante. Uma maneira de testar isso é via o teste Diebold-Mariano, que compara os erros de previsão de dois modelos. Implicitamente, contudo, ele assume que a diferença entre os erros de previsão é covariância-estacionária (também conhecido como estacionário de segunda ordem ou fracamente estacionário). Dependendo do contexto, esta pode ser uma hipótese mais ou menos razoável.\n\ncompute_error &lt;- function(model, test) {\n  y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    y - yhat\n}\n\ncompute_error_metrics &lt;- function(model, test) {\n\n    y &lt;- test\n    yhat &lt;- model$mean\n    train &lt;- model$x\n\n    # Calcula o erro de previsao\n    error &lt;- y - yhat\n    \n    # Raiz do erro quadrado medio\n    rmse &lt;- sqrt(mean(error^2))\n    # Erro medio absoluto\n    mae &lt;- mean(abs(error))\n    # Erro medio percentual\n    mape &lt;- mean(abs(100 * error / y))\n    # Root mean squared scaled error\n    rmsse &lt;- sqrt(mean(error^2 / snaive(train)$mean))\n\n    # Devolve os resultados num list\n    list(rmse = rmse, mae = mae, mape = mape, rmsse = rmsse)\n    \n\n}\n\ncompute_ics &lt;- function(model) {\n\n    # Extrai criterios de informacao\n    aic  &lt;- AIC(model)\n    bic  &lt;- BIC(model)\n\n    # Devolve os resultados num list\n    list(aic = aic, bic = bic)\n\n} \n\nfcomparison &lt;- lapply(yhat, function(yhat) compute_error_metrics(yhat, test))\nicc &lt;- lapply(models, compute_ics)\n\ncomp_error &lt;- do.call(rbind.data.frame, fcomparison)\nrownames(comp_error) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\ncomp_ics &lt;- do.call(rbind.data.frame, icc)\nrownames(comp_ics) &lt;- c(\"AutoArima\", \"Manual SARIMA\", \"OLS\")\n\nA tabela abaixo mostra os critérios AIC e BIC para os três modelos. Em ambos os casos, o SARIMA(0, 1, 1)(0, 1, 1)[12] parece ser o escolhido. Este tipo de feliz coincidência não costuma acontecer frequentemente na prática, mas neste caso ambos os critérios apontam para o mesmo modelo.\n\ncomp_ics\n\n                    aic       bic\nAutoArima     -389.0155 -380.9970\nManual SARIMA -382.2807 -371.5894\nOLS           -342.2930 -303.2681\n\n\nNa comparação de medidas de erro, o SARIMA(0, 1, 1)(0, 1, 1)[12] realmente tem uma melhor performance, seguido pelo OLS e pelo SARIMA(1, 1, 1)(1, 1, 0)[12].\n\ncomp_error\n\n                    rmse        mae     mape      rmsse\nAutoArima     0.09593236 0.08959921 1.463477 0.03939512\nManual SARIMA 0.11688549 0.10780460 1.762201 0.04806577\nOLS           0.10333715 0.09384411 1.549261 0.04266214\n\n\nSerá que esta diferença é significante? Vamos comparar os modelos SARIMA. Pelo teste DM ela é sim. Lembre-se que o teste DM é, essencialmente, um teste Z de que \\(e_{1} - e_{2} = 0\\) ou \\(e_{1} = e_{2}\\), onde os valores são a média dos erros de previsão dos modelos.\n\nerrors &lt;- lapply(yhat, function(yhat) compute_error(yhat, test))\ne1 &lt;- errors[[1]]\ne2 &lt;- errors[[2]]\n\ndm.test(e1, e2, power = 2)\n\n\n    Diebold-Mariano Test\n\ndata:  e1e2\nDM = -4.4826, Forecast horizon = 1, Loss function power = 2, p-value =\n0.0001691\nalternative hypothesis: two.sided\n\n\nVale notar que o teste DM serve para comparar os erros de previsão de quaisquer modelos. Como o teste não faz qualquer hipótese sobre “de onde vem” os erros de previsão, ele pode ser utilizado livremente. Vale lembrar também que este teste não deve ser utilizado para escolher o melhor modelo, já que ele compara apenas a capacidade preditiva de dois modelos alternativos.\nOutro ponto, também complicado, é de qual a medida de erro que se deve escolher. O teste DM implicitamente usa o erro médio quadrático, mas há várias outras alternativas. Uma breve discussão pode ser vista aqui.\nPor fim, o gráfico abaixo mostra as previsões dos modelos alternativos contra a série real.\n\nplot(test, ylim = c(5.8, 6.5), col = \"#8ecae6\", lwd = 2, type = \"o\")\nlines(yhat_mean[[1]], col = \"#ffb703\")\nlines(yhat_mean[[2]], col = \"#fb8500\")\nlines(yhat_mean[[3]], col = \"#B86200\")\ngrid()\nlegend(\"topleft\", lty = 1,\n       legend = c(\"Test\", \"AutoArima\", \"Arima\", \"OLS\"),\n       col = c(\"#8ecae6\", \"#ffb703\", \"#fb8500\", \"#B86200\"))"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "A inflação voltou a ser uma pauta, não só no Brasil, mas no mundo todo, nos últimos meses. No Brasil, a combinação de câmbio desvalorizado, desajustes logísticos, crise hídrica e choques de preços externos, culminaram no maior nível de inflação desde 2002.\nMesmo em países avançados, os níveis de inflação estão em altas históricas. Nos Estados Unidos, por exemplo, o nível do CPI está no valor mais alto desde o final dos anos 1970.\nVisualizar a magnitude da inflação no Brasil pode ser um pouco desafiador. A série do IPCA é calculada desde 1979. O número de cidades avaliadas pelo índice cresceu no tempo: nos primeiros anos o índice contemplava Rio de Janeiro, Porto Alegre, Belo Horizonte, Recife, São Paulo, Brasília, Belém, Fortaleza, Salvador e Curitiba. Em 1991, Goiânia entra no índice e, mais recentemente, em 2014, Vitória e Campo Grande também entraram no cômputo do índice.\nMais importante do que a variação no número das cidades, é o período hiperinflacionário da década de 1980. Os números da inflação são incomparavelmente mais altos do que os atuais. Como regra, os cortes temporais mais relevantes para enxergar a inflação são Jul/94 (Plano Real), Jul/99 (Regime de Metas de Inflação), Mai/00 (Lei de Responsabilidade Fiscal) e Mai/03 (pós choque de 2002).\nNeste post vou mostrar o comportamento da inflação desde 1999.\n\n\n\n# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)\n\n\n\n\nImporto os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  right_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)\n\n\n\n\nO cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nUma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso.\n\n\n\n\nOutra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com um\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#pacotes",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#pacotes",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "# Os pacotes utilizados\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(spiralize)\nlibrary(showtext)\nlibrary(lubridate)\nlibrary(GetBCBData)\nlibrary(forecast)"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#importando-os-dados",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#importando-os-dados",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Importo os dados diretamente da API do Banco Central usando o pacote GetBCBData.\n\n# Importar os dados\nipca &lt;- gbcbd_get_series(id = 433, first.date = as.Date(\"1998-01-01\"))\n\nO código abaixo cria alguns elementos que serão necessários para as visualizações.\n\ngrid &lt;- tibble(\n  ref.date = seq(as.Date(\"1998-01-01\"),as.Date(\"2022-12-01\"), \"1 month\"))\n\nipca_meta &lt;- tibble(\n  year = 1999:2022,\n  meta = c(8, 6, 4, 3.5, 4, 5.5, rep(4.5, 14), 4.25, 4, 3.75, 3.5),\n  banda = c(rep(2, 4), rep(2.5, 3), rep(2, 11), rep(1.5, 6)),\n  banda_superior = meta + banda,\n  banda_inferior = meta - banda)\n\n\nipca &lt;- ipca %&gt;% \n  right_join(grid) %&gt;% \n  fill() %&gt;% \n  mutate(month = month(ref.date, label = TRUE, abbr = TRUE, locale = \"pt_BR\"),\n         year = year(ref.date),\n         value = value / 100,\n         acum12m = RcppRoll::roll_prodr(1 + value, n = 12) - 1,\n         acum12m = acum12m * 100) %&gt;% \n  left_join(ipca_meta) %&gt;% \n  mutate(deviation = acum12m - meta)"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#inflação",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#inflação",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "O cálculo do IPCA avalia a variação de preços de uma cesta fixa de bens. A composição desta cesta vem da Pesquisa de Orçamentos Familiares (POF), também feita pelo IBGE, que tem como objetivo representar a “cesta de consumo média” dos brasileiros.\nFormalmente, o IPCA é um índice de Laspeyeres que compara o preço de mercado de uma cesta de bens \\(W = (w_{1}, w_{2}, \\dots, w_{n})\\) no período \\(t\\) e compara o preço de mercado desta mesma cesta de bens no período anterior \\(t-1\\).\n\\[\nI_{t} = \\frac{\\sum_{i}w_{i}P_{i, t}}{\\sum_{i}w_{i}P_{i, t-1}}\n\\]\nNa prática, esta cesta de bens agrega um misto de gastos com habitação, alimentos, vestuário, combustíveis, serivços, etc. Como existe uma clara sazonalidade tanto na oferta como na demanda por estes bens, o IPCA também apresenta sazonalidade.\nIsso fica claro, quando se visualiza a variação do IPCA mês a mês como no gráfico abaixo. Tipicamente, os meses de nov-dez-jan-fev apresentam valores mais altos do que os meses de mai-jun-jul-ago.\nO ponto fora da curva em novembro é referente ao ano de 2002. Ele é resultado, em parte, da incerteza econômica e da enorme desvalorização cambial que se seguiu à primeira eleição do ex-presidente Lula.\n\ny &lt;- ts(ipca$value * 100, start = c(1998, 1), frequency = 12)\n\nggseasonplot(y) +\n  scale_color_viridis_d(name = \"\") +\n  labs(title = \"Sazonalidade do IPCA\",\n       x = NULL,\n       y = \"%\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#no-longo-prazo",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#no-longo-prazo",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Uma das maneiras de contornar o problema da sazonalidade é acumulando o índice anualmente, pois aí temos o efeito sazonal de todos os meses. O problema é que aí teríamos de sempre comparar anos completos, impossibilitando um diagnóstico da inflação em maio do ano corrente.\nA solução é acumular o índice em doze meses, criando um “ano artificial”. A lógica é que, independentemente do momento do tempo em que estamos, contamos o efeito de todos os meses individualmente. Chamando de \\(z_{t}\\) o índice acumulado e \\(y_{t}\\) o valor do IPCA no período \\(t\\) temos que:\n\\[\nz_{t} = [(1 + y_{t-12})(1 + y_{t-11})\\dots(1 + y_{t-1})] - 1\n\\]\nAssim, para o mês de maio de 2022 teríamos\n\\[\nz_{\\text{maio/22}} = [(1 + y_{\\text{abril/22}})(1 + y_{\\text{mar/22}})\\dots(1 + y_{\\text{jun/21}})] - 1\n\\]\n\n\nO código abaixo apresenta o histórico do IPCA acumulado em 12 meses junto com a sua média histórica.\nVemos como o período de 2002 é muito fora da curva. Também se nota como o Brasil quase sempre tem uma inflação relativamente alta, próxima de 5-6%. O único período de inflação baixa na série é no período 2017-19.\nVale notar que só faz sentido em falar na “média histórica” de uma série se ela for estacionária.\n\navgipca &lt;- mean(ipca$acum12m, na.rm = TRUE)\nlabel &lt;- paste0(\"Média: \", round(avgipca, 1))\ndftext &lt;- tibble(x = as.Date(\"2019-01-01\"), y = 7, label = label)\n\nggplot(na.omit(ipca), aes(x = ref.date, y = acum12m)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0) +\n  geom_hline(yintercept = avgipca,\n             linetype = 2) +\n  geom_text(data = dftext,\n            aes(x = as.Date(\"2019-01-01\"), y = 7, label = label),\n            family = \"Helvetica\",\n            size = 3) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  labs(title = \"Inflação Acumulada em 12 Meses\",\n       x = NULL,\n       y = \"%\",\n       caption = \"Fonte: BCB\") +\n  theme_minimal() +\n  theme(\n    text = element_text(family = \"Helvetica\"),\n    axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nA comparação com a média histórica é interessante, mas é mais relevante olhar para a meta de inflação. Desde 1999, o Conselho Monetário Nacional (CMN) define, em geral com 2 anos de antecedência, a meta de inflação que o Banco Central deve perseguir. Este tipo de estrutura institucional é bastante popular mundo afora e há uma sólida básica empírica e teórica que a sustenta.\nBasicamente, ao estabelecer antecipadamente uma meta de inflação, o CMN tenta ancorar as expectativas das pessoas na economia. Se a meta for crível, as pessoas tendem a montar planos de negócios, firmar contratos e tomar escolhas tomando a inflação futura como a meta de inflação.\nA meta de inflação também incentiva o Banco Central a priorizar o controle dos preços acima de outras prioridades concorrentes.\n\nhistorico_meta &lt;- ipca %&gt;% \n  select(ref.date, acum12m, meta, banda_inferior, banda_superior) %&gt;% \n  pivot_longer(cols = -ref.date) %&gt;% \n  na.omit()\n\nggplot() +\n  geom_line(data = filter(historico_meta, name == \"acum12m\"),\n            aes(x = ref.date, y = value),\n            color = \"#e63946\") +\n  geom_line(data = filter(historico_meta, name != \"acum12m\"),\n            aes(x = ref.date, y = value, color = name)) +\n  geom_hline(yintercept = 0) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  scale_color_manual(name = \"\",\n                     values = c(\"#264653\", \"#264653\", \"#2a9d8f\"),\n                     labels = c(\"Banda Inf.\", \"Banda Sup.\", \"Meta\")) +\n  labs(title = \"Histórico de Metas de Inflação\",\n       y = \"% (acum. 12 meses)\",\n       x = NULL,\n       caption = \"Fonte: BCB\") +\n  theme_vini\n\n\n\n\n\n\n\n\nNo Brasil, o CMN define a meta e também a sua banda superior e inferior. No gráfico acima, vemos que a inflação esteve relativamente dentro da meta no período 2003-2014, ainda que depois de 2008 o índice tenha ficado sempre acima do centro da meta.\nApós a alta de 2016 o índice cai para níveis bastante baixos e o CMN inclusive começa a progressivamente reduzir a meta de inflação. Este é o único período desde 1999 em que a inflação fica, em alguns momentos, abaixo da meta.\nEvidentemente, a inflação dispara no período recente. O enorme descolamento com a meta põe o sistema em cheque, à medida em que as pessoas acreditam cada vez menos na habilidade do Banco Central em honrar seu compromisso."
  },
  {
    "objectID": "posts/general-posts/repost-ipca-visualizacao/index.html#enxergando-a-distribuição",
    "href": "posts/general-posts/repost-ipca-visualizacao/index.html#enxergando-a-distribuição",
    "title": "Visualizando o IPCA",
    "section": "",
    "text": "Outra maneira de enxergar o IPCA é olhar para a distribuição dos seus valores. O gráfico abaixo é um histograma com um\n\nggplot(ipca, aes(x = value * 100)) +\n  geom_histogram(aes(y = ..density..), bins = 38,\n                 fill = \"#264653\",\n                 color = \"white\") +\n  geom_hline(yintercept = 0) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(-0.5, 3, 0.25)) +\n  labs(title = \"Distribuição do IPCA mensal\", x = NULL, y = \"Densidade\") +\n  theme_vini\n\n\n\n\n\n\n\n\nUm tipo de visualização interessante é montar um “grid” como o abaixo. Um problema é que a escala de cores acaba um pouco deturpada por causa do período 2002-3.\n\nggplot(filter(ipca, ref.date &gt;= as.Date(\"1999-01-01\")),\n       aes(x = month, y = year, fill = acum12m)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_c(name = \"\", option = \"magma\", breaks = seq(0, 16, 2)) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nAlgum nível de arbitrariedade é necessário para resolver isso. Uma opção é agrupar os dados em grupos. Como a meta mais longa que tivemos foi a de 4.5 com intervalo de 2, monto os grupos baseado nisso. A aceleração recente da inflação, a meu ver, fica mais evidente desta forma. Inclusive, conseguimos enxergar melhor como a inflação recente é mais intensa do que a de 2015-17.\n\ngrupos &lt;- ipca %&gt;% \n  filter(ref.date &gt;= as.Date(\"1999-01-01\")) %&gt;% \n  mutate(ipca_group = findInterval(acum12m, c(0, 2.5, 4.5, 6.5, 10, 15)),\n         ipca_group = factor(ipca_group))\nlabels &lt;- c(\"&lt; 2.5\", \"2.5-4.5\", \"4.5-6.5\", \"6.5-10\", \"10-15\", \"&gt; 15\")\n\nggplot(grupos,\n       aes(x = month, y = year, fill = ipca_group)) +\n  geom_tile(color = \"gray90\") +\n  scale_y_continuous(breaks = 1999:2022) +\n  scale_fill_viridis_d(name = \"\", option = \"magma\", labels = labels) +\n  labs(x = NULL, y = NULL) +\n  theme_vini\n\n\n\n\n\n\n\n\nTambém podemos plotar a distribuição dos dados a cada ano. Note que a visualização abaixo não é nada convencional do ponto de vista estatístico.\nAinda assim, é interessante ver a dispersão enorme dos valores em 2002-3 e também como a inflação ficou relativamente bem-comportada nos anos seguintes até se tornar mais volátil e enviesada para a direita em 2016.\n\nggplot(ipca, aes(x = factor(year), y = value * 100, fill = factor(year))) +\n  stat_halfeye(\n    justification = -.5,\n    .width = 0, \n    point_colour = NA) +\n  \n  scale_fill_viridis_d(option = \"magma\") +\n  scale_y_continuous(breaks = seq(-0.5, 3, 0.5)) +\n  guides(fill = \"none\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL, y = NULL) +\n  theme(plot.background = element_rect(fill = \"gray80\", color = NA),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nPor fim, uma visualização que eu acho muito interessante, mas que é ainda menos convencional é de espiral. Eu sinceramente não sei se é possível enxergar muita coisa de interessante desta forma e eu, infelizmente, não entendo o pacote spiralize o suficiente para entender de que forma é possível ajustar a escala das cores.\nFica uma visualização bacana, mas não acho que seja a mais apropriada.\n\nipca &lt;- na.omit(ipca)\n\nspiral_initialize_by_time(xlim = range(ipca$ref.date),\n                          unit_on_axis = \"months\",\n                          period = \"year\",\n                          period_per_loop = 1,\n                          padding = unit(2, \"cm\"))\n                          #vp_param = list(x = unit(0, \"npc\"), just = \"left\"))\nspiral_track(height = 0.8)\nlt = spiral_horizon(ipca$ref.date, ipca$acum12m, use_bar = TRUE)\n\ns = current_spiral()\nd = seq(30, 360, by = 30) %% 360\nmonth_name &lt;- as.character(lubridate::month(1:12, label = TRUE, abbr = FALSE, locale = \"pt_BR\"))\nfor(i in seq_along(d)) {\n  foo = polar_to_cartesian(d[i]/180*pi, (s$max_radius + 1)*1.05)\n  grid.text(month_name[i], x = foo[1, 1], y = foo[1, 2], default.unit = \"native\",\n            rot = ifelse(d[i] &gt; 0 & d[i] &lt; 180, d[i] - 90, d[i] + 90), gp = gpar(fontsize = 10))\n}"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html",
    "title": "OLS com matrizes",
    "section": "",
    "text": "Uma forma instrutiva de entender o modelo de regressão linear é expressando ele em forma matricial. Os cursos introdutórios de econometria costumam omitir esta abordagem e expressam todas as derivações usando somatórios, deixando a abordagem matricial para cursos mais avançados. É bastante simples computar uma regressão usando apenas matrizes no R.\nDe fato, um dos objetos fundamentais do R é a e muitas das operações matriciais (decomposições, inversa, transposta, etc.) já estão implementadas em funções base. Uma var \\(k\\) .\nNeste post vou mostrar como fazer uma regressão linear usando somente matrizes no R. Além disso, vou computar algumas estatísticas típicas (t, F)\nO modelo linear é da forma\n\\[\ny_{t} = x^\\intercal_{t} \\beta + e_{t}\n\\]\nonde \\(x^\\intercal\\) é o vetor transposto de \\(x\\) . É importante sempre ter em mente a dimensão destes vetores. O vetor \\(y_{t}\\) é \\(n\\times1\\) onde \\(n\\) representa o número de observações na amostra. O vetor \\(\\beta\\) é \\(k\\times1\\) onde \\(k\\) é o número de regressores (ou variáveis explicativas). Como há \\(n\\) observações para cada uma dos \\(k\\) regressores, \\(x\\) é \\(k\\times1\\) ; o detalhe é que \\(x = (1 \\, \\,x_{1} \\, \\dots \\,x_{k-1})\\) , onde cada \\(x_{i}\\) é \\(n\\times 1\\) e \\(1\\) é um vetor de uns \\(n\\times1\\) . Finalmente, \\(e_{t}\\) é \\(n\\times1\\) . Temos então que:\n\\[\n\\begin{pmatrix}\ny_{1} \\\\\ny_{2}\\\\\n\\vdots\\\\\ny_{n}\n\\end{pmatrix}=\n\\begin{pmatrix}\n1\\\\\nx_{1}\\\\\n\\vdots\\\\\nx_{k-1}\n\\end{pmatrix}^\\intercal\n\\begin{pmatrix}\n\\beta_{0}\\\\\n\\beta_{1}\\\\\n\\vdots\\\\\n\\beta_{k-1}\n\\end{pmatrix}+\n\\begin{pmatrix}\ne_{1}\\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{n}\n\\end{pmatrix}\n\\]\nonde:\n\\[\n\\begin{bmatrix}\nx_{1} & = (x_{11} & x_{12} & x_{13} & \\dots & x_{1n})\\\\\nx_{2} & = (x_{21} & x_{22} & x_{23} & \\dots & x_{2n})\\\\\nx_{3} & = (x_{31} & x_{32} & x_{33} & \\dots & x_{3n})\\\\\n\\vdots\\\\\nx_{k-1} & = (x_{(k-1)1} & x_{(k-1)2} & x_{(k-1)3} & \\dots & x_{(k-1)n})\n\\end{bmatrix}\n\\]\nQueremos encontrar o vetor \\(\\hat{\\beta}\\) que minimiza o a soma do quadrado dos erros, isto é, que minimiza\n\\[\nS(\\beta) = \\sum_{t = 1}^{T}(y_{t} - x^\\intercal_{t}\\beta)^{2}\n\\]\nEncontramos o ponto crítico derivando a expressão acima e igualando-a a zero. O resultado é o conhecido estimador de mínimos quadrados:\n\\[\n\\hat{\\beta} = \\left ( \\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} \\right )^{-1} \\sum_{t = 1}^{T}x_{t}y_{t}\n\\]\nPara reescrever as equações acima usando matrizes usamos os seguintes fatos:\n\\[\n\\sum_{t = 1}^{T}x_{t}x_{t}^{^\\intercal} = X^\\intercal X\n\\]\nonde \\(X\\) é uma matriz \\(n \\times k\\)\n\\[\n\\sum_{t = 1}^{T}x_{t}y_{t} = X^\\intercal y\n\\]\nonde \\(X^\\intercal y\\) é \\(k \\times 1\\) . Lembre-se que uma hipótese do modelo linear é de que \\(X\\) é uma matriz de posto completo, logo \\(X^\\intercal X\\) possui inversa e podemos escrever:\n\\[\n\\hat{\\beta} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html#exemplo-salário-wooldridge",
    "title": "OLS com matrizes",
    "section": "Exemplo: salário (Wooldridge)",
    "text": "Exemplo: salário (Wooldridge)\nComo exemplo vou usar um exemplo clássico de livro texto de econometria: uma regressão de salário (rendimento) contra algumas variáveis explicativas convencionais: anos de educação, sexo, anos de experiência, etc. As bases de dados do livro Introductory Econometrics estão disponíveis no pacote wooldridge. O código abaixo carrega a base de dados .\n\nlibrary(wooldridge)\n# Carrega a base\ndata(wage2)\n# Remove os valores ausentes (NAs)\nsal &lt;- na.omit(wage2)\n\n\n\n\n\n\n\n\nwage\nhours\nIQ\nKWW\neduc\nexper\ntenure\nage\nmarried\nblack\nsouth\nurban\nsibs\nbrthord\nmeduc\nfeduc\nlwage\n\n\n\n\n1\n769\n40\n93\n35\n12\n11\n2\n31\n1\n0\n0\n1\n1\n2\n8\n8\n6.645091\n\n\n3\n825\n40\n108\n46\n14\n11\n9\n33\n1\n0\n0\n1\n1\n2\n14\n14\n6.715383\n\n\n4\n650\n40\n96\n32\n12\n13\n7\n32\n1\n0\n0\n1\n4\n3\n12\n12\n6.476973\n\n\n5\n562\n40\n74\n27\n11\n14\n5\n34\n1\n0\n0\n1\n10\n6\n6\n11\n6.331502\n\n\n7\n600\n40\n91\n24\n10\n13\n0\n30\n0\n0\n0\n1\n1\n2\n8\n8\n6.396930\n\n\n9\n1154\n45\n111\n37\n15\n13\n1\n36\n1\n0\n0\n0\n2\n3\n14\n5\n7.050990\n\n\n10\n1000\n40\n95\n44\n12\n16\n16\n36\n1\n0\n0\n1\n1\n1\n12\n11\n6.907755\n\n\n11\n930\n43\n132\n44\n18\n8\n13\n38\n1\n0\n0\n0\n1\n1\n13\n14\n6.835185\n\n\n14\n1318\n38\n119\n24\n16\n7\n2\n28\n1\n0\n0\n1\n3\n1\n10\n10\n7.183871\n\n\n15\n1792\n40\n118\n47\n16\n9\n9\n34\n1\n0\n0\n1\n1\n1\n12\n12\n7.491087\n\n\n\n\n\n\n\nA base traz 663 observações de 17 variáveis. A função str é útil para entender a estrutura dos dados.\n\n# Dimensão da base (# linhas  # colunas)\ndim(sal)\n\n[1] 663  17\n\n# Descrição da base\nstr(sal)\n\n'data.frame':   663 obs. of  17 variables:\n $ wage   : int  769 825 650 562 600 1154 1000 930 1318 1792 ...\n $ hours  : int  40 40 40 40 40 45 40 43 38 40 ...\n $ IQ     : int  93 108 96 74 91 111 95 132 119 118 ...\n $ KWW    : int  35 46 32 27 24 37 44 44 24 47 ...\n $ educ   : int  12 14 12 11 10 15 12 18 16 16 ...\n $ exper  : int  11 11 13 14 13 13 16 8 7 9 ...\n $ tenure : int  2 9 7 5 0 1 16 13 2 9 ...\n $ age    : int  31 33 32 34 30 36 36 38 28 34 ...\n $ married: int  1 1 1 1 0 1 1 1 1 1 ...\n $ black  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ south  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urban  : int  1 1 1 1 1 0 1 0 1 1 ...\n $ sibs   : int  1 1 4 10 1 2 1 1 3 1 ...\n $ brthord: int  2 2 3 6 2 3 1 1 1 1 ...\n $ meduc  : int  8 14 12 6 8 14 12 13 10 12 ...\n $ feduc  : int  8 14 12 11 8 5 11 14 10 12 ...\n $ lwage  : num  6.65 6.72 6.48 6.33 6.4 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n - attr(*, \"na.action\")= 'omit' Named int [1:272] 2 6 8 12 13 19 20 21 31 36 ...\n  ..- attr(*, \"names\")= chr [1:272] \"2\" \"6\" \"8\" \"12\" ...\n\n\nO modelo proposto é o abaixo:\n\\[\n\\text{lwage}_{t} = \\beta_{0} + \\beta_{1}\\text{educ}_{t} + \\beta_{2}\\text{exper}_{t} + \\beta_{3}\\text{exper}^{2}_{t} + \\beta_{4}\\text{tenure}_{t} + \\beta{5}\\text{married}_{t} + u_{t}\n\\]\nonde:\n\nlwage = logaritmo natural do salário\neduc = anos de educação\nexper = anos de experiência (trabalhando)\ntenure = anos trabalhando com o empregador atual\nmarried = dummy (1 = casado, 0 = não-casado)\n\nHá 6 coeficientes para estimar logo \\(k = 6\\) . Além disso, como há \\(663\\) observações temos que \\(n = 663\\) . A matriz de “dados” é da forma:\n\\[\nX = \\begin{bmatrix}\n1 & 12 & 11 & 121 & 2 & 1\\\\\\\\\n1 & 14 & 11 & 121 & 9 & 1\\\\\\\\\n1 & 12 & 13 & 169 & 7 & 1\\\\\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\\n1 & 13 & 10 & 100 & 3 & 1\n\\end{bmatrix}_{663\\times 6}\n\\]\nO código abaixo monta a matriz \\(X\\) acima. As funções head e tail podem ser usadas para verificar as primeiras e últimas linhas da matriz. Vale esclarecer dois pontos sobre o código a ser usado neste post. Primeiro, quando se cria um objeto usando o operador &lt;- pode-se forçar o R a imprimir o seu valor colocando a expressão entre parêntesis. Por exemplo, teste (x &lt;- 3). O segundo ponto é que o operador de multiplicação matricial é %*%.\n\n# Define alguns valores úteis: \n## N = número de observações\n## k = número de regressores\n## const = vetor com 1^\\intercals (uns)\nN &lt;- 663; k &lt;- 6; const &lt;- rep(1, 663)\n# Monta a matriz de observações da regressão\nX &lt;- cbind(const, sal$educ, sal$exper, sal$exper^2, sal$tenure, sal$married)\nX &lt;- as.matrix(X)\n# Define o nome das colunas da matriz de observações\ncolnames(X) &lt;- c(\"const\", \"educ\", \"exper\", \"exper2\", \"tenure\", \"married\")\n# Função para verificar as primeiras linhas da matriz X\nhead(X)\n\n     const educ exper exper2 tenure married\n[1,]     1   12    11    121      2       1\n[2,]     1   14    11    121      9       1\n[3,]     1   12    13    169      7       1\n[4,]     1   11    14    196      5       1\n[5,]     1   10    13    169      0       0\n[6,]     1   15    13    169      1       1\n\n# Função para verificar as últimas linhas da matriz X\ntail(X)\n\n       const educ exper exper2 tenure married\n[658,]     1   12     9     81      2       1\n[659,]     1   16     8     64     10       1\n[660,]     1   12    11    121      3       1\n[661,]     1   12     9     81      3       1\n[662,]     1   16    10    100      9       1\n[663,]     1   13    10    100      3       1\n\n\nLembrando que o problema de mínimos quadrados é de encontrar os valores de \\(\\beta\\) que minimizam a soma dos erros ao quadrado.\n\\[\n\\underset{\\beta}{\\text{Min }} e^\\intercal e\n\\]\nAbrindo mais a expressão acima:\n\\[\n\\begin{align}\n  e^\\intercal e & = (y - X\\beta )^\\intercal(y - X\\beta ) \\\\\\\\\n      & = y^\\intercal y - y^\\intercal X\\beta - \\beta ^\\intercal X^\\intercal y + \\beta ^\\intercal X^\\intercal X \\beta \\\\\\\\\n      & = y^\\intercal y - 2 y^\\intercal X \\beta + \\beta^\\intercal X^\\intercal X \\beta\n\\end{align}\n\\]\nDerivando em relação a \\(\\beta\\) e igualando a zero chega-se no estimador de MQO\n\\[\n\\beta_{\\text{MQO}} = (X^\\intercal X)^{-1}X^\\intercal y\n\\]\nO código abaixo computa \\(\\beta_{\\text{MQO}}\\) . Note que os parêntesis por fora da expressão forçam o R a imprimir o valor do objeto. Além disso, como estamos multiplicando matrizes/vetores usamos %*%.\n\n# Define o vetor y (log do salário)\ny &lt;- sal$lwage\n# Computa a estimativa para os betas\n(beta &lt;- solve(t(X) %*% X) %*% t(X) %*% y)\n\n                [,1]\nconst   5.3563606713\neduc    0.0767258959\nexper   0.0104985672\nexper2  0.0002881339\ntenure  0.0091039254\nmarried 0.2002468574\n\n\nOs valores estimados dos betas são reportados na tabela abaixo.\n\ntabela &lt;- as.data.frame(round(beta, 4))\ncolnames(tabela) &lt;- c(\"Coeficiente estimado\")\nround(beta, 4) %&gt;%\n  kable(align = \"c\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\n\nconst\n5.3564\n\n\neduc\n0.0767\n\n\nexper\n0.0105\n\n\nexper2\n0.0003\n\n\ntenure\n0.0091\n\n\nmarried\n0.2002"
  },
  {
    "objectID": "posts/general-posts/repost-ols-com-matrizes/index.html#resíduo-e-variância",
    "href": "posts/general-posts/repost-ols-com-matrizes/index.html#resíduo-e-variância",
    "title": "OLS com matrizes",
    "section": "Resíduo e variância",
    "text": "Resíduo e variância\nO resíduo do modelo é simplesmente a diferença entre o observado \\(y_{t}\\) e o estimado \\(\\hat{y_{t}}\\) . Isto é,\n\\[\n\\hat{e}_{t} = y_{t} - \\hat{y}_{t} = y_{t} - x_{t}^\\intercal\\hat{\\beta}\n\\]\nou, de forma equivalente,\n\\[\n\\hat{e} = y - X\\hat{\\beta}\n\\]\n\n# Computa o resíduo da regressão\nu_hat &lt;- y - X %*% beta\n\nUsando o histograma pode-se visualizar a distribuição dos resíduos.\n\nhist(u_hat, breaks = 30, freq = FALSE, main = \"Histograma dos resíduos\")\n\n\n\n\nO estimador da variância é dado por:\n\\[\n\\hat{\\sigma}^{2} = \\frac{1}{N-k}\\sum_{t = 1}^{N}\\hat{e}_{t}^{2}\n\\]\nSubstituindo os valores calculados acima chegamos em:\n\\[\n\\hat{\\sigma}^{2} = \\frac{\\hat{e}^\\intercal\\hat{e}}{N-k} = 0.1403927\n\\]"
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html",
    "href": "posts/general-posts/repost-definindo-objetos/index.html",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Há dois operadores para definir um objeto no R: = e &lt;-. A maior parte dos usuários parece preferir o último apesar dele parecer um tanto inconveniente. Em teclados antigos, havia uma tecla específica com o símbolo &lt;-, mas em teclados ABNT modernos ele exige três teclas para ser escrito.\nPara contornar este incômodo é comum criar um atalho no teclado para esse símbolo; o RStudio, por exemplo, tem um atalho usando a teclas Alt e - em conjunto. Mas ainda assim fica a questão: por que não utilizar o =? A resposta curta é que o símbolo &lt;- é a melhor e mais consistente forma de definir objetos R. Na prática, contudo, há poucas diferenças entre as expressões e elas dificilmente vão fazer alguma diferença. Podemos começar com um exemplo bastante simples para entender estas diferenças.\n\n\nO código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1\n\n\n\n\n\nEu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%.\n\n\n\nNo geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html#qual-a-diferença",
    "href": "posts/general-posts/repost-definindo-objetos/index.html#qual-a-diferença",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "O código abaixo cria duas variáveis, x e y, cujos valores são a sequência \\(1, 2, 3, 4, 5\\). Até aí tudo igual. A função all.equal certifica que os objetos são iguais e a função rm “deleta” os objetos. Esta última vai ser conveniente para manter os exemplos organizados.\n\nx = 1:5\ny &lt;- 1:5\n\nall.equal(x, y)\n\n[1] TRUE\n\nrm(x, y)\n\nAgora considere o código abaixo. A função median está sendo aplicada em x &lt;- 1:5. O que acontece desta vez? O resultado é que é criada uma variável x com valor 1 2 3 4 5 e também é impresso a mediana deste vetor, i.e., 3.\n\nmedian(x &lt;- 1:5)\n\n[1] 3\n\nx\n\n[1] 1 2 3 4 5\n\nrm(x)\n\nPoderíamos fazer o mesmo usando =, certo? Errado. Aí está uma das primeiras diferenças entre estes operadores. O código abaixo calcula a mediana do vetor, mas não cria um objeto chamado x com valor 1 2 3 4 5. Por quê? O problema é que o operador = tem duas finalidades distintas. Ele serve tanto para definir novos objetos, como em x = 2, como também para definir o valor dos argumentos de uma função, como em rnorm(n = 10, mean = 5, sd = 1). Coincidentemente, o nome do primeiro argumento da função median é x. Logo, o código abaixo é interpretado como: tire a mediana do vetor 1 2 3 4 5. O mesmo acontece com outras funções (ex: mean, var, etc.)\n\nmedian(x = 1:5)\n\n[1] 3\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nrm(x)\n\nWarning in rm(x): object 'x' not found\n\n\nOutro exemplo em que há divergência entre os operadores é com o comando lm. Usando &lt;- podemos escrever, numa única linha, um comando que define um objeto lm (resultado de uma regressão) ao mesmo tempo em que pedimos ao R para imprimir os resultados desta regressão. O código abaixo faz justamente isto.\n\n# Imprime os resultados da regressão e salva as info num objeto chamado 'fit'\nsummary(fit &lt;- lm(mpg ~ wt, data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# Verifica que é, de fato, um objeto lm\nclass(fit)\n\n[1] \"lm\"\n\nrm(fit)\n\nNote que isto não é possível com o operador =. Isto acontece, novamente, porque o = é interpretado de maneira diferente quando aparece dentro de uma função. É necessário quebrar o código em duas linhas.\n\n# Este exemplo não funciona\nsummary(fit = lm(mpg ~ wt, data = mtcars))\n\nError in summary.lm(fit = lm(mpg ~ wt, data = mtcars)): argument \"object\" is missing, with no default\n\n\n\n# É preciso reescrever o código em duas linhas\nfit = lm(mpg ~ wt, data = mtcars)\nsummary(fit)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\nrm(fit)\n\nHá também algumas pequenas divergências pontuais. Os primeiros dois argumentos da função lm são formula e data. Considere o código abaixo. Sem executar o código qual deve ser o resultado?\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\n\nEstamos aplicando a função lm em dois argumentos. O primeiro deles se chama formula e é definido como mpg ~ wt, o segundo é chamado data e é definido como os valores no data.frame mtcars. Ou seja, o resultado deve ser o mesmo do exemplo acima com median(x &lt;- 1:5). A função é aplicada sobre os argumentos e os objetos formula e data são criados.\n\nfit &lt;- lm(formula &lt;- mpg ~ wt, data &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = formula &lt;- mpg ~ wt, data = data &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nformula\n\nmpg ~ wt\n\nhead(data)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(fit, formula, data)\n\nNote que usei os nomes dos argumentos apenas para exemplificar o caso. Pode-se colocar um nome diferente, pois não estamos “chamando” o argumento e sim especificando qual valor/objeto a função deve utilizar.\n\n# Exemplo utilizando nomes diferentes\nfit &lt;- lm(a &lt;- \"mpg ~ wt\", b &lt;- mtcars)\nfit\n\n\nCall:\nlm(formula = a &lt;- \"mpg ~ wt\", data = b &lt;- mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nprint(a)\n\n[1] \"mpg ~ wt\"\n\nhead(b)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nrm(list = ls())\n\nO mesmo não é possível com =, por causa da duplicidade apontada acima.\n\nfit = lm(a = \"mpg ~ wt\", b = mtcars)\n\nError in terms.formula(formula, data = data): argument is not a valid model\n\n\nHá ainda mais alguns exemplos estranhos, resultados da ordem que o R processa os comandos. O segundo código abaixo, por exemplo, não funciona. Isto acontece porque o &lt;- tem “prioridade” e é lido primeiro.\n\n(x = y &lt;- 5)\n\n[1] 5\n\n(x &lt;- y = 5)\n\nError in x &lt;- y = 5: could not find function \"&lt;-&lt;-\"\n\n\nÉ difícil encontrar desvatagens em usar o &lt;- (além da dificuldade de escrevê-lo num teclado normal). Mas há pelo menos um caso em que ele pode levar a problemas. O código abaixo mostra como este operador pode ser sensível a espaços em branco. No caso, define-se o valor de x como -2. O primeiro teste verifica se o valor de x é menor que \\(-1\\). Logo, espera-se que o código imprima \"ótimo\" pois -2 é menor que -1. Já o segundo teste faz quase o mesmo. A única diferença é um espaço em branco, mas agora ao invés de um teste, a linha de código define o valor de x como 1 e imprime \"ótimo\", pois o valor do teste (por padrão) é TRUE.\nAssim como muitos dos exemplos acima, é difícil imaginar que isto possa ser um problema real. Eventualmente, podemos apagar espaços em branco usando o ‘localizar e substituir’ e isto talvez leve a um erro como o abaixo.\n\n# Define x como -2\nx &lt;- -2\n# Se x for menor que -1 (menos um) então \"ótimo\"\nif (x &lt; -1) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Verifica o valor de x\nx\n\n[1] -2\n\n# Mesma linha com uma diferença sutil\nif (x &lt;-1 ) \"ótimo\" else \"errado\"\n\n[1] \"ótimo\"\n\n# Agora acabamos de mudar o valor de x (e não há aviso algum!)\nx\n\n[1] 1"
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html#mais-um-adendo",
    "href": "posts/general-posts/repost-definindo-objetos/index.html#mais-um-adendo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "Eu citei apenas dois operadores: = e &lt;-; mas na verdade há ainda outros: &lt;&lt;-, -&gt; e -&gt;&gt; (veja help(\"assignOps\")). Os operadores com “flecha dupla” são comumente utilizadas dentro de funções para usos específicos. Algumas pessoas acham que o operador -&gt; é mais intuitivo quando usado com “pipes”. Pessoalmente, acho este tipo de código abominável.\n\nAirPassengers |&gt; \n  log() |&gt; \n  window(start = c(1955, 1), end = c(1958, 12)) -&gt; sub_air_passengers\n\nsub_air_passengers\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1955 5.488938 5.451038 5.587249 5.594711 5.598422 5.752573 5.897154 5.849325\n1956 5.648974 5.624018 5.758902 5.746203 5.762051 5.924256 6.023448 6.003887\n1957 5.752573 5.707110 5.874931 5.852202 5.872118 6.045005 6.142037 6.146329\n1958 5.828946 5.762051 5.891644 5.852202 5.894403 6.075346 6.196444 6.224558\n          Sep      Oct      Nov      Dec\n1955 5.743003 5.613128 5.468060 5.627621\n1956 5.872118 5.723585 5.602119 5.723585\n1957 6.001415 5.849325 5.720312 5.817111\n1958 6.001415 5.883322 5.736572 5.820083\n\n\nAlém dos operadores base, o popular pacote magrittr também possui um novo tipo de operador. Muitos conhecem o %&gt;% que ajuda a formar “pipes” e carregar objetos progressivamente em funções distintas. Menos conhecido, mas igualmente poderoso, o %&lt;&gt;% faz algo parecido.\n\nlibrary(magrittr)\nx &lt;- 1:5\nprint(x)\n\n[1] 1 2 3 4 5\n\nx %&lt;&gt;% mean() %&lt;&gt;% sin()\nprint(x)\n\n[1] 0.14112\n\n\nNote que o código acima é equivalente ao abaixo. A vantagem do operador %&lt;&gt;% é de evitar a repetição do x &lt;- x ... o que pode ser conveniente quando o nome do objeto é longo. Da mesma forma, também pode ser aplicado para transformar elementos em uma lista ou colunas num data.frame.\n\nx &lt;- 1:5\nx &lt;- x %&gt;% mean() %&gt;% sin()\n\nNão recomendo o uso nem do -&gt; e nem do %&lt;&gt;%."
  },
  {
    "objectID": "posts/general-posts/repost-definindo-objetos/index.html#resumo",
    "href": "posts/general-posts/repost-definindo-objetos/index.html#resumo",
    "title": "Definindo objetos no R. = ou <- ?",
    "section": "",
    "text": "No geral, o operador &lt;- é a forma mais “segura” de se definir objetos. De fato, atualmente, este operador é considerado o mais apropriado. O livro Advanced R, do influente autor Hadley Wickham, por exemplo, recomenda que se use o operador &lt;- exclusivamente para definir objetos.\nA inconveniência de escrevê-lo num teclado moderno é contornada, como comentado acima, por atalhos como o Alt + - no RStudio. Em editores de texto como o VSCode, Sublime Text ou Notepad++ também é possível criar atalhos personalizados para o &lt;-. Pessoalmente, eu já me acostumei a digitar &lt;- manualmente e não vejo grande problema nisso. Acho que o = tem seu valor por ser mais intutivo, especialmente para usuários que já tem algum conhecimento de programação, mas acabo usando mais o &lt;-. Por fim, o &lt;- fica bem bonito quando se usa uma fonte com ligaturas como o Fira Code.\nÉ importante frisar que o &lt;- continua sendo o operador de predileção da comunidade dos usuários de R e novas funções/pacotes devem ser escritas com este sinal. Por sorte há opções bastante simples que trocam os = para &lt;- corretamente como o formatR apresentado abaixo. Veja também o addin do pacote styler. Ou seja, é possível escrever seu código usando = e trocá-los por &lt;- de maneira automática se for necessário.\n\nlibrary(formatR)\ntidy_source(text = \"x = rnorm(n = 10, mean = 2)\", arrow = TRUE)\n\nx &lt;- rnorm(n = 10, mean = 2)\n\n\nO ponto central deste post, na verdade, é mostrar como os operadores &lt;- e = são muito similares na prática e que a escolha entre um ou outro acaba caindo numa questão subjetiva. Há quem acredite ser mais cômodo usar o = não só porque ele é mais fácil de escrever, mas também porque ele é mais próximo de universal. Várias linguagens de programação comuns para a/o economista (Matlab, Python, Stata, etc.) usam o sinal de igualdade para definir objetos e parece estranho ter que usar o &lt;- somente para o R.\nEm livros de econometria ambos os operadores são utilizados. Os livros Introduction to Econometrics with R, Applied Econometrics with R, Arbia. Spatial Econometrics, entre outros, usam o &lt;-. Já os livros Tsay. Financial Time Series e Shumway & Stoffer. Time Series Analysis usam =.\nNo fim, use o operador que melhor"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html",
    "href": "posts/general-posts/repost-emv-no-r/index.html",
    "title": "EMV no R",
    "section": "",
    "text": "A estimação por máxima verossimilhança possui várias boas propriedades. O estimador de máxima verossimilhança (EMV) é consistente (converge para o valor verdadeiro), normalmente assintótico (distribuição assintórica segue uma normal padrão) e eficiente (é o estimador de menor variância possível). Por isso, e outros motivos, ele é um estimador muito comumemente utilizado em estatística e econometria.\nA intuição do EMV é a seguinte: temos uma amostra e estimamos os parâmetros que maximizam a probabilidade de que esta amostra tenha sido gerada por uma certa distribuição de probabilidade. Em termos práticos, eu primeiro suponho a forma da distribuição dos meus dados (e.g. normal), depois eu estimo os parâmetros \\(\\mu\\) e \\(\\sigma\\) de maneira que eles maximizem a probabilidade de que a minha amostra siga uma distribuição normal (tenha sido “gerada” por uma normal).\nHá vários pacotes que ajudam a implementar a estimação por máxima verossimilhança no R. Neste post vou me ater apenas a dois pacotes: o optimx e o maxLik. O primeiro deles agrega funções de otimização de diversos outros pacotes numa sintaxe unificada centrada em algumas poucas funções. O último é feito especificamente para estimação de máxima verossimilhança então traz algumas comodidades como a estimação automática de erros-padrão.\nVale lembrar que o problema de MV é, essencialmente, um problema de otimização, então é possível resolvê-lo simplesmente com a função optim do R. Os dois pacotes simplesmente trazem algumas comodidades.\n\nlibrary(maxLik)\nlibrary(optimx)\n# Para reproduzir os resultados\nset.seed(33)"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-optimx",
    "href": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-optimx",
    "title": "EMV no R",
    "section": "Usando o pacote optimx",
    "text": "Usando o pacote optimx\nA função optim já é bastante antiga e um novo pacote, chamado optimx, foi criado. A ideia do pacote é de agregar várias funções de otimização que estavam espalhadas em diversos pacotes diferentes. As principais funções do pacote são optimx e optimr. Mais informações sobre o pacote podem ser encontradas aqui.\nA sintaxe das funções é muito similar à sintaxe original do optim. O código abaixo faz o mesmo procedimento de estimação que o acima. Por padrão a função executa dois otimizadores: o BFGS e Nelder-Mead\n\nsummary(fit &lt;- optimx(par = 1, fn = ll_pois, x = amostra))\n\n                p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nNelder-Mead 4.9375 2202.837     32     NA    NA        0 TRUE TRUE 0.001\nBFGS        4.9380 2202.837     34      9    NA        0 TRUE TRUE 0.002\n\n\nUma das principais vantagens do optimx é a possibilidade de usar vários métodos de otimização numérica numa mesma função.\n\nfit &lt;- optimx(\n  par = 1,\n  fn = ll_pois,\n  x = amostra,\n  method = c(\"nlm\", \"BFGS\", \"Rcgmin\", \"nlminb\")\n  )\n\nfit\n\n             p1    value fevals gevals niter convcode kkt1 kkt2 xtime\nnlm    4.937998 2202.837     NA     NA     8        0 TRUE TRUE 0.001\nBFGS   4.938000 2202.837     34      9    NA        0 TRUE TRUE 0.002\nRcgmin 4.937999 2202.837    708    112    NA        1 TRUE TRUE 0.039\nnlminb 4.938000 2202.837     10     12     9        0 TRUE TRUE 0.001\n\n\nComo este exemplo é bastante simples os diferentes métodos parecem convergir para valores muito parecidos."
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-maxlik",
    "href": "posts/general-posts/repost-emv-no-r/index.html#usando-o-pacote-maxlik",
    "title": "EMV no R",
    "section": "Usando o pacote maxLik",
    "text": "Usando o pacote maxLik\nA função maxLik (do pacote homônimo) traz algumas comodidades: primeiro, ela maximiza as funções de log-verossimilhança, ou seja, não é preciso montar a função com sinal de menos como fizemos acima; segundo, ela já calcula erros-padrão e estatísticas-t dos coeficientes estimados. Além disso, ela também facilita a implementação de gradientes e hessianas analíticos e conta com métodos de otimização bastante populares como o BHHH. Mais detalhes sobre a função e o pacote podem ser encontradas aqui.\nPara usar a função precisamos primeiro reescrever a função log-verossimilhança, pois agora não precisamos mais buscar o negativo da função. Como o R já vem com as funções de densidade de várias distribuições podemos tornar o código mais enxuto usando o dpois que implementa a função densidade da Poisson. O argumento log = TRUE retorna as probabilidades \\(p\\) como \\(log(p)\\).\n\nll_pois &lt;- function(x, theta) {\n    ll &lt;- dpois(x, theta, log = TRUE)\n    return(sum(ll))\n}\n\nO comando abaixo executa a estimação. Note que a saída agora traz várias informações relevantes.\n\nsummary(fit &lt;- maxLik(ll_pois, start = 1, x = amostra))\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 8 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07617   64.83  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos implementar manualmente o gradiente e a hessiana da função. Neste caso, a estimativa do parâmetro continua a mesma mas o erro-padrão diminui um pouco. Note que também podemos fornecer estas informações para a função optimx. Derivando a função de log-verossimilhança:\n\\[\n\\begin{align}\n  \\frac{\\partial \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda} & = \\frac{1}{\\lambda}\\sum_{k = 1}^{n}x_{k} - n \\\\\n  \\frac{\\partial^2 \\mathcal{L}(\\lambda ; x)}{\\partial\\lambda^2} & = -\\frac{1}{\\lambda^2}\\sum_{k = 1}^{n}x_{k}\n\\end{align}\n\\]\nO código abaixo implementa o gradiente e a hessiana e faz a estimação. O valor estimado continua praticamente o mesmo, mas o erro-padrão fica menor.\n\ngrad_pois &lt;- function(x, theta) {\n  (1 / theta) * sum(x) - length(x)\n  }\n\nhess_pois &lt;- function(x, theta) {\n    -(1 / theta^2) * sum(x)\n}\n\nfit2 &lt;- maxLik(\n  ll_pois,\n  grad = grad_pois,\n  hess = hess_pois,\n  start = 1,\n  x = amostra\n  )\n\nsummary(fit2)\n\n--------------------------------------------\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 7 iterations\nReturn code 1: gradient close to zero (gradtol)\nLog-Likelihood: -2202.837 \n1  free parameters\nEstimates:\n     Estimate Std. error t value Pr(&gt; t)    \n[1,]  4.93800    0.07027   70.27  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#consistência",
    "href": "posts/general-posts/repost-emv-no-r/index.html#consistência",
    "title": "EMV no R",
    "section": "Consistência",
    "text": "Consistência\nVamos montar um experimento simples: simulamos 5000 amostras aleatórias de tamanho 1000 seguindo uma distribuição \\(N(2, 3)\\); computamos as estimativas para \\(\\mu\\) e \\(\\sigma\\) e suas respectivas variâncias assintóticas e depois analisamos suas propriedades.\n\nSimular uma amostra segundo uma distribuição.\nEstimata os parâmetros da distribuição.\nCalcula a variância assintótica dos estimadores.\nRepete 5000 vezes os passos 1-3.\n\nO código abaixo implementa exatamente este experimento. Note que a matriz de informação de Fisher é aproximada pela hessiana.\n\nr &lt;- 5000\nn &lt;- 1000\n\nestimativas &lt;- matrix(ncol = 4, nrow = r)\n\nfor(i in 1:r) {\n    x &lt;- rnorm(n = n, mean = 2, sd = 3)\n    \n    fit &lt;- optimr(\n      par = c(1, 1),\n      fn = ll_norm,\n      method = \"BFGS\",\n      hessian = TRUE\n      )\n    # Guarda o valor estimado do parâmetro\n    estimativas[i, 1:2] &lt;- fit$par\n    estimativas[i, 3:4] &lt;- diag(n * solve(fit$hess))\n}\n\nA consistência dos estimadores \\(\\hat{\\theta}_{MV}\\) significa que eles aproximam os valores verdadeiros do parâmetros \\(\\theta_{0}\\) à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande \\(\\mathbb{N} \\to \\infty\\) então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros \\(\\hat{\\theta}_{\\text{MV}} \\to \\theta_{0}\\)\nO código abaixo calcula a média das estimativas para cada parâmetro - lembrando que \\(\\mu_{0} = 2\\) e que \\(\\sigma_{0} = 3\\). Além disso, o histograma das estimativas mostra como as estimativas concentram-se em torno do valor verdadeiro do parâmetro (indicado pela linha vertical).\n\n# | fig-width: 10\npar(mfrow = c(1, 2))\n# Consistência dos estimadores de MV\nmu &lt;- estimativas[, 1]; sigma &lt;- estimativas[, 2]\nmean(mu)\n\n[1] 2.000883\n\nmean(sigma)\n\n[1] 2.997335\n\nhist(mu, main = bquote(\"Estimativas para \"~~mu), freq = FALSE, xlim = c(1.5, 2.5))\nabline(v = 2, col = \"indianred\")\nhist(sigma, main = bquote(\"Estimativas para \"~~sigma), freq = FALSE, xlim = c(2.7, 3.3))\nabline(v = 3, col = \"indianred\")"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#normalmente-assintótico",
    "href": "posts/general-posts/repost-emv-no-r/index.html#normalmente-assintótico",
    "title": "EMV no R",
    "section": "Normalmente assintótico",
    "text": "Normalmente assintótico\nDizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Especificamente, temos que:\n\\[\nz_{\\theta} = \\sqrt{N}\\frac{\\hat{\\theta}_{MV} - \\theta}{\\sqrt{\\text{V}_{ast}}} \\to \\mathbb{N}(0, 1)\n\\]\nonde \\(\\text{V}_{ast}\\) é a variância assintótica do estimador. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\nNo loop acima usamos o fato que a matriz de informação de Fisher pode ser estimada pela hessiana. O código abaixo calcula a expressão acima para os dois parâmetros e apresenta o histograma dos dados com uma normal padrão superimposta.\n\n# Normalidade assintótica\n\n# Define objetos para facilitar a compreensão\nmu_hat &lt;- estimativas[, 1]\nsigma_hat &lt;- estimativas[, 2]\nvar_mu_hat &lt;- estimativas[, 3]\nvar_sg_hat &lt;- estimativas[, 4]\n\n# Centra a estimativa\nmu_centrado &lt;- mu_hat - 2 \nsigma_centrado &lt;- sigma_hat - 3\n# Computa z_mu z_sigma\nmu_normalizado &lt;- sqrt(n) * mu_centrado / sqrt(var_mu_hat)\nsigma_normalizado &lt;- sqrt(n) * sigma_centrado / sqrt(var_sg_hat)\n\n\n# Monta o gráfico para mu\n\n# Eixo x\ngrid_x &lt;- seq(-3, 3, 0.01)\n\nhist(\n  mu_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[mu]),\n  freq = FALSE,\n  xlim = c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[mu]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")\n\n\n\n\n\n\n\n\n\n# Monta o gráfico para sigma2\nhist(\n  sigma_normalizado,\n  main = bquote(\"Histograma de 5000 simulações para z\"[sigma]),\n  freq = FALSE,\n  xlim =c(-3, 3),\n  breaks = \"fd\",\n  xlab = bquote(\"z\"[sigma]),\n  ylab = \"Densidade\"\n  )\nlines(grid_x, dnorm(grid_x, mean = 0, sd = 1), col = \"dodgerblue\")\nlegend(\"topright\", lty = 1, col = \"dodgerblue\", legend = \"Normal (0, 1)\")"
  },
  {
    "objectID": "posts/general-posts/repost-emv-no-r/index.html#escolha-de-valores-inicias",
    "href": "posts/general-posts/repost-emv-no-r/index.html#escolha-de-valores-inicias",
    "title": "EMV no R",
    "section": "Escolha de valores inicias",
    "text": "Escolha de valores inicias\nComo comentei acima, o método de estimação por MV exige que o usuário escolha valores iniciais (chutes) para os parâmetros que se quer estimar.\nO exemplo abaixo mostra o caso em que a escolha de valores iniciais impróprios leva a estimativas muito ruins.\n\n# sensível a escolha de valores inicias\nx &lt;- rnorm(n = 1000, mean = 15, sd = 4)\nfit &lt;- optim(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = \"BFGS\",\n  hessian = TRUE\n  )\n\nfit\n\n$par\n[1] 618.6792 962.0739\n\n$value\n[1] 7984.993\n\n$counts\nfunction gradient \n     107      100 \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]\n[1,]  0.001070703 -0.0013531007\n[2,] -0.001353101  0.0001884928\n\n\nNote que as estimativas estão muito distantes dos valores corretos \\(\\mu = 15\\) e \\(\\sigma = 4\\). Uma das soluções, já mencionada acima, é de usar os momentos da distribuição como valores iniciais.\nO código abaixo usa os momentos empíricos como valores inicias para \\(\\mu\\) e \\(\\sigma\\):\n\\[\n\\begin{align}\n  \\mu_{inicial} & = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\\\\n  \\sigma_{inicial} & = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n} (x_{i} - \\mu_{inicial})^2}\n\\end{align}\n\\]\n\n(chute_inicial &lt;- c(mean(x), sqrt(var(x))))\n\n[1] 14.859702  3.930849\n\n(est &lt;- optimx(par = chute_inicial, fn = ll_norm))\n\n                  p1       p2    value fevals gevals niter convcode kkt1 kkt2\nNelder-Mead 14.85997 3.929097 2787.294     47     NA    NA        0 TRUE TRUE\nBFGS        14.85970 3.928884 2787.294     15      2    NA        0 TRUE TRUE\n            xtime\nNelder-Mead 0.001\nBFGS        0.001\n\n\nAgora as estimativas estão muito melhores. Outra opção é experimentar com otimizadores diferentes. Aqui a função optimx se prova bastante conveniente pois admite uma grande variedade de métodos de otimizãção.\nNote como os métodos BFGS e CG retornam valores muito distantes dos verdadeiros. Já o método bobyqa retorna um valor corretor para o parâmetro da média, mas erra no parâmetro da variânica. Já os métodos nlminb e Nelder-Mead ambos retornam os valores corretos.\n\n# Usando outros métodos numéricos\noptimx(\n  par = c(1, 1),\n  fn = ll_norm,\n  method = c(\"BFGS\", \"Nelder-Mead\", \"CG\", \"nlminb\", \"bobyqa\")\n  )\n\n                   p1         p2    value fevals gevals niter convcode  kkt1\nBFGS        618.67917 962.073907 7984.993    107    100    NA        1  TRUE\nNelder-Mead  14.85571   3.929621 2787.294     83     NA    NA        0  TRUE\nCG           46.43586 628.570987 7358.601    204    101    NA        1  TRUE\nnlminb       14.85970   3.928883 2787.294     23     47    19        0  TRUE\nbobyqa       15.20011   8.993240 3211.556    109     NA    NA        0 FALSE\n             kkt2 xtime\nBFGS        FALSE 0.007\nNelder-Mead  TRUE 0.001\nCG          FALSE 0.008\nnlminb       TRUE 0.001\nbobyqa      FALSE 0.033\n\n\nVale notar também alguns detalhes técnicos da saída. Em particular, convcode == 0 significa que o otimizador conseguiu convergir com sucesso, enquanto convcode == 1 indica que o otimizador chegou no límite máximo de iterações sem convergir. Vemos que tanto o BFGS e o CG falharam em convergir e geraram os piores resultados.\nJá o kkt1 e kkt2 verificam as condições de Karush-Kuhn-Tucker (às vezes apresentadas apenas como condições de Kuhn-Tucker). Resumidamente, a primeira condição verifica a parte necessária do teorema enquanto a segunda condição verifica a parte suficiente. Note que o bobyqa falha em ambas as condições (pois ele não é feito para este tipo de problema).\nOs métodos que retornam os melhores valores, o Nelder-Mead e nlminb são os únicos que convergiram com sucesso e que atenderam a ambas as condições de KKT. Logo, quando for comparar os resltados de vários otimizadores distintos, vale estar atento a estes valores.\nMais detalhes sobre os métodos podem ser encontrados na página de ajuda da função ?optimx."
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html",
    "title": "Usando fontes com showtext no R",
    "section": "",
    "text": "Criar boas visualizações é parte importante de qualquer análise de dados.\nA tipografia de um texto deve complementar a mensagem e o tom que se quer comunicar e o mesmo vale para visualizações com dados. A fonte do texto ajuda a transmitir informação e pode comunicar, por exemplo, maior sobriedade, profissionalismo, etc.\nO pacote showtext, desenvolvido por yixuan, facilita a importação e o uso de fontes em gráficos no R. O pacote funciona com uma variedade de extensões de fontes, não sendo limitado como o extrafont, por exemplo, a arquivos .ttf."
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html#base-r",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html#base-r",
    "title": "Usando fontes com showtext no R",
    "section": "Base R",
    "text": "Base R\nPara modificar a fonte dos elementos textuais dos gráficos feitos com o plot() é preciso ajustar o argumento family. Usando as funções base do R, este argumento aparece dentro da função par (que configura vários parâmetros dos gráficos).\nO código abaixo mostra como trocar a fonte do gráfico.\n\n# Define a fonte padrão do gráfico\npar(family = \"alice\")\n# Monta um scatter plot de exemplo\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30)\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nNote que todos os objetos textuais (título, legenda, etc.) são convertidos para a mesma fonte. Caso se queira fontes diferentes para estes elementos é preciso especificá-los adequadamente. Por exemplo, para trocar somente a fonte do título\ntitle(\"nome_do_titulo\", family = \"nome_fonte\")\n\nplot(mpg ~ wt, data = mtcars, ylab = \"Milhas por galão\", xlab = \"Peso (ton.)\")\ntitle(\"Peso x eficiência de carros\", family = \"RobCond\")\ntext(labels = \"exemplo de texto\", x = 3, y = 30, family = \"Montserrat\")\nlegend(\"topright\", legend = \"exemplo de legenda\", pch = 1)\n\n\n\n\n\n\n\n\nVale notar que, uma vez definida a fonte usando a função par, todos os gráficos subsequentes vão usar esta fonte. Para trocar a fonte é preciso usar a função par novamente. Além da fonte também é possível trocar a ênfase (e.g. face = c(\"bold\", \"italic\")) e também o tamanho da letra (e.g. cex.axis = 1.5, cex.main = 2)."
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html#ggplot2",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html#ggplot2",
    "title": "Usando fontes com showtext no R",
    "section": "ggplot2",
    "text": "ggplot2\nTambém é possível trocar a fonte de gráficos feitos com outros pacotes, como o ggplot2. O exemplo abaixo monta um gráfico similar ao que foi feito acima. Modifica-se a fonte dentro da função theme. Esta função é um tanto particular, então vale a pena discorrer um pouco sobre ela. Ela é basicamente usada para modificar elementos do gráfico. Há quatro elementos principais, dos quais só nos interessa um: o element_text. São seis os principais elementos textuais que pode-se modificar:\n\naxis.text - texto dos eixos (em geral, os números do eixo);\naxis.title - nome do eixo (e.g. “Milhas por galão” no exemplo acima);\nlegend.text - texto da legenda;\nlegend.title - título da legenda;\nplot.title - título do gráfico;\ntext - todos os acima.\n\nPode-se ser mais específico com o texto dos eixos usando axis.text.x e axis.text.y, por exemplo. O último dos elementos listados acima funciona como um “coringa”, ele serve para modificar de uma vez só todos os elementos textuais de um gráfico. No exemplo abaixo modifico somente o text.\n\nlibrary(ggplot2)\n\np &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE) +\n  labs(\n    x = \"Peso (ton.)\",\n    y = \"Milhas por galão\",\n    title = \"Eficiência e peso de carros\",\n    subtitle = \"Regressão entre o peso de diferentes carros e sua eficiência energética\",\n    caption = \"Fonte: Motor Trend US Magazine 1974\"\n    )\n\n\np + theme(text = element_text(family = \"Montserrat\", size = 10))\n\n\n\n\n\n\n\n\nO próximo exemplo mostra como modificar alguns dos diferentes elementos do gráfico. Aproveito a variável cyl (cilindradas) para diferenciar os carros em três grupos para que o gráfico agora tenha uma legenda.\n\np +\n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme(\n    # Modifica o texto (números) dos eixos x e y\n    axis.text = element_text(family = \"alice\"),\n    # Modifica o título do eixo (i.e. Milhas por galão)\n    axis.title = element_text(family = \"Montserrat\"),\n    # Modifica o título da legenda (Cilindros)\n    legend.title = element_text(family = \"HelveticaNeue\"),\n    # Modifica o texto da legenda (i.e. 4, 6, 8)\n    legend.text = element_text(family = \"HelveticaNeue\"),\n    # Modifica o título do gráfico\n    plot.title = element_text(family = \"RobCond\", size = 20)\n    )\n\n\n\n\n\n\n\n\nTalvez o jeito mais sensato de usar fontes com o ggplot2 seja primeiro especificar uma fonte padrão para o gráfico usando text e depois calibrar as exceções. Os elementos textuais como axis.title e legend.text copiam as propriedades definidas em text.\nNo exemplo abaixo defino que todos os elementos textuais ser escritos em Arial simples em tamanho 12 na cor \"gray20\". Depois disso defino que o título deve ter mais destaque com Arial em negrito (bold) num tamanho maior e numa cor mais escura. Por fim, defino que o rodapé do gráfico seja escrito em fonte menor e em itálico.\n\ntheme_custom &lt;- theme_light() +\n  theme(\n    # Modifica todos os elementos textuais do gráfico\n    text = element_text(family = \"Arial\", size = 12, color = \"gray20\"),\n    # Modifica apenas o título\n    plot.title = element_text(face = \"bold\", size = 14, color = \"gray10\"),\n    # Modifica apenas a nota no rodapé\n    plot.caption = element_text(face = \"italic\", size = 8)\n  )\n\np + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n\n\n\n\n\n\n\nPor último, também pode ser interessante usar fontes diferentes para representar dados diferentes. Isto é possível usando o argumento family dentro do aes. Da mesma forma, seria possível também representar grupos de dados diferentes com tamanhos de fontes diferentes ou mesmo destacar algum grupo específico com itálico.\n\nnomes &lt;- row.names(mtcars)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_text(aes(label = nomes, family = c(\"Arial\", \"alice\", \"Montserrat\")[cyl]))"
  },
  {
    "objectID": "posts/general-posts/repost-tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "href": "posts/general-posts/repost-tutorial-showtext/index.html#anexo-problemas-com-dpi-e-rmarkdown",
    "title": "Usando fontes com showtext no R",
    "section": "Anexo: problemas com DPI e RMarkdown",
    "text": "Anexo: problemas com DPI e RMarkdown\nApesar de muito conveniente, o showtext não é inteiramente desprovido de problemas. Dois problemas que enfrento com alguma recorrência são diferenças de DPI na hora de exportar gráficos e problemas com RMarkdown.\nO problema com o RMarkdown é mais simples. Em versões antigas do RMarkdown e do showtext era necessário adicionar um argumento fig.showtext = TRUE em todos os chunks em que um gráfico usando showtext fosse renderizado. Alternativamente, podia-se modificar esta opção globalmente inserido o seguinte código no início do documento RMarkdown.\n\nknitr::opts_chunk$set(\n  fig.showtext = TRUE,\n  fig.retina = 1\n  )\n\nAcredito, mas não tenho certeza, de que este problema sumiu em versões mais recentes dos pacotes, pois com frequência eu esqueço de adicionar estes argumentos mas não encontro problemas na prática.\nO problema com o DPI na hora de exportar gráficos é mais complicado. Por problemas de DPI quero dizer quando o showtext “desenha” o texto num DPI diferente do ggplot2. O resultado é que o texto fica ou grande ou pequeno demais. Por padrão o showtext utiliza o DPI em 96.\nVamos montar um gráfico para ilustrar o problema.\n\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exportar o gráfico em alta resolução\nggsave(\"meu_grafico.jpeg\", grafico, dpi = 300)\n\nQuando vamos abrir o arquivo que foi exportado temos o resultado abaixo.\n\nComo a imagem foi salva com DPI mais elevado o texto fica menor do que deveria; em casos mais extremos o texto fica minúsculo a ponto de ser ilegível. Há duas formas de tentar contornar este problema: (1) reduzir o DPI dentro de ggsave; ou (2) modificar o DPI do showtext.\nVamos tentar a primeira solução: modificar o ggsave para o DPI padrão do showtext. Agora o texto está maior mas a proporção dos elementos está péssima! O resultado é pior do que o problema inicial.\n\n# Exportar a imagem num dpi menor\nggsave(\"meu_grafico_96.jpeg\", grafico, dpi = 96)\n\n\n\n\n\n\nA segunda solução é modificar as opções internas do showtext. Isto é bastante simples e pode ser feito com o showtext_opts(dpi = 300) e chamando novamente a função showtext_auto().\n\n# Ajusta o DPI do showtext\nshowtext_opts(dpi = 300)\n# \"Ativa\" o showtext novamente\nshowtext_auto()\n\n# Refaz o gráfico (isto é importante!)\ngrafico &lt;- p + \n  geom_point(aes(color = as.factor(cyl))) +\n  scale_color_discrete(name = \"Cilindros\") +\n  theme_custom\n\n# Exporta um novo gráfico\nggsave(\"meu_grafico_300.jpeg\", grafico, dpi = 300)\n\nAgora o tamanho do texto está correto e a imagem como um todo está em alta resolução. Um problema é que a imagem ficou bastante grande, mas isto pode ser ajustado variando os argumentos width e height da função ggsave.\n\nVale notar que, a depender do seu sistema operacional, modificar o DPI padrão do showtext pode distorcer os gráficos dentro do R ou RStudio. Na prática o melhor workflow pode ser de modificar o DPI do showtext apenas no momento de exportar os gráficos."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html",
    "href": "posts/general-posts/2023-09-tidyverse/index.html",
    "title": "A filosofia do Tidyverse",
    "section": "",
    "text": "O tidyverse é um metapacote, ou conjunto de pacotes. Pode-se pensar no tidyverse como uma família de pacotes, unidos por uma filosofia comum; grosso modo, os pacotes que compõem o tidyverse tem o mesmo objetivo: facilitar a manipulação de dados. Estes pacotes criaram uma nova forma de se escrever código, que substitui boa parte ou mesmo todas as funções base do R. Atualmente, o tidyverse parece estar se consolidando como a variante dominante do R. De fato, a maior parte dos pacotes do tidyverse consta na lista dos mais baixados no repositório CRAN.\nO tidyverse é intimamente ligado com o estatístico Hadley Wickham, criador ou co-criador da maioria dos seus pacotes, e autor do influente artigo em que ele define o que é “tidy” data. Ele também é autor de diversos livros didáticos como R for Data Science e ggplot2: Elegant graphics for Data Analysis, que ajudaram a popularizar o tidyverse.\nWickham também tem posição de liderança dentro da Posit (antigamente conhecida como RStudio), que mantém o GUI mais popular de R e que patrocina inúmeras atividades vinculadas com o aprendizado de R, que costumam enfatizar os pacotes do tidyverse. De fato, tornou-se lugar comum começar a se ensinar R pelo tidyverse como se vê pela prevalência de cursos no Coursera ou Datacamp.\nQuando se olha para a curta história do tidyverse é difícil explicar o porquê do seu enorme sucesso, mas é fato que este conjunto de pacotes se tornou um dialeto dominante dentro da comunidade do R. As funções do tidyverse tem algumas vantagens importantes sobre o base-R.\n\n\nAs funções do tidyverse possuem uma característica ausente na maior parte das funções base do R: consistência. As funções do tidyverse oferecem consistência sintática: o nome da funções segue certas convenções e a ordem dos argumentos segue regras previsíveis.\nUm exemplo imediato é o pacote stringr, que serve para manipulação de strings. Todas as funções deste pacote começam com prefixo str_ e seus argumentos seguem a lógica: string e pattern como em str_detect(string, pattern)1. Além disso, as funções são mais otimizadas em relação às funções base do R.\nO purrr faz algo similar, ao simplificar a família de funções apply em diversas funções map_*. Neste caso, além da consistência sintática, as funções map_* também garantem a consistência do output, em termos da classe do objeto que é retornado como resultado da função. Isto é uma grande vantagem, especialmente quando comparado com a função sapply que “simplifica” o output de maneiras às vezes imprevisíveis.\nEm termos de eficiência, o tidyverse costuma ganhar das funções equivalentes em base-R. O dplyr/tidyr, de maneira geral, garante manipulações de dados muito mais velozes2, assim como o readr importa dados mais rapidamente3. As funções map também tem paralelos simples na família future_ do pacote furrr, que permite usar processamento paralelo no R.\n\n\n\nHá muito material de apoio para tidyverse: livros, materiais didáticos, posts em blogs, respostas em fóruns, etc. Como citado acima, o próprio Posit produz inúmeros materiais didáticos e livros que ajudam a aprender e a ensinar tidyverse. Na medida em que o tidyverse consolida-se como o dialeto dominante isto tende a se tornar um ciclo virtuoso.\nO R é uma linguagem bastante versátil, que reúne pesquisadores de campos distintos. Recentemente, parece haver uma convergência para o tidyverse. O campo de séries de tempo, por exemplo, agora tem o tidyquant, fable e modeltime que utilizam os princípios do tidyverse. Com o tempo, deve-se observar movimentos similares de outros campos.\n\n\n\nO tidyverse oferece funções que se aplicam a cada uma das etapas de uma análise de dados. Neste sentido, ele vai de ponta-a-ponta, cobrindo importação, limpeza, modelagem e visualização de dados. A natureza autocontida do tidyverse é bastante atraente pois oferece um caminho seguro para novatos no R, especialmente para quem tem interesse em ciência de dados.\n\n\n\nO conhecimento no R muitas vezes é bastante horizontal. Cada pacote novo traz funções diferentes, que funcionam de novas maneiras e este conhecimento adquirido nem sempre se traduz para outras tarefas. Já sintaxe do dplyr é bastante geral, pode ser utilizada em vários contextos. O dbplyr, por exemplo, é um backend para databases (como BigQuery, PostgreSQL, etc.) que usa a sintaxe do dplyr como frontend. O mesmo acontece com dtplyr/tidytable que permite usar a sintaxe do dplyr junto com a eficiência do data.table. Até para dados complexos já existe o pacote srvyr que usa o survey como backend.\n\n\n\nEste último ponto é bastante mais contencioso. Eu acredito que o tidyverse é mais fácil do que base-R. Eu tenho um conhecimento razoável de base-R e avançado tanto de tidyverse como de data.table. Na minha opinião, a lógica do tidyverse de usar o nome das colunas de um data.frame como objetos é muito poderosa e intuitiva. Não só torna o código mais legível como também evita uma sintaxe carregada com operadores estranhos como $.\nA integração com pipes também simplifica muito o workflow da análise de dados. Com o tempo, a leitura de um código em pipes torna-se natural. Por fim, fazer funções com tidyverse também é muito fácil. Especialmente no caso de funções simples, a sintaxe {{x}} e !!x facilita bastante e, de maneira geral, considero mais simples programar usando princípios “tidy” do que programar usando base-R."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html#o-tidyverse-em-números",
    "href": "posts/general-posts/2023-09-tidyverse/index.html#o-tidyverse-em-números",
    "title": "A filosofia do Tidyverse",
    "section": "O tidyverse em números",
    "text": "O tidyverse em números\nOlhando para as estatísticas do CRAN, vê-se que os pacotes do tidyverse são muito relevantes dentro do ecossistema. Os dados compilados abaixo mostram o retrato dos pacotes do CRAN em 05 de setembro de 2023, quando havia cerca de 19.800 pacotes ativos.\nAtualmente, cerca de um terço dos pacotes no CRAN dependem de algum dos pacotes core do tidyverse. O crescimento desta razão tem sido crescente: de todos os pacotes ativos em 2023, 40% dependem diretamente do tidyverse. Note que no gráfico abaixo, o ano de publicação reflete o ano da versão mais recente de cada pacote. Assim, pacotes ativos cuja última atualização foi anterior a 2016 dificilmente vão possuir alguma dependência com os pacotes do tidyverse já que a maioria deles não existia nesta época.\n\n\n\n\n\n\n\n\n\nVale lembrar que há três tipos de “dependência” entre pacotes no R: imports, depends e suggests. Tipicamente, se um pacote A usa algumas funções de outro pacote B, então o pacote A importa (imports) o pacote B. Isto é, ele assume que o usuário tenha o pacote B instalado. Já a relação depends é mais estrita: se um pacote A depends de um pacote C então os pacotes são carregados conjuntamente quando se chama library()4. Por fim se um pacote A usa um pacote D, em algum contexto específico, mas não requer que o usuário tenha o pacote D instalado, então o pacote A suggests o pacote D5.\nOlhando os dados por pacote vê-se que o ggplot2 e dplyr são os mais populares."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html#a-filosofia-do-tidyverse",
    "href": "posts/general-posts/2023-09-tidyverse/index.html#a-filosofia-do-tidyverse",
    "title": "A filosofia do Tidyverse",
    "section": "A filosofia do tidyverse",
    "text": "A filosofia do tidyverse\nA filosofia geral do tidyverse toma muito emprestado da gramática. As funções têm nomes de verbos que costumam ser intuitivos e são encadeados via “pipes”6 que funcionam como conectivos numa frase. Em tese, isto torna o código mais legível e até mais didático. A tarefa de renomear colunas, criar variáveis e calcular uma média nos grupos torna-se “linear” no mesmo sentido em que uma frase com sujeito-verbo-objeto é linear.\n\nPipes\nO pipe, essencialmente, carrega o resultado de uma função adiante numa cadeia de comandos: objeto |&gt; função1 |&gt; função2 |&gt; função3. Isto tem duas vantagens: primeiro, evita que você use funções compostas que são lidas “de dentro para fora” como exp(mean(log(x))); e, segundo, dispensa a criação de objetos intermediários “inúteis” que estão ali somente para segurar um valor que não vai ser utilizado mais adiante.\n\nmodel &lt;- lm(log(AirPassengers) ~ time(AirPassengers))\n\n#&gt; Função composta\nmean(exp(fitted(model)))\n#&gt; Usando pipes\nmodel |&gt; fitted() |&gt; exp() |&gt; mean()\n#&gt; Usando objetos intermediários\nx1 &lt;- fitted(model)\nx2 &lt;- exp(x1)\nx3 &lt;- mean(x2)\n\nNum contexto de manipulação de dados pode-se ter algo como o código abaixo.\n\ntab_vendas_cidade &lt;- dados |&gt; \n  #&gt; Renomeia colunas\n  rename(date = data, city = cidade, variable = vendas, value = valor) |&gt; \n  #&gt; Transforma colunas\n  mutate(\n    value = value / 1000,\n    date = readr::parse_date(date, format = \"%Y-%b%-d\", locale = readr::locale(\"pt\")),\n    year = lubridate::year(date)\n    ) |&gt; \n  #&gt; Agrupa pela coluna year e calcula algumas estatísticas\n  group_by(year) |&gt; \n  summarise(\n    total = sum(value),\n    count = n()\n  )\n\nEm base-R o mesmo código ficaria algo como o descrito abaixo.\n\nnames(dados) &lt;- c(\"date\", \"city\", \"variable\", \"value\")\n\ndados$value &lt;- dados$value / 1000\ndados$date &lt;- readr::parse_date(\n  dados$date, format = \"%Y-%b%-d\", locale = readr::locale(\"pt\")\n  )\ndados$year &lt;- lubridate::year(dados$date)\n\ntab_vendas_cidade &lt;- tapply(\n  dados$value,\n  dados$city,\n  \\(x) {data.frame(total = sum(x), count = length(x))}\n  )\n\nHá um tempo atrás argumentava-se contra o uso de “pipes”, pois estes dificultavam a tarefa de encontrar bugs no código. Isto continua sendo parcialmente verdade, mas as funções do tidyvserse atualmente têm mensagens de erro bastante ricas e permitem encontrar a fonte do erro com relativa facilidade. Ainda assim, não se recomenda encadear funções em excesso, i.e., pipes com 10 funções ou mais7.\n\n\nFunções\nOutra filosofia do tidyverse é de que tarefas rotineiras devem ser transformadas em funções específicas. Neste sentido, os pacotes dplyr, tidyr e afins são recheados de funções, às vezes com nomes muito semelhantes e com usos redundantes. As funções starts_with e ends_with, por exemplo, são casos específicos da função matches. Há funções que permitem até duas formas de grafia como summarise e summarize. Outras como slice_min e slice_max são convenientes mas são literalmente: arrange + slice.\nSomando somente os dois principais pacotes, dplyr e tidyr, há 360 funções disponíveis. Contraste isto com o data.table que permite fazer 95% das transformações de dados somente com dt[i, j, by = c(), .SDcols = cols].\nMesmo as funções base do R costumam ser mais sucintas do que códigos em tidyverse. No exemplo abaixo, a função tapply consegue o mesmo resultado que o código mais extenso feito com dplyr.\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg = mean(cyl))\n\nAs vantagens do tidyverse se tornam mais evidentes com o tempo. De fato, o pacote permite abstrações muito poderosas, e eventualmente, pode-se fazer um código centenas de vezes mais sucinto combinando as suas funções. Em outros casos, as funções do tidyverse são simplesmente muito convenientes.\nTome a starts_with, por exemplo, que seleciona as colunas que começam de uma certa forma. Suponha uma tabela simples em que há múltiplas colunas cujos nomes começam com a letra “x”. O código em tidyverse é muito mais limpo que o código em base-R.\n\ndf &lt;- df |&gt; \n  select(date, starts_with(\"x\"))\n\ndf &lt;- df[, c(\"date\", names(df)[grep(\"^x\", names(df))])]\ndf &lt;- df[, c(\"date\", names(df)[stringr::str_detect(names(df), \"^x\")])]\n\nO exemplo abaixo é inspirado neste post, que mostra como calcular lags de uma série de tempo que esteja em um data.frame. Calcular defasagens de uma série de tempo é uma tarefa um pouco árdua quando se usa somente funções base. O código abaixo mostra não somente a elegância do tidyverse mas também a facilidade em se criar funções a partir do tidyverse.\n\ncalculate_lags &lt;- function(df, var, lags) {\n  \n map_lag &lt;- lags |&gt; map(~partial(lag, n = .x))\n out &lt;- df |&gt;\n   mutate(\n     across(.cols = {{var}},\n            .fns = map_lag,\n            .names = \"{.col}_lag{lags}\")\n     )\n \n return(out)\n}\n\ndf &lt;- data.frame(\n  date = time(AirPassengers),\n  value = as.numeric(AirPassengers)\n)\n\ndf |&gt; calculate_lags(value, 1:3) |&gt; head()\n#       date value value_lag1 value_lag2 value_lag3\n# 1 1949.000   112         NA         NA         NA\n# 2 1949.083   118        112         NA         NA\n# 3 1949.167   132        118        112         NA\n# 4 1949.250   129        132        118        112\n# 5 1949.333   121        129        132        118\n# 6 1949.417   135        121        129        132\n\n\n\nDesvantagens\nO lado negativo da abordagem “gramatical” é que para não-falantes de inglês muitas destas vantagens são despercebidas8 e o resultado é somente um código “verborrágico”, cheio de funções. Além disso, pode-se argumentar que há ambiguidades inerentes na linguagem. A função filter, por exemplo, é utilizada para filtrar as linhas de um data.frame, mas podia, igualmente, chamar-se select, que selecionaria as linhas de um data.frame. A função select, contudo, é usada para selecionar as colunas de um data.frame.\nUm fato particularmente irritante do tidyverse é a frequência com que os pacotes mudam. Na maior parte das vezes, as mudanças são positivas, mas isto faz com que o código escrito em tidyverse não seja sustentável ao longo do tempo.\nEu demorei um bom tempo para entender as funções tidyr::gather e tidyr::spread e, atualmente, ambas foram descontinuadas e substituídas pelas funções pivot_longer e pivot_wider9. As funções mutate_if, mutate_at e similares do dplyr foram todas suprimidas pela sinataxe mais geral do across. A função tidyr::separate agora está sendo substituída por separate_wider_position e separate_wider_delim.\nMesmo um código bem escrito há poucos anos atrás tem grandes chances de não funcionar mais porque as funções foram alteradas ou descontinuadas. Em 2021, Wickham discutiu este problema abertamente numa palestra. Desde então, o tidyverse tem melhorado a sua política de manutenção de funções.\nA velocidade e eficiência das funções do tidyverse pode ser um problema, mas atualmente existem diversas boas soluções como o já citado tidytable. Particularmente, são raras as situações em que a velocidade do tidyverse me incomoda.\nAtualmente, parece haver um consenso crescente de que a melhor forma de começar a aprender R é começando pelo tidyverse; esta visão não é livre de críticos como de Norm Matloff, professor de estatística da UC Davis. Essencialmente, Matloff considera que o tidyverse é muito complexo para iniciantes: há muitas funções para se aprender e o incentivo à programação funcional torna o código muito abstrato. O tidyverse também esconde o uso do base-R e não ensina operadores básicos como [[ e $. Matloff também considera que “pipes” prejudicam o aprendizado pois dificultam a tarefa de encontrar a fonte dos erros no código."
  },
  {
    "objectID": "posts/general-posts/2023-09-tidyverse/index.html#footnotes",
    "href": "posts/general-posts/2023-09-tidyverse/index.html#footnotes",
    "title": "A filosofia do Tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs funções base gsub, grep e grepl, por exemplo seguem o padrão pattern, string. Já a função strsub usa o padrão string, pattern. Para mais diferenças entre as funções base para manipulação de texto e o stringr consulte este material.↩︎\nExistem diversos benchmarks que atestam os ganhos do dplyr em relação ao base-R. Veja, por exemplo, este comparativo. Apesar disto, o dplyr é menos eficiente que seu concorrente data.table. Existem algumas alternativas como dtplyr e, mais recentemente, tidytable, que fornecem a velocidade do data.table com a sintaxe do dplyr.↩︎\nSimilarmente ao dplyr, o readr também perde para seu concorrente data.table::fread. Contudo, o pacote vroom oferece uma alternativa mais veloz ao readr dentro do universo tidyverse.↩︎\nO pacote ggforce, por exemplo, depends do ggplot2. Isto significa que library(ggforce) automaticamente carrega o pacote ggplot2.↩︎\nEm geral, os pacotes sugeridos (suggests) são listados para os desenvolvedores do pacote ou utilizados para testes e exemplos. Você provavelmente já deve ter visto algum exemplo que começa com: if (require(\"pacote\")) { … }. O pacote nnet, por exemplo, não depende nem importa outros pacotes, mas utiliza o pacote MASS em seus exemplos; assim, o pacote nnet suggests o pacote MASS. Também existem casos onde um pacote tem mais capacidades ou melhor performance quando os pacotes sugeridos estão instalados; nestes casos, recomenda-se instalar o pacote com install.packages(dependencies = TRUE)`.↩︎\nPara saber mais sobre pipes e a diferença entre o novo pipe nativo |&gt; e o pipe |&gt; do magrittr veja meu post sobre o assunto.↩︎\nPara mais sobre pipes consulte o meu post sobre o assunto.↩︎\nNo fundo, isto é ainda mais um incentivo para aprender inglês.↩︎\nTecnicamente, elas foram “superseded”, ou suplatandas. Isto significa que elas continuam existindo exatamente da forma como sempre existiram e que não receberão mais atualizações.↩︎"
  }
]